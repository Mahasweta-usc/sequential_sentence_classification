,Unnamed: 0,text,url,sender,mentor,label
84,85,"Happy to take the extra time to facilitate a talk about ""features"" and ""issues""...

Get Outlook for iOS




On Tue, Aug 16, 2016 at 1:28 PM -0700, ""siddharth anand"" <sanand@apache.org> wrote:










Great!
I just tweeted it via our ApacheAirflow twitter account! 



-s
On Tue, Aug 16, 2016 at 11:26 AM, Jeff Balogh <jbalogh@stripe.com.invalid> wrote:
Stripe is hosting the next Airflow Meetup on Wednesday, September 21st

in San Francisco. You can sign up here:



http://www.meetup.com/Bay-Area-Apache-Airflow-Incubating-Meetup/events/233316814/



We're still looking for one more speaker to give a 15 minute talk. Let

me know if you'd like to volunteer.



Cheers,

jeff









",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<70EBD4C69287065F.D74AEE77-2187-4E71-8B43-98A327073693@mail.outlook.com>,Ben Tallman <...@apigee.com>,0,1
104,105,"Hi

I updated the jackrabbit version from 0.9.16 to 1.0 and when I try to redeploy my application..it
comes with the following error...Pls suggest me

20:10:23,337 WARN  [MultiIndex] Found uncommitted redo log. Applying changes now...
20:10:43,103 ERROR [RepositoryImpl] Unable to start repository. forcing shutdown.
20:10:43,103 INFO  [ObservationManagerFactory] Notification of EventListeners stopped.

20:10:43,103 INFO  [STDOUT] Caused by: javax.jcr.RepositoryException: 0: 0: 0
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:125)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1141)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.RepositoryImpl.getSearchManager(RepositoryImpl.java:550)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:412)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:204)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:358)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:116)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.jndi.BindableRepository.create(BindableRepository.java:104)
20:10:43,103 INFO  [STDOUT]     at org.apache.jackrabbit.core.jndi.RegistryHelper.registerRepository(RegistryHelper.java:57)
20:10:43,118 INFO  [STDOUT]     ... 30 more
20:10:43,118 INFO  [STDOUT] Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
20:10:43,118 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:271)
20:10:43,118 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addValue(NodeIndexer.java:210)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeIndexer.java:162)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDocument(NodeIndexer.java:116)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(SearchIndex.java:254)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.MultiIndex.addNodePersistent(MultiIndex.java:456)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:209)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:88)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:39)
20:10:43,134 INFO  [STDOUT]     at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:123)
20:10:43,134 INFO  [STDOUT]     ... 39 more

TIA
Mohan
",http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200507.mbox/<1120662834.42cbf5326d7ea@mail.sify.com>,Srinivasa Mohan <oxyg...@sify.com>,0,0
203,204,"That's only a recent addition, and I've only got 2005 & 2008 myself to test on. Perhaps
someone else here can offer you some more advice, or work with you to debug the issue.

- Brett

On 17/03/2011, at 10:09 AM, Eric Kolotyluk wrote:

> So I uninstalled and reinstalled NPanday, but I have the same problem with
> VS 2010 - so it seems to be 2010 related.
> 
> The plugin seems to work for VS 2005, so I'll experiment with that. In the
> end I really need this to work in VS 2010 as that is our development
> environment.
> 
> I don't have VS 2008.
> 
> Cheers, Eric
> 
> On Mon, Mar 14, 2011 at 4:32 PM, Brett Porter <brett@apache.org> wrote:
> 
>> The problem is that the wizard doesn't install some things in the GAC yet,
>> making the redundant Maven command necessary. What failure did you get from
>> that?
>> 
>> On 15/03/2011, at 10:14 AM, Eric Kolotyluk wrote:
>> 
>> That's the funny thing, I have never installed NPanday before - this was
>> the first time.
>> 
>> One thing that is confusing is the Install Wizard vs the mvn install. I ran
>> the wizard first, but then I tried the mvn install as the instructions are
>> not all that clear. The mvn install failed anyway, is that what could have
>> screwed things up?
>> 
>> Cheers, Eric
>> 
>> On Mon, Mar 14, 2011 at 2:33 PM, Brett Porter <brett@apache.org> wrote:
>> 
>>> It sounds like you had an old version installed beforehand. We should
>>> handle this better, so feel free to file a bug. However, to get it working I
>>> suggest you follow the uninstall instructions on the web site (make sure
>>> everything is removed, particularly the AddIn file in your documents
>>> folder), make sure it does not appear in VS and then try installing again.
>>> 
>>> On 15/03/2011, at 1:57 AM, Eric Kolotyluk wrote:
>>> 
>>>> After running the NPanday installation wizard, when I start VS 2010 I
>>> get the following message (see attached)
>>>> 
>>>> Exception adding NPanday to the tools m...
>>>> A command with that name already exists.
>>>> 
>>>> NPanday shows up in the Tools menu, but it does nothing when I click it,
>>> and then the menu item goes disabled.
>>>> 
>>>> Does anyone know how to fix this?
>>>> 
>>>> Cheers, Eric
>>>> 
>>>> 
>>> 
>>> --
>>> Brett Porter
>>> brett@apache.org
>>> http://brettporter.wordpress.com/
>>> http://au.linkedin.com/in/brettporter
>>> 
>>> 
>>> 
>>> 
>>> 
>> 
>> --
>> Brett Porter
>> brett@apache.org
>> http://brettporter.wordpress.com/
>> http://au.linkedin.com/in/brettporter
>> 
>> 
>> 
>> 
>> 

--
Brett Porter
brett@apache.org
http://brettporter.wordpress.com/
http://au.linkedin.com/in/brettporter





",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201103.mbox/%3c8A081154-D0BD-42B7-8963-4D1064273F2A@apache.org%3e,Brett Porter <br...@apache.org>,1,1
251,252,"
     [ https://issues.apache.org/jira/browse/JOHNZON-48?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Hendrik Saly closed JOHNZON-48.
-------------------------------

Released with 0.9-incubating

> Complex types in collection with converter
> ------------------------------------------
>
>                 Key: JOHNZON-48
>                 URL: https://issues.apache.org/jira/browse/JOHNZON-48
>             Project: Johnzon
>          Issue Type: Bug
>          Components: Mapper
>    Affects Versions: 0.9-incubating
>            Reporter: Karl Grosse
>            Priority: Critical
>             Fix For: 0.9-incubating
>
>         Attachments: JOHNZON-48.patch, JOHNZON-48.patch
>
>
> Complex objects in a list get serialized as json object without using registered converters.
This leads to malformed json strings which can't be deserialized if there is no default constructor
available (as is the case with enums). 



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",http://mail-archives.apache.org/mod_mbox/johnzon-dev/201508.mbox/<JIRA.12842548.1435916780000.93274.1439892646443@Atlassian.JIRA>,"""Hendrik Saly (JIRA)"" <j...@apache.org>",0,0
197,198,"I disagree. Num_runs should NOT be used anymore and I would really like to know ‘stuck’
schedulers on release or on master, preferably with celery executor (LocalExecutor can sometimes
look stuck but isn’t). Restarting should only be required for clearing up database connections
as we are not very good at that yet.

- Bolke

> Op 9 aug. 2016, om 20:30 heeft Lance Norskog <lance.norskog@gmail.com> het volgende
geschreven:
> 
> Yes, it is still current advice.
> 
> My experience is that after running for (let's say) days, the app develops
> memory corruption. I've seen three different ways that memory corruption
> shows up. The scheduler failure is just one of these three symptoms.
> 
> The other two symptoms are
> 1) the main page of the UI shows a different list of running DAGs than is
> what is really configured,
> 2) a task contains some configuration data that should be in a neighboring
> task, and fails.
> 
> Frankly, I would configure all 5 daemons to restart periodically, not just
> the scheduler daemon.
> 
> 
> On Tue, Aug 9, 2016 at 8:50 AM, Andrew Phillips <andrewp@apache.org> wrote:
> 
>> Hi all
>> 
>> I just wanted to check to what extent the advice in [1] and [2], namely to
>> restart the scheduler ""every once in a while"", is still considered accurate?
>> 
>> ""Restart your scheduler process to get a clean environment every once in a
>> while. Use --num_runs N scheduler CLI option to make it stop after N runs
>> and have some supervisor ensuring it is always running. See issue 698""
>> 
>> ""The scheduler should be restarted frequently
>> 
>> In our experience, a long running scheduler process, at least with the
>> CeleryExecutor, ends up not scheduling some tasks. We still don’t know the
>> exact cause, unfortunately.
>> 
>> Fortunately, airflow has a built-in workaround in the form of the —
>> num_runs flag. It specifies a number of iterations for the scheduler to run
>> of its loop before it quits. We’re running it with 10 iterations, Airbnb
>> runs it with 5. Note that this will cause problems when using the
>> LocalExecutor.""
>> 
>> Both documents are pretty now, so I assume this is considered still
>> relevant. Could you give some guidance on what kind of frequency is
>> recommended here, or is that very dependent on the particular installation?
>> 
>> Also, which of the current JIRA issues (if any) is the new version of
>> ""issue 698"" as mentioned in the first quote? There seem to be quite a few
>> issues relating to the scheduler getting stuck [3] - which one(s) should we
>> follow and/or add information to to best track progress on this topic?
>> 
>> Thanks!
>> 
>> ap
>> 
>> 
>> [1] https://cwiki.apache.org/confluence/display/AIRFLOW/Common+Pitfalls
>> [2] https://medium.com/handy-tech/airflow-tips-tricks-and-pitfal
>> ls-9ba53fba14eb#.ahcprdr9r
>> [3] https://issues.apache.org/jira/browse/AIRFLOW-39?jql=project
>> %20%3D%20AIRFLOW%20AND%20text%20~%20%22scheduler%22
>> 
> 
> 
> 
> -- 
> Lance Norskog
> lance.norskog@gmail.com
> Redwood City, CA


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<0DD434BB-4E6B-40F1-8C1C-0D5F251D05B1@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,0
266,267,"Rasul Osmanov created AMBARI-3062:
-------------------------------------

             Summary: ambari-server doesn't work
                 Key: AMBARI-3062
                 URL: https://issues.apache.org/jira/browse/AMBARI-3062
             Project: Ambari
          Issue Type: Bug
          Components: build
    Affects Versions: 1.3.0
         Environment: linux 2.6
            Reporter: Rasul Osmanov


I'm trying to run the latest version of ambari-server.
I do the following steps:
I clone git repo from git://git.apache.org/ambari.git
I build it using maven: mvn -X -B -e clean install site package rpm:rpm -Dpython.ver=""python
>= 2.6"" -DenableExperimental=true
I take ambari-server rpm file and install it with rpm -i ambari-server.rpm
Then I start ambari service and several minutes later i go to http://servername:8080/ and
there I get only:
Directory: /

Maybe I'm doing something wrong?

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators
For more information on JIRA, see: http://www.atlassian.com/software/jira

",http://mail-archives.apache.org/mod_mbox/ambari-dev/201308.mbox/<JIRA.12666406.1377900581889.63044.1377900651582@arcas>,"""Rasul Osmanov (JIRA)"" <j...@apache.org>",0,0
76,77,"Thanks to Daniel and Arthur, we now have a few versions of the docs up on
ReadTheDocs.
https://readthedocs.org/projects/airflow/

I updated the wiki with the links. Let me know if you think we need to
publish older versions of the docs.

Max

On Wed, Mar 8, 2017 at 2:16 PM, Maxime Beauchemin <
maximebeauchemin@gmail.com> wrote:

> Thanks for taking that on. It's mostly an effort in mocking all the libs
> that cannot be built in the readthedocs environment. This may require a
> specific `requirements.txt`->`readthedocs-reqs.txt` for that purpose with
> only the core dependencies needed to parse the modules.
>
> Max
>
> On Wed, Mar 8, 2017 at 1:40 PM, Daniel Huang <dxhuang@gmail.com> wrote:
>
>> Just filed https://issues.apache.org/jira/browse/AIRFLOW-956 to setup
>> readthedocs. I'll give it a shot.
>>
>> On Wed, Mar 8, 2017 at 12:29 PM, Jake Gysland <jgysland@optoro.com>
>> wrote:
>>
>> > I'm glad there's new documentation available for 1.8.0, but replacing
>> the
>> > documentation for the latest stable release with the documentation for a
>> > future release candidate can lead to experiences that are ...
>> frustrating,
>> > as I've discovered this afternoon while trying to help a colleague QA a
>> DAG
>> > and finding documentation for CLI flags that don't exist.
>> >
>> > Can we at least get a version number (or... a commit hash?) into the
>> > documentation somewhere that's reasonably visible?
>> >
>> > On Mon, Mar 6, 2017 at 12:20 PM, Maxime Beauchemin <
>> > maximebeauchemin@gmail.com> wrote:
>> >
>> > > I just pushed a new version of the docs out.
>> > >
>> > > We may want to set up a service like https://readthedocs.org/ to
>> offer
>> > > versioned docs. It would require setting up some ""mocking"" of external
>> > libs
>> > > that can't be installed on the RTD build nodes and that actually
>> > shouldn't
>> > > be required to build the docs. From my understanding the mocking of
>> libs
>> > > would have to be done here in this file:
>> > > https://github.com/airbnb/superset/blob/master/docs/conf.py
>> > >
>> > > It would be great if someone wanted to take that on.
>> > >
>> > > Max
>> > >
>> > > On Mon, Mar 6, 2017 at 8:46 AM, Michael Gong <gonwg@hotmail.com>
>> wrote:
>> > >
>> > > > Hi,
>> > > >
>> > > >
>> > > > Does anyone know where I can find the documentation for the coming
>> > > airflow
>> > > > 1.8.0 release ?
>> > > >
>> > > >
>> > > > thanks.
>> > > >
>> > > > Michael
>> > > >
>> > > >
>> > > > ________________________________
>> > > > From: Michael Gong <gonwg@hotmail.com>
>> > > > Sent: Monday, March 6, 2017 2:37 PM
>> > > > To: dev; Michael Gong
>> > > > Subject: Re: Airflow running different with different user id ?
>> > > >
>> > > > Hi, Dan,
>> > > >
>> > > > The only doc mentioning ""run_as"" was in the airflow.operators.
>> > > > HiveOperator.
>> > > >
>> > > > Is this what you mean ""run_as""?
>> > > >
>> > > > If not, can you provide more information about it? We are very
>> > interested
>> > > > to know more .
>> > > >
>> > > > Thanks.
>> > > > Michael
>> > > >
>> > > > Sent from my PP•KING™ smartphone
>> > > >
>> > > > On Mar 3, 2017 3:57 PM, Michael Gong <gonwg@hotmail.com> wrote:
>> > > > Hi, Dan,
>> > > >
>> > > >
>> > > > Thanks for the encouraging news.
>> > > >
>> > > >
>> > > > Maybe you could direct me to the documentation about ""run_as"" so I
>> > could
>> > > > see whether it is for my purpose.
>> > > >
>> > > >
>> > > > I guess it will be passed as an argument for the DAG object , right
>> ?
>> > > >
>> > > >
>> > > > Thanks again.
>> > > >
>> > > >
>> > > > Michael
>> > > >
>> > > >
>> > > >
>> > > >
>> > > > ________________________________
>> > > > From: Dan Davydov <dan.davydov@airbnb.com.INVALID>
>> > > > Sent: Friday, March 3, 2017 8:44 PM
>> > > > To: dev@airflow.incubator.apache.org
>> > > > Subject: Re: Airflow running different with different user id ?
>> > > >
>> > > > Within a couple of weeks.
>> > > >
>> > > > On Fri, Mar 3, 2017 at 12:34 PM, Michael Gong <gonwg@hotmail.com>
>> > wrote:
>> > > >
>> > > > > When approximately will it be released?
>> > > > >
>> > > > > Sent from my PP•KING™ smartphone
>> > > > >
>> > > > > On Mar 3, 2017 1:42 PM, Dan Davydov <dan.davydov@airbnb.com
>> .INVALID>
>> > > > > wrote:
>> > > > > Yes it is starting on 1.8.0 which will be released soon, you
can
>> look
>> > > in
>> > > > > the documentation/grep for ""run_as"".
>> > > > >
>> > > > > On Mar 3, 2017 8:50 AM, ""Michael Gong"" <gonwg@hotmail.com>
wrote:
>> > > > >
>> > > > > > Hi,
>> > > > > >
>> > > > > >
>> > > > > > Suppose I have 1 airflow instance running 2 different DAGs,
is
>> it
>> > > > > possible
>> > > > > > to specify the 2 DAGs running under 2 different ids ?
>> > > > > >
>> > > > > >
>> > > > > > Any advises are welcomed.
>> > > > > >
>> > > > > >
>> > > > > > Thanks.
>> > > > > >
>> > > > > > Michael
>> > > > > >
>> > > > > >
>> > > > > >
>> > > > > >
>> > > > > >
>> > > > >
>> > > >
>> > >
>> >
>> >
>> >
>> > --
>> >
>> > Jake Gysland
>> >
>> > Data Pipeline Engineer
>> >
>> > www.optoro.com
>> >
>> > [image: Optoro, Inc.] <http://www.optoro.com/>
>> >
>>
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CAHEEp7VghYVNauaB0eEkJLKoiP6izVM2kCH97g3TqqzLFKv6zg@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,1
199,200,"Jason,

I don't believe Airflow cares about Celery's backend as long as the task
API remains the same. You should be OK (though I haven't tested to
confirm).

J

On Sat, Jan 28, 2017 at 5:09 PM Jason Chen <chingchien.chen@gmail.com>
wrote:

> Hi Airflow team,
>
> Celery 4 supports AWS SQS
>  http://docs.celeryproject.org/en/latest/getting-started/brokers/sqs.html
>
> We are using Airflow 1.7.1.3
> Is there any problem, if we change config to use SQS for CeleryExecutor ?
>
> Thanks.
>
> Jason
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CADsgxrEH3+RhDDbpFT0_BhXRJwvv6-5yJJLx0-S0oJvJ2uztKQ@mail.gmail.com>,Jeremiah Lowin <jlo...@apache.org>,0,0
77,78,"Nice, thanks for fixing this!

Were you able to clear the cache? Seems like I can't terminate builds since
I don't have admin rights on the underlying repo, so I was assuming I
couldn't clear the cache as welll...

Max

On Tue, Aug 9, 2016 at 6:15 AM, Bolke de Bruin <bdbruin@gmail.com> wrote:

> This is fixed. This was due to an issue with Travis’ cache: it can corrupt
> the cache and therefore
> untarring fails. Now hive is re-downloaded in case this happens.
>
> - Bolke
>
> > Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org> het
> volgende geschreven:
> >
> > We have a few non-deterministic unit test failures that are affecting
> many
> > -- but not all -- PRs. I believe they are being ignored as ""unrelated""
> but
> > they have the potential to mask real issues and should be addressed.
> > Unfortunately they're out of my expertise so I'm going to list the ones
> > I've identified and hope someone smarter than me can see if they can
> help!
> >
> > In particular, we have a number of simple PR's that should obviously have
> > no problems (typos, readme edits, etc.) that are nonetheless failing
> tests,
> > causing frustration for all. Here is one from just this morning:
> > https://github.com/apache/incubator-airflow/pull/1705/files
> >
> > Thanks in advance!
> >
> > 1. Python 3 Mysql (this one is pretty common), due to not being able to
> > find ""beeline"" which I believe is related to Hive. This is the error:
> >
> > ======================================================================
> >
> > ERROR: test_mysql_to_hive_partition (tests.TransferTests)
> >
> > ----------------------------------------------------------------------
> >
> > Traceback (most recent call last):
> >
> >  File ""/home/travis/build/apache/incubator-airflow/tests/
> operators/operators.py"",
> > line 208, in test_mysql_to_hive_partition
> >
> >    t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/models.py"",
> > line 2350, in run
> >
> >    force=force,)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/utils/db.py"",
> > line 54, in wrapper
> >
> >    result = func(*args, **kwargs)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/models.py"",
> > line 1388, in run
> >
> >    result = task_copy.execute(context=context)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/
> operators/mysql_to_hive.py"",
> > line 131, in execute
> >
> >    recreate=self.recreate)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/
> hooks/hive_hooks.py"",
> > line 322, in load_file
> >
> >    self.run_cli(hql)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/
> hooks/hive_hooks.py"",
> > line 212, in run_cli
> >
> >    cwd=tmp_dir)
> >
> >  File ""/opt/python/3.4.2/lib/python3.4/subprocess.py"", line 858, in
> __init__
> >
> >    restore_signals, start_new_session)
> >
> >  File ""/opt/python/3.4.2/lib/python3.4/subprocess.py"", line 1456, in
> > _execute_child
> >
> >    raise child_exception_type(errno_num, err_msg)
> >
> > nose.proxy.FileNotFoundError: [Errno 2] No such file or directory:
> 'beeline'
> >
> >
> > 2. Python 3 Postgres (this one is really infrequent):
> >
> > ======================================================================
> >
> > FAIL: Test that ignore_first_depends_on_past doesn't affect results
> >
> > ----------------------------------------------------------------------
> >
> > Traceback (most recent call last):
> >
> >  File ""/home/travis/build/apache/incubator-airflow/tests/jobs.py"",
> > line 349, in test_dagrun_deadlock_ignore_depends_on_past
> >
> >    run_kwargs=dict(ignore_first_depends_on_past=True))
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/utils/db.py"",
> > line 54, in wrapper
> >
> >    result = func(*args, **kwargs)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/tests/jobs.py"",
> > line 221, in evaluate_dagrun
> >
> >    self.assertEqual(ti.state, expected_state)
> >
> > nose.proxy.AssertionError: None != 'success'
> >
> > 3. Mysql (py2 and py3, infrequent). This appears to happen when the
> > SLA code is called wiht mysql. Bizarrely, this doesn't appear to
> > actually raise an error in the test -- it just prints a logging error.
> > It must be trapped somewhere.
> >
> > ERROR [airflow.jobs.SchedulerJob] Boolean value of this clause is not
> defined
> >
> > Traceback (most recent call last):
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/jobs.py"",
> > line 667, in _do_dags
> >
> >    self.manage_slas(dag)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/utils/db.py"",
> > line 53, in wrapper
> >
> >    result = func(*args, **kwargs)
> >
> >  File ""/home/travis/build/apache/incubator-airflow/airflow/jobs.py"",
> > line 301, in manage_slas
> >
> >    .all()
> >
> >  File ""/home/travis/build/apache/incubator-airflow/.tox/py34-
> cdh-airflow_backend_mysql/lib/python3.4/site-packages/
> sqlalchemy/sql/elements.py"",
> > line 2760, in __bool__
> >
> >    raise TypeError(""Boolean value of this clause is not defined"")
> >
> > TypeError: Boolean value of this clause is not defined
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<CAHEEp7UTx49wFjeC8N7NpR0UzkSEnVvM0jvop+-nLebn35yvkA@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
270,271,"@Bolke, yea, you're right, according to:

https://blogs.apache.org/infra/entry/apache_gains_additional_travis_ci

> And can we setup long running services (ie. Hadoop, KDC, Presto, MySQL,
etc) in the Jenkins environment

I don't think so.

> To my knowledge what Apache offers is more suited for unit tests (ie. the
hadoop-qa runs) than integration tests.

Yea, you're right. I agree. I did most integration testing in private
environments when RCs were cut.

Travis seems like it might be a good way to go, if we can make it work. I
agree with Li Xuan that failures in Travis can be a pain to replicate--I
just suffered through this on another open source project--but it is
do-able.

On Tue, Dec 20, 2016 at 12:13 PM, Li Xuan Ji <xuanji@gmail.com> wrote:

> Chris, is this the authoritative list of services apache provides to
> projects? https://www.apache.org/dev/services.html
>
> The builds section / page links to https://ci.apache.org/ and lists the
> following build services: buildbot, gump, jenkins (the link to continuum is
> broken). From my brief reading, it seems they use a shared pool of machines
> (for all projects) are designed for running unit/integration tests that can
> be torn down after running, not long-running services.
>
> There's no references to Travis on those pages, there are some tickets
> related to travis (
> https://www.google.ca/search?sitesearch=*.apache.org&q=travis) but I think
> they are referring to integration with the public travis-ci.org service
> that we already use.
>
> The page also says that FreeBSD jails and Ubuntu VMs are available,
> although I'm not sure what they are normally used for.
>
> One thing I don't really like about the ""official"" infrastructure is
> access, eg we can't ssh into the travis-ci.org builds to debug errors, and
> the travis build script we have is quite verbose, presumably for debugging
> purposes.
>
> On 20 December 2016 at 13:31, Georg Walther <georg.walther@markovian.com>
> wrote:
>
> > Hi,
> >
> >
> > why would you want these services to be long-running? Why not write
> > integration tests against clean instances of each service or clean
> > instances with data fixtures on top?
> > Travis offers spinning up a variety of services (
> > https://docs.travis-ci.com/user/database-setup/) so does wercker (
> > http://devcenter.wercker.com/docs/services).
> > These services are docker containers run off of pre-build Docker images.
> >
> >
> > Best,
> >
> > Georg
> >
> >
> > On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com>
> wrote:
> >
> > > Hi Chris,
> > >
> > > Didn’t they offer a dedicated Travis thing as well? And can we setup
> long
> > > running services (ie. Hadoop, KDC, Presto, MySQL, etc) in the Jenkins
> > > environment, because that would be sufficient I guess? To my knowledge
> > what
> > > Apache offers is more suited for unit tests (ie. the hadoop-qa runs)
> than
> > > integration tests.
> > >
> > > What are your thoughts?
> > >
> > > Bolke
> > >
> > > > Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <
> criccomini@apache.org
> > >
> > > het volgende geschreven:
> > > >
> > > > Hey Bolke,
> > > >
> > > > Thanks for stepping up. The ""traditional"" Apache infra is Jenkins
> with
> > a
> > > > pool of machines that they provide. That might or might not be
> > > satisfactory
> > > > for us (it's certainly antiquated technology). If we decide we don't
> > like
> > > > it, I think that's OK, as long as we move forward knowing that any
> > other
> > > > testing solution can disappear at any time.
> > > >
> > > > My 2c. :)
> > > >
> > > > Cheers,
> > > > Chris
> > > >
> > > > On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <
> > > > dan.davydov@airbnb.com.invalid> wrote:
> > > >
> > > >> This is extremely generous of you! I do agree with the approach of
> > > trying
> > > >> to get funding from Apache and having shared resources (e.g. so that
> > we
> > > >> don't depend on any one company or individual for the uptime of the
> > > >> integration environment, plus so we would have public cloud
> > integration
> > > >> potentially).
> > > >>
> > > >> On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com>
> > > wrote:
> > > >>
> > > >>> Hi,
> > > >>>
> > > >>> I have been thinking about an integration test environment. Aside
> > from
> > > >> any
> > > >>> technical requirements we need a place to do it. I am willing
to
> > offer
> > > a
> > > >>> place in Lab env I am running or to fund an environment in
> AWS/GCloud
> > > if
> > > >>> Apache cannot make these kind of resources available.
> > > >>>
> > > >>> If running in our Lab there is virtually no restriction what we
> could
> > > do,
> > > >>> however I will hand select people who have access to this
> > environment.
> > > I
> > > >>> will also hold ultimate power to remove access from anyone. I
even
> > > might
> > > >>> ask for a confirmation that you will behave when using our property
> > > >> (don’t
> > > >>> worry won’t cover it with legal wording). This is a IAAS service
so
> > we
> > > >> need
> > > >>> to cover the things we need ourselves, but the upside is we can
and
> > it
> > > is
> > > >>> free. We could setup a Gitlab instance that mirrors from Apache
a
> > kicks
> > > >> off
> > > >>> runners to do testing. Downside 1) it might not be entirely Apache
> > > like.
> > > >>> Sorry cant help that. 2) there is no guaranteed up time 3) I might
> > need
> > > >> to
> > > >>> remove it in the future e.g. when I change jobs for example :).
4)
> No
> > > >>> public cloud integration, it’s a private stack after all.
> > > >>>
> > > >>> I can also fund on AWS/GCloud. Again, I probably want to have
> > ultimate
> > > >>> power on access to this environment - it’s my company’s money
on
> the
> > > line
> > > >>> after all. Major downside to this is that it is dependent on and
> > > limited
> > > >> by
> > > >>> the budget I can make available. Upside is that it is not company
> > > >> property.
> > > >>> Also I personally have less exposure to public cloud environments
> due
> > > to
> > > >>> company restrictions.
> > > >>>
> > > >>> Are there any other options? Any thoughts?
> > > >>>
> > > >>> Bolke
> > > >>>
> > > >>>
> > > >>>
> > > >>>
> > > >>>
> > > >>>
> > > >>
> > >
> > >
> >
>
>
>
> --
> Im Xuan Ji!
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<CABYbY7e0TFhTMXDJxMhoaRieohfuSBmVHG5PgFgS_PzQppjhaw@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
89,90,"On Sun, May 6, 2018 at 3:17 PM, Carlos Santana <csantana23@gmail.com> wrote:
> ...if many go we can have an OpenWhisk BoF ...

I'll be at ApacheCon as well, giving a talk on Thursday. I'll have a
busy schedule, not sure if I'll be able to attend a BoF but interested
in principle!

-Bertrand

",http://mail-archives.apache.org/mod_mbox/openwhisk-dev/201805.mbox/<CAEWfVJmKRSk7bB0vYC-9PM7fb+4Smr4qCtC1L_JuszzjqDuEhA@mail.gmail.com>,Bertrand Delacretaz <bdelacre...@apache.org>,1,0
157,158,"Forwarding to a couple other related lists for feedback.  Responses
back to ooo-dev@incubator.apache.org, please.

-Rob


---------- Forwarded message ----------
From: Noah Tilton <noahktilton@gmail.com>
Date: Tue, May 8, 2012 at 1:15 AM
Subject: ODF Command Line Tools -- Request for community feedback
To: odf-dev@incubator.apache.org


Hello,

I was selected as a gsoc2012 participant by the Apache Software
Foundation (ASF) for the ODF Command Line Tools (OCLT) project.  At
Rob Weir's suggestion, I have proposed an initial feedback period
(during Community Bonding) to gauge the interest and leanings of the
community regarding the OCLT tools.  My proposal document is less a
roadmap and more a hodgepodge of different ideas to solving the
problem of the OCLS:

https://docs.google.com/open?id=0B8g_FgudO4EdbWk0T01Zbmo4RFU

Please feel free to discuss pros/cons of the different approaches I
have proposed, or to propose new ideas.

Cheers,

--
Noah

",http://mail-archives.apache.org/mod_mbox/incubator-odf-users/201205.mbox/%3cCAP-ksoi58P1wN3TQYxVEgm4hf_mTMqHg703TfBc6ihkjVFfLJg@mail.gmail.com%3e,Rob Weir <robw...@apache.org>,1,1
262,263,"DFDL spec says ""If this property appears on an element declaration or element reference schema
component, the appearance of any other DFDL properties on that component is a schema definition
error. ""

So you have dfdl:choiceBranchKey on these elements also.

This restriction in DFDL is also unnecessary. dfdl:choiceBranchKey causes no issues in conjunction
with dfdl:inputValueCalc, because choiceBranchKey is not a format property.

You can workaround by wrapping a sequence around each of your elements and putting the dfdl:choiceBranchKey
on the sequence.

________________________________
From: Costello, Roger L. <costello@mitre.org>
Sent: Tuesday, September 24, 2019 1:30 PM
To: users@daffodil.apache.org <users@daffodil.apache.org>
Subject: Re: Why can't the element referenced by dfdl:choiceBranchKey use dfdl:inputValueCalc?


Hi Mike,



But, but, but, …



Even when I make the elements local (not global), I get the same error message. Here’s the
local version of the DFDL schema:



<xs:element name=""input"">
    <xs:complexType>
        <xs:sequence>
            <xs:element name=""Magic-Number"" type=""xs:string"" />
            <xs:choice>
                <xs:choice dfdl:choiceDispatchKey=""{./Magic-Number}"">
                    <xs:element name=""Executable"" type=""xs:string""
                                    dfdl:choiceBranchKey=""MZ""
                                    dfdl:inputValueCalc=""{'Windows executable'}"" />
                    <xs:element name=""Image"" type=""xs:string""
                                    dfdl:choiceBranchKey=""PNG""
                                    dfdl:inputValueCalc=""{'PNG image'}"" />
                    <xs:element name=""Archive"" type=""xs:string""
                                    dfdl:choiceBranchKey=""PK""

                                    dfdl:inputValueCalc=""{'Zip archive'}"" />
                </xs:choice>
                <xs:element name=""Unrecognized"" type=""xs:string""
                                    dfdl:inputValueCalc=""{'Unrecognized magic number'}"" />
            </xs:choice>
        </xs:sequence>
    </xs:complexType>
</xs:element>









From: Beckerle, Mike <mbeckerle@tresys.com>
Sent: Tuesday, September 24, 2019 1:18 PM
To: users@daffodil.apache.org
Subject: [EXT] Re: Why can't the element referenced by dfdl:choiceBranchKey use dfdl:inputValueCalc?



A global element is not allowed to have inputValueCalc property.



This is an arbitrary restriction in DFDL. At one point before there were any implementations
of inputValueCalc property, someone thought this could keep implementors out of trouble.



Having implemented this now, I see no reason for this restriction.



DFDL workgroup is supposed to be getting an ""experience document"" about calculated value properties
from our experience with it. We have a draft of that on the Daffodil Wiki.



See: https://s.apache.org/daffodil-experience-with-computed-elements



Unless it is already there, I will update this page with a note that this restriction is unnecessary.



________________________________

From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org>>
Sent: Tuesday, September 24, 2019 1:11 PM
To: users@daffodil.apache.org<mailto:users@daffodil.apache.org> <users@daffodil.apache.org<mailto:users@daffodil.apache.org>>
Subject: Why can't the element referenced by dfdl:choiceBranchKey use dfdl:inputValueCalc?



Hello DFDL community,



If my input file is this:



MZ



Then I want the output XML to be this:



<input>

    <Magic-Number>MZ</Magic-Number>

   <Executable>Windows executable</Executable>

</input>



If my input file is this:



PNG



Then I want the output XML to be this:



<input>

    <Magic-Number>PNG</Magic-Number>

   <Image>PNG image</Image>

</input>



If my input file is this:



PK



Then I want the output XML to be this:



<input>

    <Magic-Number>PK</Magic-Number>

   <Archive>Zip archive</Archive>

</input>



If the input is none of those, then I want the output XML to be this:



<input>

    <Magic-Number>…</Magic-Number>

    <Unrecognized>Unrecognized magic number</Unrecognized>

</input>



Below is my DFDL schema. It produces this error:



element reference {}Executable cannot have the dfdl:inputValueCalc property.



Why am I getting that error?  /Roger



<xs:element name=""input"">
    <xs:complexType>
        <xs:sequence>
            <xs:element name=""Magic-Number"" type=""xs:string"" />
            <xs:choice>
                <xs:choice dfdl:choiceDispatchKey=""{./Magic-Number}"">
                    <xs:element ref=""Executable"" dfdl:choiceBranchKey=""MZ"" />
                    <xs:element ref=""Image"" dfdl:choiceBranchKey=""PNG"" />
                    <xs:element ref=""Archive"" dfdl:choiceBranchKey=""PK"" />
                </xs:choice>
            </xs:choice>
            <xs:element name=""Unrecognized"" type=""xs:string"" dfdl:inputValueCalc=""{'Unrecognized
magic number'}"" />
        </xs:sequence>
    </xs:complexType>
</xs:element>

<xs:element name=""Executable"" type=""xs:string"" dfdl:inputValueCalc=""{'Windows executable'}""
/>
<xs:element name=""Image"" type=""xs:string"" dfdl:inputValueCalc=""{'PNG image'}"" />
<xs:element name=""Archive"" type=""xs:string"" dfdl:inputValueCalc=""{'Zip archive'}"" />



",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201909.mbox/%3cSN6PR1501MB2158E73D936FE5E931AACE0DD3840@SN6PR1501MB2158.namprd15.prod.outlook.com%3e,"""Beckerle, Mike"" <mbecke...@tresys.com>",0,0
39,40,"I wonder if there's a commandMenuItem you can use in your version (I think
commandLink is intended for stand-alone hyperlinks).

Regards,
Matt

On 12/6/06, Matt Cooper <matt.faces@gmail.com> wrote:
>
> In Trinidad, the solution for this is to use navigationPane hint=""tabs""
> with 2 commandNavigationItem children.  CommandNavigationItem has both
> action and destination attributes (so you'd want to use the ""action""
> attribute and set partialSubmit=""true"").
>
> Regards,
> Matt
>
> On 12/6/06, Marcus Bond <marcus_bonds@hotmail.com> wrote:
> >
> > Hi, I'm using ADF menuTabs with goMenuItem tags to basically give me a 2
> > tab
> > pane. I don't want to redirect to another page i just wish to change
> > which
> > tab is visible when the user clicks one. For me this would involve a
> > user
> > clicking a tab which then runs a server script to change the visibility
> > of a
> > property in a view bean, one to false and the other to true (The
> > rendered
> > property of the pane tags comes from a view bean) then the page would
> > refresh.
> > I have tried adding commandLinks to the goMenuItem's with no success, I
> > have
> > also tried putting an EL expression in the destination attribute with
> > the
> > method that returns the destination (the same page) having code to
> > change
> > the visibility of each pane but that also doesn't work...
> >
> > Anyone have a solution to this?
> > Thanks,
> > Marcus.
> >
> > _________________________________________________________________
> > Windows Live™ Messenger has arrived. Click here to download it for free!
> >
> > http://imagine-msn.com/messenger/launch80/?locale=en-gb
> >
> >
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200612.mbox/%3cf8eab54d0612060902x2688520bx282957da5a250e72@mail.gmail.com%3e,"""Matt Cooper"" <matt.fa...@gmail.com>",0,0
217,218,"Hello DFDL community,

In the following DFDL schema the label element uses dfdl:lengthPattern to specify that the
label should be uppercase, while the label element's simpleType pattern facet says that the
label should be lowercase.

[cid:image001.png@01D4F90A.61470320]

I tested the schema against an input with an uppercase label and an input with a lowercase
label. I tested those with the three --validate values (on, off, limited). See below. I am
surprised that there is no output when the input is lowercase. Would you explain why there
is no output, please?

[cid:image002.png@01D4F90A.61470320]
[cid:image003.png@01D4F90A.61470320]




",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201904.mbox/%3cSN6PR0901MB2351A8FCAD232D9A1C14F315C8220@SN6PR0901MB2351.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
282,283,"Possibly useful to you is the variable dags_are_paused_at_creation = True you
can set in airflow.cfg or in environment variables to cause the metadata
database to consider all DAGs paused when they are first started up.

I do think I saw something fly by on this dev list that environment
variables for config got written out in the newest version. Not sure if
it's true as I haven't tried to upgrade from 1.7.0 yet... partially for
fear of tearing out our dependence on being able to control other airflow
config with environment variables :)

On Thu, Jun 30, 2016 at 4:59 PM, Tyrone Hinderson <thinderson@reonomy.com>
wrote:

> Hey there Chris,
>
> I agree that's a solid use case--mine is more development-oriented. Rather
> than have all DAGs attempt to run upon scheduler start, a developer
> probably just wants to observe how one DAG behaves. To that end, a
> scheduler startup script can simply pause all DAGs before starting the dev
> instance of the scheduler; then the developer can selectively unpause that
> DAG via the UI or CLI (assuming those two controls are related).
>
> On Thu, Jun 30, 2016 at 4:06 PM Chris Riccomini <criccomini@apache.org>
> wrote:
>
> > Hey Tyrone,
> >
> > This would be a useful feature for us as well. Right now, we just turn
> the
> > scheduler off, which is kind of annoying.
> >
> > Use case is when things go really haywire (e.g. 100s of failures per
> > minute), we just want everything to stop.
> >
> > Cheers,
> > Chris
> >
> > On Thu, Jun 30, 2016 at 1:02 PM, Tyrone Hinderson <
> thinderson@reonomy.com>
> > wrote:
> >
> > > Hi, is there a way to ""pause"" all DAGs via the cli or otherwise?
> > >
> > > In the same vein, does ""airflow pause"" flip the same state which is
> > flipped
> > > by the on/off button next to each DAG in the UI?
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CAJQxwiF+HxFaK_-tOW-P9mRnFXAK11SMiVympk+aX9m=tvjo5w@mail.gmail.com>,Laura Lorenz <llor...@industrydive.com>,0,0
223,224,"Hi,
  Are you following the instructions at
https://gobblin.readthedocs.io/en/latest/user-guide/Gobblin-Schedulers/
  Which scheduler are you using to launch the shell script?



On Wed, Jul 24, 2019 at 12:13 AM Amith Prasanna <amith.prasanna@sentienz.com>
wrote:

> Hi all,
>
> I am working on scheduling a job which pulls records from kafka to hdfs.
> This works fine while running in standalone mode. But on trying in
> map-reduce mode using gobblin-mapreduce.sh script I'm getting class and
> method not found errors. I'm using hadoop-2.8.1 and gobblin-0.13. And also
> somehow I came to know that mapreduce.sh triggers job only once. Can anyone
> please give details or changes to be done for scheduling the job in
> mapreduce or yarn mode?
>
> This is how jobconf file looks like:
>
> job.name=KafkatoHdfsJob1
> job.group=GobblinKafka
> job.description=Gobblin quick start job for Kafka
> job.lock.enabled=false
> kafka.brokers=<host>:9092
> job.schedule=0/20 * * * * ?
> topic.whitelist=test
>
> source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource
> extract.namespace=org.apache.gobblin.extract.kafka
> writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder
> writer.file.path.type=tablename
> writer.destination.type=HDFS
> writer.output.format=json
> simple.writer.delimiter=\n
> data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher
> launcher.type=MAPREDUCE
> mr.job.max.mappers=1
> mr.include.task.counters=100
> mr.job.root.dir=/tmp/gobblin/mr-job
> metrics.reporting.file.enabled=true
> metrics.log.dir=/data/temp/gobblin-kafka/metrics
> metrics.reporting.file.suffix=txt
> bootstrap.with.offset=earliest
> fs.uri=hdfs://<host>:8020/
> writer.fs.uri=hdfs://<host>:8020/
> state.store.fs.uri=hdfs://<host>:8020/
> mr.job.root.dir=/data/temp/gobblin-kafka/working
> writer.staging.dir=/data/temp/gobblin-kafka/writer-staging
> writer.output.dir=/data/temp/gobblin-kafka/writer-output
> state.store.dir=/data/temp/gobblin-kafka/state-store
> task.data.root.dir=/data/temp/jobs/kafkaetl/gobblin/gobblin-kafka/task-data
> data.publisher.final.dir=/data/temp/test
>
>
> Regards,
> Amith
>

",http://mail-archives.apache.org/mod_mbox/incubator-gobblin-user/201907.mbox/%3cCAA+GDLooPwFcDvTf_RGiFDLSs_Z4m7c1dE4sFDvEfcrcW_d-Vw@mail.gmail.com%3e,Shirshanka Das <shirsha...@apache.org>,0,1
105,106,"Hello DFDL community,

My input consists of a string containing one to five characters. Each character must be a
digit character. Here is a sample input:

	99999

My DFDL schema uses the xs:unsignedInt datatype to constrain the string to digits. It uses
dfdl:length=5 to constrain the length of the string. 

I am trying to be very precise in my wording. Is the above wording correct?

Here is my DFDL schema:

<xs:element name=""input"">
    <xs:complexType>
        <xs:sequence>
            <xs:element name=""value"" 
 		type=""xs:unsignedInt"" 
 		dfdl:lengthKind=""explicit"" 
 		dfdl:length=""5"" 
 		dfdl:lengthUnits=""characters"" />
        </xs:sequence>
    </xs:complexType>
</xs:element>

Parsing the input 99999 produces this XML:

<input>
  <value>99999</value>
</input>

Unparsing produces this error message:

[error] Unparse Error: Data too long by 8 bits. Unable to truncate.

Here is the output of unparsing:

99,999

Why does unparsing produce that error message? Is it a bug in Daffodil?

/Roger

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201906.mbox/%3cCY4PR09MB1448342EA621DE7DE287A471C8E50@CY4PR09MB1448.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
216,217,"On Tue, Aug 5, 2014 at 10:54 AM, Ameya Aware <ameya.aware@gmail.com> wrote:
> It returns some of the rows of table but not all.. :(

How are you interacting with Blur?  the Shell? the thrift API?
Console?  By default Blur limits the number of results returned but
those can be adjusted through the BlurQuery[1] object in the API.  Or,
if you're using the shell, just adjust it with command line switches
(e.g. query test *:* -min 1000 -fetch 1000 )

--tim

[1] - http://incubator.apache.org/blur/docs/0.2.3/Blur.html#Struct_BlurQuery

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201408.mbox/%3cCAG_bHowuj=Qaju5rmNJ8j4LRZ2za8yzuC0hKBDy6JMe+xrbpaw@mail.gmail.com%3e,Tim Williams <william...@gmail.com>,1,0
225,226,"Hi,

I have tried to use the ""@once"" scheduling option for the first time for a
DAG that I only wish to run once. I would possibly want to run it again at
some random point in the future, but not on regular intervals.

I thought that if I set a start date to the DAG, it would be scheduled only
once, with the start date. However it seemed to be scheduled once with the
start date, and another time with the time point when I unpause the DAG.
Also, when I tried clearing all executions of the DAG and running a
backfill, nothing was scheduled.

Maybe I am confused as to how this type of one time scheduling works. Can
somebody please provide some insight?

Thank you,

Cheers,

Tamara.

-- 

[image: logo]   
  <http://www.facebook.com/hellofreshde>   <http://twitter.com/HelloFreshde>
   <http://instagram.com/hellofreshde/>   <http://blog.hellofresh.de/>   
<https://app.adjust.com/ayje08?campaign=Hellofresh&deep_link=hellofresh%3A%2F%2F&post_deep_link=https%3A%2F%2Fwww.hellofresh.com%2Fapp%2F%3Futm_medium%3Demail%26utm_source%3Demail_signature&fallback=https%3A%2F%2Fwww.hellofresh.com%2Fapp%2F%3Futm_medium%3Demail%26utm_source%3Demail_signature>
 
*HelloFresh App –Download Now!* 
<https://app.adjust.com/ayje08?campaign=Hellofresh&deep_link=hellofresh%3A%2F%2F&post_deep_link=https%3A%2F%2Fwww.hellofresh.com%2Fapp%2F%3Futm_medium%3Demail%26utm_source%3Demail_signature&fallback=https%3A%2F%2Fwww.hellofresh.com%2Fapp%2F%3Futm_medium%3Demail%26utm_source%3Demail_signature>
*We're active in:* 
US <https://www.hellofresh.com/?utm_medium=email&utm_source=email_signature>
 |  DE 
<https://www.hellofresh.de/?utm_medium=email&utm_source=email_signature> |  
UK 
<https://www.hellofresh.co.uk/?utm_medium=email&utm_source=email_signature> 
|  NL 
<https://www.hellofresh.nl/?utm_medium=email&utm_source=email_signature> |  
AU 
<https://www.hellofresh.com.au/?utm_medium=email&utm_source=email_signature>
 |  BE 
<https://www.hellofresh.be/?utm_medium=email&utm_source=email_signature> |  
AT <https://www.hellofresh.at/?utm_medium=email&utm_source=email_signature> 
|  CH 
<https://www.hellofresh.ch/?utm_medium=email&utm_source=email_signature> | 
CA <https://www.hellofresh.ca/?utm_medium=email&utm_source=email_signature> 

www.HelloFreshGroup.com 
<http://www.hellofreshgroup.com/?utm_medium=email&utm_source=email_signature>
 
We are hiring around the world – Click here to join us 
<https://www.hellofresh.com/jobs/?utm_medium=email&utm_source=email_signature>

-- 

<https://www.hellofresh.com/jobs/?utm_medium=email&utm_source=email_signature>
HelloFresh AG, Berlin (Sitz der Gesellschaft) | Vorstände: Dominik S. 
Richter (Vorsitzender), Thomas W. Griesel, Christian Gärtner | Vorsitzender 
des Aufsichtsrats: Jeffrey Lieberman | Eingetragen beim Amtsgericht 
Charlottenburg, HRB 171666 B | USt-Id Nr.: DE 302210417

*CONFIDENTIALITY NOTICE:* This message (including any attachments) is 
confidential and may be privileged. It may be read, copied and used only by 
the intended recipient. If you have received it in error please contact the 
sender (by return e-mail) immediately and delete this message. Any 
unauthorized use or dissemination of this message in whole or in parts is 
strictly prohibited.

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<CAC9UTSa+Dfgz83+EG2sxUy4UE+-UW7B2bWc+adwWuuTwJkvurA@mail.gmail.com>,Tamara Mendt ...@hellofresh.com>,0,0
146,147,"correct

On Tue, Nov 15, 2016 at 1:11 AM <nux@sroff.com> wrote:

Thanks siddharth for your answer. I'm going to look at the extra ""failure
processing branch"" as you suggested.

 >It is intended that a DAGRun be deemed successful in all cases except for
 > failure. So, skipped nodes F and G, would result in a Successful DagRun.
A DAGRun is deemed succesful or failed based solely on the status of the
last tasks right ? Airflow does not consider non-terminal failed tasks ?


At moment, I want my privacy to be protected.
https://mytemp.email/

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201611.mbox/<CANLtMieejXdmk4qC3ZpB+bwhcYED27yEikQOOHAfaxMRhQsKBg@mail.gmail.com>,siddharth anand <san...@apache.org>,0,0
46,47,"can't verify this.

Do you have something special in your settings.xml ?

-M

On 1/11/07, Pfau, Oliver <oliver.pfau@siemens.com> wrote:
> Hi,
>
> I tried to build the trunk and stable-06-dec branch but got an
> excpetion. What's wrong ?
>
> [INFO]
> ------------------------------------------------------------------------
> ----
> [INFO] Building Maven i18n Plugin
> [INFO]    task-segment: [install]
> [INFO]
> ------------------------------------------------------------------------
> ----
> [INFO] [plugin:descriptor]
> [INFO] Using 2 extractors.
> [INFO] Applying extractor for language: java
> [INFO]
> ------------------------------------------------------------------------
> [ERROR] FATAL ERROR
> [INFO]
> ------------------------------------------------------------------------
> [INFO] syntax error @[861,1] in
> file:/D:/3rdparty/plugins/maven-i18n-plugin/src/main/java/org/apache/myf
> aces/trinidadbuild/plugin/i18n/uixto
> ols/JSLocaleElementsGenerator.java
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Trace
> com.thoughtworks.qdox.parser.ParseException: syntax error @[861,1] in
> file:/D:/3rdparty/plugins/maven-i18n-plugin/src/main/java/org/apache/m
> yfaces/trinidadbuild/plugin/i18n/uixtools/JSLocaleElementsGenerator.java
>         at
> com.thoughtworks.qdox.parser.impl.Parser.yyerror(Parser.java:638)
>         at
> com.thoughtworks.qdox.parser.impl.Parser.yyparse(Parser.java:747)
>         at
> com.thoughtworks.qdox.parser.impl.Parser.parse(Parser.java:619)
>         at
> com.thoughtworks.qdox.JavaDocBuilder.addSource(JavaDocBuilder.java:300)
>         at
> com.thoughtworks.qdox.JavaDocBuilder.addSource(JavaDocBuilder.java:316)
>         at
> com.thoughtworks.qdox.JavaDocBuilder.addSource(JavaDocBuilder.java:312)
>         at
> com.thoughtworks.qdox.JavaDocBuilder$1.visitFile(JavaDocBuilder.java:369
> )
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:43)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca
> nner.java:34)
>         at
> com.thoughtworks.qdox.directorywalker.DirectoryScanner.scan(DirectorySca
> nner.java:52)
>         at
> com.thoughtworks.qdox.JavaDocBuilder.addSourceTree(JavaDocBuilder.java:3
> 66)
>         at
> org.apache.maven.tools.plugin.extractor.java.JavaMojoDescriptorExtractor
> .execute(JavaMojoDescriptorExtractor.java:520)
>         at
> org.apache.maven.tools.plugin.scanner.DefaultMojoScanner.populatePluginD
> escriptor(DefaultMojoScanner.java:84)
>         at
> org.apache.maven.plugin.plugin.AbstractGeneratorMojo.execute(AbstractGen
> eratorMojo.java:135)
>         at
> org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginMa
> nager.java:412)
>         at
> org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(Default
> LifecycleExecutor.java:534)
>         at
> org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalWithLifec
> ycle(DefaultLifecycleExecutor.java:475)
>         at
> org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultL
> ifecycleExecutor.java:454)
>         at
> org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandle
> Failures(DefaultLifecycleExecutor.java:306)
>         at
> org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(
> DefaultLifecycleExecutor.java:273)
>         at
> org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifec
> ycleExecutor.java:140)
>         at
> org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:322)
>         at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:115)
>         at org.apache.maven.cli.MavenCli.main(MavenCli.java:256)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
> a:39)
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
> Impl.java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
>         at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
>         at
> org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
>         at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
>
>
>


-- 
Matthias Wessendorf
http://tinyurl.com/fmywh

further stuff:
blog: http://jroller.com/page/mwessendorf
mail: mwessendorf-at-gmail-dot-com

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200701.mbox/%3c71235db40701110611l554dbfa5v97f1ca2cba461c73@mail.gmail.com%3e,"""Matthias Wessendorf"" <mat...@apache.org>",1,0
87,88,"I believe this JIRA could be related:

https://issues.apache.org/jira/browse/PIG-2693


On Sun, May 13, 2012 at 8:41 PM, Travis Crawford
<traviscrawford@gmail.com>wrote:

> Hey Aniket -
>
> I allegedly fixed this in:
>
> Automagically setting parallelism based on input file size does not
> work with HCatalog
> https://issues.apache.org/jira/browse/PIG-2573
>
> HCatLoader should report its input size so pig can estimate the number
> of reducers
> https://issues.apache.org/jira/browse/HCATALOG-328
>
> If its not working we should fix it. Can you provide more details?
>
> --travis
>
>
> On Sun, May 13, 2012 at 7:24 PM, Aniket Mokashi <aniket486@gmail.com>
> wrote:
> > Hi,
> >
> > I have observed that pig's reducer estimator does not work with
> HCatalog. I
> > am wondering if there is a way to make it work? If not, should we have a
> > jira with pig to track this?
> >
> > Thanks,
> > Aniket
>



-- 
*Note that I'm no longer using my Yahoo! email address. Please email me at
billgraham@gmail.com going forward.*

",http://mail-archives.apache.org/mod_mbox/incubator-hcatalog-user/201205.mbox/%3cCAB-acjPTzydjqm21eA5HnVqz+L3+50y24XE9RfG5sGCP_hexig@mail.gmail.com%3e,Bill Graham <billgra...@gmail.com>,0,0
200,201,"We are running 1.7.0 in prod for about a month as well. It's working well
for us now, but we had to cherry pick the scheduler fix:
https://github.com/airbnb/airflow/pull/1374

On Wed, Apr 27, 2016 at 9:51 PM, Dan Davydov <dan.davydov@airbnb.com.invalid
> wrote:

> All of the blockers were fixed as of yesterday (there was some issue that
> Jeremiah was looking at with the last release candidate which I think is
> fixed but I'm not sure). I started staging the airbnb_1.7.1rc3 tag earlier
> today, so as long as metrics look OK and the 1.7.1rc2 issues seem resolved
> tomorrow I will release internally either tomorrow or Monday (we try to
> avoid releases on Friday). If there aren't any issues we can push the 1.7.1
> tag on Monday/Tuesday.
>
> @Sid
> I think we were originally aiming to deploy internally once every two weeks
> but we decided to do it once a month in the end. I'm not too sure about
> that so Max can comment there.
>
> We have been running 1.7.0 in production for about a month now and it
> stable.
>
> I think what really slowed down this release cycle is some commits that
> caused severe bugs that we decided to roll-forward with instead of rolling
> back. We can potentially try reverting these commits next time while the
> fixes are applied for the next version, although this is not always trivial
> to do.
>
> On Wed, Apr 27, 2016 at 9:31 PM, Siddharth Anand <
> siddharthanand@yahoo.com.invalid> wrote:
>
> > Btw, is anyone of the committers running 1.7.0 or later in any staging or
> > production env? I have to say that given that 1.6.2 was the most stable
> > release and is 4 or more months old does not say much for our release
> > cadence or process. What's our plan for 1.7.1?
> >
> > Sent from Sid's iPhone
> >
> > > On Apr 27, 2016, at 9:05 PM, Chris Riccomini <criccomini@apache.org>
> > wrote:
> > >
> > > Hey all,
> > >
> > > I just wanted to check in on the 1.7.1 release status. I know there
> have
> > > been some major-ish bugs, as well as several people doing tests. Should
> > we
> > > create a 1.7.1 release JIRA, and track outstanding issues there?
> > >
> > > Cheers,
> > > Chris
> >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201604.mbox/<CABYbY7cx2+NM7DUUOxfiZ2JvABPRcQ3_B8qK4TV5BMCzyZj0AA@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
162,163,"Thanx for the work Henri..

Mvgr,
Martin

Henri Yandell wrote:
> SVN is retired (rw to the incubator pmc, should still be anonymous r).
> Wiki is read-only.
> The rest of the RetiringPodlings page has been done, so now I've
> requested that these mailing lists be shutdown:
> 
> https://issues.apache.org/jira/browse/INFRA-1056
> 
> Hen
> 
> On 11/19/06, Henri Yandell <flamefew@gmail.com> wrote:
>> So the plan is to retire Agila. As far as I can tell everyone is
>> agreed on doing it, so I'm planning to do the items listed on
>> http://wiki.apache.org/incubator/RetiringPodlings next weekend.
>>
>> This is to let people know that the mailing lists will be shut down
>> (archives don't go away) as a part of that and the various resources
>> (jira/code etc) will be read-only.
>>
>> Retiring podlings is pretty novel, so please point out if anything is
>> missing or if anything there is a bad idea.
>>
>> Hen
>>
> 
> 

",http://mail-archives.apache.org/mod_mbox/incubator-agila-user/200701.mbox/%3c459FCBDF.9070607@mvdb.net%3e,Martin van den Bemt <mll...@mvdb.net>,0,1
218,219,"Hello DFDL community,

My input file contains a bunch of Import Directory Entries. The last Entry is empty (filled
with null values). I figured that the way to handle this is with a lengthPattern (see below),
but that results in this error message:

[error] Schema Definition Error: Element element reference {}Import_Directory_Entry does not
meet the requirements of Pattern-Based lengths and Scanability.

What is the right way to express this?  /Roger

<xs:element name=""Import_Directory_Table"">
    <xs:complexType>
        <xs:sequence>
            <xs:element ref=""Import_Directory_Entry"" maxOccurs=""unbounded""
                dfdl:lengthKind=""pattern"" dfdl:lengthUnits=""bytes""
                dfdl:lengthPattern="".+?(?=((\x00){20,20}|$))"" />
        </xs:sequence>
    </xs:complexType>
</xs:element>

<xs:element name=""Import_Directory_Entry"">
    <xs:complexType>
        <xs:sequence>
            <xs:element name=""Import_Lookup_Table_RVA"" type=""addressType"" />
            <xs:element name=""Date_Time_Stamp"" type=""xs:dateTime"" dfdl:length=""4""
                dfdl:lengthKind=""explicit"" dfdl:binaryCalendarRep=""binarySeconds""
                dfdl:lengthUnits=""bytes"" dfdl:binaryCalendarEpoch=""1970-01-01T00:00:00"" />
            <xs:element name=""Forwarder_Chain"" type=""unsignedint32"" />
            <xs:element name=""Name_RVA"" type=""addressType"" />
            <xs:element name=""Import_Address_Table_RVA"" type=""addressType"" />
        </xs:sequence>
    </xs:complexType>
</xs:element>


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201902.mbox/%3cSN6PR0901MB2351EA4A00BF6D0DE07B8FD7C86D0@SN6PR0901MB2351.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
312,313,"Thank you Sid!!

On Tue, Jan 3, 2017 at 10:08 AM, Chris Nauroth <cnauroth@gmail.com> wrote:

> Thank you for putting together the report.  I have entered my mentor
> sign-off.
>
> On a side note, it's very exciting to see the push for an Apache release as
> the new year begins!
>
> Chris Nauroth
>
> On Sat, Dec 31, 2016 at 12:58 PM, siddharth anand <sanand@apache.org>
> wrote:
>
> > Hi Folks!
> > Here's the quarterly Podling Report for Apache Airflow. Feel free to
> > suggest edits. If you are a committer/maintainer, you can directly edit
> it.
> >
> > https://wiki.apache.org/incubator/January2017
> >
> > -s
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CAKL5WoEtxtGTyyEVt8JXmDBmxjExyAQh58zvdnhR2c1BUL7+Eg@mail.gmail.com>,Gurer Kiratli <gurer.kira...@airbnb.com.INVALID>,0,1
211,212,"Hi Ali,

Sounds like a better thing to do. If the response gets too big, you'll
probably want to store the results as a file or otherwise
immediately process them in a bit of python code.

If you want to write a custom operator for this depends on how many times
you wish to reuse this functionality, which depends on
how many interfaces return larger responses and if it's appropriate to
treat them in a standardized way for those cases.

Another factor driving this decision is debuggability. If you have a
standardized location on the cloud to store the file that you
ingest, you can go back to that later to figure out which line or what went
wrong. If you treat the response immediately,
you probably don't have the file available and you should worry a bit more
about general logging and fault handling.


The easiest approach for a ""one-off"" is to use the Hook in a PythonOperator
directly and treat the response there.

Rgds,

Gerard


On Wed, Mar 8, 2017 at 5:18 PM, Ali Uz <aliuz1@gmail.com> wrote:

> Hello,
>
> I want to use the HttpOperator to issue a GET request and pass a fairly
> large response to another task. Is there any best way to handle this? I
> think pushing a large object to xcom is not recommended. Should I just
> write a custom operator that will fetch from the api and handle it as
> necessary?
>
> Would love to hear some thoughts.
>
> Thanks!
>
>  -Ali
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CAM5819Sk8PVn6vKDwKaxQ+oZeBG73o1Noz+DAV5Thxfr2MFMwA@mail.gmail.com>,Gerard Toonstra <gtoons...@gmail.com>,0,0
49,50,"Hi Neda,

First off welcome!


On Mon, Apr 7, 2014 at 11:27 AM, Neda Grbic <neda.grbic@mangora.org> wrote:

> Hi,
>
> It seems Blur is all I need for my project, but I have some trouble
> finding the best solution, since I do not know anything about zookeeper as
> well.
>
> I have two application servers, one hadoop name node server and four data
> nodes.
> I was wondering if you could give me an idea where to put blur,
> zookeper(s), controller and shard servers.
> I presume I would have two blur servers on my application servers; shard
> servers on data node servers. But I'm not sure where zookeeper(s) and
> controllers should be.
>

I would run blur shard servers on the same nodes as the hadoop data nodes.
As for the controllers and zk nodes, that is really up to you.  I agree
with Garret that assuming that your namenode has enough resources you could
run them there.  You could also run a controller beside each shard server
but that may be overkill for your current setup.  NOTE: Both the
controllers and shard servers need to be able to talk to both ZooKeeper and
HDFS.

I would suggest that you run code from the apache-blur-0.2 branch.  The
version will be 0.2.2 when released.

To check it out and compile yourself:

git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git
git checkout apache-blur-0.2
mvn install -DskipTests -Dhadoop1

You could also grab the latest build binaries from Jenkins Apache CI server:

https://builds.apache.org/job/Blur-master-jdk6/

Download links:

https://builds.apache.org/job/Blur-master-jdk6/212/org.apache.blur$apache-blur/

Let us know if you have any questions.

Aaron


>
> Thanks,
> Neda
>

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201404.mbox/%3cCAB6tTr3MaMKgYfn792bqAYk7MOwES08HKBpWSwTqFu0sVBAYzw@mail.gmail.com%3e,Aaron McCurry <amccu...@gmail.com>,0,1
30,31,"With Daylight Savings Time upon us, I was wondering if anyone has had to
address this issue -- While I understand that right now Airflow is not
timezone-aware, and runs all of its jobs in GMT/UTC time, my team delivers
reports to stakeholders that want to consistently see all data reported
through Midnight **Eastern Time**.

Right now we have a DAG is scheduled to run at 05:00 GMT, which correlates
to Midnight Eastern time.   After this weekend, we'll need the DAG
scheduled to run at 04:00GMT instead, so that it still correlates to
Midnight eastern.   If we just try to modify the DAG Python definition to
change the 'start_date', this doesn't seem to take effect - that is, the
scheduler continues running the DAG at 05:00GMT. So, a few questions:

(1) Once a DAG has been running, why don't changes to the Python
'start_date' seem to take effect?  It seems we always need to create a
different dag with a different dag_id.   Is this something about the way
the history is stored in the database, and is it something we could
possibly tweak in the database directly if we wanted to?

(2) Has anyone else dealt with this issue of needing to adjust a large set
of DAGs for DST?  Or am I the only unlucky ones whose stakeholders don't
speak GMT?

Thanks for all of the help!

-rob

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CAK-QJMGNXoTOwZPqjo40eHzr+YMH4o7GcZny0Hxi2Fh_vbeifQ@mail.gmail.com>,Rob Goretsky <robert.goret...@gmail.com>,0,0
9,9,"keyurkarnik closed pull request #634: URL: https://github.com/apache/usergrid/pull/634         ---------------------------------------------------------------- This is an automated message from the Apache Git Service. To respond to the message, please log on to GitHub and use the URL above to go to the specific comment.  For queries about this service, please contact Infrastructure at: users@infra.apache.org ",http://mail-archives.apache.org/mod_mbox/usergrid-dev/202009.mbox/raw/%3C159920798889.32230.643142987579713014.asfpy%40gitbox.apache.org%3E,GitBox <...@apache.org>,0,0
187,188,"Subscribe

",http://mail-archives.apache.org/mod_mbox/incubator-s2graph-users/201605.mbox/%3cCAOk5v9MSgGFDqhRdvANUEFJjOOYWo1Lm1=5mAdHCWZ5p=+Pmtg@mail.gmail.com%3e,Jason Plurad <plur...@gmail.com>,0,0
143,144,"The unit test that runs the example_subdag_operator DAG is failing
non-deterministically. I see it sporadically in the Travis results, for
different environments, but I can't figure out what's causing the failure
(and why it doesn't happen every time!) Anyone have a thought?

Here's a log from a failed run, :
https://travis-ci.org/jlowin/airflow/jobs/203149863#L8494. This is the same
master branch that's currently passing in the apache repo.

J

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CADsgxrHM_Lh9Z6CJnPdfb1Ui=Ob2eZ8=OgLtRZvpbrRgTDKBog@mail.gmail.com>,Jeremiah Lowin <jlo...@apache.org>,0,0
34,35,"Hi Simon,

Thanks for your reply.

Currently, it is about required fields, but you know, it depends on the customer. ;-)
Does it mean I can't do that?
Do you have a workaround for my problem?

I'm surprised by the fact that the parent component panelFromLayout affects the styling of
its children.
Is it a design strategy, a side effect of using CSS, or just a bug?
Today, I have some trouble with Trinidad skinning because I can't predict the impact when
I change a skin selector. Sorry to say that, I know you guys are doing your best!

Thanks,
Markus

------------------------------------------
Markus Heinisch      

Dipl.-Inform. 
Consultant 

Trivadis GmbH 
Freischützstrasse 92 
81927 München 
Germany 
 
Tel.: +49-89-99275930
Fax : +49-89-99275959 
Mobile: +49-162-2959616 
mailto:markus.heinisch@trivadis.com 
http://www.trivadis.com
 

> -----Ursprüngliche Nachricht-----
> Von: Simon Lessard [mailto:simon.lessard.3@gmail.com] 
> Gesendet: Donnerstag, 14. September 2006 14:22
> An: adffaces-user@incubator.apache.org
> Betreff: Re: tr:inputText Skinning Question
> 
> Hello Markus,
> 
> Is the use case only about required fields or also some other 
> states? If it's about required only, there's a patche pending 
> that will fix it.
> 
> 
> Regards,
> 
> ~ Simon
> 
> On 9/14/06, Markus Heinisch <Markus.Heinisch@trivadis.com> wrote:
> >
> > Hi,
> >
> > I want to paint the background color of a tr:inputText depending of 
> > some
> > state:
> >
> > <tr:inputText value=""#{bindings.Title.inputValue}""
> >               label=""#{bindings.Title.label}""
> >            required=""#{bindings.Title.mandatory}""
> >          styleClass=""#{bindings.Title.mandatory ? 'required' :
> > 'nonrequired'"" />
> >
> > I read in an posting some weeks ago that I can use the
> > af|inputText::content selector. I tried following definition:
> >
> > .required af|inputText::content{
> >   background-color: red;
> > }
> >
> > It works when the tr:inputText is *not* used inside of a 
> > tr:panelFormLayout!
> > But most of the time I use a tr:panelFormLayout parent for all my 
> > input components.
> >
> > I tried the following definition:
> >
> > .required af|panelFormLayout::content-cell af|inputText::content{
> >   background-color: red;
> > }
> >
> > Does not paint my inputText inside the panelFormLayout!
> > Of course, using
> >
> > af|panelFormLayout::content-cell af|inputText::content{
> >   background-color: red;
> > }
> >
> > paints all inputs red inside of a paneFormLayout.
> >
> > How can I style the background color of an input inside of a 
> > panelFormLayout depending on a state?
> >
> > Any hints are welcome!
> >
> > Thanks,
> > Markus
> >
> >
> >
> >
> >
> > ------------------------------------------
> > Markus Heinisch
> >
> > Dipl.-Inform.
> > Consultant
> >
> > Trivadis GmbH
> > Freischützstrasse 92
> > 81927 München
> > Germany
> >
> > Tel.: +49-89-99275930
> > Fax : +49-89-99275959
> > Mobile: +49-162-2959616
> > mailto:markus.heinisch@trivadis.com
> > http://www.trivadis.com <http://www.trivadis.com/>
> >
> >
> 

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200609.mbox/%3cF2C9CCA71510B442AF71446CAE8AEBAF0961BB@MSXVS04.trivadis.com%3e,"""Markus Heinisch"" <Markus.Heini...@trivadis.com>",0,0
283,284,"Hi Folks!
Here's the quarterly Podling Report for Apache Airflow. Feel free to
suggest edits. If you are a committer/maintainer, you can directly edit it.

https://wiki.apache.org/incubator/January2017

-s

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<CANLtMifAUohgQKrRqbb4gckOi3y-WedMnWt2wiC9Zc0dDUA7vQ@mail.gmail.com>,siddharth anand <san...@apache.org>,0,1
291,292,"Hi Barbara:

I'm on an x86 box with:

[gunnar@localhost ~]$ lsb_release -a
LSB Version:
 :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch
Distributor ID: CentOS
Description:    CentOS release 6.8 (Final)
Release:        6.8
Codename:       Final

It may be too weak for what's needed here. I can spin up a AWS micro
instance once I know what OS to install.

Thanks,

Gunnar

On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <
barbaramaltagomes@gmail.com> wrote:

> Hi Gunnar,
>
> Which Operational System are you using?
> *sbt *picks the order of the tests execution and it might be the problem
> on your machine.
>
> As you can see in the picture bellow, I pulled the repo from github and
> run the tests and all of them succeeded
>
> [image: Inline image 1]
>
> I'm using Mac OS
>
> About the *ERROR *messages during the tests, those are fey logs caused
> intentionally by the tests.
>
> Also, try to run the tests separated:
>
> >> sbt
> >> project fey-core
> >> test
>
> Regards,
>
> On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com> wrote:
>
>> Thanks for pointing this out will take a look.
>> -Tony
>>
>>
>> On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com>
>> wrote:
>>
>> Hi,
>>
>> I verified the required versions and then I ran assembly of the fey-core
>> project. 5 failures.
>>
>> [info] - should result in creating a global Performer child actor with
>> the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/G
>> LOBAL-TEST'
>> [info] - should result in creating a Performer child actor with the name '
>> akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFORMER-SCHEDULER
>> '
>> [info] - should result in one global actor created for orchestration
>> [info] - should result in right number of running actors
>> [info] Stopping performer inside ensemble
>> [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8]
>> [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer
>> PERFORMER-SCHEDULER
>> org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER
>>         at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(
>> Ensemble.scala:60)
>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>         at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29)
>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(Death
>> Watch.scala:44)
>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.
>> java:260)
>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(
>> ForkJoinPool.java:1339)
>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPoo
>> l.java:1979)
>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinW
>> orkerThread.java:107)
>>
>> [info] - should Send stop message to monitor
>> [info] Stopping ensemble
>> [info] - should Send stop message to monitor
>> [info] - should result in no orchestration running
>> [info] - should not affect global performer
>> [info] Stopping global performer
>> [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7]
>> [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer
>> GLOBAL-TEST
>> org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer
>> GLOBAL-TEST
>>         at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.
>> applyOrElse(GlobalPerformer.scala:49)
>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>         at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerf
>> ormer.scala:28)
>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(Death
>> Watch.scala:44)
>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>         at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestAct
>> orRef.scala:60)
>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>         at akka.testkit.CallingThreadDispatcher.process$1(CallingThread
>> Dispatcher.scala:250)
>>         at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadD
>> ispatcher.scala:283)
>>         at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingT
>> hreadDispatcher.scala:191)
>>         at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch
>> .scala:147)
>>         at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)
>>         at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:402)
>>         at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$
>> FaultHandling$$finishTerminate(FaultHandling.scala:213)
>>         at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandli
>> ng.scala:172)
>>         at akka.actor.ActorCell.terminate(ActorCell.scala:374)
>>         at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)
>>         at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)
>>         at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.
>> scala:282)
>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)
>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.
>> java:260)
>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(
>> ForkJoinPool.java:1339)
>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPoo
>> l.java:1979)
>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinW
>> orkerThread.java:107)
>>
>> [info] - should result in restart the orchestration
>> [info] - should all previous actors restarted
>> [info] Stopping orchestration
>> [info] - should result in empty global
>> [info] EnsembleSpec:
>> [info] Creating a simple Ensemble MY-ENSEMBLE-0005
>> [info] - should result in creation of Ensemble actor '
>> akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'
>> [info] - should result in sending START to monitor actor
>> [info] - should result in creation of Performer 'TEST-0004'
>> [info] - should result in Empty state variable Ensemble.connectors
>> [info] - should result in one entry added to state variable
>> Ensemble.performer
>> [info] - should result in one right entry to state variable
>> Ensemble.performers_metadata
>> [info] - should result in two paths added to IdentifyFeyActors.actorsPath
>> [info] Sending Ensemble.STOP_PERFORMERS to Ensemble
>> [info] - should result in Terminate message of actor 'TEST-0004' and
>> throw RestartEnsemble Exception
>> [info] - should result in Performer 'TEST-0004' restarted
>> [info] - should result in two paths added to IdentifyFeyActors.actorsPath
>> [info] Sending PoisonPill to Ensemble
>> [info] - should result in termination of actor 'MY-ENSEMBLE-0005'
>> [info] - should result in sending TERMINATE to monitor actor
>> [info] - should result in termination of ensemble and performer
>> [info] - should result in empty IdentifyFeyActors.actorsPath
>> [info] creating more detailed Ensemble
>> [info] - should result in creation of Ensemble actor
>> [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'
>> [info] - should result in creation of Performer 'PERFORMER-PARAMS'
>> [info] - should create connection PERFORMER-SCHEDULER -> PERFORMER-PARAMS
>> [info] - should create 'PERFORMER-SCHEDULER' with schedule time equal to
>> 200ms
>> [info] - should create 'PERFORMER-SCHEDULER' with connection to
>> 'PERFORMER-PARAMS'
>> [info] - should create 'PERFORMER-PARAMS' with no connections
>> [info] - should create 'PERFORMER-PARAMS' with specified params
>> [info] 'PERFORMER-SCHEDULER'
>> [info] - should produce 5 messages in 1 seconds
>> [info] - should produce 10 messages in 2 seconds
>> [info] 'PERFORMER-PARAMS'
>> [info] - should process 5 messages in 1 seconds
>> [info] - should produce 10 messages in 2 seconds
>> [info] Stopping any Performer that belongs to the Ensemble
>> [info] - should force restart of entire Ensemble
>> [info] - should result in sending STOP - RESTART to monitor actor
>> [info] - should keep ensemble actorRef when restarted
>> [info] - should stop and start the performer with a new reference
>> [info] Restarting an Ensemble
>> [info] - should Consuming left messages on Process
>> [info] - should Cleanup TestProbs
>> [info] Redefining TestProbe for performers
>> [info] - should start receiving messages
>> [info] Sending PoisonPill to detailed Ensemble
>> [info] - should result in termination of Ensemble
>> [info] - should result in empty IdentifyFeyActors.actorsPath
>> [info] creating Ensemble with Backoff performer
>> [info] - should result in creation of Ensemble actor
>> [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'
>> [info] - should result in creation of Performer 'PERFORMER-PARAMS'
>> [info] - should create 'PERFORMER-PARAMS' with backoff time equal to 1
>> second
>> [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal to true
>> [info] Performer with backoff enabled
>> [info] - should not process messages during the backoff period
>> [info] Performer with autoScale
>> [info] - should result in router and routees created
>> [info] IdentifyFeyActorsSpec:
>> [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>> [info] - should result in one path added to IdentifyFeyActors.actorsPath
>> [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'
>> [info] Creating a new actor in the system and sending
>> IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>> [info] - should result in two paths added to IdentifyFeyActors.actorsPath
>> [info] - should result in matching paths
>> [info] Stopping previous added actor and sending
>> IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>> [info] - should result in going back to have just one path added to
>> IdentifyFeyActors.actorsPath
>> [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'
>> [info] WatchServiceReceiverSpec:
>> [info] Creating WatchServiceReceiver
>> [info] - should process initial files in the JSON repository
>> [info] Start a Thread with WatchServiceReceiver
>> [info] - should Start Thread
>> [info] Start watching directory
>> [info] - should Starting receiving CREATED event
>> [info] - should Starting receiving UPDATE event
>> [info] processJson
>> [info] - should log to warn level when json has invalid schema
>> [info] interrupt watchservice
>> [info] - should interrupt thread
>> [info] FeyCoreSpec:
>> [info] Creating FeyCore
>> [info] - should result in creating a child actor with the name
>> 'FEY_IDENTIFIER'
>> [info] - should result in sending START message to Monitor actor
>> [info] Sending FeyCore.START to FeyCore
>> [info] - should result in creating a child actor with the name
>> 'JSON_RECEIVER'
>> [info] - should result in starting FeyWatchService Thread
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to
>> FeyCore
>> [info] - should result in creating an Orchestration child actor with the
>> name 'TEST-ACTOR'
>> [info] - should result in creating an Ensemble child actor with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0001'
>> [info] - should result in creating an Ensemble child actor with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0002'
>> [info] - should result in creating a Performer child actor with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'
>> [info] - should result in creating a Performer child actor with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'
>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
>> with key 'TEST-ACTOR'
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command to
>> FeyCore
>> [info] - should result in creating a new Performer child actor with the
>> name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command and
>> DELETE ensemble to FeyCore
>> [info] - should result in termination of Ensemble with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0001'
>> [info] - should result in termination of Performer with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'
>> [info] - should result in termination of Performer with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command and
>> same Timestamp to FeyCore
>> [info] - should result in logging a 'not recreated' message at Warn
>> [info] Sending FeyCore.JSON_TREE to FeyCore
>> [info] - should result in logging a 6 path messages at Info
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command to
>> FeyCore
>> [info] - should result in termination of Orchestration with the name
>> 'TEST-ACTOR'
>> [info] - should result in sending TERMINATE message to Monitor actor
>> [info] - should result in termination of Ensemble with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0002'
>> [info] - should result in termination of Performer with the name
>> 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'
>> [info] - should result in removing key 'TEST-ACTOR' at
>> FEY_CACHE.activeOrchestrations
>> [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore
>> [info] - should result in termination of 'TEST-ORCH-2' *** FAILED ***
>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2"" ->
>> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]))
>> had size 2 instead of expected size 1 (FeyCoreSpec.scala:144)
>> [info] - should result in sending Terminate message to Monitor actor ***
>> FAILED ***
>> [info]   java.lang.AssertionError: assertion failed: timeout (1 second)
>> during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TE
>> RMINATE
>> [info]   at scala.Predef$.assert(Predef.scala:170)
>> [info]   at akka.testkit.TestKitBase$class.expectMsgClass_internal(TestK
>> it.scala:435)
>> [info]   at akka.testkit.TestKitBase$class.expectMsgClass(TestKit.scala:
>> 431)
>> [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)
>> [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$
>> mcV$sp$37.apply(FeyCoreSpec.scala:150)
>> [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$
>> mcV$sp$37.apply(FeyCoreSpec.scala:150)
>> [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>> [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>> [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
>> [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
>> [info]   ...
>> [info] - should result in empty FEY_CACHE.activeOrchestrations *** FAILED
>> ***
>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2"" ->
>> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]))
>> was not empty (FeyCoreSpec.scala:153)
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to
>> FeyCore of a GenericReceiverActor
>> [info] - should result in creating an Orchestration child actor with the
>> name 'RECEIVER_ORCHESTRATION'
>> [info] - should result in creating an Ensemble child actor with the name
>> 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'
>> [info] - should result in creating a Performer child actor with the name
>> 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'
>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
>> with key 'RECEIVER_ORCHESTRATION'
>> [info] Sending PROCESS message to the Receiver Performer
>> [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore
>> [info] - should result in creating an Orchestration child actor with the
>> name 'RECEIVED-BY-ACTOR-RECEIVER'
>> [info] - should result in creating an Ensemble child actor with the name
>> 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'
>> [info] - should result in creating an Ensemble child actor with the name
>> 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'
>> [info] - should result in creating a Performer child actor with the name
>> 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'
>> [info] - should result in creating a Performer child actor with the name
>> 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'
>> [info] - should result in one new entry to FEY_CACHE.activeOrchestrations
>> with key 'RECEIVED-BY-ACTOR-RECEIVER' *** FAILED ***
>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null),
>> ""RECEIVED-BY-ACTOR-RECEIVER"" -> (213263914979,Actor[akka://FEY
>> -MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]),
>> ""TEST-ORCH-2"" -> (213263914979,Actor[akka://FEY
>> -TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), ""RECEIVER_ORCHESTRATION""
>> -> (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCH
>> ESTRATION#-560956299])) had size 4 instead of expected size 2
>> (FeyCoreSpec.scala:200)
>> [info] Sending PROCESS message to the Receiver Performer with command
>> DELETE
>> [info] - should STOP running orchestration
>> [info] - should result in one entry in FEY_CACHE.activeOrchestrations ***
>> FAILED ***
>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2"" ->
>> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]),
>> ""RECEIVER_ORCHESTRATION"" -> (591997890,Actor[akka://FEY-TE
>> ST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 3 instead
>> of expected size 1 (FeyCoreSpec.scala:213)
>> [info] Sending PROCESS message to Receiver with checkpoint enabled
>> [info] - should Save received JSON to checkpoint dir
>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND GLOBAL
>> performer command to FeyCore
>> [info] - should result in creating an Orchestration child actor with the
>> name 'GLOBAL-PERFORMER'
>> [info] - should result in creating an Ensemble child actor with the name
>> 'GLOBAL-PERFORMER/ENS-GLOBAL'
>> [info] - should result in creating a global Performer child actor with
>> the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'
>> [info] - should result in creating a Performer child actor with the name
>> 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'
>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
>> with key 'GLOBAL-PERFORMER'
>> [info] - should result in one global actor created for orchestration
>> [info] - should result in globa metadata add to table
>> [info] - should result in right running actors
>> [info] Stopping Global actor
>> [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3]
>> [akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer
>> GLOBAL-TEST
>> org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer
>> GLOBAL-TEST
>>         at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.
>> applyOrElse(GlobalPerformer.scala:49)
>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>         at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerf
>> ormer.scala:28)
>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(Death
>> Watch.scala:44)
>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.
>> java:260)
>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(
>> ForkJoinPool.java:1339)
>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPoo
>> l.java:1979)
>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinW
>> orkerThread.java:107)
>>
>> [info] - should result in sending logging error
>> [info] - should result in orchestration restarted
>> [info] - should all previous actors restarted
>> [info] Stopping orchestration with global performer
>> [info] - should result in sending TERMINATE message to Monitor actor
>> [info] - should result in no global actors for orchestration
>> [info] Stopping FeyCore
>> [info] - should result in sending STOP message to Monitor actor
>> [info] BaseAkkaSpec:
>> [info] JsonReceiverSpec:
>> [info] Executing validJson in JsonReceiver
>> [info] - should return false when json schema is not right
>> [info] - should log message to Error
>> [info] - should return true when Json schema is valid
>> [info] Executing checkForLocation in JsonReceiver
>> [info] - should log message at Debug level
>> [info] - should download jar dynamically from URL
>> [info] Start a Thread with the JSON receiver
>> [info] - should Start Thread
>> [info] - should execute execute() method inside run
>> [info] Interrupting the receiver Thread
>> [info] - should Throw Interrupted exception
>> [info] - should execute exceptionOnRun method
>> [info] FeyGenericActorSpec:
>> [info] Creating a GenericActor with Schedule time defined
>> [info] - should result in scheduler started
>> [info] - should result in onStart method called
>> [info] - should result in START message sent to Monitor
>> [info] - should result in one active actor
>> [info] Backoff of GenericActor
>> [info] - should be zero until the first PROCESS message
>> [info] - should change when first PROCESS message was received
>> [info] Sending PROCESS message to GenericActor
>> [info] - should call processMessage method
>> [info] customReceive method
>> [info] - should process any non treated message
>> [info] Sending PROCESS message to GenericActor
>> [info] - should be discarded when backoff is enabled
>> [info] - should be processed when backoff has finished
>> [info] Calling startBackoff
>> [info] - should set endBackoff with time now
>> [info] Calling propagateMessage
>> [info] - should send message to connectTo actors
>> [info] Scheduler component
>> [info] - should call execute() method
>> [info] Sending EXCEPTION(IllegalArgumentException) message to
>> GenericActor
>> [info] - should Throw IllegalArgumentException
>> [info] - should Result in restart of the actor with sequence of
>> Monitoring: STOP -> RESTART -> START
>> [info] - should call onStart method
>> [info] - should call onRestart method
>> [info] - should restart scheduler
>> [info] Sending STOP to GenericActor
>> [info] - should terminate GenericActor
>> [info] - should call onStop method
>> [info] - should cancel scheduler
>> [info] - should send STOP - TERMINATE message to Monitor
>> [info] - should result in no active actors
>> [info] Creating GenericActor with schedule anc backoff equal to zero
>> [info] - should not start a scheduler
>> [info] - should result in one active actor
>> [info] - should result in no discarded PROCESS messages
>> [info] FeyGenericActorReceiverSpec:
>> [info] Creating a GenericActor with Schedule time defined
>> [info] - should result in scheduler started
>> [info] - should result in onStart method called
>> [info] - should result in START message sent to Monitor
>> [info] - should result in one active actor
>> [info] - should result in normal functioning of GenericActor
>> [info] Sending PROCESS message to GenericReceiver
>> [info] - should log message to Warn saying that the JSON could not be
>> forwarded to FeyCore when JSON is invalid
>> [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON to be
>> processed has a valid schema
>> [info] - should Download jar from location and send
>> ORCHESTRATION_RECEIVED to FeyCore when JSON has a location defined
>> [info] Scheduler component
>> [info] - should call execute() method
>> [info] Sending EXCEPTION(IllegalArgumentException) message to
>> GenericActor
>> [info] - should Throw IllegalArgumentException
>> [info] - should Result in restart of the actor with sequence of
>> Monitoring: STOP -> RESTART -> START
>> [info] - should call onStart method
>> [info] - should call onRestart method
>> [info] - should restart scheduler
>> [info] Sending STOP to GenericActor
>> [info] - should terminate GenericActor
>> [info] - should call onStop method
>> [info] - should cancel scheduler
>> [info] - should send STOP - TERMINATE message to Monitor
>> [info] - should result in no active actors
>>
>> CLeaning up[info] Run completed in 44 seconds, 724 milliseconds.
>> [info] Total number of tests run: 243
>> [info] Suites: completed 12, aborted 0
>> [info] Tests: succeeded 238, failed 5, canceled 0, ignored 0, pending 0
>> [info] *** 5 TESTS FAILED ***
>> [error] Failed tests:
>> [error]         org.apache.iota.fey.FeyCoreSpec
>> [error] (fey-core/test:test) sbt.TestsFailedException: Tests unsuccessful
>> [error] Total time: 46 s, completed Feb 19, 2017 12:36:25 AM
>>
>>
>> --
>> Thanks,
>>
>> Gunnar
>> *If you think you can you can, if you think you can't you're right.*
>>
>>
>>
>
>
> --
> Barbara Gomes
> Computer Engineer
> San Jose, CA
>



-- 
Thanks,

Gunnar
*If you think you can you can, if you think you can't you're right.*

",http://mail-archives.apache.org/mod_mbox/iota-dev/201702.mbox/<CAB2AVJ5_9BRAbZkmnRjB0jGh4L8cjL1uDvtkzQLtqH65iNz0sA@mail.gmail.com>,Gunnar Tapper <tapper.gun...@gmail.com>,0,0
52,53,"Github user ottobackwards commented on the issue:

    https://github.com/apache/incubator-metron/pull/531
  
    Can you edit the title to start with METRON-854?  If it doesn't the scripts won't work
with jira


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

",http://mail-archives.apache.org/mod_mbox/metron-dev/201704.mbox/<20170425105658.E751AE00A3@git1-us-west.apache.org>,ottobackwards <...@git.apache.org>,0,0
65,66,"On 11/26/2014 10:56 AM, alaninmcr wrote:
> On 26/11/2014 15:14, Hoeftberger, Johann wrote:
>> On 11/26/2014 08:55 AM, alaninmcr wrote:
>>> On 25/11/2014 16:36, Hoeftberger, Johann wrote:
>>>> Can Taverna make use of a Sun Grid Engine Cluster and is there a good
>>>> explanation how to do that?
>>>
>>> What do you mean by ""make use of"" ? Do you want to have a workflow be
>>> run on a node in the cluster or to have individual steps within a
>>> workflow run be executed on nodes in the cluster?
>>
>> I think both.
>> The most important need for me seems to be to have the ability that
>> individual steps within a workflow run on our SGE cluster transparently
>> for the user which uses the workflow. So those workflow steps should be
>> executed on nodes of the SGE cluster, the whole handshake for the usage
>> of those steps should be done by the Taverna system itself.
>
> What is the normal interface for ""talking"" to the SGE?

That's the question whose answer I hoped the SGE integration in Taverna 
would give.
(For our current jobs we use qsub and related tools.)

It seems currently there doesn't exist a real integration of a SGE Grid 
Engine in Taverna. And the only available solution is over Tool Service, 
SSH and qsub commands.
Although I don't have any experience with that I see the theoretical 
approach behind it. But this results in a polling strategy of the Grid 
results from a workflow engine perspectice. I thought / hoped there 
exists a tighter integrated solution for the connection between Taverna 
and SGE.


> A quick way to try things would be use the tool service to run qsub (and
> related) commands to your SGE. Then you could group the choreography of
> submit -> poll * -> retrieve results, into a component.

Yes, that's the best I could found and you confirm that somehow.


> Alternatively, if there is a Java library, you could write an equivalent
> Beanshell.

I also find very interesting the ""Tool Activity pluggable execution 
point"" Alan mentioned in his last post. But this would take a much 
bigger development approach I guess. I have to figure out what fits best 
in our situation.

Thank you all for your suggestions so far.


Regards,
Johann


-- 


The information in this e-mail is intended only for the person to whom it is
addressed. If you believe this e-mail was sent to you in error and the e-mail
contains patient information, please contact the Partners Compliance HelpLine at
http://www.partners.org/complianceline . If the e-mail was sent to you in error
but does not contain patient information, please contact the sender and properly
dispose of the e-mail.


",http://mail-archives.apache.org/mod_mbox/incubator-taverna-users/201411.mbox/%3c8405D05C2179D64F80FFFE79E22C07BD0E877F8D@PHSX10MB4.partners.org%3e,"""Hoeftberger, Johann"" <Johann_Hoeftber...@DFCI.HARVARD.EDU>",0,0
276,277,"
     [ https://issues.apache.org/jira/browse/LENS-63?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Amareshwari Sriramadasu updated LENS-63:
----------------------------------------
    Priority: Critical  (was: Major)

> fact query on jdbc is failing with IllegalArgumentException
> -----------------------------------------------------------
>
>                 Key: LENS-63
>                 URL: https://issues.apache.org/jira/browse/LENS-63
>             Project: Apache Lens
>          Issue Type: Bug
>    Affects Versions: 2.0
>            Reporter: Raghavendra Singh
>            Assignee: Sushil Mohanty
>            Priority: Critical
>             Fix For: 2.0
>
>         Attachments: LENS-63.patch
>
>
> {CODE}
> ERROR org.apache.lens.driver.jdbc.JDBCResultSet  - Error getting JDBC type information:
Decimal precision out of allowed range [1,38]
> java.lang.IllegalArgumentException: Decimal precision out of allowed range [1,38]
> 	at org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.validateParameter(HiveDecimalUtils.java:73)
> 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:432)
> 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:305)
> 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromTypeString(TypeInfoUtils.java:759)
> 	at org.apache.lens.driver.jdbc.JDBCResultSet$1.getColumns(JDBCResultSet.java:113)
> 	at org.apache.lens.server.api.driver.LensResultSetMetadata.toQueryResultSetMetadata(LensResultSetMetadata.java:46)
> 	at org.apache.lens.server.query.QueryExecutionServiceImpl.getResultSetMetadata(QueryExecutionServiceImpl.java:1528)
> 	at org.apache.lens.server.query.QueryServiceResource.getResultSetMetadata(QueryServiceResource.java:714)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:622)
> 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
> 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)
> 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)
> 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)
> 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)
> 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)
> 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)
> 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
> 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
> 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
> 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
> 	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
> 	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
> 	at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
> 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
> 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
> 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
> 	at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
> 	at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
> 	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
> 	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
> 	at java.lang.Thread.run(Thread.java:701)
> 24 Nov 2014 10:22:54,367 [Grizzly-worker(5)] WARN  org.glassfish.jersey.server.ServerRuntime$Responder
 - WebApplicationException cause:
> java.lang.NullPointerException
> 	at org.apache.lens.server.api.driver.LensResultSetMetadata.toQueryResultSetMetadata(LensResultSetMetadata.java:46)
> 	at org.apache.lens.server.query.QueryExecutionServiceImpl.getResultSetMetadata(QueryExecutionServiceImpl.java:1528)
> 	at org.apache.lens.server.query.QueryServiceResource.getResultSetMetadata(QueryServiceResource.java:714)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:622)
> 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
> 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)
> 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)
> 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)
> 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)
> 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)
> 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)
> 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
> 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
> 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
> 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
> 	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
> 	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
> 	at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
> 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
> 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
> 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
> 	at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
> 	at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
> 	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
> 	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
> 	at java.lang.Thread.run(Thread.java:701)
> {CODE}



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",http://mail-archives.apache.org/mod_mbox/lens-dev/201411.mbox/<JIRA.12757690.1416922754000.25302.1416998772906@Atlassian.JIRA>,"""Amareshwari Sriramadasu (JIRA)"" <j...@apache.org>",1,0
205,206,"Hi Matthias,

ok, i've created https://issues.apache.org/jira/browse/ADFFACES-440

Good look for your disk.

Kuno

-----Ursprüngliche Nachricht-----
Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com]Im Auftrag von
Matthias Wessendorf
Gesendet: Donnerstag, 5. April 2007 13:13
An: adffaces-user@incubator.apache.org
Betreff: Re: Skin Swap


Ah!

Now I understand what you are talking about. The old (second link)
demo renders as <img> and the new (the link, containing the date) not.

Can you open a jira issue. My hard disk on my working mashine is gone...

-M

On 4/5/07, Kuno.Baeriswyl@bkw-fmb.ch <Kuno.Baeriswyl@bkw-fmb.ch> wrote:
> äh sorry..
>
> I've send you a link to the latest build of the demo app. If you select the purple Skin
demo, you will see that the serverside buttons won't be rendered. However, in this (http://www.irian.at/trinidad-demo/faces/demos/panelPageSkinDemo.jspx)
build, the serverside buttons work well.
>
> Thanks
> Kuno
>
>
>
> -----Ursprüngliche Nachricht-----
> Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com]Im Auftrag von
> Matthias Wessendorf
> Gesendet: Donnerstag, 5. April 2007 11:21
> An: adffaces-user@incubator.apache.org
> Betreff: Re: Skin Swap
>
>
> what should I see there ?
> what is ""broken"" ?
>
>
>
>
> On 4/5/07, Kuno.Baeriswyl@bkw-fmb.ch <Kuno.Baeriswyl@bkw-fmb.ch> wrote:
> > Never mind! It's not a big issue to me.
> >
> > However, you might check it here: http://example.irian.at/trinidad-demo-20070405/faces/demos/panelPageSkinDemo.jspx
> >
> > Regards
> > Kuno
> >
> > -----Ursprüngliche Nachricht-----
> > Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com]Im Auftrag von
> > Matthias Wessendorf
> > Gesendet: Donnerstag, 5. April 2007 10:49
> > An: adffaces-user@incubator.apache.org
> > Betreff: Re: Skin Swap
> >
> >
> > my environement is broken, now.
> > cant check.
> >
> > sorry
> >
> > On 4/5/07, Kuno.Baeriswyl@bkw-fmb.ch <Kuno.Baeriswyl@bkw-fmb.ch> wrote:
> > > Hi Matthias,
> > >
> > > I've just checked out the source from the trunk and I wanted to let you that
it didn't work. :-{
> > > Thanks
> > >
> > > Kuno
> > >
> > > -----Ursprüngliche Nachricht-----
> > > Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com]Im Auftrag von
> > > Matthias Wessendorf
> > > Gesendet: Dienstag, 3. April 2007 13:38
> > > An: adffaces-user@incubator.apache.org
> > > Betreff: Re: Skin Swap
> > >
> > >
> > > there was a fix for that.
> > >
> > > that should be in the release and also, when you do the svn checkout
> > > on your own.
> > >
> > > -M
> > >
> > > On 4/3/07, Kuno.Baeriswyl@bkw-fmb.ch <Kuno.Baeriswyl@bkw-fmb.ch> wrote:
> > > > Hello again!
> > > >
> > > > By the way, do you get the serverside buttons rendered? I've been using
the latest snapshot, but always the text based fall-back buttons.
> > > >
> > > > Thanks for your reply.
> > > >
> > > > Kuno
> > > >
> > > >
> > > > -----Ursprüngliche Nachricht-----
> > > > Von: mwessendorf@gmail.com im Auftrag von Matthias Wessendorf
> > > > Gesendet: Di 03.04.2007 10:41
> > > > An: adffaces-user@incubator.apache.org
> > > > Betreff: Re: Skin Swap
> > > >
> > > > Hey John,
> > > >
> > > > this is cool!
> > > >
> > > > Thx,
> > > > Matthias
> > > >
> > > > On 4/3/07, John Conner <conner.john@gmail.com> wrote:
> > > > >
> > > > > I have opened up a google project as a place to swap skins.  The
project
> > > > > is located at http://code.google.com/p/trinidad-skin-swap/ and the
first
> > > > > skin is one created by Mark Robinson of http://www.mizar.com/.
> > > > >
> > > > > If anyone else has a skin they would like added to this project,
shot me
> > > > > an email and I will give you project membership status so you can
> > > > > add/change your skin.
> > > > >
> > > > > John....
> > > > >
> > > >
> > > >
> > > > --
> > > > Matthias Wessendorf
> > > > http://tinyurl.com/fmywh
> > > >
> > > > further stuff:
> > > > blog: http://jroller.com/page/mwessendorf
> > > > mail: mwessendorf-at-gmail-dot-com
> > > >
> > > >
> > >
> > >
> > > --
> > > Matthias Wessendorf
> > > http://tinyurl.com/fmywh
> > >
> > > further stuff:
> > > blog: http://jroller.com/page/mwessendorf
> > > mail: mwessendorf-at-gmail-dot-com
> > >
> >
> >
> > --
> > Matthias Wessendorf
> > http://tinyurl.com/fmywh
> >
> > further stuff:
> > blog: http://jroller.com/page/mwessendorf
> > mail: mwessendorf-at-gmail-dot-com
> >
>
>
> --
> Matthias Wessendorf
> http://tinyurl.com/fmywh
>
> further stuff:
> blog: http://jroller.com/page/mwessendorf
> mail: mwessendorf-at-gmail-dot-com
>


-- 
Matthias Wessendorf
http://tinyurl.com/fmywh

further stuff:
blog: http://jroller.com/page/mwessendorf
mail: mwessendorf-at-gmail-dot-com

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200704.mbox/%3c8CB94491C38B5D479753A775AED68ADF118120@HSB106EX.main.root.bkw-fmb.ch%3e,<Kuno.Baeris...@bkw-fmb.ch>,0,0
289,290,"

hi, 

I actually tried it, but i got the same result 

baojian Zhou 
schrieb:  
i think that is caused by
'commons-logging:commons-logging:1.1.1@jar' not found, it maybe means
that your maven have something wrong, 
you can try this, may can help
you:

http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201208.mbox/%3CF4DF62C8CDB9C746BE2BEFDB55D5E4760E97B854@PALLENE.office.hd%3E
[1]

2013/5/30 Aboubakr Benabbas 

hi, 

after building s4, i wanted to
try it through the example in the s4 piper walkthrough page, when i
reached the step to build ""myApp"", i got the following error after
running : 

./s4 deploy -appName=myApp -c=cluster1 -b=`pwd`/build.gradle
-a=hello.HelloApp

 at
org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:34)
~[na:na]
 at
org.gradle.launcher.daemon.server.exec.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
~[na:na]

 at
org.gradle.launcher.daemon.server.exec.HandleStop.execute(HandleStop.java:34)
~[na:na]
 at
org.gradle.launcher.daemon.server.exec.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
~[na:na]

 at
org.gradle.launcher.daemon.server.exec.CatchAndForwardDaemonFailure.execute(CatchAndForwardDaemonFailure.java:32)
~[na:na]
 at
org.gradle.launcher.daemon.server.exec.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
~[na:na]

 at
org.gradle.launcher.daemon.server.exec.HandleClientDisconnectBeforeSendingCommand.execute(HandleClientDisconnectBeforeSendingCommand.java:21)
~[na:na]
 at
org.gradle.launcher.daemon.server.exec.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
~[na:na]

 at
org.gradle.launcher.daemon.server.exec.StopConnectionAfterExecution.execute(StopConnectionAfterExecution.java:27)
~[na:na]
 at
org.gradle.launcher.daemon.server.exec.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
~[na:na]

 at
org.gradle.launcher.daemon.server.exec.DefaultDaemonCommandExecuter.executeCommand(DefaultDaemonCommandExecuter.java:55)
~[na:na]
 at
org.gradle.launcher.daemon.server.Daemon.run(Daemon.java:123) ~[na:na]


at
org.gradle.messaging.concurrent.DefaultExecutorFactory$StoppableExecutorImpl.run(DefaultExecutorFactory.java:66)
~[gradle-core-1.0.jar:1.0]
Caused by:
org.gradle.messaging.remote.internal.PlaceholderException: Artifact
'commons-logging:commons-logging:1.1.1@jar' not found.

 at
org.gradle.api.internal.artifacts.ivyservice.ivyresolve.UserResolverChain$ModuleVersionRepositoryBackedArtifactResolver.resolve(UserResolverChain.java:168)
~[na:na]
 at
org.gradle.api.internal.artifacts.ivyservice.ivyresolve.LazyDependencyToModuleResolver$ErrorHandlingArtifactResolver.resolve(LazyDependencyToModuleResolver.java:59)
~[na:na]

 at
org.gradle.api.internal.artifacts.ivyservice.ResolvedArtifactFactory.create(ResolvedArtifactFactory.java:39)
~[na:na]
 at
org.gradle.api.internal.artifacts.ivyservice.ResolvedArtifactFactory.create(ResolvedArtifactFactory.java:37)
~[na:na]

 at
org.gradle.cache.internal.DefaultCacheAccess.useCache(DefaultCacheAccess.java:126)
~[gradle-core-1.0.jar:1.0]
 at
org.gradle.cache.internal.DefaultPersistentDirectoryStore.useCache(DefaultPersistentDirectoryStore.java:99)
~[gradle-core-1.0.jar:1.0]

 at
org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.useCache(DefaultCacheLockingManager.java:49)
~[na:na]
 at
org.gradle.api.internal.artifacts.ivyservice.ResolvedArtifactFactory.get(ResolvedArtifactFactory.java:37)
~[na:na]

 at
org.gradle.api.internal.artifacts.DefaultResolvedArtifact.getFile(DefaultResolvedArtifact.java:107)
~[na:na]
 at
org.gradle.api.internal.artifacts.ivyservice.DefaultLenientConfiguration$ArtifactFileResolver.getFile(DefaultLenientConfiguration.java:137)
~[na:na]

 at
org.gradle.api.internal.artifacts.ivyservice.DefaultLenientConfiguration.getFiles(DefaultLenientConfiguration.java:115)
~[na:na]
 at
org.gradle.api.internal.artifacts.ivyservice.DefaultLenientConfiguration.getFilesStrict(DefaultLenientConfiguration.java:98)
~[na:na]

 at
org.gradle.api.internal.artifacts.ivyservice.DefaultResolvedConfiguration.getFiles(DefaultResolvedConfiguration.java:45)
~[na:na]
 at
org.gradle.api.internal.artifacts.ivyservice.SelfResolvingDependencyResolver.getFiles(SelfResolvingDependencyResolver.java:56)
~[na:na]

 at
org.gradle.api.internal.artifacts.ivyservice.ErrorHandlingArtifactDependencyResolver$ErrorHandlingResolvedConfiguration.getFiles(ErrorHandlingArtifactDependencyResolver.java:78)
~[na:na]
 ... 92 common frames omitted

and it does not build, what is
wrong ?

 

Links:
------
[1]
http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201208.mbox/%3CF4DF62C8CDB9C746BE2BEFDB55D5E4760E97B854@PALLENE.office.hd%3E
[2]
mailto:aboubakr.benabbas@tu-ilmenau.de

",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201305.mbox/%3c3780fa4d6de0d0fd7b0a148ac2240bb2@localhost%3e,Aboubakr Benabbas <aboubakr.benab...@tu-ilmenau.de>,0,0
3,3,      [ https://issues.apache.org/jira/browse/USERGRID-881?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Michael Russo updated USERGRID-881: -----------------------------------     Sprint: Usergrid 37  (was: Usergrid 36)  > Remove Fields and Map to Entity Conversions from Core > ----------------------------------------------------- > >                 Key: USERGRID-881 >                 URL: https://issues.apache.org/jira/browse/USERGRID-881 >             Project: Usergrid >          Issue Type: Story >          Components: Stack >    Affects Versions: 2.1.0 >            Reporter: Shawn Feldman >            Assignee: Michael Russo >             Fix For: 2.1.2 > > > we convert from a map to entity and back 2 - 3 times for every read and write.  We should remove these type conversions    -- This message was sent by Atlassian JIRA (v6.3.4#6332) ,http://mail-archives.apache.org/mod_mbox/usergrid-dev/201603.mbox/raw/%3CJIRA.12849675.1438104091000.196212.1456937478395%40Atlassian.JIRA%3E,"""Michael Russo (JIRA)"" <j...@apache.org>",0,0
128,129,"Daniel,

Hey, I did see it, but I'm currently plowed under at work.
This might be:
 http://issues.apache.org/jira/browse/ADFFACES-24
... which desparately needs some attention.

-- Adam


On 1/3/07, Daniel Hannum <dhannum@quovadx.com> wrote:
> Hi. I'm reposting this from last week because I'm afraid no one saw it
> during the vacation week. This is still a very serious issue for me and
> I'd welcome any feedback you can provide. I've added references at the
> bottom.
>
>
>
> --
>
>
>
> I have a form with a required field, and a commandMenuItem that goes to
> another page (but has immediate set to true). As expected, I can click
> the menu link and go to the other page, and the immediate attribute
> bypasses validations.
>
>
>
> However, if I add that page a control that has autoSubmit=""true"", even
> if the autoSubmit has nothing to do with the required field, now when I
> click the menu link, it will still stop me with validation errors. If I
> click the menu link again, it will go to the page as expected. I saw
> reports on the Oracle forums about ADF issues in this vein [1, 2]. I
> don't know if they have been fixed in Trinidad. Seems like this has to
> be a bug, though.
>
>
>
> To recap:
>
>
>
> 1.    Make a page with a required text field, an autoSubmit checkbox,
> and an immediate link to another page.
>
> 2.    Leave the field blank and click the link. You go to the page fine.
> Validations are skipped by immediate=""true""
>
> 3.    Go back. Still leave the field blank, but this time click the
> autoSubmit checkbox. Now click the link. You'll get validation errors
> (despite the immediate=""true""... bug?)
>
> 4.    Once you have the validation errors on screen, ignore them and
> click the link a second time. Now you go to the page as expected.
>
>
>
> Not being an expert in the JSF lifecycle or in the implementation of
> autoSubmit, can someone explain what's going on? I feel validations
> should always be bypassed with immediate=""true"", not just when not using
> autoSubmit. I love autoSubmit, but this makes for a bad user experience.
>
>
>
> Thanks
>
> Dan
>
>
>
> [1]
> http://forums.oracle.com/forums/thread.jspa?messageID=1387162&#1387162
>
> [2] http://www.orablogs.com/fnimphius/archives/001787.html (seems to be
> a solution but it involves customizing the ADF lifecycle by extending
> Oracle-specific classes. How I would adapt this for the version I'm on,
> I don't know)
>
>
>
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200701.mbox/%3c6dac79b90701030842r6e22ebb6xee81ed57033a621e@mail.gmail.com%3e,"""Adam Winer"" <awi...@gmail.com>",0,0
51,52,"Interesting. Thanks for this solution Lance, gonna try it
> On Jul 6, 2016, at 19:11, Lance Norskog <lance.norskog@gmail.com> wrote:
> 
> You could use the XCOM feature to post a semaphore at the start of the task
> and then remove it at the end.
> Another task would see the semaphore and immediately quit.
> If you get a race condition, the third 15-minute task will take care of the
> problem, 15 minutes late.
> 
> Lance
> 
> On Wed, Jul 6, 2016 at 2:50 PM, Cyril Scetbon <cyril.scetbon@free.fr> wrote:
> 
>> Hi,
>> 
>> I have dags with tasks that use same configuration and same schedule time
>> frequencies.
>> 
>> When I have x tasks in a dag they run in //. My dag is scheduled every 15
>> minutes, but sometimes I have at least a task that runs for more than 15
>> minutes and 2 identical tasks should not run at the same time (in my case)
>> but it's what happens.
>> 
>> So when the dag is kicked off (every 15 min) I need Airflow to run tasks
>> only for those that are not running, which means :
>> 
>> - if at time t1 tasks tk1 is running and tk2, ..., tkn are not running, I
>> need Airflow to run only tk2,.., tkn but not tk1 cause it's already running
>> - if at time t2 tasks tk1, ..., tkn are not running, I need Airflow to run
>> tk1,.., tkn
>> 
>> I already tried using depends_on_past=True, however I need failed tasks to
>> be kicked off, cause I can get some temporary issues.
>> 
>> I also use an upstream task connected to all tasks in my dag to be able to
>> run manually all tasks if I need to. (I don't use a frequency of 15 minutes
>> in all dags)
>> 
>> Any idea ?
> 
> 
> 
> 
> -- 
> Lance Norskog
> lance.norskog@gmail.com
> Redwood City, CA


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201607.mbox/<F04705EC-9F3E-45BE-B55F-D944873C39E0@free.fr>,Cyril Scetbon <cyril.scet...@free.fr>,0,0
62,63,"Hila,
I commented just now.

-s

On Sun, Sep 25, 2016 at 10:11 PM, הילה ויזן <hilaviz@gmail.com> wrote:

> Hi guys,
> I posted this issue 2 weeks ago.
> ExternalTaskSensor fails in each dag run - query to airflow DB fails.
>
> maybe one of you can help?
>
> https://issues.apache.org/jira/browse/AIRFLOW-503
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<CANLtMicfpQuyQec9Aw3STOFbvzetkWYj0VnM5-B3j7LZrQSf-A@mail.gmail.com>,siddharth anand <san...@apache.org>,0,0
2,2,"Hello Usergrid Team,  We are suddenly facing ""out of memory"" exceptions in our Tomcat Severs, under low load conditions. Please note, our usergrid installations have been very stable over the last 6 months, and we have ""not"" seen such issues before. Our setup configuration is as below: Environment: Ubuntu 14.04, Tomcat 7, JDK 1.8.0_65 (Oracle); Cassandra version: 2.2.6 (DataStax); Usergrid version: 2.2.0 (Master branch, 3rd May, 2016)  I am pasting a few logs that have suddenly started showing up.  ------------------------------------------------------------ Nov 09 16:15:26 catalina.out: 05:45:26,812  WARN EntityMappingParser:116 - Encountered 2 collections consecutively.  N+1 dimensional arrays are unsupported, only arrays of depth 1 are supported ------------------------------------------------------------ Nov 09 17:22:12 catalina.out: 06:52:12,848  WARN AsyncEventServiceImpl:362 - No index operation messages came back from event processing for msg: ------------------------------------------------------------ Nov 09 17:39:56 catalina.out: 07:09:56,177  INFO transport:470 - [ip-10-0-2-128] failed to get local cluster state for [#transport#-3][ip-10-0-2-128][inet[/10.0.4.205:9300]], disconnecting... Nov 09 17:39:56 catalina.out: org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[/10.0.4.205:9300]][cluster:monitor/state] request_id [11652] timed out after [5247ms] Nov 09 17:39:56 catalina.out:  at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529) Nov 09 17:39:56 catalina.out:  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) Nov 09 17:39:56 catalina.out:  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) Nov 09 17:39:56 catalina.out:  at java.lang.Thread.run(Thread.java:745) ------------------------------------------------------------ Nov 09 17:40:17 catalina.out: 07:10:17,557  WARN transport:415 - [ip-10-0-2-128] Received response for a request that has timed out, sent [10887ms] ago, timed out [3ms] ago, action [cluster:monitor/state], node [[bluedls__us-east-1a__db__10.0.4.63][T6OWiR1US9m5ABxHh0tW0w][ip-10-0-4-63][inet[/10.0.4.63:9300]]{zone=us-east-1__us-east-1a}], id [11678] ------------------------------------------------------------ Nov 09 17:43:05 catalina.out: 07:13:05,091 ERROR AbstractExceptionMapper:74 - com.netflix.hystrix.exception.HystrixRuntimeException 5XX Uncaught Exception (500) Nov 09 17:43:05 catalina.out: com.netflix.hystrix.exception.HystrixRuntimeException: ConsistentReplayCommand timed-out and fallback failed. .. Nov 09 17:43:05 catalina.out: Caused by: java.util.concurrent.TimeoutException .. Nov 09 17:43:05 catalina.out: Caused by: rx.exceptions.OnErrorThrowable$OnNextValue: OnError while emitting onNext value: org.apache.usergrid.persistence.collection.mvcc.stage.CollectionIoEvent.class .. Nov 09 17:43:05 catalina.out: 07:13:05,123 ERROR AbstractExceptionMapper:108 - Server Error (500): Nov 09 17:43:05 catalina.out: {""error"":""hystrix_runtime"",""timestamp"":1510229585122,""duration"":0,""error_description"":""ConsistentReplayCommand timed-out and fallback failed."",""exception"":""com.netflix.hystrix.exception.HystrixRuntimeException""} ------------------------------------------------------------  Our monitoring indicates there is no issue in cassandra and elasticseach clusters. Look forward to your help.  Thanks Jaskaran ",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201711.mbox/raw/%3CCAFBZ5wwDSpXe6B9fr2mOv7C1nba5ShOJP-fZjS6TwKDxekafwg%40mail.gmail.com%3E,Jaskaran Singh <jaskaran1...@gmail.com>,0,0
66,67,"Hi, I'm just getting started trying out NPanday and I can't get the VS
installer to run. Any help would be appreciated.

My environment:

Maven: 3.0.3
Java: 1.6.0_25
OS: Win 7 Ultimate
Visual Studio: 2010 Premium (10.0.40219.1 SP1Rel)
.Net: 4.0.30319 SP1Rel


Here is the error that I get when trying to install the plugin. I have
started the cmd shell as administrator and VS is not running.  Any tips?

C:\Windows\system32>mvn -U -e
org.apache.npanday.plugins:maven-vsinstaller-plugin:1.4.0-incubating:install
[INFO] Error stacktraces are turned on.
[INFO] Scanning for projects...
[INFO]
[INFO]
------------------------------------------------------------------------
[INFO] Building Maven Stub Project (No POM) 1
[INFO]
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-vsinstaller-plugin:1.4.0-incubating:install (default-cli) @
standalone-pom ---
[WARNING] NPANDAY-102-001: No NPanday settings available. Using Defaults.
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 2.477s
[INFO] Finished at: Tue Sep 13 14:16:09 CDT 2011
[INFO] Final Memory: 5M/111M
[INFO]
------------------------------------------------------------------------
[ERROR] Failed to execute goal
org.apache.npanday.plugins:maven-vsinstaller-plugin:1.4.0-incubating:install
(default-cli) on project standal
one-pom: Execution default-cli of goal
org.apache.npanday.plugins:maven-vsinstaller-plugin:1.4.0-incubating:install
failed. NullPointerExcep
tion -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal org.apache.npanday.plugins:maven-vsinstaller-plugin:1.4.0-inc
ubating:install (default-cli) on project standalone-pom: Execution
default-cli of goal org.apache.npanday.plugins:maven-vsinstaller-plugin:1
.4.0-incubating:install failed.
        at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:225)
        at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
        at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
        at
org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
        at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:319)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)
        at
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)
        at
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)
        at
org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)
Caused by: org.apache.maven.plugin.PluginExecutionException: Execution
default-cli of goal org.apache.npanday.plugins:maven-vsinstaller-plug
in:1.4.0-incubating:install failed.
        at
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:110)
        at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
        ... 19 more
Caused by: java.lang.NullPointerException
        at
npanday.plugin.vsinstaller.VsInstallerMojo.isDotNetHandler(VsInstallerMojo.java:227)
        at
npanday.plugin.vsinstaller.VsInstallerMojo.execute(VsInstallerMojo.java:157)
        at
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
        ... 20 more
[ERROR]
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please
read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
C:\Windows\system32>

",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201109.mbox/%3cCAGpFaXQJ+PmnbZXBOHCFcGTfvsOKtDdHoRYNCu6=B3Ccpjzb-w@mail.gmail.com%3e,Ryan Kruse <r...@kruseonline.net>,0,0
73,74,"On 4/5/07, Gareth Paglinawan <gareth.paglinawan@gmail.com> wrote:
>
> hi,
>
> Do you have any getting start guide in using Trinidad in eclipse and tomcat.
> I have a problem that when I start the application, it says the resource is
> not available. HTTP404. Please send me the complete step how to develop one
>
> page. I already developed using JDeveloper but I want to use eclipse and
> Tomcat. I followed the steps in apache doc but I am not successful.
>
> Please send me step by step guide to do this.
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200704.mbox/%3c2aeff41c0704050810p509a3a3dt1560cfa439113566@mail.gmail.com%3e,"""Gareth Paglinawan"" <gareth.paglina...@gmail.com>",0,0
132,133,"Hello Arvind,

DAGs in Airflow are simple Python objects that only need to be registered
in the globals namespace. It's easy to generate DAGs from some
configuration or database, I'm using that for example to generate several
image processing pipelines based on configuration.

Here is an example:
https://github.com/LREN-CHUV/airflow-mri-preprocessing-dags/blob/master/mri_pipelines_init.py

Ludovic


2017-01-16 12:28 GMT+01:00 Arvind Nedumaran <arvindamirtaa@gmail.com>:

> Hello everyone,
>
> I'm new to Airflow and I'm evaluating it for implementation at our org. I
> find that there are a lot of use cases where workflows are not known during
> development. Is there a way to programatically generate, and manage DAGs.
>
> Better yet, is there a way to manage them directly from a DB instead of
> generating them from templates and managing them as files. We're about to
> spend considerable time implementing this, so I just want to know about the
> various existing options.
>
> Does it help if the tasks are pre-defined and the DAGs only happen to be
> some permutation of the pre-existing tasks?
>
> Anybody who's tried something like before, I'd be grateful for any input on
> this issue.
>
> Thanks,
> Arvind
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CAKOr0jtjCN+YTBqkOzvjQPBO=VDPaTsTzS2NzrRkmNv5TLpCfw@mail.gmail.com>,Ludovic Claude <ludovic.claud...@gmail.com>,0,0
243,244,"Thanks a lot Sumit & Max.

Finally I could resolve it by following the above  stack overflow link and
then doing `pip install airflow[mysql]`

Reagrds,
Madhu

On Mon, Jun 20, 2016 at 8:27 AM, Maxime Beauchemin <
maximebeauchemin@gmail.com> wrote:

> `mysqlclient` is the [in place drop in, compatible with `MySQLdb`] library
> you want to use as it is maintained and py3 compatible.
>
> `pip install airflow[mysql]` will install mysqlclient with a version that
> has been tested with Airflow.
>
> Max
>
> On Mon, Jun 20, 2016 at 2:51 AM, Sumit Maheshwari <sumeet.manit@gmail.com>
> wrote:
>
> > mysql_hook uses MySQLdb. Just see if you are not hit by this issue:
> >
> >
> >
> http://stackoverflow.com/questions/6383310/python-mysqldb-library-not-loaded-libmysqlclient-18-dylib
> >
> >
> >
> > On Mon, Jun 20, 2016 at 5:58 AM, Msr Msr <msrmaillist@gmail.com> wrote:
> >
> > > Hi,
> > >
> > > I have installed Airflow on Mac and trying to use  MySqlOperator.
> > >
> > > It is giving below error message.
> > >
> > > ------------------
> > >
> > > mms-MacBook-Pro:~ mm$ airflow initdb
> > >
> > > [2016-06-19 17:21:06,794] {__init__.py:36} INFO - Using executor
> > > SequentialExecutor
> > >
> > > DB: sqlite:////Users/mm/airflow/airflow.db
> > >
> > > [2016-06-19 17:21:07,345] {db.py:222} INFO - Creating tables
> > >
> > > INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
> > >
> > > INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
> > >
> > > ERROR [airflow.models.DagBag] Failed to import:
> > > /Users/mm/airflow/dags/S3test.py
> > >
> > > Traceback (most recent call last):
> > >
> > >   File
> > ""/Users/mm/anaconda/lib/python2.7/site-packages/airflow/models.py"",
> > > line 247, in process_file
> > >
> > >     m = imp.load_source(mod_name, filepath)
> > >
> > >   File ""/Users/mm/airflow/dags/S3test.py"", line 4, in <module>
> > >
> > >     from airflow.operators import MySqlOperator
> > >
> > > ImportError: cannot import name MySqlOperator
> > >
> > > -------------------
> > >
> > >
> > > pip install airflow[mysql] always gives Requirements met
> > >
> > >
> > > Could someone please suggest what could be the reason for this error
> and
> > > how to resolve it
> > >
> > >
> > > Thanks,
> > >
> > > msr
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CABzfHRpK+4VNsR0kvuCAzhqku+K=vgryELLHrLEH_TTbDB_itw@mail.gmail.com>,Msr Msr <msrmaill...@gmail.com>,0,0
53,54,"Hi, the gradle wrapper is not included anymore in the source distribution from RC4 , but you
can follow the instructions in the README file to install gradle.

The approach is similar to other projects that rely on maven but do not include it in the
release: you have to install maven in order to build the project.

Note we haven't voted yet on RC4, and we also need to update the walkthrough for installing
gradle.

Regards,

Matthieu

On Apr 13, 2013, at 10:05 , Dingyu Yang wrote:

> Hi, I download the src of S4-RC4.
> I find the gradlew file missing.
> Is there other compile files?
> 


",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201304.mbox/%3c01130ED8-7810-4B22-855F-1607F240B9A6@apache.org%3e,Matthieu Morel <mmo...@apache.org>,0,0
142,143,"Thinking back it may have been 1.8.0rc5-> 1.8.0 regressions. I am still
worried about the large number of PRs in 1.8.1 even if they are all bug
fixes though (known issues that we already have patches for vs unknown new
issues introduced with the 1.8.1 patches) , but I agree with your sentiment
that these PRs should most likely make things more stable.

On Thu, May 4, 2017 at 10:55 AM, Alex Guziel <alex.guziel@airbnb.com> wrote:

> I don't think any of the fixes I did were regressions.
>
> On Thu, May 4, 2017 at 8:11 AM, Bolke de Bruin <bdbruin@gmail.com> wrote:
>
>> I know of one that Alex wanted to get in, but wasn’t targeted for 1.8.1
>> in Jira and thus didn’t make the cut at RC time. There is is another one
>> out that seems to have stalled a bit (https://github.com/apache/inc
>> ubator-airflow/pull/2205).
>>
>> Reading the changelog of 1.8.1 I see bug fixes, apache requirements and
>> one “new” feature (UI lightning bolt). Regressions could have happened but
>> we have been quite vigilant on the fact that these bug fixes needed proper
>> tests, so I am very interested in 1.8.0 -> 1.8.1 regressions. If it is a
>> pre-backfill-change 1.8.0 to 1.8.1 regression then I would also like to
>> know, cause I made that change and feel responsible for it.
>>
>> Cheers
>> Bolke
>>
>>
>> On 3 May 2017, at 22:13, Dan Davydov <dan.davydov@airbnb.com> wrote:
>>
>> cc Alex and Rui who were working on fixes, I'm not sure if their commits
>> got in before 1.8.1.
>>
>> On Wed, May 3, 2017 at 1:09 PM, Bolke de Bruin <bdbruin@gmail.com> wrote:
>>
>>> Hi Dan,
>>>
>>> (Thread renamed to make sure it does not clash, dev@ now added)
>>>
>>> It surprises me that you found regression from 1.8.0 to 1.8.1 as 1.8.1
>>> is very much focused on bug fixes. Were the regressions shared yet?
>>>
>>> The whole 1.8.X release will be bug fix focused (per release management)
>>> and minor feature updates. The 1.9.0 release will be the first release with
>>> major feature updates. So what you want, more robustness and focus on
>>> stability, is now underway. I agree with beefing up tests and including the
>>> major operators in this. Executors should also be on this list btw. Turning
>>> on coverage reporting might be a first step in helping this (it isn’t the
>>> solution obviously).
>>>
>>> Cheers
>>> Bolke
>>>
>>>
>>> On 3 May 2017, at 20:28, Dan Davydov <dan.davydov@airbnb.com> wrote:
>>>
>>> We saw several regressions moving from 1.8.0 to 1.8.1 the first time we
>>> tried, and while I think we merged all our fixes to master (not sure if
>>> they all made it into 1.8.1 however), we have put releasing on hold due to
>>> stability issues from the last couple of releases. It's either the case
>>> that:
>>> A) Airbnb requires more robustness from new releases.
>>> or
>>> B) Most companies using Airflow require more robustness and we should
>>> halt on feature work until we are more confident in our testing
>>>
>>> I think the biggest problem currently is the lack of unit testing
>>> coverage, e.g. when the backfill framework was refactored (which was the
>>> right long-term fix), it caused a lot of breakages that weren't caught by
>>> tests. I think we need to audit the major operators/classes and beef up the
>>> unit testing coverage. The coverage metric does not necessarily cover these
>>> cases (e.g. cyclomatic complexity). Writing regression tests is good but we
>>> shouldn't have so many new blocker issues in our releases.
>>>
>>> We are fighting some fires internally at the moment (not Airflow
>>> related), but Alex and I have been working on some stuff that we will push
>>> to the community once we are done. Alex is working on a good solution for
>>> python package isolation, and I'm working on integration with Kubernetes at
>>> the executor level.
>>>
>>> Feel free to forward any of my messages to the dev mailing list.
>>>
>>> On Wed, May 3, 2017 at 11:18 AM, Bolke de Bruin <bdbruin@gmail.com>
>>> wrote:
>>>
>>>> Grrr, I seriously dislike to send button on the touch bar…here goes
>>>> again.
>>>>
>>>> Hi Dan,
>>>>
>>>> (Please note I would like to forward the next message to dev@, but let
>>>> me know if you don’t find it comfortable)
>>>>
>>>> I understand your point. The gap between 1.7.1 was large in terms of
>>>> functionality changes etc. It was going to be a (bit?) rough and as you
>>>> guys are using many of the edge cases you probably found more issues than
>>>> any of us. Still, between 1.8.0 and 1.8.1 we have added many tests
>>>> (coverage increased from 67% to close to 69%, which is a lot as you know).
>>>> It would be nice if you can share where your areas of concern are so we can
>>>> address those and a suggestion on how to proceed with integration tests is
>>>> also welcome.
>>>>
>>>> You guys (=Airbnb) have been a bit quiet over the past couple of days,
>>>> so I am getting a bit worried in terms of engagement. Is that warranted?
>>>>
>>>> Cheers
>>>> Bolke
>>>>
>>>>
>>>> On 3 May 2017, at 20:13, Bolke de Bruin <bdbruin@gmail.com> wrote:
>>>>
>>>> Hi Dan,
>>>>
>>>> (Please note I would like to forward the next message to dev@, but let
>>>> me know if you don’t find it comfortable)
>>>>
>>>> I understand your point. The gap between 1.7.1 was large in terms of
>>>> functionality changes etc. It was going to be a (bit?) rough and as you
>>>> guys are using many of the edge cases you probably found more issues than
>>>> any of us. Still, between 1.8.0 and 1.8.1 we have added many tests
>>>> (coverage increased from 67
>>>>
>>>> On 3 May 2017, at 19:41, Arthur Wiedmer <arthur.wiedmer@airbnb.com>
>>>> wrote:
>>>>
>>>> As a counterpoint,
>>>>
>>>> I am comfortable voting +1 on this release in the sense that it fixes
>>>> some of the issues with 1.8.0. It is unfortunate that we cannot test it on
>>>> the Airbnb production for now and we should definitely invest in increasing
>>>> testing coverage, but some of the fixes are needed for ease of use/adoption
>>>> (See for instance AIRFLOW-832), and this release is a step in the right
>>>> direction.
>>>>
>>>> Best,
>>>> Arthur
>>>>
>>>> On Wed, May 3, 2017 at 10:30 AM, Dan Davydov <dan.davydov@airbnb.com>
>>>> wrote:
>>>>
>>>>> I'm not comfortable voting without doing comprehensive staging and we
>>>>> aren't comfortable doing an internal lease for now until we fix the state
>>>>> of unit test coverage and integration tests.
>>>>>
>>>>> On May 3, 2017 8:42 AM, ""Bolke de Bruin"" <bdbruin@gmail.com> wrote:
>>>>>
>>>>>> Hey Guys,
>>>>>>
>>>>>> Chris has been preparing the 1.8.1 release and the vote is running
>>>>>> for it. Only one day left though! Would you mind casting your vote?
Only
>>>>>> Chris and I have voted binding until so far.
>>>>>>
>>>>>> (Please reply to the message on the list, not this message).
>>>>>>
>>>>>> Cheers!
>>>>>> Bolke
>>>>>>
>>>>>> Begin forwarded message:
>>>>>>
>>>>>> *From: *Chris Riccomini <criccomini@apache.org>
>>>>>> *Subject: **[VOTE] Release Airflow 1.8.1 RC2*
>>>>>> *Date: *1 May 2017 at 19:58:41 GMT+2
>>>>>> *To: *dev@airflow.incubator.apache.org
>>>>>> *Reply-To: *dev@airflow.incubator.apache.org
>>>>>>
>>>>>> Dear All,
>>>>>>
>>>>>> _WARN: The package version for this RC is 1.8.1 (does not include
RC2
>>>>>> in
>>>>>> version number). As such, any future 1.8.1 installatinos will have
to
>>>>>> be
>>>>>> force installed. PIP will not be able to distinguish between RCs
and
>>>>>> final
>>>>>> versions. Again, you'll have to force install the package. This can
>>>>>> be done
>>>>>> by adding `--force-reinstall` to your `pip install` commands._
>>>>>>
>>>>>> I've made Airflow 1.8.1 RC2 available at:
>>>>>> https://dist.apache.org/repos/dist/dev/incubator/airflow, public
>>>>>> keys are
>>>>>> available at https://dist.apache.org/repos/
>>>>>> dist/release/incubator/airflow.
>>>>>>
>>>>>> New issues fixed in 1.8.1 RC2:
>>>>>>
>>>>>> [AIRFLOW-1142] SubDAG Tasks Not Executed Even Though All Dependen
>>>>>> [AIRFLOW-1004] `airflow webserver -D` runs in foreground
>>>>>> [AIRFLOW-492] Insert into dag_stats table results into failed ta
>>>>>>
>>>>>> Issues fixed in 1.8.1 RC0/RC1, and included in RC2:
>>>>>>
>>>>>> [AIRFLOW-1138] Add licenses to files in scripts directory
>>>>>> [AIRFLOW-1127] Move license notices to LICENSE instead of NOTICE
>>>>>> [AIRFLOW-1124] Do not set all task instances to scheduled on back
>>>>>> [AIRFLOW-1120] Update version view to include Apache prefix
>>>>>> [AIRFLOW-1062] DagRun#find returns wrong result if external_trigg
>>>>>> [AIRFLOW-1054] Fix broken import on test_dag
>>>>>> [AIRFLOW-1050] Retries ignored - regression
>>>>>> [AIRFLOW-1033] TypeError: can't compare datetime.datetime to None
>>>>>> [AIRFLOW-1017] get_task_instance should return None instead of th
>>>>>> [AIRFLOW-1011] Fix bug in BackfillJob._execute() for SubDAGs
>>>>>> [AIRFLOW-1001] Landing Time shows ""unsupported operand type(s) fo
>>>>>> [AIRFLOW-1000] Rebrand to Apache Airflow instead of Airflow
>>>>>> [AIRFLOW-989] Clear Task Regression
>>>>>> [AIRFLOW-974] airflow.util.file mkdir has a race condition
>>>>>> [AIRFLOW-906] Update Code icon from lightning bolt to file
>>>>>> [AIRFLOW-858] Configurable database name for DB operators
>>>>>> [AIRFLOW-853] ssh_execute_operator.py stdout decode default to A
>>>>>> [AIRFLOW-832] Fix debug server
>>>>>> [AIRFLOW-817] Trigger dag fails when using CLI + API
>>>>>> [AIRFLOW-816] Make sure to pull nvd3 from local resources
>>>>>> [AIRFLOW-815] Add previous/next execution dates to available def
>>>>>> [AIRFLOW-813] Fix unterminated unit tests in tests.job (tests/jo
>>>>>> [AIRFLOW-812] Scheduler job terminates when there is no dag file
>>>>>> [AIRFLOW-806] UI should properly ignore DAG doc when it is None
>>>>>> [AIRFLOW-794] Consistent access to DAGS_FOLDER and SQL_ALCHEMY_C
>>>>>> [AIRFLOW-785] ImportError if cgroupspy is not installed
>>>>>> [AIRFLOW-784] Cannot install with funcsigs > 1.0.0
>>>>>> [AIRFLOW-780] The UI no longer shows broken DAGs
>>>>>> [AIRFLOW-777] dag_is_running is initlialized to True instead of
>>>>>> [AIRFLOW-719] Skipped operations make DAG finish prematurely
>>>>>> [AIRFLOW-694] Empty env vars do not overwrite non-empty config v
>>>>>> [AIRFLOW-139] Executing VACUUM with PostgresOperator
>>>>>> [AIRFLOW-111] DAG concurrency is not honored
>>>>>> [AIRFLOW-88] Improve clarity Travis CI reports
>>>>>>
>>>>>> I would like to raise a VOTE for releasing 1.8.1 based on release
>>>>>> candidate
>>>>>> 2.
>>>>>>
>>>>>> Please respond to this email by:
>>>>>>
>>>>>> +1,0,-1 with *binding* if you are a PMC member or *non-binding* if
>>>>>> you are
>>>>>> not.
>>>>>>
>>>>>> Vote will run for 72 hours (ends this Thursday).
>>>>>>
>>>>>> Thanks!
>>>>>> Chris
>>>>>>
>>>>>> My VOTE: +1 (binding)
>>>>>>
>>>>>>
>>>>>>
>>>>
>>>>
>>>> --
>>>> Arthur Wiedmer
>>>> (Pronouns: He, Him)
>>>> Data Engineering, Airbnb
>>>> www.airbnb.com
>>>>
>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CABekQG7LYELk3UwweCqrdO=x49k_DvY444v5socDH6x+4dfnqw@mail.gmail.com>,Dan Davydov <dan.davy...@airbnb.com.INVALID>,0,1
48,49,"CC'ing olio-user as that is the right forum for these questions.
Answers below.

Shanti

On Tue, Jun 8, 2010 at 11:31 PM, hao zhang <julius.haozhang@gmail.com>wrote:

> Hi,All
> I have some doubt about using the apache-olio-java-0.2. Is it possible
> to use one master to control multiple pairs of workload driver and
> SUT?
>

 I assume you are using the term SUT to mean the system or set of systems
that are running the Olio application.


> Suppose that I have four machines. Two of them are used as workload
> driver and two as SUTs. One workload driver drives load to one SUT.
> Can I use one faban driver to control them? Thanks.
>
>
If you're question is whether you can have the application run on multiple
systems (ie. two instances on two systems) with one driver running each
server and benchmark the whole thing as one unit, you can certainly do that.
You can run the master on any one of the driver systems. You can specify as
many host:port pairs as you want for both driver systems as well as the web
server systems they will be connecting to. By sure to run at least 1 agent
on each driver system (i.e. total of 2 drivers for the above config).


> - Hui
>

Shanti

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/201006.mbox/%3cAANLkTinbezcfVAxmmg4ilXjcJJ7Z0YWry2jImXuv7-xY@mail.gmail.com%3e,Shanti Subramanyam <shanti.subraman...@gmail.com>,0,1
134,135,"Hi,

I have a use case in which:
- I have a single file having 10M records in it.
- I want to run multiple jobs against this file.

So, right now what is happening with using simple Apache Spark is that I
need to load this 10M file for each job that I submit.

I was hoping Apache Livy could help me here. Help me understand Livy
better. Below is my understanding of Livy.

1) If I create a job (i.e a spark context) called to say ""load file in
memory"" and use Livy to run it, then Spark will read this file and keep
this RDD cached at the spark context level which at the end will reside
in-memory at Livy's end.

1.1) My Spark jobs then can die and the file is still cached at the spark
context level. This will consume some memory on Livy's end.

1.2) At this point, my task slots are all free to pick up another jobs and
no Spark executors are running.

2) I can then later create more jobs and submit using Livy. This time I
will use the context created by the above job. This way I will be able to
use the file cached by Livy.


Please help me understand this better.

-- 

Warm Regards,
Suraj Sharma
Phone: +91-9741370819
Skype: surajsharma121

",http://mail-archives.apache.org/mod_mbox/incubator-livy-user/201908.mbox/%3cCAEpg4zddw59drNTKSt3sokE1reBLc=_dm8tzQV5E5Ujkzd_kaw@mail.gmail.com%3e,Suraj Sharma <surajsharma...@gmail.com>,0,0
100,101,"Hey Bolke,

I'm installing the latest alpha as we speak.

As far as I'm concerned, we've hit no remaining blockers in our dev
environment. And I'm ready for beta whenever you cut it (I'll install the
beta in our prod cluster).

Re: dropping scheduler change, I'm fine with that. I would rather favor
stability anyway, and messing with the scheduler this late in a release
doesn't seem prudent to me.

Cheers,
Chris

On Wed, Jan 11, 2017 at 6:39 AM, Alex Van Boxel <alex@vanboxel.be> wrote:

> Hey Bolke, I'll be of for a few days but I started looking at the
> CHANGELIST to make sure we now have a list of changes for 1.8 (seems like a
> big list...).
>
> I'll will also drop the trigger issue for the 1.8 and see what we can do
> for the next version. I'll make it into a documentation issue.
>
> On Wed, Jan 11, 2017 at 1:46 PM Bolke de Bruin <bdbruin@gmail.com> wrote:
>
> > Dear All,
> >
> > I would like to drop ""Schedule all pending DAG runs in a single scheduler
> > loop” from the 1.8.0 release (updated:
> > https://github.com/apache/incubator-airflow/pull/1980 <
> > https://github.com/apache/incubator-airflow/pull/1980>, original:
> > https://github.com/apache/incubator-airflow/pull/1906 <
> > https://github.com/apache/incubator-airflow/pull/1906>). The reason for
> > this is that it, imho, biases the scheduler towards a single DAG as it
> > fills the queue with tasks from one DAG and then goes to the next DAG.
> > Starving DAGs that come after the first for resources. As such it should
> be
> > updated and that will take time.
> >
> > Please let me know if I am incorrect.
> >
> > Thanks
> > Bolke
> >
> > > On 10 Jan 2017, at 09:25, Bolke de Bruin <bdbruin@gmail.com> wrote:
> > >
> > > Dear All,
> > >
> > > I have made Airflow 1.8.0 alpha 4 available at
> > https://people.apache.org/~bolke/ <https://people.apache.org/~bolke/> .
> > Again no Apache release yet - this is for testing purposes. I consider
> this
> > Alpha to be a Beta if not for the pending features. If the pending
> features
> > are merged within a reasonable time frame (except for **, as no progress
> > currently) then I am planning to mark the tarball as Beta and only allow
> > bug fixes and (very) minor features. This week hopefully.
> > >
> > > Blockers:
> > >
> > > * None
> > >
> > > Fixed issues
> > > * Regression in email
> > > * LDAP case sensitivity
> > > * one_failed task not being run: now seems to pass suddenly (so fixed?)
> > -> need to investigate why
> > > * Email attachments
> > > * Pinned jinja2 to < 2.9.0 (2.9.1 has a confirmed regression)
> > > * Improve time units for task performance charts
> > > * XCom throws an duplicate / locking error
> > > * Add execution_date to trigger_dag
> > >
> > > Pending features:
> > > * DAG.catchup : minor changes needed, documentation still required,
> > integration tests seem to pass flawlessly
> > > * Cgroups + impersonation: clean up of patches on going, more tests and
> > more elaborate documentation required. Integration tests not executed yet
> > > * Schedule all pending DAG runs in a single scheduler loop: no progress
> > (**)
> > >
> > > Cheers!
> > > Bolke
> >
> > --
>   _/
> _/ Alex Van Boxel
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CABYbY7e+Tf0n6uNpBOZ=EkeE9+Gh6-8JSJH8tYp0qY2oqrPFPw@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
71,72,"Nice! Why don’t we also merge 1 into master then and fix issues from there? It will get wider
exposure (We run master in our dev environments for example), it won’t put everything into
the lab or Airbnb. Master hardly should be considered production so imho it is allowed some
time to settle before it is stable again.

Does someone want to pick up 2 say by the end of week or next week to fix all headers?

- Bolke


> Op 14 jun. 2016, om 20:01 heeft Maxime Beauchemin <maximebeauchemin@gmail.com>
het volgende geschreven:
> 
> I think (1) is good to go. I can cherry pick it into our production to make
> sure it's ready for release.
> 
> (2) should be fine, git should be able to merge easily, otherwise it's
> super easy to resolve any conflicts
> 
> Max
> 
> On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <criccomini@apache.org>
> wrote:
> 
>> Hey Bolke,
>> 
>> I think your list is good. (1) is what I'm most concerned about, as it
>> requires actually touching the code, and is blocking on graduation. I
>> *think* Max had a partial PR on that, but don't know the current state.
>> 
>> Re: (2), agree. Should just do a bulk PR for it.
>> 
>> Cheers,
>> Chris
>> 
>> On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com> wrote:
>> 
>>> Hi,
>>> 
>>> I am wondering what needs to be done to get to an Apache release? I think
>>> now 1.7.1.3 is out we should be focused on getting one out as we are kind
>>> of half way the incubation process. What comes to my mind is:
>>> 
>>> 1. Replace highcharts by D3 (WIP:
>>> https://github.com/apache/incubator-airflow/pull/1469)
>>> 2. Add license headers everywhere (TM) (Sucks, as it will break many PRs
>> -
>>> but lets do it quickly)
>>> 3. Have a review by Apache
>>> 
>>> Anything I am missing?
>>> 
>>> - Bolke
>> 


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<C3BB5984-32BA-409D-A065-20209CE98714@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,1
26,27,"> SO, to return to the most important question of the moment:
> I'm gonna assume that all of the ""non-controversial"" name changes
> have general approval, so that one of the committers can go ahead
> and make the change...  To review, that list is now:
>
> decorateCollection  panelCollection
> navigationLevel     navigationPane
> objectIcon          icon
> objectImage         image
> objectMedia         media
> objectSeparator     separator
> objectSpacer        spacer
> panelBorder         panelBorderLayout
> panelForm           panelFormLayout
> panelGroup          panelGroupLayout
> panelHorizontal     panelHorizontalLayout
> selectInputColor    inputColor
> selectInputDate     inputDate
> showOneAccordion    panelAccordion
> showManyAccordion   panelAccordion (merge with showOneAccordion)
> showOneChoice       panelChoice
> showOneRadio        panelRadio
> showOneTabs         panelTabbed


+1


> -- Adam
>
>
> On 7/13/06, Benj Fayle <bfayle@maketechnologies.com> wrote:
> > Agreed which is why abbreviations such as inputLOV should be avoided
> > inputListOfValues is much more self-explanatory.
> >
> > -----Original Message-----
> > From: Adam Winer [mailto:awiner@gmail.com]
> > Sent: Wednesday, July 12, 2006 10:13 AM
> > To: adffaces-user@incubator.apache.org
> > Subject: Re: Re: Re: Tag renaming proposal
> >
> > On 7/12/06, Mike Kienenberger <mkienenb@gmail.com> wrote:
> > >
> > > On 7/12/06, Matthias Wessendorf <matzew@apache.org> wrote:
> > > > > I've gotten a decent bit of feedback in the past that people
> > > > > just couldn't find selectInputDate or selectInputColor, but
> > > > > if they'd been called inputDate or inputColor, they would have
> > > > > been found.
> > > >
> > > > yes, that is true for me, to be honest .
> > >
> > > On the other hand, if someone explains up-front in the component table
> > > of contents that ""input"" meant unassisted entry and ""select"" meant
> > > assisted entry, and ""inputSelect"" means both, I could have figured
> > > this out.
> >
> >
> >
> > ... but as ""selectInputDate"", anyone looking alphabetically is just
> > totally
> > lost.
> >
> > One of the goals of the renaming is to minimize any need for up-front
> > explanation;  the best name is one that requires no documentation.
> >
> > -- Adam
> >
>


-- 
Matthias Wessendorf

further stuff:
blog: http://jroller.com/page/mwessendorf
mail: mwessendorf-at-gmail-dot-com

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200607.mbox/%3c71235db40607131719j78959a75j7df090e07039acdc@mail.gmail.com%3e,"""Matthias Wessendorf"" <mat...@apache.org>",1,0
16,16,"From dev-return-418-apmail-provisionr-dev-archive=provisionr.apache.org@provisionr.incubator.apache.org  Mon Nov 11 05:36:29 2013 Return-Path: <dev-return-418-apmail-provisionr-dev-archive=provisionr.apache.org@provisionr.incubator.apache.org> X-Original-To: apmail-provisionr-dev-archive@minotaur.apache.org Delivered-To: apmail-provisionr-dev-archive@minotaur.apache.org Received: from mail.apache.org (hermes.apache.org [140.211.11.3]) 	by minotaur.apache.org (Postfix) with SMTP id A92C21083F 	for <apmail-provisionr-dev-archive@minotaur.apache.org>; Mon, 11 Nov 2013 05:36:29 +0000 (UTC) Received: (qmail 40243 invoked by uid 500); 11 Nov 2013 05:36:29 -0000 Delivered-To: apmail-provisionr-dev-archive@provisionr.apache.org Received: (qmail 39858 invoked by uid 500); 11 Nov 2013 05:36:26 -0000 Mailing-List: contact dev-help@provisionr.incubator.apache.org; run by ezmlm Precedence: bulk List-Help: <mailto:dev-help@provisionr.incubator.apache.org> List-Unsubscribe: <mailto:dev-unsubscribe@provisionr.incubator.apache.org> List-Post: <mailto:dev@provisionr.incubator.apache.org> List-Id: <dev.provisionr.incubator.apache.org> Reply-To: dev@provisionr.incubator.apache.org Delivered-To: mailing list dev@provisionr.incubator.apache.org Received: (qmail 39829 invoked by uid 99); 11 Nov 2013 05:36:22 -0000 Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)     by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 05:36:22 +0000 X-ASF-Spam-Status: No, hits=-2000.0 required=5.0 	tests=ALL_TRUSTED X-Spam-Check-By: apache.org Received: from [140.211.11.8] (HELO aegis.apache.org) (140.211.11.8)     by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 05:36:15 +0000 Received: from aegis.apache.org (localhost [127.0.0.1]) 	by aegis.apache.org (Postfix) with ESMTP id 37B10C00E4; 	Mon, 11 Nov 2013 05:35:54 +0000 (UTC) Date: Mon, 11 Nov 2013 05:35:53 +0000 (UTC) From: Apache Jenkins Server  <jenkins@builds.apache.org> To: dev@provisionr.incubator.apache.org, null@apache.org Message-ID: <301905732.54.1384148154008.JavaMail.hudson@aegis> In-Reply-To: <636472627.497.1384070125369.JavaMail.hudson@aegis> References: <636472627.497.1384070125369.JavaMail.hudson@aegis> Subject: Build failed in Jenkins: provisionr-master #174 MIME-Version: 1.0 Content-Type: text/plain; charset=UTF-8 Content-Transfer-Encoding: 7bit X-Jenkins-Job: provisionr-master X-Jenkins-Result: FAILURE X-Virus-Checked: Checked by ClamAV on apache.org  See <https://builds.apache.org/job/provisionr-master/174/>  ------------------------------------------ [...truncated 3788 lines...] ------------------------------------------------------- Running org.apache.provisionr.karaf.CustomKarafDistributionTest SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/home/jenkins/jenkins-slave/maven-repositories/0/org/apache/karaf/org.apache.karaf.client/2.3.1/org.apache.karaf.client-2.3.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/home/jenkins/jenkins-slave/maven-repositories/0/org/slf4j/slf4j-simple/1.6.6/slf4j-simple-1.6.6.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/home/jenkins/jenkins-slave/maven-repositories/0/org/ops4j/pax/logging/pax-logging-api/1.7.0/pax-logging-api-1.7.0.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory] [org.ops4j.pax.url.mvn.internal.Connection] : Resolving [mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.junit/4.10_1] [org.ops4j.pax.url.mvn.internal.Connection] : Collecting versions from repository [file:/home/jenkins/jenkins-slave/maven-repositories/0/,releases=true,snapshots=true] [org.ops4j.pax.url.mvn.internal.Connection] :   Resolving exact version [org.ops4j.pax.url.mvn.internal.Connection] : Collecting versions from repository [http://osgi.sonatype.org/content/groups/pax-runner/,releases=true,snapshots=false] [org.ops4j.pax.url.mvn.internal.Connection] :   Resolving exact version [org.ops4j.pax.url.mvn.internal.Connection] : Collecting versions from repository [http://repo1.maven.org/maven2/,releases=true,snapshots=false] [org.ops4j.pax.url.mvn.internal.Connection] :   Resolving exact version [org.ops4j.pax.url.mvn.internal.Connection] : Collecting versions from repository [http://repository.ops4j.org/maven2/,releases=true,snapshots=false] [org.ops4j.pax.url.mvn.internal.Connection] :   Resolving exact version [org.ops4j.pax.url.mvn.internal.Connection] : Collecting versions from repository [http://repository.springsource.com/maven/bundles/release/,releases=true,snapshots=false] [org.ops4j.pax.url.mvn.internal.Connection] :   Resolving exact version [org.ops4j.pax.url.mvn.internal.Connection] : Collecting versions from repository [http://repository.springsource.com/maven/bundles/external/,releases=true,snapshots=false] [org.ops4j.pax.url.mvn.internal.Connection] :   Resolving exact version 2013-11-11 05:27:14,891 | INFO  | FelixStartLevel  | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/etc/org.apache.felix.fileinstall-deploy.cfg 2013-11-11 05:27:14,897 | INFO  | FelixStartLevel  | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/etc/org.apache.karaf.features.obr.cfg 2013-11-11 05:27:14,899 | INFO  | FelixStartLevel  | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/etc/org.apache.karaf.management.cfg 2013-11-11 05:27:14,956 | INFO  | FelixStartLevel  | BlueprintExtender                | rint.container.BlueprintExtender  138 | 7 - org.apache.aries.blueprint.core - 1.1.0 | No quiesce support is available, so blueprint components will not participate in quiesce operations 2013-11-11 05:27:15,064 | INFO  | rint Extender: 3 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.shell.console is waiting for namespace handlers [http://aries.apache.org/blueprint/xmlns/blueprint-ext/v1.0.0] 2013-11-11 05:27:15,120 | INFO  | FelixStartLevel  | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Starting JMX OSGi agent 2013-11-11 05:27:15,183 | INFO  | FelixStartLevel  | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering MBean with ObjectName [osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e] for service with service.id [10] 2013-11-11 05:27:15,190 | INFO  | rint Extender: 3 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.shell.packages is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,198 | INFO  | rint Extender: 3 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.deployer.kar is waiting for namespace handlers [http://aries.apache.org/blueprint/xmlns/blueprint-ext/v1.0.0] 2013-11-11 05:27:15,201 | INFO  | rint Extender: 3 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.diagnostic.command is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,216 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.shell.osgi is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,243 | INFO  | rint Extender: 2 | BlueprintContainerImpl           | container.BlueprintContainerImpl  344 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.diagnostic.management is waiting for dependencies [(objectClass=javax.management.MBeanServer)] 2013-11-11 05:27:15,243 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.features.command is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,306 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.shell.ssh is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,323 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.shell.commands is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,334 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  303 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.shell.log is waiting for namespace handlers [http://karaf.apache.org/xmlns/shell/v1.0.0] 2013-11-11 05:27:15,341 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  344 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.features.management is waiting for dependencies [(objectClass=org.apache.karaf.features.FeaturesService), (objectClass=javax.management.MBeanServer)] 2013-11-11 05:27:15,347 | INFO  | rint Extender: 1 | BlueprintContainerImpl           | container.BlueprintContainerImpl  344 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.karaf.admin.management is waiting for dependencies [(objectClass=javax.management.MBeanServer)] 2013-11-11 05:27:15,442 | INFO  | JMX OSGi Agent   | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering org.osgi.jmx.framework.FrameworkMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@84de3c with name osgi.core:type=framework,version=1.7,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e 2013-11-11 05:27:15,442 | INFO  | JMX OSGi Agent   | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering org.osgi.jmx.framework.PackageStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@84de3c with name osgi.core:type=packageState,version=1.5,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e 2013-11-11 05:27:15,443 | INFO  | JMX OSGi Agent   | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering org.osgi.jmx.framework.wiring.BundleWiringStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@84de3c with name osgi.core:type=wiringState,version=1.1,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e 2013-11-11 05:27:15,444 | INFO  | JMX OSGi Agent   | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering org.osgi.jmx.framework.BundleStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@84de3c with name osgi.core:type=bundleState,version=1.7,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e 2013-11-11 05:27:15,445 | INFO  | JMX OSGi Agent   | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering org.osgi.jmx.service.cm.ConfigurationAdminMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@84de3c with name osgi.compendium:service=cm,version=1.3,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e 2013-11-11 05:27:15,445 | INFO  | JMX OSGi Agent   | core                             | ?                                   ? | 27 - org.apache.aries.jmx.core - 1.1.1 | Registering org.osgi.jmx.framework.ServiceStateMBean to MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer@84de3c with name osgi.core:type=serviceState,version=1.7,framework=org.apache.felix.framework,uuid=95246204-baf0-4fa7-b1bd-292145faa88e     _                    _                              /_\  _ __   __ _  ___| |__   ___                   //_\\| '_ \ / _` |/ __| '_ \ / _ \             /  _  \ |_) | (_| | (__| | | |  __/                 \_/ \_/ .__/ \__,_|\___|_| |_|\___|                   |_|                                               ___                _     _                          / _ \_ __ _____   _(_)___(_) ___  _ __  _ __       / /_)/ '__/ _ \ \ / / / __| |/ _ \| '_ \| '__|  / ___/| | | (_) \ V /| \__ \ | (_) | | | | |      \/    |_|  \___/ \_/ |_|___/_|\___/|_| |_|_|       Apache Provisionr  (0.5.0-incubating-SNAPSHOT)   http://provisionr.incubator.apache.org/  Hit '<tab>' for a list of available commands and '[cmd] --help' for help on a specific command. Hit '<ctrl-d>' to shutdown the service.  Apache Provisionr [0.5.0-incubating-SNAPSHOT] $ 2013-11-11 05:27:15,936 | INFO  | rint Extender: 3 | SecurityUtils                    | e.sshd.common.util.SecurityUtils   80 | 29 - org.apache.sshd.core - 0.8.0 | BouncyCastle not registered, using the default JCE provider 2013-11-11 05:27:16,002 | INFO  | rint Extender: 1 | KarArtifactInstaller             | eployer.kar.KarArtifactInstaller   73 | 20 - org.apache.karaf.deployer.kar - 2.3.1 | Karaf archives will be extracted to /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/system 2013-11-11 05:27:16,002 | INFO  | rint Extender: 1 | KarArtifactInstaller             | eployer.kar.KarArtifactInstaller   74 | 20 - org.apache.karaf.deployer.kar - 2.3.1 | Timestamps for Karaf archives will be extracted to /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/system/.timestamps 2013-11-11 05:27:16,011 | INFO  | fbe535297/deploy | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/deploy/4c0af96e-fb35-4bd7-8872-1087daf670bb_4.10_1.jar 2013-11-11 05:27:16,039 | INFO  | fbe535297/deploy | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Started bundle: file:/home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/deploy/4c0af96e-fb35-4bd7-8872-1087daf670bb_4.10_1.jar 2013-11-11 05:27:18,017 | INFO  | c8cfbe535297/etc | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/etc/org.apache.felix.fileinstall-templates.cfg 2013-11-11 05:27:18,497 | INFO  | c8cfbe535297/etc | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/etc/org.apache.provisionr.cfg 2013-11-11 05:27:18,923 | INFO  | c8cfbe535297/etc | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/
rovisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/etc/org.apache.provisionr.amazon.cfg 2013-11-11 05:27:21,955 | INFO  | Thread-8         | Activator                        | x.web.service.internal.Activator  115 | 76 - org.ops4j.pax.web.pax-web-runtime - 1.1.12 | Pax Web started 2013-11-11 05:27:22,110 | INFO  | b Runtime worker | Server                           | org.eclipse.jetty.server.Server   266 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | jetty-7.6.8.v20121106 2013-11-11 05:27:22,146 | INFO  | b Runtime worker | AbstractConnector                | e.jetty.server.AbstractConnector  338 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | Started BlockingChannelConnector@0.0.0.0:8181 2013-11-11 05:27:22,154 | INFO  | b Runtime worker | HttpServiceFactoryImpl           | .internal.HttpServiceFactoryImpl   33 | 76 - org.ops4j.pax.web.pax-web-runtime - 1.1.12 | Binding bundle: [org.apache.karaf.webconsole.console [80]] to http service 2013-11-11 05:27:22,173 | INFO  | rint Extender: 3 | FeaturesPlugin                   | bconsole.features.FeaturesPlugin   79 | 82 - org.apache.karaf.webconsole.features - 2.3.1 | Features plugin activated 2013-11-11 05:27:22,294 | INFO  | rint Extender: 2 | BlueprintContainerImpl           | container.BlueprintContainerImpl  344 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.aries.transaction.blueprint is waiting for dependencies [(objectClass=javax.transaction.TransactionManager)] 2013-11-11 05:27:22,367 | INFO  | b Runtime worker | ContextHandler                   | ty.server.handler.ContextHandler  744 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | started HttpServiceContext{httpContext=org.apache.felix.webconsole.internal.servlet.OsgiManagerHttpContext@1aefd7b} 2013-11-11 05:27:22,613 | INFO  | rint Extender: 3 | BlueprintContainerImpl           | container.BlueprintContainerImpl  344 | 7 - org.apache.aries.blueprint.core - 1.1.0 | Bundle org.apache.provisionr.rundeck is waiting for dependencies [(objectClass=org.activiti.engine.ProcessEngine)] 2013-11-11 05:27:22,828 | INFO  | Thread-29        | Extender                         | org.activiti.osgi.Extender        325 | 98 - org.activiti.osgi - 5.10.0 | Found ScriptEngineFactory in groovy-all 2013-11-11 05:27:22,873 | INFO  | Thread-29        | Extender                         | org.activiti.osgi.Extender        325 | 98 - org.activiti.osgi - 5.10.0 | Found ScriptEngineFactory in org.activiti.engine 2013-11-11 05:27:24,353 | INFO  | rint Extender: 2 | DbSqlSession                     | viti.engine.impl.db.DbSqlSession  787 | 97 - org.activiti.engine - 5.10.0 | performing create on engine with resource org/activiti/db/create/activiti.h2.create.engine.sql 2013-11-11 05:27:24,421 | INFO  | rint Extender: 2 | DbSqlSession                     | viti.engine.impl.db.DbSqlSession  787 | 97 - org.activiti.engine - 5.10.0 | performing create on history with resource org/activiti/db/create/activiti.h2.create.history.sql 2013-11-11 05:27:24,442 | INFO  | rint Extender: 2 | DbSqlSession                     | viti.engine.impl.db.DbSqlSession  787 | 97 - org.activiti.engine - 5.10.0 | performing create on identity with resource org/activiti/db/create/activiti.h2.create.identity.sql 2013-11-11 05:27:24,485 | INFO  | rint Extender: 2 | ProcessEngineImpl                | ti.engine.impl.ProcessEngineImpl   82 | 97 - org.activiti.engine - 5.10.0 | ProcessEngine default created 2013-11-11 05:27:24,486 | INFO  | rint Extender: 2 | JobExecutor                      | ine.impl.jobexecutor.JobExecutor   62 | 97 - org.activiti.engine - 5.10.0 | Starting up the JobExecutor[org.activiti.engine.impl.jobexecutor.DefaultJobExecutor]. 2013-11-11 05:27:24,489 | INFO  | Thread-32        | AcquireJobsRunnable              | .jobexecutor.AcquireJobsRunnable   46 | 97 - org.activiti.engine - 5.10.0 | JobExecutor[org.activiti.engine.impl.jobexecutor.DefaultJobExecutor] starting to acquire jobs 2013-11-11 05:27:24,514 | INFO  | Thread-8         | BpmnDeployer                     | .impl.bpmn.deployer.BpmnDeployer   74 | 97 - org.activiti.engine - 5.10.0 | Processing resource OSGI-INF/activiti/amazonPoolManagement.bpmn20.xml 2013-11-11 05:27:24,520 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   47 | 98 - org.activiti.osgi - 5.10.0 | removed Activiti service from delegate cache null 2013-11-11 05:27:24,522 | INFO  | tall-./templates | PoolTemplateInstaller            | .templates.PoolTemplateInstaller   74 | 106 - org.apache.provisionr.core - 0.5.0.incubating-SNAPSHOT | Installing Pool template from  /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/templates/cdh4.xml 2013-11-11 05:27:24,560 | INFO  | tall-./templates | PoolTemplateInstaller            | .templates.PoolTemplateInstaller   82 | 106 - org.apache.provisionr.core - 0.5.0.incubating-SNAPSHOT | Registered new template with ID: cdh4 2013-11-11 05:27:24,561 | INFO  | tall-./templates | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/templates/cdh4.xml 2013-11-11 05:27:24,561 | INFO  | tall-./templates | PoolTemplateInstaller            | .templates.PoolTemplateInstaller   74 | 106 - org.apache.provisionr.core - 0.5.0.incubating-SNAPSHOT | Installing Pool template from  /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/templates/cdh3.xml 2013-11-11 05:27:24,574 | INFO  | tall-./templates | PoolTemplateInstaller            | .templates.PoolTemplateInstaller   82 | 106 - org.apache.provisionr.core - 0.5.0.incubating-SNAPSHOT | Registered new template with ID: cdh3 2013-11-11 05:27:24,574 | INFO  | tall-./templates | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/templates/cdh3.xml 2013-11-11 05:27:24,574 | INFO  | tall-./templates | PoolTemplateInstaller            | .templates.PoolTemplateInstaller   74 | 106 - org.apache.provisionr.core - 0.5.0.incubating-SNAPSHOT | Installing Pool template from  /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/templates/jenkins.xml 2013-11-11 05:27:24,586 | INFO  | tall-./templates | PoolTemplateInstaller            | .templates.PoolTemplateInstaller   82 | 106 - org.apache.provisionr.core - 0.5.0.incubating-SNAPSHOT | Registered new template with ID: jenkins 2013-11-11 05:27:24,587 | INFO  | tall-./templates | fileinstall                      | ?                                   ? | 6 - org.apache.felix.fileinstall - 3.2.6 | Installed /home/hudson/jenkins-slave/workspace/provisionr-master/karaf/assembly-tests/target/exam/376800f7-f33c-472c-9020-c8cfbe535297/templates/jenkins.xml 2013-11-11 05:27:24,610 | INFO  | Thread-8         | BpmnParse                        | ngine.impl.bpmn.parser.BpmnParse  279 | 97 - org.activiti.engine - 5.10.0 | XMLSchema currently not supported as typeLanguage 2013-11-11 05:27:24,610 | INFO  | Thread-8         | BpmnParse                        | ngine.impl.bpmn.parser.BpmnParse  285 | 97 - org.activiti.engine - 5.10.0 | XPath currently not supported as expressionLanguage 2013-11-11 05:27:25,026 | INFO  | rint Extender: 2 | SecurityUtils                    | tyUtils$BouncyCastleRegistration   69 | 102 - net.schmizz.sshj - 0.8.1 | BouncyCastle registration succeeded 2013-11-11 05:27:25,030 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache installRepositories 2013-11-11 05:27:25,030 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache installPackages 2013-11-11 05:27:25,031 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache checkSshPortIsOpen 2013-11-11 05:27:25,031 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache downloadFiles 2013-11-11 05:27:25,560 | INFO  | Thread-8         | BpmnDeployer                     | .impl.bpmn.deployer.BpmnDeployer   74 | 97 - org.activiti.engine - 5.10.0 | Processing resource OSGI-INF/activiti/amazonMachineSetup.bpmn20.xml 2013-11-11 05:27:25,592 | INFO  | Thread-8         | BpmnParse                        | ngine.impl.bpmn.parser.BpmnParse  279 | 97 - org.activiti.engine - 5.10.0 | XMLSchema currently not supported as typeLanguage 2013-11-11 05:27:25,592 | INFO  | Thread-8         | BpmnParse                        | ngine.impl.bpmn.parser.BpmnParse  285 | 97 - org.activiti.engine - 5.10.0 | XPath currently not supported as expressionLanguage 2013-11-11 05:27:25,717 | INFO  | Thread-8         | HttpServiceFactoryImpl           | .internal.HttpServiceFactoryImpl   33 | 76 - org.ops4j.pax.web.pax-web-runtime - 1.1.12 | Binding bundle: [org.apache.provisionr.rundeck [107]] to http service 2013-11-11 05:27:25,720 | INFO  | Thread-8         | ContextHandler                   | ty.server.handler.ContextHandler  744 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | started HttpServiceContext{httpContext=DefaultHttpContext{bundle=org.apache.provisionr.rundeck [107]}} 2013-11-11 05:27:25,727 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_checkSetupProcessesEnded 2013-11-11 05:27:25,729 | INFO  | Thread-8         | AdminPlugin                      | raf.webconsole.admin.AdminPlugin   61 | 81 - org.apache.karaf.webconsole.admin - 2.3.1 | Admin plugin activated 2013-11-11 05:27:25,729 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_setupAdminAccess 2013-11-11 05:27:25,730 | WARN  | Thread-8         | ServletTracker                   | .internal.tracker.ServletTracker   96 | 114 - org.ops4j.pax.web.pax-web-extender-whiteboard - 1.1.12 | Registered servlet [org.apache.karaf.webconsole.admin.AdminPlugin@7f7d58] did not contain a valid alias or url patterns property 2013-11-11 05:27:25,734 | INFO  | Thread-8         | GogoPlugin                       | karaf.webconsole.gogo.GogoPlugin   89 | 83 - org.apache.karaf.webconsole.gogo - 2.3.1 | Gogo plugin activated 2013-11-11 05:27:25,735 | WARN  | Thread-8         | ServletTracker                   | .internal.tracker.ServletTracker   96 | 114 - org.ops4j.pax.web.pax-web-extender-whiteboard - 1.1.12 | Registered servlet [org.apache.karaf.webconsole.gogo.GogoPlugin@12b65bf] did not contain a valid alias or url patterns property 2013-11-11 05:27:25,735 | WARN  | Thread-8         | ServletTracker                   | .internal.tracker.ServletTracker   96 | 114 - org.ops4j.pax.web.pax-web-extender-whiteboard - 1.1.12 | Registered servlet [org.apache.karaf.webconsole.features.FeaturesPlugin@121b1f2] did not contain a valid alias or url patterns property 2013-11-11 05:27:25,735 | WARN  | Thread-8         | ServletTracker                   | .internal.tracker.ServletTracker   96 | 114 - org.ops4j.pax.web.pax-web-extender-whiteboard - 1.1.12 | Registered servlet [org.apache.felix.webconsole.plugins.event.internal.PluginServlet@1887119] did not contain a valid alias or url patterns property 2013-11-11 05:27:25,739 | INFO  | Thread-8         | HttpServiceFactoryImpl           | .internal.HttpServiceFactoryImpl   33 | 76 - org.ops4j.pax.web.pax-web-runtime - 1.1.12 | Binding bundle: [org.apache.karaf.deployer.features [37]] to http service 2013-11-11 05:27:25,740 | INFO  | Thread-8         | ContextHandler                   | ty.server.handler.ContextHandler  744 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | started HttpServiceContext{httpContext=DefaultHttpContext{bundle=org.apache.karaf.deployer.features [37]}} 2013-11-11 05:27:25,765 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_checkNoRequestsAreOpen 2013-11-11 05:27:25,767 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_terminateInstances 2013-11-11 05:27:25,769 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_checkAllInstancesAreTerminated 2013-11-11 05:27:25,771 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_checkAllInstancesAreRunning 2013-11-11 05:27:25,775 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_runOnDemandInstances 2013-11-11 05:27:25,782 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_spawnMachineSetupProcesses 2013-11-11 05:27:25,784 | INFO  | rint Extender: 1 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_checkAllRequestsAreActive 2013-11-11 05:27:25,785 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_killMachineSetUpProcesses 2013-11-11 05:27:25,788 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_ensureKeyPairExists 2013-11-11 05:27:25,790 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_cancelSpotRequests 2013-11-11 05:27:25,791 | INFO  | rint Extender: 1 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_getInstanceIdsFromSpotRequests 2013-11-11 05:27:25,793 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_dumpConsoleOutput 2013-11-11 05:27:25,795 | INFO  | rint Extender: 1 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_ensureSecurityGroupExists 2013-11-11 05:27:25,796 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_runSpotInstances 2013-11-11 05:27:25,798 | INFO  | rint Extender: 1 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_publishListOfMachines 2013-11-11 05:27:25,799 | INFO  | rint Extender: 2 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_deleteSecurityGroup 2013-11-11 05:27:25,800 | INFO  | rint Exte
der: 1 | BlueprintELResolver              | gi.blueprint.BlueprintELResolver   39 | 98 - org.activiti.osgi - 5.10.0 | added Activiti service to delegate cache amazon_deleteKeyPair 2013-11-11 05:27:25,818 | INFO  | rint Extender: 3 | AmazonProvisionr                 | ovisionr.amazon.AmazonProvisionr   58 | 112 - org.apache.provisionr.amazon - 0.5.0.incubating-SNAPSHOT | Default provider for AmazonProvisionr is Provider{id='amazon', endpoint='', accessKey='access', options='{region=us-east-1}'} 2013-11-11 05:27:25,880 | INFO  | Thread-8         | container                        | er.impl.PersistenceBundleManager  269 | 124 - org.apache.aries.jpa.container - 1.0.0 | The file org.apache.aries.jpa.container.properties was not found in bundle org.apache.aries.jpa.container/1.0.0. The default properties {} will be used. 2013-11-11 05:27:25,886 | WARN  | rint Extender: 2 | aries                            | a.blueprint.aries.impl.NSHandler  288 | 123 - org.apache.aries.jpa.blueprint.aries - 1.0.1 | Managed persistence context support is no longer available for use with the Aries Blueprint container. 2013-11-11 05:27:25,888 | INFO  | Thread-8         | container                        | er.impl.PersistenceBundleManager  633 | 124 - org.apache.aries.jpa.container - 1.0.0 | No quiesce support is available, so managed persistence units will not participate in quiesce operations. 2013-11-11 05:27:25,897 | INFO  | Thread-8         | context                          | pl.JTAPersistenceContextRegistry  228 | 125 - org.apache.aries.jpa.container.context - 1.0.1 | A TransactionSynchronizationRegistry service is now available in the runtime. Managed persistence contexts will now integrate with JTA transactions using [javax.transaction.TransactionManager, javax.transaction.TransactionSynchronizationRegistry, javax.transaction.UserTransaction, org.apache.geronimo.transaction.manager.RecoverableTransactionManager, org.springframework.transaction.PlatformTransactionManager]. 2013-11-11 05:27:25,899 | INFO  | Thread-8         | context                          | xt.impl.GlobalPersistenceManager  253 | 125 - org.apache.aries.jpa.container.context - 1.0.1 | No quiesce support is available, so managed persistence units will not participate in quiesce operations. 2013-11-11 05:27:25,931 | INFO  | Thread-8         | ContextLoaderListener            | .activator.ContextLoaderListener  354 | 135 - org.springframework.osgi.extender - 1.2.1 | Starting [org.springframework.osgi.extender] bundle v.[1.2.1] 2013-11-11 05:27:26,099 | INFO  | Thread-8         | ExtenderConfiguration            | al.support.ExtenderConfiguration  150 | 135 - org.springframework.osgi.extender - 1.2.1 | No custom extender configuration detected; using defaults... 2013-11-11 05:27:26,107 | INFO  | Thread-8         | TimerTaskExecutor                | heduling.timer.TimerTaskExecutor  106 | 91 - org.springframework.context - 3.1.4.RELEASE | Initializing Timer 2013-11-11 05:27:26,318 | INFO  | Executor: 1      | WebXmlObserver                   | nder.war.internal.WebXmlObserver  120 | 118 - org.ops4j.pax.web.pax-web-extender-war - 1.1.12 | Using [activiti-explorer] as web application context name 2013-11-11 05:27:26,321 | INFO  | Executor: 1      | WebXmlObserver                   | nder.war.internal.WebXmlObserver  141 | 118 - org.ops4j.pax.web.pax-web-extender-war - 1.1.12 | Using [] as web application root path 2013-11-11 05:27:26,350 | INFO  | Executor: 1      | HttpServiceFactoryImpl           | .internal.HttpServiceFactoryImpl   33 | 76 - org.ops4j.pax.web.pax-web-runtime - 1.1.12 | Binding bundle: [org.apache.provisionr.activiti-karaf-web-explorer [146]] to http service 2013-11-11 05:27:26,359 | INFO  | Executor: 1      | ContextHandler                   | ty.server.handler.ContextHandler  744 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | started HttpServiceContext{httpContext=org.ops4j.pax.web.extender.war.internal.WebAppWebContainerContext@b8df14} 2013-11-11 05:27:26,363 | INFO  | Executor: 1      | ContextHandler                   | ty.server.handler.ContextHandler  795 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | stopped HttpServiceContext{httpContext=org.ops4j.pax.web.extender.war.internal.WebAppWebContainerContext@b8df14} 2013-11-11 05:27:26,409 | INFO  | Executor: 1      | /activiti-explorer               | r.handler.ContextHandler$Context 1931 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | Initializing Spring root WebApplicationContext 2013-11-11 05:27:26,410 | INFO  | Executor: 1      | ContextLoader                    | mework.web.context.ContextLoader  272 | 137 - org.springframework.web - 3.1.4.RELEASE | Root WebApplicationContext: initialization started 2013-11-11 05:27:26,466 | INFO  | Executor: 1      | giBundleXmlWebApplicationContext | pport.AbstractApplicationContext  503 | 91 - org.springframework.context - 3.1.4.RELEASE | Refreshing OsgiBundleXmlWebApplicationContext(bundle=org.apache.provisionr.activiti-karaf-web-explorer, config=/WEB-INF/applicationContext.xml): startup date [Mon Nov 11 05:27:26 UTC 2013]; root of context hierarchy 2013-11-11 05:27:26,467 | INFO  | Executor: 1      | giBundleXmlWebApplicationContext | ractOsgiBundleApplicationContext  365 | 91 - org.springframework.context - 3.1.4.RELEASE | Application Context service already unpublished 2013-11-11 05:27:26,520 | INFO  | Executor: 1      | XmlBeanDefinitionReader          | tory.xml.XmlBeanDefinitionReader  315 | 92 - org.springframework.beans - 3.1.4.RELEASE | Loading XML bean definitions from URL [bundle://146.0:0/WEB-INF/applicationContext.xml] 2013-11-11 05:27:26,808 | INFO  | Executor: 1      | XmlBeanDefinitionReader          | tory.xml.XmlBeanDefinitionReader  315 | 92 - org.springframework.beans - 3.1.4.RELEASE | Loading XML bean definitions from URL [bundle://146.0:0/WEB-INF/activiti-ui-context.xml] 2013-11-11 05:27:26,854 | INFO  | (3)-67.195.138.9 | ContainerTestRunner              | nit.internal.ContainerTestRunner   69 | 162 - org.ops4j.pax.exam.invoker.junit - 2.6.0 | running testAllFeaturesStartAsExpected in reactor 2013-11-11 05:27:26,927 | INFO  | Executor: 1      | PropertyPlaceholderConfigurer    | .support.PropertiesLoaderSupport  177 | 90 - org.springframework.core - 3.1.4.RELEASE | Loading properties file from OSGi resource[classpath:ui.properties|bnd.id=146|bnd.sym=org.apache.provisionr.activiti-karaf-web-explorer] 2013-11-11 05:27:26,938 | INFO  | Executor: 1      | DefaultListableBeanFactory       | pport.DefaultListableBeanFactory  577 | 92 - org.springframework.beans - 3.1.4.RELEASE | Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@12b644e: defining beans [processEngine,repositoryService,runtimeService,taskService,historyService,managementService,identityService,demoDataGenerator,activitiLoginHandler,activitiUiPropertyPlaceholder,navigatorManager,attachmentRendererManager,formPropertyRendererManager,variableRendererManager,componentFactories,userCache,navigationFragmentChangeListener,mainWindow,explorerApp,i18nManager,notificationManager,viewManager]; root of factory hierarchy 2013-11-11 05:27:26,945 | INFO  | (3)-67.195.138.9 | CustomKarafDistributionTest      | araf.CustomKarafDistributionTest  155 | 166 - PAXEXAM-PROBE-18570e90-9e7c-479e-bcfa-729baf2a5b47 - 0.0.0 | Unable to fetch http://localhost:8181/activiti-explorer/ (Server returned HTTP response code: 401 for URL: http://localhost:8181/activiti-explorer/). Trying again in 5s. 2013-11-11 05:27:26,986 | INFO  | Executor: 1      | OsgiServiceProxyFactoryBean      | al.aop.ServiceDynamicInterceptor  470 | 134 - org.springframework.osgi.core - 1.2.1 | Looking for mandatory OSGi service dependency for bean [processEngine] matching filter (objectClass=org.activiti.engine.ProcessEngine) 2013-11-11 05:27:26,987 | INFO  | Executor: 1      | OsgiServiceProxyFactoryBean      | al.aop.ServiceDynamicInterceptor  476 | 134 - org.springframework.osgi.core - 1.2.1 | Found mandatory OSGi service for bean [processEngine] 2013-11-11 05:27:27,104 | INFO  | Executor: 1      | giBundleXmlWebApplicationContext | ractOsgiBundleApplicationContext  327 | 91 - org.springframework.context - 3.1.4.RELEASE | Publishing application context as OSGi service with properties {org.springframework.context.service.name=org.apache.provisionr.activiti-karaf-web-explorer, Bundle-SymbolicName=org.apache.provisionr.activiti-karaf-web-explorer, Bundle-Version=0.5.0.incubating-SNAPSHOT} 2013-11-11 05:27:27,110 | INFO  | Executor: 1      | ContextLoader                    | mework.web.context.ContextLoader  312 | 137 - org.springframework.web - 3.1.4.RELEASE | Root WebApplicationContext: initialization completed in 700 ms 2013-11-11 05:27:27,110 | INFO  | Executor: 1      | ContextHandler                   | ty.server.handler.ContextHandler  744 | 59 - org.eclipse.jetty.util - 7.6.8.v20121106 | started HttpServiceContext{httpContext=org.ops4j.pax.web.extender.war.internal.WebAppWebContainerContext@b8df14} 2013-11-11 05:27:31,987 | INFO  | qtp23295306-69   | TrieBasedUserCache               | xplorer.cache.TrieBasedUserCache   73 | 146 - org.apache.provisionr.activiti-karaf-web-explorer - 0.5.0.incubating-SNAPSHOT | Caching users 0 to 25 2013-11-11 05:27:32,090 | INFO  | (3)-67.195.138.9 | TestBundleObserver               | tender.intern.TestBundleObserver  101 | 159 - org.ops4j.pax.exam.extender.service - 2.6.0 | Unregistered testcase [org.ops4j.pax.exam.raw.extender.intern.Probe@1f172aa.] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.199 sec  Results :  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  [JENKINS] Recording test results [INFO]  [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ provisionr-assembly-tests --- [INFO] Building jar: <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/target/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.jar> [INFO]  [INFO] --- maven-site-plugin:3.2:attach-descriptor (attach-descriptor) @ provisionr-assembly-tests --- [WARNING] Failed to getClass for org.apache.maven.plugin.source.SourceJarMojo [INFO]  [INFO] --- maven-source-plugin:2.2.1:jar (default) @ provisionr-assembly-tests --- [INFO] Building jar: <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/target/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT-sources.jar> [INFO]  [INFO] --- apache-rat-plugin:0.9:check (default) @ provisionr-assembly-tests --- [INFO] 51 implicit excludes (use -debug for more details). [INFO] Exclude: **/*.md [INFO] Exclude: NOTICE [INFO] Exclude: .git/** [INFO] Exclude: .repository/** [INFO] Exclude: .gitignore [INFO] Exclude: .idea/** [INFO] Exclude: **/*.iml [INFO] Exclude: **/*.project [INFO] Exclude: **/*.classpath [INFO] Exclude: **/*.prefs [INFO] Exclude: **/id_rsa_test [INFO] Exclude: **/id_rsa_test.pub [INFO] Exclude: **/*.bpmn20.xml [INFO] Exclude: **/*.activiti [INFO] Exclude: **/*.csv [INFO] Exclude: **/target/** [INFO] 3 resources included (use -debug for more details) [INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence. [INFO]                                                                          [INFO] ------------------------------------------------------------------------ [INFO] Building Apache Provisionr :: Aggregator 0.5.0-incubating-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO]  [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ provisionr-aggregator --- [INFO]  [INFO] --- maven-remote-resources-plugin:1.4:process (default) @ provisionr-aggregator --- [INFO]  [INFO] --- maven-site-plugin:3.2:attach-descriptor (attach-descriptor) @ provisionr-aggregator --- [WARNING] Failed to getClass for org.apache.maven.plugin.source.SourceJarMojo [INFO]  [INFO] --- maven-source-plugin:2.2.1:jar (default) @ provisionr-aggregator --- [INFO]  [INFO] --- apache-rat-plugin:0.9:check (default) @ provisionr-aggregator --- [INFO] 70 implicit excludes (use -debug for more details). [INFO] Exclude: **/*.md [INFO] Exclude: NOTICE [INFO] Exclude: .git/** [INFO] Exclude: .repository/** [INFO] Exclude: .gitignore [INFO] Exclude: .idea/** [INFO] Exclude: **/*.iml [INFO] Exclude: **/*.project [INFO] Exclude: **/*.classpath [INFO] Exclude: **/*.prefs [INFO] Exclude: **/id_rsa_test [INFO] Exclude: **/id_rsa_test.pub [INFO] Exclude: **/*.bpmn20.xml [INFO] Exclude: **/*.activiti [INFO] Exclude: **/*.csv [INFO] Exclude: **/target/** [INFO] 7 resources included (use -debug for more details) [INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence. [INFO]  [INFO] --- maven-install-plugin:2.3.1:install (default-install) @ provisionr-aggregator --- [INFO] Installing <https://builds.apache.org/job/provisionr-master/ws/pom.xml> to /home/jenkins/jenkins-slave/maven-repositories/0/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/provisionr-aggregator-0.5.0-incubating-SNAPSHOT.pom Build timed out (after 60 minutes). Marking the build as failed. channel stopped [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO]  Archiving artifacts ",https://mail-archives.apache.org/mod_mbox/provisionr-dev/201311.mbox/raw/%3C301905732.54.1384148154008.JavaMail.hudson%40aegis%3E,Apache Jenkins Server  <jenk...@builds.apache.org>,0,0
108,109,"I think I mentioned this on another similar thread and I think our use case
might be somewhat similar. We have a daily ETL that loads data in to a
database in one DAG, and then need to do a weekly rollup report every
Tuesday that is in another DAG. The first DAG has a final
TriggerDagRunOperator that decides if today is Tuesday or not, and if yes,
triggers the weekly rollup DAG that operates on the data in the database at
that moment - which, since it's Tuesday, is all the data we want. If that
sounds like what you're trying to do, your first DAG might have a
TriggerDagRunOperator that decides if today is the first of the month, and
then triggers some other DAG.

Laura

On Thu, Jun 30, 2016 at 12:09 PM, Jeremiah Lowin <jlowin@apache.org> wrote:

> Interesting -- this could be an extension of open enhancement AIRFLOW-100
> https://issues.apache.org/jira/browse/AIRFLOW-100. Let me see if I can
> restate this correctly:
>
> - You have a daily ETL job
> - You have a monthly reporting job, for arguments sake lets say it runs on
> the last day of each month with an execution date equal to the last day of
> the prior month (for example on 7/31/2016 the task with execution date
> 6/30/2016 will run).
> You want the monthly job with execution date 6/30/2016 to wait for (and
> include) the daily ETLs through 7/31/2016. In some months, that requires a
> 31 day delta, in others 30 (in others 28... and forget about leap years).
>
> It sounds like the simplest solution (and the one proposed in A-100) is to
> allow ExternalTaskSensor to accept not just a static delta, but potentially
> a callable that accepts the current execution date and returns the desired
> execution date for the sensed task. In this case, it would take in
> 6/30/2016 and return 7/31/2016 as the last day of the following month. I
> don't think any headway has been made on actually implementing the solution
> but it should be straightforward -- I will try to get to it if I have some
> time in the next few days.
>
>
> On Wed, Jun 29, 2016 at 11:25 AM Adrian Bridgett <adrian@opensignal.com>
> wrote:
>
> > I'm hitting a bit of an annoying problem and wondering about the best
> > course of action.
> >
> > We have several dags:
> > - a daily ETL job
> > - several reporting jobs (daily, weekly or monthly) which use the data
> > from previous ETL jobs
> >
> > I wish to have a dependency such that the reporting jobs depend upon the
> > last ETL job that the report uses.   We're happy to set depends_on_past
> > in the ETL job.
> >
> > Daily jobs are easy - ExternalTaskSensor, job done.
> > Weekly jobs are a little trickier - we need to work out the
> > execution_delta - normally +6 for us (we deliberately run a day late to
> > prioritise other jobs).
> > Monthly jobs.... this is where I'm struggling - how to work out the
> > execution_delta.   I guess the ideal would be an upgrade from timedelta
> > to dateutil.relativedelta?   tomorrow_ds and ds_add don't help either.
> >
> > I must admit, ds being the time that's just gone has caused me no end of
> > brain befudledness, especially when trying to get the initial job right
> > (so much so that I wrote this up in our DAG README, posting here for
> > others):
> >
> > When adding a new job, it's critical to ensure that you've set the
> > schedule correctly:
> > - frequency (monthly, weekly, daily)
> > - schedule_interval (""0 0 2 * *"", ""0 0 * * 0"", ""0 0 * * *"")
> > - start_date (choose a day that matches schedule_interval at least one
> > interval ago)
> > -- e.g if today is Thursday 2016-06-09, go back in time to when the
> > schedule will trigger,
> >     then work out what ""ds"" (execution date) would be (remembering
> > that's the lapsed date)
> > --- for a monthly job, last trigger=2016-06-02, ds=2016-05-02
> > --- for a weekly job, last trigger=2016-06-05, ds=2016-05-29
> > --- for a daily job, last trigger=2016-06-09, ds=2016-06-08
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CAJQxwiGGRuJ7O9s4D3ExF8SHtdyN9rDxkwVcbB7+CiNaV0YXpA@mail.gmail.com>,Laura Lorenz <llor...@industrydive.com>,0,0
214,215,"Wikipedia says there are 3 implementations:

1. Apache Daffodil

2. IBM has a DFDL processor included in their products

3. The European Space Agency has a DFDL processor

Is that list correct and complete?

/Roger

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201910.mbox/%3cBL0PR0901MB31241AC1D67B9252FEFBFA6FC8660@BL0PR0901MB3124.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
309,310,"Hi Leonidas,

Thank you for your help. The newest version fix my original problem.
The result is now delivered.

However from a non root user
I get a similar exception:
[marcos@namenode mrql]$ /home/hadoop/mrql-0.9.4/bin/mrql.spark -dist -nodes
1 /tmp/script_etienne.mrql
.
.
java.io.FileNotFoundException: File does not exist:
/user/marcos/tmp/hadoop_data_source_dir.txt

Where hadoop is my hadoop root user and marcos my regular user.
If I *ls* the tmp directory I get:
[marcos@namenode mrql]$ hadoop fs -ls /user/marcos/tmp/
Found 10 items
-rw-r--r--   2 marcos users        213 2015-01-08 09:51
/user/marcos/tmp/marcos_data_source_dir.txt

Regards,

Étienne

On 7 January 2015 at 22:03, Leonidas Fegaras <fegaras@cse.uta.edu> wrote:

>  Hi Etienne,
> Both problems were fixed after MRQL 0.9.2 was released. You can get the
> latest 0.9.4 src tarball using:
> git clone https://git-wip-us.apache.org/repos/asf/incubator-mrql.git
> mrql-0.9.4
> The EndpointWriter are Spark errors during Spark shutdown; you may simply
> ignore them.
> Best regards,
> Leonidas
>
>
>
> On 01/07/2015 06:53 AM, Etienne Dumoulin wrote:
>
>   Hi MRQL users,
>
>  I am using mrql 0.9.2 and spark 1.0.2.
>
>  I have a little issue with the spark mode.
>  When I try to execute a small test job that is successful in MapReduce
> mode I get an error.
>
>
> RMAT and pagerank examples throw a lot of warnings but I get the result at
> the end.
>
>  [hadoop@namenode ~]$
> /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1
> mrql-0.9.2-incubating-src/queries/RMAT.mrql 100 1000
> Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)
> Query type: ( int, int, int, int ) -> ( int, int )
> Query type: !bag(( int, int ))
> Physical plan:
> MapReduce:
>    input: Generator
> Run time: 6.165 secs
> 15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode2:40321]:
> Error [Shut down address: akka.tcp://sparkExecutor@datanode2:40321] [
> akka.remote.ShutDownAssociation: Shut down address:
> akka.tcp://sparkExecutor@datanode2:40321
> Caused by: akka.remote.transport.Transport$InvalidAssociationException:
> The remote system terminated the association because it is shutting down.
> ]
> 15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode3:39739]:
> Error [Shut down address: akka.tcp://sparkExecutor@datanode3:39739] [
> akka.remote.ShutDownAssociation: Shut down address:
> akka.tcp://sparkExecutor@datanode3:39739
> Caused by: akka.remote.transport.Transport$InvalidAssociationException:
> The remote system terminated the association because it is shutting down.
> ]
>
> /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1
> mrql-0.9.2-incubating-src/queries/pagerank.mrql
> Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)
> Query type: long
> Physical plan:
> Aggregate:
>    input: MapAggregateReduce:
>              input: Source (binary): ""tmp/graph.bin""
> Run time: 6.352 secs
> Query type: string
> Result:
> ""*** number of nodes: 76""
> Run time: 0.055 secs
> Query type: !list(< node: int, rank: double >)
> Physical plan:
> MapReduce:
>    input: Repeat (x_55):
>              init: MapReduce:
>                       input: Source (binary): ""tmp/graph.bin""
>              step: MapReduce:
>                       input: x_55
> Repeat #1: 67 true results
> 15/01/07 12:32:08 WARN scheduler.TaskSetManager: Lost TID 6 (task 5.0:0)
> 15/01/07 12:32:08 WARN scheduler.TaskSetManager: Loss was due to
> java.lang.Error
> java.lang.Error: Cannot up-coerce the numerical value null
>     at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)
>     at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)
>     at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from
> JavaSourceFromString:32)
>     at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)
>     at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)
>     at
> scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
>     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>     at
> scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
>     at
> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
>     at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>     at org.apache.spark.scheduler.Task.run(Task.scala:51)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> Repeat #2: 61 true results
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 9 (task 9.0:0)
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to
> java.lang.Error
> java.lang.Error: Cannot up-coerce the numerical value null
>     at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)
>     at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)
>     at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from
> JavaSourceFromString:32)
>     at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)
>     at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)
>     at
> scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
>     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>     at
> scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
>     at
> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
>     at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>     at org.apache.spark.scheduler.Task.run(Task.scala:51)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 10 (task 9.0:0)
> Repeat #3: 41 true results
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 13 (task 14.0:0)
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to
> java.lang.Error
> java.lang.Error: Cannot up-coerce the numerical value null
>     at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)
>     at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)
>     at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from
> JavaSourceFromString:32)
>     at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)
>     at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)
>     at
> scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
>     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>     at
> scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
>     at
> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
>     at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>     at org.apache.spark.scheduler.Task.run(Task.scala:51)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 14 (task 14.0:0)
> 15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 15 (task 14.0:0)
> Repeat #4: 6 true results
> Repeat #5: 0 true results
> Result:
> [ < node: 0, rank: 0.07872101046749681 >, < node: 6, rank:
> 0.05065660974066882 >, < node: 3, rank: 0.0459596116701186 >, < node: 25,
> rank: 0.04376273127207268 >, < node: 12, rank: 0.04188453600736429 >, <
> node: 1, rank: 0.04040564312366409 >, < node: 50, rank:
> 0.033875663833967604 >, < node: 31, rank: 0.023745510555659932 >, < node:
> 28, rank: 0.022415986790041594 >, < node: 9, rank: 0.021938982955425616 >,
> < node: 75, rank: 0.021586163633336066 >, < node: 18, rank:
> 0.02116068257012926 >, < node: 15, rank: 0.02026443639558245 >, < node: 62,
> rank: 0.017767555008348448 >, < node: 53, rank: 0.017410210069182946 >, <
> node: 7, rank: 0.017195867587074802 >, < node: 56, rank:
> 0.01699938245303271 >, < node: 10, rank: 0.01654553114240747 >, < node: 26,
> rank: 0.01650909559014266 >, < node: 37, rank: 0.016466833143024252 >, ... ]
> Run time: 5.379 secs
> 15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode2:32794]:
> Error [Shut down address: akka.tcp://sparkExecutor@datanode2:32794] [
> akka.remote.ShutDownAssociation: Shut down address:
> akka.tcp://sparkExecutor@datanode2:32794
> Caused by: akka.remote.transport.Transport$InvalidAssociationException:
> The remote system terminated the association because it is shutting down.
> ]
> 15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode3:35978]:
> Error [Shut down address: akka.tcp://sparkExecutor@datanode3:35978] [
> akka.remote.ShutDownAssociation: Shut down address:
> akka.tcp://sparkExecutor@datanode3:35978
> Caused by: akka.remote.transport.Transport$InvalidAssociationException:
> The remote system terminated the association because it is shutting down.
> ]
>
>
>  However when I run a simple select script (that works in MapReduce mode):
> ds =
> source(line,'/user/marcos/hdfs_file2/Text1Comma',',',type(<city:string,country:string,pop:string>));
> SELECT (t.city, t.country) FROM t in ds;
>
> /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark -dist -nodes 1
> /tmp/script_etienne.mrql
> Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)
> Query type: !bag(( string, string ))
> Physical plan:
> cMap:
>    input: Source (line): ""/user/marcos/hdfs_file2/Text1Comma""
> 15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)
> 15/01/07 12:32:48 WARN scheduler.TaskSetManager:
> *Loss was due to java.io.FileNotFoundException
> java.io.FileNotFoundException: File does not exist:
> /tmp/hadoop_data_source_dir.txt*
>     at
> org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)
>     at
> org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1834)
>     at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)
>     at
> org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)
>     at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)
>     at
> org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)
>     at
> org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)
>     at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
>     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
>     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>     at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>     at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>     at org.apache.spark.scheduler.Task.run(Task.scala:51)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> 15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:0)
> 15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 2 (task 0.0:0)
> 15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 3 (task 0.0:0)
> 15/01/07 12:32:48 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 4
> times; aborting job
> *** MRQL System Error at line 8: java.lang.Error:
> org.apache.spark.SparkException: Job aborted due to stage failure: Task
> 0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on
> host datanode3: java.io.FileNotFoundException:* File does not exist:
> /tmp/hadoop_data_source_dir.txt*
>
> org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)
>
> org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1834)
>         org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)
>
> org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)
>         org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)
>
> org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)
>
> org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)
>         org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
>         org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
>         org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
>         org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
>         org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
>         org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
>         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>         java.lang.Thread.run(Thread.java:662)
> Driver stacktrace:
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode2:47194]:
> Error [Shut down address: akka.tcp://sparkExecutor@datanode2:47194] [
> akka.remote.ShutDownAssociation: Shut down address:
> akka.tcp://sparkExecutor@datanode2:47194
> Caused by: akka.remote.transport.Transport$InvalidAssociationException:
> The remote system terminated the association because it is shutting down.
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode3:60893]:
> Error [Shut down address: akka.tcp://sparkExecutor@datanode3:60893] [
> akka.remote.ShutDownAssociation: Shut down address:
> akka.tcp://sparkExecutor@datanode3:60893
> Caused by: akka.remote.transport.Transport$InvalidAssociationException:
> The remote system terminated the association because it is shutting down.
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] -> [akka.tcp://spark@datanode2:56494]:
> Error [Association failed with [akka.tcp://spark@datanode2:56494]] [
> akka.remote.EndpointAssociationException: Association failed with
> [akka.tcp://spark@datanode2:56494]
> Caused by:
> akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:
> Connection refused: datanode2/192.168.11.90:56494
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] -> [akka.tcp://spark@datanode2:56494]:
> Error [Association failed with [akka.tcp://spark@datanode2:56494]] [
> akka.remote.EndpointAssociationException: Association failed with
> [akka.tcp://spark@datanode2:56494]
> Caused by:
> akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:
> Connection refused: datanode2/192.168.11.90:56494
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] -> [akka.tcp://spark@datanode2:56494]:
> Error [Association failed with [akka.tcp://spark@datanode2:56494]] [
> akka.remote.EndpointAssociationException: Association failed with
> [akka.tcp://spark@datanode2:56494]
> Caused by:
> akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:
> Connection refused: datanode2/192.168.11.90:56494
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] -> [akka.tcp://spark@datanode3:42480]:
> Error [Association failed with [akka.tcp://spark@datanode3:42480]] [
> akka.remote.EndpointAssociationException: Association failed with
> [akka.tcp://spark@datanode3:42480]
> Caused by:
> akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:
> Connection refused: datanode3/192.168.11.91:42480
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] -> [akka.tcp://spark@datanode3:42480]:
> Error [Association failed with [akka.tcp://spark@datanode3:42480]] [
> akka.remote.EndpointAssociationException: Association failed with
> [akka.tcp://spark@datanode3:42480]
> Caused by:
> akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:
> Connection refused: datanode3/192.168.11.91:42480
> ]
> 15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError
> [akka.tcp://spark@namenode:43495] -> [akka.tcp://spark@datanode3:42480]:
> Error [Association failed with [akka.tcp://spark@datanode3:42480]] [
> akka.remote.EndpointAssociationException: Association failed with
> [akka.tcp://spark@datanode3:42480]
> Caused by:
> akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:
> Connection refused: datanode3/192.168.11.91:42480
> ]
>
>
>  The spark installation seems fine as I can use the spark shell and
> Zeppelin is working.
>  There are no useful spark logs and I am not sure where to start the
> troubleshooting.
>
>  Thanks for your help,
>
> Étienne
>  --
>  Étienne Dumoulin
> Head of product development
>
> Idiro Technologies
> Clarendon House,
> 34-37 Clarendon St,
> Dublin 2
> Ireland
>
>
>


-- 
Étienne Dumoulin
Head of product development

Idiro Technologies
Clarendon House,
34-37 Clarendon St,
Dublin 2
Ireland
Email: etienne.dumoulin@idiro.com

",http://mail-archives.apache.org/mod_mbox/incubator-mrql-user/201501.mbox/%3cCAOkn+GiaUW5fvSG2r1pdm7NF-c0Y66nemjwWMy6hryDtxO_pqQ@mail.gmail.com%3e,Etienne Dumoulin <etienne.dumou...@idiro.com>,0,0
154,155,"Hi,

which OS and which version of Java are you using?

do you have a trace? If not, can you try adding the -debug or option to 
the gradle command? That may help diagnose the issue.

Thanks,

Matthieu

On 8/14/12 11:22 AM, Qun Huang wrote:
> Dear all,
>
> I'm a new user of S4.
> I checked out the latest version from the git and followed the
> instruction in the S4 website.
> However, when I type in the command
>      ""./gradlew install -DskipTests"",
> it stops at the point
>      "" Building > :s4-tools:compileJava > Resolving dependencies
> ':s4-tools:compile' ""
> and then cannot continue.
>
> Similarly, when I use the command
>      ""./gradlew eclipse"",
> it stops at
>      "" Building > :s4-tools:compileJava > Resolving dependencies
> ':s4-tools:compile' "".
>
> Can anybody help me?
>
> Thanks very much!
>


",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201208.mbox/%3c502A1A92.3090708@apache.org%3e,Matthieu Morel <mmo...@apache.org>,0,0
189,190,"Often, I find myself bang in the middle of a query, when BlurClientManager
comes up with this error. Happens both ways. When my app-server talks to
controller-server as well as controller-server talks to shard-server. This
is affecting search experience quite a bit nowadays in production!!

BlurException(message:Unknown error during remote call to node
[AAA.BB.CCC.DD:40020],
stackTraceStr:org.apache.blur.thrift.BadConnectionException: Could not
connect to controller/shard server. All connections are bad. at
org.apache.blur.thrift.BlurClientManager.execute(BlurClientManager.java:243)
at
org.apache.blur.thrift.BlurClientManager.execute(BlurClientManager.java:314)
at
org.apache.blur.thrift.BlurControllerServer$BlurClientRemote$1.call(BlurControllerServer.java:132)
at
org.apache.blur.thrift.BlurControllerServer$BlurClientRemote.execute(BlurControllerServer.java:139)

When do we get such an Exception? In-correct timeout settings or
shard-server restarts etc...

Any help is much appreciated

--
Ravi

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201612.mbox/%3cCAGW2whSO8KLCUCxR0un66CSNnvCgrzBkEiNYsF6w41CsQ4LmRg@mail.gmail.com%3e,Ravikumar Govindarajan <ravikumar.govindara...@gmail.com>,0,0
196,197,"Hi
Thank you. I will check out the twitter example then.

Thankyou

",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201208.mbox/%3cCABmtsnkG1Jo2Zvub2wqk1bbLKd=NYA-Bu_Lx6O6qQghT+Zh_PA@mail.gmail.com%3e,Raghavendar TS <raghav280...@gmail.com>,0,0
94,95,"Martin,

have you already committed that change? As I pointed out in another
thread, I've submitted a patch that will likely conflict with yours if
commited.


Ciao
Cosma



2006/7/17, Martin Marinschek <martin.marinschek@gmail.com>:
> As a workaround for the form problem, I've made the form searching
> routing of MyFaces aware of the Trinidad form family - so the trinidad
> form should be found as well.
>
> I'd be glad about any test results!
>
> regards,
>
> Martin
>
>
>
> On 5/26/06, Cosma Colanicchia <cosmacol@gmail.com> wrote:
> > Thomas, I'm using the MyFaces RI. I think that the <t:commandLink>
> > problem is related to *not* using the MyFaces renderer for the
> > <h:form> component (like in TOMAHAWK-416).
> >
> >
> >
> > 2006/5/25, Thomas Spiegl <thomas.spiegl@gmail.com>:
> > > Cosma, seems like you are using the Sun RI as JSF implementation.
> > >
> > > see http://issues.apache.org/jira/browse/TOMAHAWK-416
> > >
> > >
> > > On 5/25/06, Martin Marinschek <martin.marinschek@gmail.com> wrote:
> > > > So it is a must that a renderer for a certain component family needs
> > > > to be able to render out all components belonging to this family? I
> > > > don't see why you couldn't just use a different renderer-type for the
> > > > UIXForm component...
> > > >
> > > > regards,
> > > >
> > > > Martin
> > > >
> > > > On 5/24/06, Adam Winer <awiner@gmail.com> wrote:
> > > > > On 5/24/06, Martin Marinschek <martin.marinschek@gmail.com>
wrote:
> > > > > > Hi Adam,
> > > > > >
> > > > > > I don't remember a thread talking about component families,
we only
> > > > > > talked about extending from UIForm - so UIXForm does not even
have the
> > > > > > same component family as UIForm?
> > > > >
> > > > > No, it doesn't, which is more-or-less implied by not extending UIForm.
> > > > > In general, component family should follow subclassing
> > > > > as well, because otherwise you'd get ClassCastExceptions in
> > > > > Renderers.
> > > > >
> > > > > -- Adam
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > > regards,
> > > > > >
> > > > > > Martin
> > > > > >
> > > > > > On 5/24/06, Adam Winer <awiner@gmail.com> wrote:
> > > > > > > Actually, probably not...  UIXForm is in a different component
> > > > > > > family from UIForm.  (An earlier thread talked about that.)
> > > > > > >
> > > > > > > -- Adam
> > > > > > >
> > > > > > >
> > > > > > > On 5/24/06, Martin Marinschek <martin.marinschek@gmail.com>
wrote:
> > > > > > > > Problem 1) should have been fixed by Mario's recent
addition of
> > > > > > > > searching for the component family instead of the
instance of UIForm.
> > > > > > > >
> > > > > > > > regards,
> > > > > > > >
> > > > > > > > Martin
> > > > > > > >
> > > > > > > > On 5/24/06, Cosma Colanicchia <cosmacol@gmail.com>
wrote:
> > > > > > > > > I've done some other tries:
> > > > > > > > >
> > > > > > > > > 1) When using the <af:form> component,
a dummy form and its clear
> > > > > > > > > method are created. Clicking on <t:commandLink>
or
> > > > > > > > > <t:commandSortHeader> does trigger a submit,
but the expected
> > > > > > > > > behaviour isn't executed. Probably this is because
the _link_hidden_
> > > > > > > > > value is set telling JSF that a ""linkDummyForm:commandLinkId""
has been
> > > > > > > > > triggered, but the actual name of the component
is different (its name
> > > > > > > > > contaner has a different id).
> > > > > > > > >
> > > > > > > > > 2) When using an <h:form> component, the
dummy form isn't created
> > > > > > > > > (seems that the <t:commandLink> recognize
its parent form), but no
> > > > > > > > > clear_ method is generated in the page, so the
onclick javascript
> > > > > > > > > fails to submit the form.
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > Cosma
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > 2006/5/23, Cosma Colanicchia <cosmacol@gmail.com>:
> > > > > > > > > > It would be useful to share info about making
they work togheter, if
> > > > > > > > > > someone has resolved these problems.
> > > > > > > > > >
> > > > > > > > > > Anyway I'm using the latest snapshots..
and I tried with both <h:form>
> > > > > > > > > > and <af:form>. I had some issues with
dummyForm at first, if I can get
> > > > > > > > > > some detail I'll try to post it tomorrow,
now I'm leaving the office.
> > > > > > > > > >
> > > > > > > > > > Bye
> > > > > > > > > > Cosma
> > > > > > > > > >
> > > > > > > > > >
> > > > > > > > > > 2006/5/23, Matthias Wessendorf <matzew@apache.org>:
> > > > > > > > > > > Hi-
> > > > > > > > > > >
> > > > > > > > > > > time by time there are people on the
list, using both.
> > > > > > > > > > > MyFaces (incl. Tomahawk) and ADF Faces
> > > > > > > > > > >
> > > > > > > > > > >
> > > > > > > > > > > > > > 1) <t:commandLink>
components (and <t:commandSortHeader>s) don't work,
> > > > > > > > > > > > > > because the MyFaces
javascript functions such as clear_myFormId don't get
> > > > > > > > > > > > > > generated. Also the
_link_hidden_form isn't generated anymore. The rendered
> > > > > > > > > > > > > > HTML includes the string
<!-- MYFACES JAVASCRIPT -->, and looking at the
> > > > > > > > > > > > > > stack I can see that
both the adfFaces and the ExtensionsFilter are invoked,
> > > > > > > > > > > > > > but no MyFaces script
are actually rendered in the page.
> > > > > > > > > > > > >
> > > > > > > > > > > > > I'm not sure why this would
happen, but this may be a MyFaces
> > > > > > > > > > > > > bug;  I know there's been
some discussion on the main MyFaces
> > > > > > > > > > > > > list about issues like this.
> > > > > > > > > > > >
> > > > > > > > > > > > I'll try to search more carefully
in the archives.. do you know if
> > > > > > > > > > > > someone had success using MyFaces,
Tomahawk and ADF Faces in a
> > > > > > > > > > > > project?
> > > > > > > > > > > >
> > > > > > > > > > >
> > > > > > > > > > > Which version (of MyFaces) are you
using?
> > > > > > > > > > > Can you wrapp <h:form> around
commandLink?
> > > > > > > > > > >
> > > > > > > > > > > There where some issue - discussed
on myfaces dev - regarding ""auto
> > > > > > > > > > > generated form"" aka dummy form
> > > > > > > > > > >
> > > > > > > > > > > -Matthias
> > > > > > > > > > >
> > > > > > > > > >
> > > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > > --
> > > > > > > >
> > > > > > > > http://www.irian.at
> > > > > > > >
> > > > > > > > Your JSF powerhouse -
> > > > > > > > JSF Consulting, Development and
> > > > > > > > Courses in English and German
> > > > > > > >
> > > > > > > > Professional Support for Apache MyFaces
> > > > > > > >
> > > > > > >
> > > > > >
> > > > > >
> > > > > > --
> > > > > >
> > > > > > http://www.irian.at
> > > > > >
> > > > > > Your JSF powerhouse -
> > > > > > JSF Consulting, Development and
> > > > > > Courses in English and German
> > > > > >
> > > > > > Professional Support for Apache MyFaces
> > > > > >
> > > > >
> > > >
> > > >
> > > > --
> > > >
> > > > http://www.irian.at
> > > >
> > > > Your JSF powerhouse -
> > > > JSF Consulting, Development and
> > > > Courses in English and German
> > > >
> > > > Professional Support for Apache MyFaces
> > > >
> > >
> > >
> > > --
> > > http://www.irian.at
> > >
> > > Your JSF powerhouse -
> > > JSF Consulting, Development and
> > > Courses in English and German
> > >
> > > Professional Support for Apache MyFaces
> > >
> >
>
>
> --
>
> http://www.irian.at
>
> Your JSF powerhouse -
> JSF Consulting, Development and
> Courses in English and German
>
> Professional Support for Apache MyFaces
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200607.mbox/%3c467251f60607162353n72ebff7k516d1e0401c2704e@mail.gmail.com%3e,"""Cosma Colanicchia"" <cosma...@gmail.com>",0,0
67,68,"Thomas, I'm using the MyFaces RI. I think that the <t:commandLink>
problem is related to *not* using the MyFaces renderer for the
<h:form> component (like in TOMAHAWK-416).



2006/5/25, Thomas Spiegl <thomas.spiegl@gmail.com>:
> Cosma, seems like you are using the Sun RI as JSF implementation.
>
> see http://issues.apache.org/jira/browse/TOMAHAWK-416
>
>
> On 5/25/06, Martin Marinschek <martin.marinschek@gmail.com> wrote:
> > So it is a must that a renderer for a certain component family needs
> > to be able to render out all components belonging to this family? I
> > don't see why you couldn't just use a different renderer-type for the
> > UIXForm component...
> >
> > regards,
> >
> > Martin
> >
> > On 5/24/06, Adam Winer <awiner@gmail.com> wrote:
> > > On 5/24/06, Martin Marinschek <martin.marinschek@gmail.com> wrote:
> > > > Hi Adam,
> > > >
> > > > I don't remember a thread talking about component families, we only
> > > > talked about extending from UIForm - so UIXForm does not even have the
> > > > same component family as UIForm?
> > >
> > > No, it doesn't, which is more-or-less implied by not extending UIForm.
> > > In general, component family should follow subclassing
> > > as well, because otherwise you'd get ClassCastExceptions in
> > > Renderers.
> > >
> > > -- Adam
> > >
> > >
> > >
> > >
> > > > regards,
> > > >
> > > > Martin
> > > >
> > > > On 5/24/06, Adam Winer <awiner@gmail.com> wrote:
> > > > > Actually, probably not...  UIXForm is in a different component
> > > > > family from UIForm.  (An earlier thread talked about that.)
> > > > >
> > > > > -- Adam
> > > > >
> > > > >
> > > > > On 5/24/06, Martin Marinschek <martin.marinschek@gmail.com>
wrote:
> > > > > > Problem 1) should have been fixed by Mario's recent addition
of
> > > > > > searching for the component family instead of the instance of
UIForm.
> > > > > >
> > > > > > regards,
> > > > > >
> > > > > > Martin
> > > > > >
> > > > > > On 5/24/06, Cosma Colanicchia <cosmacol@gmail.com> wrote:
> > > > > > > I've done some other tries:
> > > > > > >
> > > > > > > 1) When using the <af:form> component, a dummy form
and its clear
> > > > > > > method are created. Clicking on <t:commandLink> or
> > > > > > > <t:commandSortHeader> does trigger a submit, but
the expected
> > > > > > > behaviour isn't executed. Probably this is because the
_link_hidden_
> > > > > > > value is set telling JSF that a ""linkDummyForm:commandLinkId""
has been
> > > > > > > triggered, but the actual name of the component is different
(its name
> > > > > > > contaner has a different id).
> > > > > > >
> > > > > > > 2) When using an <h:form> component, the dummy form
isn't created
> > > > > > > (seems that the <t:commandLink> recognize its parent
form), but no
> > > > > > > clear_ method is generated in the page, so the onclick
javascript
> > > > > > > fails to submit the form.
> > > > > > >
> > > > > > >
> > > > > > > Cosma
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > 2006/5/23, Cosma Colanicchia <cosmacol@gmail.com>:
> > > > > > > > It would be useful to share info about making they
work togheter, if
> > > > > > > > someone has resolved these problems.
> > > > > > > >
> > > > > > > > Anyway I'm using the latest snapshots.. and I tried
with both <h:form>
> > > > > > > > and <af:form>. I had some issues with dummyForm
at first, if I can get
> > > > > > > > some detail I'll try to post it tomorrow, now I'm
leaving the office.
> > > > > > > >
> > > > > > > > Bye
> > > > > > > > Cosma
> > > > > > > >
> > > > > > > >
> > > > > > > > 2006/5/23, Matthias Wessendorf <matzew@apache.org>:
> > > > > > > > > Hi-
> > > > > > > > >
> > > > > > > > > time by time there are people on the list, using
both.
> > > > > > > > > MyFaces (incl. Tomahawk) and ADF Faces
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > > > > 1) <t:commandLink> components
(and <t:commandSortHeader>s) don't work,
> > > > > > > > > > > > because the MyFaces javascript
functions such as clear_myFormId don't get
> > > > > > > > > > > > generated. Also the _link_hidden_form
isn't generated anymore. The rendered
> > > > > > > > > > > > HTML includes the string <!--
MYFACES JAVASCRIPT -->, and looking at the
> > > > > > > > > > > > stack I can see that both the
adfFaces and the ExtensionsFilter are invoked,
> > > > > > > > > > > > but no MyFaces script are actually
rendered in the page.
> > > > > > > > > > >
> > > > > > > > > > > I'm not sure why this would happen,
but this may be a MyFaces
> > > > > > > > > > > bug;  I know there's been some discussion
on the main MyFaces
> > > > > > > > > > > list about issues like this.
> > > > > > > > > >
> > > > > > > > > > I'll try to search more carefully in the
archives.. do you know if
> > > > > > > > > > someone had success using MyFaces, Tomahawk
and ADF Faces in a
> > > > > > > > > > project?
> > > > > > > > > >
> > > > > > > > >
> > > > > > > > > Which version (of MyFaces) are you using?
> > > > > > > > > Can you wrapp <h:form> around commandLink?
> > > > > > > > >
> > > > > > > > > There where some issue - discussed on myfaces
dev - regarding ""auto
> > > > > > > > > generated form"" aka dummy form
> > > > > > > > >
> > > > > > > > > -Matthias
> > > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > >
> > > > > >
> > > > > > --
> > > > > >
> > > > > > http://www.irian.at
> > > > > >
> > > > > > Your JSF powerhouse -
> > > > > > JSF Consulting, Development and
> > > > > > Courses in English and German
> > > > > >
> > > > > > Professional Support for Apache MyFaces
> > > > > >
> > > > >
> > > >
> > > >
> > > > --
> > > >
> > > > http://www.irian.at
> > > >
> > > > Your JSF powerhouse -
> > > > JSF Consulting, Development and
> > > > Courses in English and German
> > > >
> > > > Professional Support for Apache MyFaces
> > > >
> > >
> >
> >
> > --
> >
> > http://www.irian.at
> >
> > Your JSF powerhouse -
> > JSF Consulting, Development and
> > Courses in English and German
> >
> > Professional Support for Apache MyFaces
> >
>
>
> --
> http://www.irian.at
>
> Your JSF powerhouse -
> JSF Consulting, Development and
> Courses in English and German
>
> Professional Support for Apache MyFaces
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200605.mbox/%3c467251f60605252350lf829f76q26a72c2252e943b1@mail.gmail.com%3e,"""Cosma Colanicchia"" <cosma...@gmail.com>",0,0
201,202,"Hi again and sorry for asking so frequently,

can you tell me, what would be the steps to provide trinidad-skinning 
for a custom component?
which files would I have to touch (xss stylesheets, SkinFactory, ...)? 
Is there a quick way to achieve this?

thanks in advance!
-clem

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200610.mbox/%3c452D5D86.6040203@gmail.com%3e,Clemens Schneider <clemens.schnei...@gmail.com>,0,0
239,240,"
    [ https://issues.apache.org/jira/browse/KAFKA-163?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13130723#comment-13130723
] 

Jun Rao commented on KAFKA-163:
-------------------------------

This seems to be a patch for 158, not 163.
                
> Ruby client needs to support new compression byte
> -------------------------------------------------
>
>                 Key: KAFKA-163
>                 URL: https://issues.apache.org/jira/browse/KAFKA-163
>             Project: Kafka
>          Issue Type: New Feature
>          Components: clients
>            Reporter: AaronR
>            Priority: Minor
>         Attachments: KAFKA-158.patch
>
>
> Ruby client updates

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa
For more information on JIRA, see: http://www.atlassian.com/software/jira

        

",http://mail-archives.apache.org/mod_mbox/kafka-dev/201110.mbox/<341220882.10932.1319040790608.JavaMail.tomcat@hel.zones.apache.org>,"""Jun Rao (Commented) (JIRA)"" <j...@apache.org>",0,0
56,57,"Oh look who it is! It just so happens I was Googling around this morning 
trying to find out whether Xdebug locks profiling files in append mode 
as I vaguely remember reading somewhere it didn't.

But that's a question for another list (or tracker). :)


On 05/06/11 18:09, Derick Rethans wrote:
> On Fri, 6 May 2011, Benjamin Eberlei wrote:
>
>> Not possible right now, this is why the patch in ZETACOMP-79 exists. Derick is
>> currently reviewing it.
>
> I just did! :)
>
> Derick
>

",http://mail-archives.apache.org/mod_mbox/incubator-zeta-users/201105.mbox/%3c4DC41EBA.2020203@overtops.org%3e,Tolan Blundell <to...@overtops.org>,0,0
45,46,"Hey I just published an article about the ""Data Engineer"" role in modern
organizations and thought it could be of interest to this community.

https://medium.com/@maximebeauchemin/the-rise-of-the-data-engineer-91be18f1e603#.5rkm4htnf

Max

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CAHEEp7X1Ewy7XOb0kwend8DBZEgj3UwhYq0Hwq=m_44Es3k3sg@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
246,247,"Hi

When I think about how to get help with Airflow, I feel we are missing
something in the middle.  Here's the order I usually go through:

1. Read the documentation and/or source code.
2. Search gitter, possibly ask a question there.
3. Search the dev mailing list, possibly ask a question there.
4. File a Jira ticket.

Before the migration to Apache Incubator, we used a google group which was
great for discussions spanning a long time, and for archiving knowledge.
Gitter is really not good at this.  And the dev mailing list is fine but
it's not a great UI, there's no search, etc.

I think we need something between steps 2 and 3 that's focused on users.
As an example, Pandas encourages posting questions on stackoverflow.  As a
result there's a huge body of curated knowledge there.

I'm not sure what the best solution is.  I'm not even sure that others see
this as a problem that needs to be addressed.  I'm curious what other
people think.

thanks,
Dennis

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<CAPUwX3Ox44QeAj+QPUjWmEMwdm2m4XoQZT3X0TrwDPX==4XzoA@mail.gmail.com>,"""Dennis O'Brien"" <den...@dennisobrien.net>",0,1
221,222,"Hello DFDL community,

If my input file is this:

MZ

Then I want the output XML to be this:

<input>
    <Magic-Number>MZ</Magic-Number>
   <Executable>Windows executable</Executable>
</input>

If my input file is this:

PNG

Then I want the output XML to be this:

<input>
    <Magic-Number>PNG</Magic-Number>
   <Image>PNG image</Image>
</input>

If my input file is this:

PK

Then I want the output XML to be this:

<input>
    <Magic-Number>PK</Magic-Number>
   <Archive>Zip archive</Archive>
</input>

If the input is none of those, then I want the output XML to be this:

<input>
    <Magic-Number>...</Magic-Number>
    <Unrecognized>Unrecognized magic number</Unrecognized>
</input>

Below is my DFDL schema. It produces this error:

element reference {}Executable cannot have the dfdl:inputValueCalc property.

Why am I getting that error?  /Roger

<xs:element name=""input"">
    <xs:complexType>
        <xs:sequence>
            <xs:element name=""Magic-Number"" type=""xs:string"" />
            <xs:choice>
                <xs:choice dfdl:choiceDispatchKey=""{./Magic-Number}"">
                    <xs:element ref=""Executable"" dfdl:choiceBranchKey=""MZ"" />
                    <xs:element ref=""Image"" dfdl:choiceBranchKey=""PNG"" />
                    <xs:element ref=""Archive"" dfdl:choiceBranchKey=""PK"" />
                </xs:choice>
            </xs:choice>
            <xs:element name=""Unrecognized"" type=""xs:string"" dfdl:inputValueCalc=""{'Unrecognized
magic number'}"" />
        </xs:sequence>
    </xs:complexType>
</xs:element>

<xs:element name=""Executable"" type=""xs:string"" dfdl:inputValueCalc=""{'Windows executable'}""
/>
<xs:element name=""Image"" type=""xs:string"" dfdl:inputValueCalc=""{'PNG image'}"" />
<xs:element name=""Archive"" type=""xs:string"" dfdl:inputValueCalc=""{'Zip archive'}"" />


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201909.mbox/%3cBL0PR0901MB31249A2BDFB7A23F3113C489C8840@BL0PR0901MB3124.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
267,268,"Hi  team,

We are using airflow v1.7.1.3 and schedule about 50 dags (each dags is
about 10 to one hour intervals). It's with LocalExecutor.

Recently, we noticed the RDS (MySQL 5.6.x with AWS) runs with ~100% CPU.
I am wondering if airflow scheduler and webserver can cause high CPU load
of MySQL, given ~50 dags?
I feel MySQL should be light load..

Thanks.
-Jason

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CAPdFGJuN4pBOpp_SCBnP59CQ97+D0zjoBwVYEHe4WKHJMj67Og@mail.gmail.com>,Jason Chen <chingchien.c...@gmail.com>,0,0
310,311,"Thanks for the follow up Chris.
It used to work for me with catchup=False in a month-old version of
Airflow. That's why I mentioned it as a regression.

Tried today catchup=True with @once seems actually tries to ""catchup"" which
does not make sense for @once schedule,
notice there is one active run and one pending/""scheduled"":
       [image: Inline image 1]

So we can't really use @once with catchup=True and it's not a workaround
for this problem.

Thanks.



-- 
Ruslan Dautkhanov

On Sat, May 6, 2017 at 10:47 AM, Chris Fei <cfei18@gmail.com> wrote:

> I wonder if your issue is the same root cause as AIRFLOW-1013[1] (which
> you seem to have reported) and AIRFLOW-1055[2]. I haven't tried it
> myself, but that second ticket seems to indicate that a workaround
> could be setting catchup = True on your DAG. Not sure if that's an
> option for you.
> On Sat, May 6, 2017, at 12:29 PM, Ruslan Dautkhanov wrote:
> > I've upgraded Airflow to today's master branch.
> >
> > Got following regression in attempt to start a DAG:
> >
> > Process DagFileProcessor209-Process:
> >> Traceback (most recent call last):
> >> File
> >> ""/opt/cloudera/parcels/Anaconda/lib/python2.7/multiprocessing/proce-
> >> ss.py"",>> line 258, in _bootstrap
> >>   self.run()
> >> File
> >> ""/opt/cloudera/parcels/Anaconda/lib/python2.7/multiprocessing/proce-
> >> ss.py"",>> line 114, in run
> >>   self._target(*self._args, **self._kwargs)
> >> File ""/opt/airflow/airflow-
> >> 20170506/src/airflow/airflow/jobs.py"", line>> 346, in helper
> >>   pickle_dags)
> >> File ""/opt/airflow/airflow-20170506/src/airflow/airflow/utils/db.py"",>>
> line 48, in wrapper
> >>   result = func(*args, **kwargs)
> >> File ""/opt/airflow/airflow-
> >> 20170506/src/airflow/airflow/jobs.py"", line>> 1584, in process_file
> >>   self._process_dags(dagbag, dags, ti_keys_to_schedule)
> >> File ""/opt/airflow/airflow-
> >> 20170506/src/airflow/airflow/jobs.py"", line>> 1173, in _process_dags
> >>   dag_run = self.create_dag_run(dag)
> >> File ""/opt/airflow/airflow-20170506/src/airflow/airflow/utils/db.py"",>>
> line 48, in wrapper
> >>   result = func(*args, **kwargs)
> >> File ""/opt/airflow/airflow-
> >> 20170506/src/airflow/airflow/jobs.py"", line>> 776, in create_dag_run
> >>   if next_start <= now:
> >> TypeError: can't compare datetime.datetime to NoneType
> >
> >
> >
> > DAG definition:
> >
> > main_dag = DAG(
> >>   dag_id                         = 'DISCOVER-Oracle-Load-Mar2017-v1',>>
>  default_args                   = default_args,                  #
> >>   dafeult operators' arguments - see above>>   user_defined_macros
>       = dag_macros,       # I do not get
> >>   different between>>   ## params                         =
> dag_macros,       #
> >>   ## user_defined_macros and params>>   #
> >>   start_date                     = datetime.now(),                #
> >>   or e.g. datetime(2015, 6, 1)>>   # 'end_date'                   =
> datetime(2016, 1, 1),
> >>   catchup                        = False,                         #
> >>   Perform scheduler catchup (or only run latest)?>>
>                                                    # -
> defaults to True>>   schedule_interval              = '@once',
>            #
> >>   '@once'=None?>>
>                #
> doesn't create multiple dag runs automatically>>   concurrency
>         = 3,                             #
> >>   task instances allowed to run concurrently>>   max_active_runs
>         = 1,                             #
> >>   only one DAG run at a time>>   dagrun_timeout                 =
> timedelta(days=4),             #
> >>   no way this dag should ran for 4 days>>   orientation
>     = 'TB',                          #
> >>   default graph view>> )
> >
> >
> > default_args:
> >
> > default_args = {
> >>   # Security:
> >>   'owner'                        : 'rdautkha',                    #
> >>   owner of the task, using the unix username is recommended>>   #
> 'run_as_user'                : None                           #
> >>   # unix username to impersonate while running the task>>   #
> Scheduling:
> >>   'start_date'                   : None,                          #
> >>   don't confuse with DAG's start_date>>   'depends_on_past'
>   : False,                         #
> >>   True makes sense... but there are bugs around that code>>
>  'wait_for_downstream'          : False,                         #
> >>   depends_on_past is forced to True if wait_for_downstream>>
>  'trigger_rule'                 : 'all_success',                 #
> >>   all_succcess is default anyway>>   # Retries
> >>   'retries'                      : 0,                             #
> >>   No retries>>   # 'retry_delay'                :
> timedelta(minutes=5),          #
> >>   # check retry_exponential_backoff and max_retry_delay too>>   #
> Timeouts and SLAs
> >>   # 'sla'                        : timedelta(hours=1),            #
> >>   # default tasks' sla - normally don't run longer>>
>  'execution_timeout'            : timedelta(hours=3),            #
> >>   no single task runs 3 hours or more>>   # 'sla_miss_callback'
>                                    # -
> >>   # function to call when reporting SLA timeouts>>   # Notifications:
> >>   'email'                        : ['rdautkhanov@epsilon.com'],
> >>   'email_on_failure'             : True,
> >>   'email_on_retry'               : True,
> >>   # Resource usage:
> >>   'pool'                         : 'DISCOVER-Prod',               #
> >>   can increase this pool's concurrency>>   # 'queue'
>   : 'some_queue',
> >>   # 'priority_weight'            : 10,
> >>   # Miscellaneous:
> >>   # on_failure_callback=None, on_success_callback=None,
> >>   # on_retry_callback=None>> }
> >
> >
> > The DAG itself has a bunch of Oracle operators.
> >
> > Any ideas?
> >
> > That's a regression from a month old Airflow.
> > No changes in DAG.
> >
> >
> >
> > Thank you,
> > Ruslan Dautkhanov
>
>
> Links:
>
>   1. https://issues.apache.org/jira/browse/AIRFLOW-1013
>   2. https://issues.apache.org/jira/browse/AIRFLOW-1055
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CACdThQ2fs20Mn=tFAJXN60J6XRUOVQbuoenmaPRW1W7w5LD=pA@mail.gmail.com>,Ruslan Dautkhanov <dautkha...@gmail.com>,0,0
286,287,"Harold Lim wrote:
> Hi Akara,
> 
> It's just that I've modified the Java implementation of olio to run with Glassfish, postgresql
and HDFS. I'm not really interested in the performance of the database but rather the filesystem.
At the very least, I'm just interested in the performance at the front end (response time
of user browsing the web app, which is indirectly related to the performance of the file system).
Ultimately, I'm interested in the throughput of each storage nodes of my HDFS.
> 
> Also, when I run faban, I get exception reading /my.cnf, which is true since postgresql
doesn't have my.cnf.

Just ignore it. We would have to edit the Olio benchmark class and 
rebuild the driver to get rid of it. Future versions will allow you to 
use different services transparently, avoiding the message altogether.
> 
> 
> One question though: In the olio driver tab, is the host = the IP of the master? or the
IP of the machine containing glassfish?
> My current set up is like this: 1 machine for master, 1 machine for postgresql, 1 machine
for glassfish.

The host in this tab refers to the driver systems. You can use ip 
addresses or host names, separated by space.

> 
> 
> I set up the host to be the IP of glassfish. When I run the benchmark, I get failed:
> 
> HomePage	0	0	0%	26.15%	FAILED
> Login	0	0	0%	10.22%	FAILED
> TagSearch	0	0	0%	33.45%	FAILED
> EventDetail	0	0	0%	24.68%	FAILED
> PersonDetail	0	0	0%	2.61%	FAILED
> AddPerson	0	0	0%	.84%	PASSED
> AddEvent	0	0	0%	2.04%	FAILED

Simply, none of the operations went through.

> I'm pretty sure that olio is working for me because I can manually go to the olio web
app from my browser and browse/create user, events, etc.

Well, we need to know what's not working, then. Check your run log.

-Akara
> 
> 
> 
> 
> 
> --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM> wrote:
> 
>> From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM>
>> Subject: Re: Olio + Faban
>> To: olio-user@incubator.apache.org
>> Date: Friday, August 14, 2009, 4:51 PM
>> I've to check whether it is exposed,
>> but you can certainly edit the 
>> config file and mark the host enabled=false. However, I
>> don't suggest 
>> that. Olio depends on the agent to reload the DB, etc.
>>
>> Can you please let me know why you don't want to run the
>> agent on the DB 
>> server?
>>
>> As for the data store, I don't think the distributed stores
>> are well 
>> integrated. Why don't you get successful runs with the
>> local store first 
>> and we can then add support for those.
>>
>> -Akara
>>
>> Harold Lim wrote:
>>> Thanks.
>>> I have a few more questions:
>>>
>>> Is there a way to only test the webapp? (i.e., I don't
>> want to run faban agent on my DB server). When I put blank
>> for host name of db server when setting up my run in faban,
>> I get like a null pointer exception.
>>> Also, in the data storage server section of faban, how
>> do I set it up if I am not using local store? I have set up
>> a distributed storage to host my file store (e.g., hdfs,
>> mogilefs).
>>>
>>>
>>> Thanks,
>>> Harold
>>>
>>>
>>> --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM>
>> wrote:
>>>> From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM>
>>>> Subject: Re: Olio + Faban
>>>> To: olio-user@incubator.apache.org
>>>> Date: Friday, August 14, 2009, 4:10 PM
>>>> The file faban.xml did not get
>>>> generated at deployment for some reason. Just
>> remove the
>>>> olio deploy directory under $FABAN/benchmarks and
>> restart
>>>> the Faban harness. As Faban unjars the Olio
>> benchmark jar it
>>>> should generate the file. Thanks.
>>>>
>>>> -Akara
>>>>
>>>> Harold Lim wrote:
>>>>> When I try to schedule a run in Faban: I get
>> the
>>>> following error in my log:
>>>>> record>
>>>>>    
>> <date>2009-08-14T15:05:34</date>
>>>>>    
>> <millis>1250276734815</millis>
>>>>>    
>> <sequence>65</sequence>
>>>>>    
>> <logger>com.sun.faban.harness.common.BenchmarkDescription</logger>
>>>>>    
>> <level>WARNING</level>
>>>>>    
>> <class>com.sun.faban.harness.common.BenchmarkDescription</class>
>>>>>    
>> <method>readDescription</method>
>>>>>     <thread>11</thread>
>>>>>     <message>Error reading
>>>> benchmark descriptor for
>>>>
>> /home/harold/Desktop/faban/benchmarks/olio</message>
>>>>>     <exception>
>>>>>   
>>>>     <message>java.io.IOException:
>>>> Element &lt;metric&gt; empty or missing
>> in
>> /home/harold/Desktop/faban/benchmarks/olio/META-INF/benchmark.xml</message>
>>>>>       <frame>
>>>>>     
>>>>    
>> <class>com.sun.faban.harness.common.BenchmarkDescription</class>
>>>>>     
>>>>    
>> <method>readDescription</method>
>>>>>     
>>>>     <line>276</line>
>>>>>       </frame>
>>>>>       <frame>
>>>>>     
>>>>    
>> <class>com.sun.faban.harness.common.BenchmarkDescription</class>
>>>>>     
>>>>    
>> <method>generateMaps</method>
>>>>>     
>>>>     <line>181</line>
>>>>>       </frame>
>>>>>       <frame>
>>>>>     
>>>>    
>> <class>com.sun.faban.harness.common.BenchmarkDescription</class>
>>>>>     
>>>>    
>> <method>checkMaps</method>
>>>>>     
>>>>     <line>165</line>
>>>>>       </frame>
>>>>>       <frame>
>>>>>     
>>>>    
>> <class>com.sun.faban.harness.common.BenchmarkDescription</class>
>>>>>     
>>>>    
>> <method>getDescription</method>
>>>>>     
>>>>     <line>126</line>
>>>>>       </frame>
>>>>>       <frame>
>>>>>     
>>>>    
>> <class>com.sun.faban.harness.engine.RunDaemon</class>
>>>>>     
>>>>    
>> <method>fetchNextRun</method>
>>>>>     
>>>>     <line>160</line>
>>>>>       </frame>
>>>>>       <frame>
>>>>>     
>>>>    
>> <class>com.sun.faban.harness.engine.RunDaemon</class>
>>>>>     
>>>>     <method>run</method>
>>>>>     
>>>>     <line>272</line>
>>>>>       </frame>
>>>>>
>>>>>
>>>>> Any ideas what's causing that?
>>>>>
>>>>>
>>>>> Thanks,
>>>>> Harold
>>>>>
>>>>>
>>>>>         
>>>
>>>
>>
> 
> 
> 


",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200908.mbox/%3c4A85D4C6.7010507@sun.com%3e,Akara Sucharitakul <Akara.Sucharita...@Sun.COM>,0,0
6,6," [ https://issues.apache.org/jira/browse/USERGRID-832?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Michael Russo updated USERGRID-832: -----------------------------------         Fix Version/s: 2.1.1     Affects Version/s: 2.1.0  > [SPIKE] Investigate and propose 2-3 approaches for indexing specific attributes of documents > -------------------------------------------------------------------------------------------- > >                 Key: USERGRID-832 >                 URL: https://issues.apache.org/jira/browse/USERGRID-832 >             Project: Usergrid >          Issue Type: Story >    Affects Versions: 2.1.0 >            Reporter: Jeffrey  >            Assignee: George Reyes >            Priority: Critical >             Fix For: 2.1.1 > > > Please investigate and propose a few different options for explicit indexing. > Things to consider: > 1) How other databases handle this > 2) Ability to index all occurrences of a given field name - for example, index 'cat' whether it is a root property, pets.cat, owners.cat, etc.    -- This message was sent by Atlassian JIRA (v6.3.4#6332)",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201603.mbox/%3cJIRA.12844533.1436756278000.60645.1459099705582@Atlassian.JIRA%3e,"""Michael Russo (JIRA)"" <j...@apache.org>",0,0
260,261,"Hi Folks,Interested in the latest news about the Apache Airflow (Incubating) project, follow
our newly-minted twitter account!
ApacheAirflow (@ApacheAirflow) | Twitter

  
|  
|   
|   
|   |    |

   |

  |
|  
|    |  
ApacheAirflow (@ApacheAirflow) | Twitter
 The latest Tweets from ApacheAirflow (@ApacheAirflow)  |   |

  |

  |

 

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201604.mbox/<1972961185.3971411.1461973969867.JavaMail.yahoo@mail.yahoo.com>,Siddharth Anand <san...@apache.org>,0,1
261,262,"Shouldn't be a problem. Did you resolve this ?

Shanti

2010/5/29 Bruno Guimarães Sousa <brgsousa@gmail.com>

> Is it a problem if faban master driver is a 32 bit system, and all nodes in
> SUT are 64 bit systems?
>
> changed the master driver's arch to 32 and ""run log"" presented errors like:
> 18:39:45 kamet SEVERE*
> exception<http://csgengenharia.homeftp.net:9980/LogReader?runId=OlioDriver.1E&exception=21>
> * UIDriverAgent[0].7: Error initializing driver object. Exception:
> Message: java.lang.NumberFormatException: For input string: """"
>
>
> regards,
> --
> Bruno Guimarães Sousa
> www.ifba.edu.br
> PONTONET - DGTI - IFBA
> Ciência da Computação UFBA
> Registered Linux user #465914
>

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/201006.mbox/%3cAANLkTilnujTMXAcRHDCtKLuiGhn5Okdc7BrUR01lZRUG@mail.gmail.com%3e,Shanti Subramanyam <shanti.subraman...@gmail.com>,0,0
295,296,"I'm a little confused about what is meant as a job here, after all this
discussion...

For ""interactive sessions"", stopping a session means stopping the
SparkContext. So the final state of any running jobs in that session should
be the same as if you stopped the SparkContext without explicitly stopping
the jobs in a normal, non-Livy application.

For batches, stopping a batch means killing the Spark application, so all
bets are off as to what happens there.


On Wed, Jan 24, 2018 at 1:08 PM, Alex Bozarth <ajbozart@us.ibm.com> wrote:

> You are correct that you are using the term Job incorrectly (at least
> according to how Spark/Livy uses it). Each spark-submit is a a single Spark
> Application and can include many jobs (which are broken down themselves
> into stages and tasks). In Livy using sessions would be like using
> spark-shell rather than spark-submit, you probably want to use batches
> instead (which utilize spark-submit), then you would use that delete
> command as mentioned earlier. As for the result being listed as FAILED and
> not CANCELLED, that is as intended. When a Livy Session is stopped
> (deleted) is sends a command to all the running jobs (in your case each of
> you apps only have one ""Job"") to set as failed.
>
> @Marcelo you wrote the code that does this, do you remember why you had
> Jobs killed instead of cancelled when a Livy session is stopped? Otherwise
> we may be able to open a JIRA and change this, but I am unsure of any
> potential consequences.
>
>
> *Alex Bozarth*
> Software Engineer
> Spark Technology Center
> ------------------------------
> *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com>
> *GitHub: **github.com/ajbozarth* <https://github.com/ajbozarth>
>
>
> 505 Howard Street
> <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g>
> San Francisco, CA 94105
> <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g>
> United States
> <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g>
>
>
>
> [image: Inactive hide details for kant kodali ---01/23/2018 11:44:26
> PM---I tried POST to sessions/{session id}/jobs/{job id}/cancel a]kant
> kodali ---01/23/2018 11:44:26 PM---I tried POST to sessions/{session
> id}/jobs/{job id}/cancel and that doesn't seem to cancel either.
>
> From: kant kodali <kanth909@gmail.com>
> To: user@livy.incubator.apache.org
> Date: 01/23/2018 11:44 PM
>
> Subject: Re: How to cancel the running streaming job using livy?
> ------------------------------
>
>
>
> I tried  POST to sessions/{session id}/jobs/{job id}/cancel and that
> doesn't seem to cancel either. I think first of all the word ""job"" is used
> in so many context that it might be misleading.
>
> Imagine for a second I don't have livy and I just use spark-submit command
> line to spawn . say I do that following
>
> spark-submit hello1.jar // streaming job1 (runs forever)
> spark-submit hello2.jar //streaming job2 (runs forever)
>
> The number of jobs I spawned is two and now I want to be able to cancel
> one of them..These jobs reads data from kafka and will be split into stages
> and task now sometimes these tasks are also called jobs according to SPARK
> UI for some reason. And looks like live may be is cancelling those with the
> above end point.
>
> It would be great help if someone could try from their end and see if they
> are able to cancel the jobs?
>
> Thanks!
>
> On Fri, Jan 19, 2018 at 4:03 PM, Alex Bozarth <*ajbozart@us.ibm.com*
> <ajbozart@us.ibm.com>> wrote:
>
>    Ah, that's why I couldn't find cancel in JobHandle, but it was
>    implemented in all it's implementations, which all implement it as would be
>    expected.
>
>
>
> *Alex Bozarth*
> Software Engineer
> Spark Technology Center
> ------------------------------
> *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com>
> *GitHub: **github.com/ajbozarth*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=fDK7aF_qwcx3-sCSfUCbzeju-yaB8rqcutS_AuW_BRs&e=>
>
>
> *505 Howard Street*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=>
> *San Francisco, CA 94105*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=>
> *United States*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=>
>
>
>
>
>    [image: Inactive hide details for Marcelo Vanzin ---01/19/2018
>    03:55:43 PM---A JobHandle (which you get by submitting a Job) is a Futur]Marcelo
>    Vanzin ---01/19/2018 03:55:43 PM---A JobHandle (which you get by submitting
>    a Job) is a Future, and Futures have a ""cancel()"" method.
>
>    From: Marcelo Vanzin <*vanzin@cloudera.com* <vanzin@cloudera.com>>
>    To: *user@livy.incubator.apache.org* <user@livy.incubator.apache.org>
>    Date: 01/19/2018 03:55 PM
>
>    Subject: Re: How to cancel the running streaming job using livy?
>    ------------------------------
>
>
>
>    A JobHandle (which you get by submitting a Job) is a Future, and
>    Futures have a ""cancel()"" method.
>
>    I don't remember the details about how ""cancel()"" is implemented in
>    Livy, though.
>
>    On Fri, Jan 19, 2018 at 3:52 PM, Alex Bozarth <*ajbozart@us.ibm.com*
>    <ajbozart@us.ibm.com>> wrote:
>       Ok so I looked into this a bit more. I misunderstood you a bit
>          before, the delete call is for ending livy sessions using the rest API, not
>          jobs and not via the Java API. As for the Job state that makes sense, if
>          you end the session the session kills all currently running jobs. What you
>          want to to send cancel requests to the jobs the session is running. From my
>          research I found that there is a way to do this via the REST API, but it
>          isn't documented for some reason. Doing a POST to /{session id}/jobs/{job
>          id}/cancel will cancel a job. As for the Java API, the feature isn't part
>          of the Java interface, but most implementations of it add it, such as the
>          Scala API which ScalaJobHandle class on sumbit which has a cancel function.
>          I'm not sure how you're submitting you jobs, but there should be a cancel
>          function available to you somewhere depending on the client you're using.
>          From this discussion I've realized our current documentation is even more
>          lacking that I had thought.
>       *Alex Bozarth*
>    Software Engineer
>    Spark Technology Center
>
> ------------------------------
> *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com>
> *GitHub: **github.com/ajbozarth*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=gMcUXOnL9YD3_CIOpwNX4jqFVWhx0l6DAsJYTKN9HVU&e=>
>
>
> *505 Howard Street*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=>
> *San Francisco, CA 94105*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=>
> *United States*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=>
>
>
>
>
>          [image: Inactive hide details for kant kodali ---01/18/2018
>          06:09:59 PM---Also just tried the below and got the state. It ended up in ""]kant
>          kodali ---01/18/2018 06:09:59 PM---Also just tried the below and got the
>          state. It ended up in ""FAILED"" stated when I expected it to be
>
>          From: kant kodali <*kanth909@gmail.com* <kanth909@gmail.com>>
>          To: *user@livy.incubator.apache.org*
>          <user@livy.incubator.apache.org>
>          Date: 01/18/2018 06:09 PM
>          Subject: Re: How to cancel the running streaming job using livy?
>
>          ------------------------------
>
>
>
>          Also just tried the below and got the state. It ended up in
>          ""FAILED"" stated when I expected it to be in ""CANCELLED"" state. Also from
>          the docs it is not clear if it kills the session or the job? if it kills
>          the session I can't spawn any other Job. Sorry cancelling job had been a
>          bit confusing for me.
>          DELETE /sessions/0
>
>
>
>          On Thu, Jan 18, 2018 at 5:55 PM, kant kodali <
>          *kanth909@gmail.com* <kanth909@gmail.com>> wrote:
>             oh this raises couple questions.
>
>                      1) Is there a programmatic way to cancel a job?
>
>                      2) is  there any programmatic way to get session id?
>                      If not, how do I get a sessionId when I spawn multiple jobs or multiple
>                      sessions?
>
>
>                      On Thu, Jan 18, 2018 at 5:39 PM, Alex Bozarth <
>                      *ajbozart@us.ibm.com* <ajbozart@us.ibm.com>> wrote:
>                      You make a DELETE call as detailed here:
>                      *http://livy.apache.org/docs/latest/rest-api.html#response*
>                      <https://urldefense.proofpoint.com/v2/url?u=http-3A__livy.apache.org_docs_latest_rest-2Dapi.html-23response&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=eAcZY6sAN_mkDv5Ves9UtZaotVvvUc3BBdkCEV_CqVg&e=>
>                      *Alex Bozarth*
>                      Software Engineer
>                      Spark Technology Center
>
> ------------------------------
> *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com>
> *GitHub: **github.com/ajbozarth*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=EV7HPze6ToE8xgFtDOw9zE2b3sGYWSW1rB-7ZhiJRok&e=>
>
>
> *505 Howard Street*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=>
> *San Francisco, CA 94105*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=>
> *United States*
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=>
>
>
>
>
>                      [image: Inactive hide details for kant kodali
>                      ---01/18/2018 05:34:07 PM---Hi All, I was able to submit a streaming
job to
>                      livy however]kant kodali ---01/18/2018 05:34:07
>                      PM---Hi All, I was able to submit a streaming job to livy however
I wasn't
>                      able to find
>
>                      From: kant kodali <*kanth909@gmail.com*
>                      <kanth909@gmail.com>>
>                      To: *user@livy.incubator.apache.org*
>                      <user@livy.incubator.apache.org>
>                      Date: 01/18/2018 05:34 PM
>                      Subject: How to cancel the running streaming job
>                      using livy?
>                      ------------------------------
>
>
>
>                      Hi All,
>
>                      I was able to submit a streaming job to livy however
>                      I wasn't able to find any way to cancel the running the job? Please
let me
>                      know.
>
>                      Thanks!
>
>
>
>
>
>
>
>    --
>    Marcelo
>
>
>
>
>


-- 
Marcelo

",http://mail-archives.apache.org/mod_mbox/incubator-livy-user/201801.mbox/%3cCAAOnQ7sDxAw+M5tyQgQ1hKQdy=Coc_S_231oMvK_fPHoPPb4wA@mail.gmail.com%3e,Marcelo Vanzin <van...@cloudera.com>,0,1
32,33,"
I've been using the unsubscribe email, an it doesn't seem to work.
Is there another option?

-----Original Message-----
From:	mwessendorf@gmail.com on behalf of Matthias Wessendorf
Sent:	Fri 8/25/2006 2:09 AM
To:	adffaces-user@incubator.apache.org
Cc:	
Subject:	Re: unsubscribe

that won't work...

use the unsubscribe email address

On 8/25/06, Hall, Peter <Peter.Hall@philotech.de> wrote:
> unsubscribe
>
>
>


-- 
Matthias Wessendorf

further stuff:
blog: http://jroller.com/page/mwessendorf
mail: mwessendorf-at-gmail-dot-com




",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200608.mbox/%3cF1F4BF4F2CBC7146A3EFF56D78104A2606D61F24@priv-srvexch4%3e,"""Rick Rodriguez"" <RRodrig...@valleywater.org>",0,0
287,288,"Could you also post the message you're sending?

Thanks,

Matthieu.

>I try to send a message to the engine and I got this error, and I
could notsee the message in the instance.  My bpel process may be
wrong.  How do yougive define a LocalPort, so that I can send the
message to the engine.[Agila] DEBUG
[http-8080-Processor23]MessageControllerImpl.acknowledge(322) | Step
5.[Agila] DEBUG [http-8080-Processor23]
AgilaEngineImpl.acknowledge(72) | Anactivity instance has been
found.[Agila] INFO
[http-8080-Processor23]DBSessionManager.commitTransaction(100) |
Commiting transaction with thread :
Thread[http-8080-Processor23,5,main][Agila] WARN
[http-8080-Processor23] AgilaEngineImpl.acknowledge(135) | Noprocess
instance (recipient) found, that is waiting for message.[Agila] DEBUG
[http-8080-Processor23] EngineWSA.sendToEngine(118) |Producing reply:
<?xml versionu003d""1.0""
encodingu003d""UTF-8""?><message><return-code>1</return-code></message>I
don't know you guys see what's wrong in this bpel and wsdl file: (or
anysuggestion?)Bpel definition file:<process
nameu003d""tecpbpel""targetNamespaceu003d""http://www.symcor.com/tecp/process/""suppressJoinFailureu003d""yes""xmlns:tnsu003d""http://www.symcor.com/tecp/definition/""xmlnsu003d""http://schemas.xmlsoap.org/ws/2003/03/business-process/"">
     <partnerLinks>            <partnerLink nameu003d""tecp""
partnerLinkTypeu003d""tns:tecpbpelLT""myRoleu003d""initiator""/>          
 <partnerLink nameu003d""exceptionservice""partnerLinkTypeu003d""tns:exceptionServiceLT""
myRoleu003d""handleException""/>      </partnerLinks>      <variables>  
         <variable nameu003d""receiveInput""
messageTypeu003d""tns:inputRequest""/>            <variable
nameu003d""input""messageTypeu003d""tns:processExceptionItemsRequest""/>  
         <variable
nameu003d""output""messageTypeu003d""tns:processExceptionItemsResponse""/>
     </variables>      <sequence>            <receive
nameu003d""receiveInput""
partnerLinku003d""tecp""portTypeu003d""tns:servicePT""
operationu003d""service""
variableu003d""receiveInput""createInstanceu003d""yes"">           
</receive>            <assign>                  <copy>                
       <from variableu003d""receiveInput""></from>                      
 <to variableu003d""output""></to>                  </copy>           
</assign>            <invoke
nameu003d""callExceptionService""partnerLinku003d""exceptionservice""
portTypeu003d""tns:ExceptionService""operationu003d""tns:processExceptionItem""
inputVariableu003d""input""outputVariableu003d""output"">           
</invoke>            <reply nameu003d""reply"" partnerLinku003d""tecp""
portTypeu003d""tns:servicePT""operationu003d""service""
variableu003d""output"">            </reply>     
</sequence></process>WSDL definition file<xml
versionu003d""1.0""?><definitions nameu003d""tecpbpel""       
targetNamespaceu003d""http://www.symcor.com/tecp/definition/""       
xmlns:tnsu003d""http://www.symcor.com/tecp/definition/""       
xmlns:plnku003d""http://schemas.xmlsoap.org/ws/2003/05/partner-link/""  
     xmlns:wsdlu003d""http://schemas.xmlsoap.org/wsdl/""       
xmlns:soapu003d""http://schemas.xmlsoap.org/wsdl/soap/""       
xmlnsu003d""http://schemas.xmlsoap.org/wsdl/"">    <types>       
<schema attributeFormDefaultu003d""unqualified""               
elementFormDefaultu003d""qualified""               
xmlnsu003d""http://www.w3.org/2001/XMLSchema""                >         
<complexType nameu003d""SearchResultSummary"">                <sequence>
                <element nameu003d""account"" nillableu003d""true""
typeu003d""string""/>                 <element nameu003d""amount""
typeu003d""double""/>                 <element nameu003d""exceptionDate""
nillableu003d""true""typeu003d""long""/>                 <element
nameu003d""fi"" nillableu003d""true"" typeu003d""string""/>                
<element nameu003d""mailTransit"" nillableu003d""true""typeu003d""string""/>
                <element nameu003d""reasonCode"" nillableu003d""true""
typeu003d""string""/>                 <element nameu003d""sequence""
nillableu003d""true"" typeu003d""string""/>                 <element
nameu003d""transitID"" nillableu003d""true"" typeu003d""string""/>          
      <element nameu003d""SLA"" nillableu003d""true"" typeu003d""string""/> 
               <element nameu003d""completionDate"" typeu003d""long""/>   
            </sequence>         </complexType>       </schema></types>
   <message nameu003d""inputRequest"">        <part
nameu003d""inputSummaries"" typeu003d""tns:SearchResultSummary""/>   
</message>    <message nameu003d""processExceptionItemsRequest"">       
<part nameu003d""summaries"" typeu003d""tns:SearchResultSummary""/>   
</message>      <message nameu003d""processExceptionItemsResponse"">    
   <part nameu003d""processExceptionItemsReturn""typeu003d""tns:SearchResultSummary""/>
   </message>    <portType nameu003d""servicePT"">      <operation
nameu003d""service"">            <input messageu003d""tns:inputRequest""
/>            <output messageu003d""tns:processExceptionItemsResponse""
/>      </operation>    </portType>    <portType
nameu003d""ExceptionService"">        <operation
nameu003d""processExceptionItem"" parameterOrderu003d""summaries"">       
    <input messageu003d""tns:processExceptionItemsRequest""/>           
<output messageu003d""tns:processExceptionItemsResponse""/>       
</operation>    </portType>       <binding
nameu003d""ExceptionServiceSoapBinding""typeu003d""tns:ExceptionService"">
     <soap:binding
styleu003d""rpc""transportu003d""http://schemas.xmlsoap.org/soap/http""/> 
      <operation nameu003d""processExceptionItem"">           
<soap:operation soapActionu003d""""/>            <input>                
 <soap:body useu003d""encoded""namespaceu003d""http://www.symcor.com/tecp""encodingStyleu003d""http://schemas.xmlsoap.org/soap/encoding/""/>
           </input>            <output>                  <soap:body
useu003d""encoded""namespaceu003d""http://www.symcor.com/tecp""encodingStyleu003d""http://schemas.xmlsoap.org/soap/encoding/""/>
           </output>        </operation>    </binding>   <service
nameu003d""ExceptionServiceService"">      <port
nameu003d""ExceptionService""bindingu003d""tns:ExceptionServiceSoapBinding"">
        <soap:addresslocationu003d""http://localhost:8080/tecpWS/services/ExceptionService""/>
     </port>   </service>    <plnk:partnerLinkType
nameu003d""tecpbpelLT"">        <plnk:role nameu003d""initiator"">        
   <plnk:portType nameu003d""tns:servicePT""/>        </plnk:role>    
</plnk:partnerLinkType>     <plnk:partnerLinkType
nameu003d""exceptionServiceLT"">        <plnk:role
nameu003d""handleException"">            <plnk:portType
nameu003d""tns:ExceptionService""/>        </plnk:role>   
</plnk:partnerLinkType></definitions>ThanksTu

",http://mail-archives.apache.org/mod_mbox/incubator-agila-user/200602.mbox/%3cfbdc6a970602080017m67a008f2o1aec13b71e827c45@mail.gmail.com%3e,Matthieu Riou <matthieu.r...@gmail.com>,1,0
4,4,"      [ https://issues.apache.org/jira/browse/USERGRID-1251?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Michael Russo updated USERGRID-1251: ------------------------------------     Sprint: Usergrid 37  (was: Usergrid 36)  > Handle index processing event case where an edge exists but entity is missing. > ------------------------------------------------------------------------------ > >                 Key: USERGRID-1251 >                 URL: https://issues.apache.org/jira/browse/USERGRID-1251 >             Project: Usergrid >          Issue Type: Story >    Affects Versions: 2.1.1, 2.1.0 >            Reporter: Michael Russo >            Assignee: Michael Russo >             Fix For: 2.1.2 > > > In the event an edge exists in graph and the entity has been deleted, ensure this is handled as OK when processing the index event in AmazonAsyncEventService.  Currently it logs an exception because when it tries to fetch the entity data it doesn't exist and therefore 0 messages are returned for an event that should have returned 1 message.  > Ideally, this should trigger a graph node compaction (read repair) to remove that edge.     -- This message was sent by Atlassian JIRA (v6.3.4#6332) ",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201603.mbox/raw/%3CJIRA.12936220.1454465186000.196217.1456937478422%40Atlassian.JIRA%3E,"""Michael Russo (JIRA)"" <j...@apache.org>",0,0
68,69,"can help too, but unsure what the next steps are

On Tue, Mar 1, 2016 at 11:25 PM, Justin Mclean <justin@classsoftware.com> wrote:
> Hi
>
>> Hmm, that's true. Wanted to add the release to tomee M3 (planned next
>> week). Sounds a bit hard to do then.
>
> We can do both.
>
>> Anyone able to help with the graduation process?
>
> Yep I should be able to. I know the incubation pages needs to updated with a few dates
etc.
>
> Thanks,
> Justin



-- 
Hendrik Saly (salyh, hendrikdev22)
@hendrikdev22
PGP: 0x22D7F6EC

",http://mail-archives.apache.org/mod_mbox/johnzon-dev/201603.mbox/<CAPm=oFC8cPOQGs0oLaaRBnfohenm=x91u_7WBAGzV9v_tqQzKw@mail.gmail.com>,Hendrik Dev <hendrikde...@gmail.com>,0,0
12,12,"     [ https://issues.apache.org/jira/browse/USERGRID-1321?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15954993#comment-15954993 ]   Ganaraj Tejasvi commented on USERGRID-1321: -------------------------------------------  Facing the same problem unable to use any version other than the main release 2.1.0 in which the elasticsearch indexes are getting corrupted and the only option I have is rebuilding the entire index just to work with usergrid  . Please prioritize so that I can pull the latest version and use.   > Usergrid 2.2 Master, Tomcat Startup Error: unconfigured columnfamily Data_Migration_Info > ---------------------------------------------------------------------------------------- > >                 Key: USERGRID-1321 >                 URL: https://issues.apache.org/jira/browse/USERGRID-1321 >             Project: Usergrid >          Issue Type: Story >          Components: Stack >    Affects Versions: 2.2.0 >         Environment: OS: Ubuntu 14.04; > Cassandra version: 2.2.6 (DataStax); > Elasticsearch version: 1.4.4; > Tomcat version: 7; > JDK version: 1.8.0_65 (Oracle); > Usergrid version: 2.2.0 Master branch >            Reporter: Jaskaran >             Fix For: 2.2.0 > > > We have been encountering INSTALLATION issues with Usergrid 2.2 Master (Since September 3, 2016).  > Specifically tomcat fails to start, giving errors which indicate that usergrid application was expecting certain keyspaces/columns, BUT did not find them. We have tried to setup the database via the standard calls (below), but apparatently it does not help. As stated, the problem started occuring after the commit on September 3, 2016 (hash tag 461076c389ba8413cbbb1b596dba9439af35a8b9) > Tomcat startup logs - specifically, we see two types of errors > *(A) unconfigured columnfamily Data_Migration_Info* > {code} > 10:47:45,021  INFO Schema:339 - Registering class org.apache.usergrid.persistence.entities.Receipt > 10:47:45,028  INFO Schema:339 - Registering class org.apache.usergrid.persistence.DynamicEntity > 10:47:45,034  INFO Schema:275 - Schema initialized > 10:47:45,159  WARN Slf4jConnectionPoolMonitorImpl:31 - BadRequestException: [host=172.31.32.243(172.31.32.243):9160, latency=53(85), attempts=1]InvalidRequestException(why:unconfigured columnfamily Data_Migration_Info) > 10:47:45,200  WARN Slf4jConnectionPoolMonitorImpl:31 - BadRequestException: [host=172.31.32.243(172.31.32.243):9160, latency=21(21), attempts=1]InvalidRequestException(why:unconfigured columnfamily Entity_Version_Data) > 10:47:45,206  WARN Slf4jConnectionPoolMonitorImpl:31 - BadRequestException: [host=172.31.32.243(172.31.32.243):9160, latency=0(0), attempts=1]InvalidRequestException(why:unconfigured columnfamily Entity_Version_Data) > 10:47:45,211  INFO XmlWebApplicationContext:1376 - Bean 'entityManagerFactory' of type [class org.apache.usergrid.corepersistence.CpEntityManagerFactory] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) > {code} > *(B) keyspace 'Locks' does not exist* > {code} > 10:47:47.842 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [http-nio-8080] > 10:47:47.853 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [ajp-nio-8009] > 10:47:47.854 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 19119 ms > 10:47:51,331  WARN Slf4jConnectionPoolMonitorImpl:31 - BadRequestException: [host=172.31.32.243(172.31.32.243):9160, latency=0(0), attempts=1]InvalidRequestException(why:Keyspace 'Locks' does not exist) > 10:47:56,333  WARN Slf4jConnectionPoolMonitorImpl:31 - BadRequestException: [host=172.31.32.243(172.31.32.243):9160, latency=1(1), attempts=1]InvalidRequestException(why:Keyspace 'Locks' does not exist) > 10:48:01,334  WARN Slf4jConnectionPoolMonitorImpl:31 - BadRequestException: [host=172.31.32.243(172.31.32.243):9160, latency=1(1), attempts=1]InvalidRequestException(why:Keyspace 'Locks' does not exist) > {code} > *Database Setup & Init calls* (we use these, but they do not help the problem) > If we try hit the http://localhost:8080/system/database/setup API (after we see the above error), the API fails with the following error/response > {code} > {""error"":""runtime"",""timestamp"":1234567890,""duration"":0,""error_description"":""Error migrating Core Persistence"",""exception"":""java.lang.RuntimeException""} > {code} > While the issue appear to link to the discussion in JIRA-https://issues.apache.org/jira/browse/USERGRID-1316 - we have tried different configurations (including Cas , Es and tomcat version changes) and that does not help. > Would appreciate your help in resolving this    -- This message was sent by Atlassian JIRA (v6.3.15#6346) ",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201704.mbox/raw/%3CJIRA.13025877.1481011076000.199936.1491303645476%40Atlassian.JIRA%3E,"""Ganaraj Tejasvi (JIRA)"" <j...@apache.org>",0,0
110,111,"Dear all,

Sorry for my previous mail, it had more details but my editor just get rid 
of them. Any way it sent the important part which is the code.

I still have the problem and actually the problem is simple. I need to 
merge a set of text documents into one text document. To regenerate my 
problem, I modified the demo7  available at
http://incubator.apache.org/odftoolkit/simple/demo/demo7.html

This demo is generating a set of documents and I modified it to also merge 
these documents into one document. I used two methods to do so:

One by using  org.odftoolkit.simple.TextDocument as following:

        public static void generateAllOffersLetterTextDocument() throws 
Exception {
                TextDocument templateDocument = TextDocument.loadDocument(
fRootDirectory + ""/""+ ""OfferTemplate.odt"");
 
                SpreadsheetDocument dataDocument = SpreadsheetDocument.
loadDocument(fRootDirectory + ""/""+ ""Candidates.ods"");
                Table table = dataDocument.getTableByName(""Sheet1"");
                int rowCount = table.getRowCount();
 
                TextDocument offerDoc = null;
                for(int i = 1; i<rowCount; i++){
                        Row row = table.getRowByIndex(i);
                        String name = 
row.getCellByIndex(0).getDisplayText();
 
                        offerDoc = TextDocument.loadDocument(fOutDirectory 
+ ""/"" + name+""'s offer letter.odt"");
                        templateDocument.insertDocument(offerDoc, ""/"" + 
name);
                }
 
                templateDocument.save(fOutDirectory + 
""/AllOffersTextDoc.odt"");
        }

And the other way is by extending 
org.odftoolkit.odfdom.doc.OdfTextDocument by a new class MyTextDocument to 
do exactly the same thing as follows:

        public static void generateAllOffersODFDocument () throws 
Exception {
                MyTextDocument allDocTextDoc = MyTextDocument.
newTextWorkProduct(fRootDirectory + ""/""+ ""OfferTemplate.ott"");
 
                SpreadsheetDocument dataDocument = SpreadsheetDocument.
loadDocument(fRootDirectory + ""/""+ ""Candidates.ods"");
                Table table = dataDocument.getTableByName(""Sheet1"");
                int rowCount = table.getRowCount();
 
                for(int i = 1; i<rowCount; i++){
                        Row row = table.getRowByIndex(i);
                        String name = 
row.getCellByIndex(0).getDisplayText();
 
                        allDocTextDoc.includeFile(fOutDirectory + ""/"" + 
name+""'s offer letter.odt"", ""/"" + name);
                }

 
                allDocTextDoc.save (fOutDirectory + ""/AllOffersODFDoc.odt""
);
        }

While the includeFile is as follows:

        public void includeFile (String filePath, String documentPath) 
throws Exception{
                OdfPackageDocument newDocument;
 
                try {
                        newDocument = OdfPackageDocument.loadDocument
(filePath);
                        insertDocument(newDocument, documentPath);
                } catch (Exception exception) {
                        throw new Exception (""Failed to include the file: 
"" + filePath, exception);
                }
        }

The results were exactly the same, the generated files didn't show the 
agregated documents, only the first contents of the document, however the 
conetns of the documents are inserted in their coresponding folders in the 
zip file. 

In the attached zip file in the previous message, I included the source 
code (FieldsDemo.java and MyTextDocument.java) as well as the source files 
and the generated files. Sorry again for the previous messgae. I am 
stucked here and I can not move forward, and I am looking for help.


Thanks & Best Regards / السلام عليكم 

Ahmed Ibrahim
MBA ,Senior IT Architect, SWG Cairo Lab Services
BPM Client Solution Manager, MEA 
IT Architect Egypt Profession Lead
Mobile: +20 100 1615 506



From:   Ahmed I Ibrahim/Egypt/IBM@IBMEG
To:     odf-users@incubator.apache.org, 
Date:   04/15/2014 10:18 AM
Subject:        Re: How to insert a text document into another text 
document?


[attachment ""ODF_FileMerge.zip"" deleted by Ahmed I Ibrahim/Egypt/IBM] 

",http://mail-archives.apache.org/mod_mbox/incubator-odf-users/201404.mbox/%3cOF65F434BE.7936674F-ON42257CBB.002EC96B-42257CBB.00308206@eg.ibm.com%3e,Ahmed I Ibrahim <AIBRA...@eg.ibm.com>,0,0
306,307,"please use the MyFaces lists.

On 5/24/07, Nigel Magnay <nigel.magnay@gmail.com> wrote:
> Hi there
>
> I'm successfully using the tr:tree control with a custom TreeModel behind it.
>
> Some of the documentation implies that I ought to be able to do some
> kind of AJAX things with this tree - e.g when a node is expanded, just
> re-render that part of the tree rather than the whole page.
>
> Unfortunately I can't seem to find out how to do this - is it possible ?
>


-- 
Matthias Wessendorf

further stuff:
blog: http://matthiaswessendorf.wordpress.com/
mail: matzew-at-apache-dot-org

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200705.mbox/%3c71235db40705290529v48accd83sbe15005dc80fbb78@mail.gmail.com%3e,"""Matthias Wessendorf"" <mat...@apache.org>",1,0
8,8,"GitHub user ayesha12 opened a pull request:      https://github.com/apache/usergrid/pull/554      Resolve merge conflicts for George's pull request https://github.com/â€¦      â€¦apache/usergrid/pull/520          Jira :     https://issues.apache.org/jira/browse/USERGRID-1120     https://issues.apache.org/jira/browse/USERGRID-1116     https://issues.apache.org/jira/browse/USERGRID-1118  You can merge this pull request into a Git repository by running:      $ git pull https://github.com/ayesha12/usergrid EventFixes  Alternatively you can review and apply these changes as the patch at:      https://github.com/apache/usergrid/pull/554.patch  To close this pull request, make a commit to your master/trunk branch with (at least) the following in the commit message:      This closes #554      ---- commit dd13f0b00cdc1a84153a582028b9aac13f98f74b Author: Ayesha Dastagiri <ayesha.amrin@gmail.com> Date:   2016-08-01T21:34:57Z      Resolve merge conflicts for George's pull request https://github.com/apache/usergrid/pull/520     Jira :     https://issues.apache.org/jira/browse/USERGRID-1120     https://issues.apache.org/jira/browse/USERGRID-1116     https://issues.apache.org/jira/browse/USERGRID-1118  ----   --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well. If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA. ---",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201608.mbox/raw/%3Cgit-pr-554-usergrid%40git.apache.org%3E,ayesha12 <...@git.apache.org>,0,0
138,139,"Davide Ling wrote:

> Hi,
> I found an error in my wsdef, targetNamespace wasn't correct in types 
> section.
> So now I made a new step toward invoke... but...
> a new strange Exception occurred!
>
> An html tag in envelope?
> In Italian: ""Dove cavolo lo va a pescare un tag html 
> nell'envelope??????"" ;-)
>
I respond to myself.
Service alias was different from service name
so the bpel process got an html error page instead of a service response.

Bye

-- 
Davide Ling
Sito personale - http://davideling.altervista.org


",http://mail-archives.apache.org/mod_mbox/incubator-agila-user/200511.mbox/%3c43724E84.20406@libero.it%3e,Davide Ling <lin...@libero.it>,0,0
55,56,"The version of json4s we're using (3.2.6 in the 2.10 branch) does seem to
depend on Jackson 2.3.0 and Scala 2.10.0:
http://mvnrepository.com/artifact/org.json4s/json4s-jackson_2.10/3.2.6


On Wed, Feb 12, 2014 at 11:29 AM, Paul Brown <prb@mult.ifario.us> wrote:

> Hi, Aaron --
>
> I can't speak to issues relevant to Spark, but it looks like json4s is
> currently using the Jackson Scala module 2.1.3 and Scala 2.9.2.  There have
> been quite a few significant changes to the Scala module and underpinnings
> between the 2.1.x and 2.3.x series, but I can't speak to how that interacts
> with json4s.  Many of those changes are convenience for direct usage of the
> Jackson Scala module in binding case classes transparently, but you
> wouldn't need or benefit from those through the json4s API.  (FWIW, we use
> Jackson Scala 2.3.2 in our Spark jobs to bind lines of JSON from text files
> to case classes.)
>
> I'll reach out to json4s and see if I can get them to update to the 2.3.x
> Jackson series and Scala 2.10, but I think it makes sense to for Spark to
> just use the released version and then update when a json4s release is
> available.
>
> Best.
> -- Paul
>
> --
> prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
>
>
> On Wed, Feb 12, 2014 at 10:38 AM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
>
> > Will, thanks for the clarifications. I think Spark's main use-case is
> > ""warm, small inputs"" right now, but the change seems reasonable to me
> > nevertheless.
> >
> > Paul, do you know if there are any issues relevant to Spark that we need
> > from 2.3.2? We would also have to wait for json4s to release a new
> version
> > that depends on 2.3.2, or else pull it in ourselves.
> >
> >
> > On Wed, Feb 12, 2014 at 9:47 AM, Paul Brown <prb@mult.ifario.us> wrote:
> >
> > > And, with my FasterXML hat on, if you ask, you'll find the Jackson
> folks
> > > will turn around issues quickly.  FWIW, there is a full-suite Jackson
> > 2.3.2
> > > release rolling right up if you wait a couple of days to pull that in.
> > >
> > > -- Paul
> > >
> > > --
> > > prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
> > >
> > >
> > > On Wed, Feb 12, 2014 at 8:12 AM, Will Benton <willb@redhat.com> wrote:
> > >
> > > > ----- Original Message -----
> > > >
> > > > > I am not sure I fully understand this reasoning. I imagine that
> > > lift-json
> > > > > is only one of hundreds of packages that would have to be built if
> > you
> > > > > wanted to build all of Spark's transitive dependencies from source.
> > > >
> > > > This is absolutely true.  However, many of Spark's dependencies are
> > > > already available in operating system distributions.  In fact, in the
> > > case
> > > > I am most familiar with (packaging Spark for Fedora), Lift is the
> > biggest
> > > > one left that isn't already available or under review.
> > > >
> > > > > Additionally, to make sure I understand the impact -- this is only
> > > > intended
> > > > > to simplify the process of packaging Spark on a new OS distribution
> > > that
> > > > > disallows pulling in binaries?
> > > >
> > > > Yes, this was my main motivation.  Since the process of building Lift
> > and
> > > > its transitive dependencies is disproportionately complex compared to
> > how
> > > > much Spark uses lift-json, I thought it would be nice to replace it
> > with
> > > > something that could be built as just a JSON library.  I would argue
> > that
> > > > -- all else being equal -- it generally makes sense to make software
> > > > development choices that facilitate packaging for distributions like
> > > Fedora
> > > > and Debian.
> > > >
> > > > There are other actual and potential advantages, though; here are a
> > few:
> > > >
> > > > 1.  Based on some simple timing runs I did, json4s-jackson is faster
> > all
> > > > around when running warm (i.e. on subsequent timing runs in the same
> VM
> > > or
> > > > timing runs with enough iterations to last for more than a few
> > seconds),
> > > > slightly slower when running cold on very small parsing tasks, and
> > > > significantly (~10x) faster on large parsing tasks whether cold or
> > warm.
> > > >  The knee in the cold lift-json performance curve is somewhere
> between
> > > 2kb
> > > > and 50kb of JSON source text.  json4s-jackson is nominally faster
> cold
> > > with
> > > > a 12kb file, 40% faster with a 50kb file, 2.6x faster with a 500kb
> file
> > > and
> > > > 10x faster with files ranging from 4-20mb.  Given how Spark uses JSON
> > at
> > > > the moment, the improved large-file parsing performance seems
> unlikely
> > to
> > > > be a huge practical advantage for json4s-jackson, but it's worth
> > noting.
> > > > 2.  The release schedule of json4s isn't coupled to the release
> > schedule
> > > > of a larger project.
> > > > 3.  json4s is intended to provide a uniform interface to Scala JSON
> > > > libraries, and it provides multiple backends, which offers potential
> > > > flexibility in the future.  (To be fair, this interface is heavily
> > based
> > > on
> > > > the one provided by Lift, so it would be only slightly more work to
> go
> > > from
> > > > lift-json to json4s, as my patch does, as it would be to switch
> between
> > > > json4s backends.)
> > > >
> > > > Again, this change is primarily motivated by a desire to make life
> > easier
> > > > for downstream packagers, but there is no obvious downside (beyond
> the
> > > > downsides inherent in changing library dependencies) and several
> minor
> > > > advantages.
> > > >
> > > >
> > > > best,
> > > > wb
> > > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/spark-dev/201402.mbox/<CANGvG8o+86U1F8tMVaMjP-nOYNhJXN1b9uwSEHvD0KReBEKf_w@mail.gmail.com>,Aaron Davidson <ilike...@gmail.com>,0,0
25,26,"Hi,

I need some advice in solving a problem with local variables in DAG.

I have a DAG < schedule intervel 30 mins >. It has 3 tasks. t1 runs a
python program on remote EC2. t2 waits for S3 file availability at
particular location. This S3 file created by t1. Once the S3 file is
available, t3 runs and process the file on S3.

I have date-time as part of my S3 file location.

dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')

bucket_key2 = ""s3://aaaaa/bbbbb/"" + dttm2 + ""/sucess""

t1 runs more than 1 hour so second instance of DAG  is already started and
it changes the variable dttm2 value so job1 task # t2 is trying to locate
the file at different location.

To overcome this I am planning to use parameter {{execution_date}} instead
of getting dttm2 value as shown above.

In situations like these, is there any better approach to keep same value
in a variable through out the particular run of DAG?

Or use XCOM feature to push and pull the values across the tasks with
different keys for each run?


part of my dag is given below:

#

dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')


NL = """"""
          cd /home/ubuntu/Scripts/ ; python2 a11.py {{params.get(""dttm2"")}}
;
""""""

t1 = BashOperator(
    task_id='E_Ns_A',
    bash_command=NL,
    params={'dttm2':dttm2},
    retries=3,
    dag=dag)

bucket_key2 = ""s3://aaaaa/bbbbb/"" + dttm2 + ""/sucess""

def detect_s3(name, dag=dag, upstream=t1):
  task = S3KeySensor(
    task_id = name,
    bucket_key=bucket_key2,
    s3_conn_id='s3conn',
    dag=dag,
    wildcard_match=True)
  task.set_upstream(upstream)
  return task

# Spark Module to clasiify data

bucket_key3 = ""s3://aaaaa/bbbbb/"" + dttm2 + ""/""
sparkcmd = """"""
           cd /home/ubuntu/SC; /home/ubuntu/anaconda3/bin/python
 NbRunner.py;
           aws s3 cp /home/ubuntu/NC.txt {{params.get(""bkey"")}} --region
us-west-1
""""""

t3 = BashOperator(
    task_id='CNs',
    bash_command=sparkcmd,
    params={""bkey"":bucket_key3},
    retries=1,
    dag=dag)

t2 = detect_s3('t2')

t3.set_upstream(t2)


Thanks,
MSR

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201607.mbox/<CABzfHRrz31xPWhMbRRK-WJcnGs1DB8rzELq0V+b1EaVxXFZ0zg@mail.gmail.com>,MSR M <msrmaill...@gmail.com>,0,0
298,299,"
     [ https://issues.apache.org/jira/browse/ISIS-110?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Dan Haywood closed ISIS-110.
----------------------------

    
> Minor edits to pom.xml's to improve operation
> ---------------------------------------------
>
>                 Key: ISIS-110
>                 URL: https://issues.apache.org/jira/browse/ISIS-110
>             Project: Isis
>          Issue Type: Sub-task
>            Reporter: Kevin Meyer
>            Priority: Trivial
>              Labels: maven
>             Fix For: 0.2.0-incubating
>
>
> Minor changes to Isis pom.xml files to improve behaviour. Overall behaviour is unaffected
by these changes, e.g. moving a dependency version definition higher up the build path, etc.

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa
For more information on JIRA, see: http://www.atlassian.com/software/jira

        

",http://mail-archives.apache.org/mod_mbox/isis-dev/201201.mbox/<441081409.63591.1327168901340.JavaMail.tomcat@hel.zones.apache.org>,"""Dan Haywood (Closed) (JIRA)"" <j...@apache.org>",0,0
264,265,"Airflow supports what we call ""slowly-changing"" dynamic DAGs.

Because the DAG files are frequently parsed, there is no issue in theory
with generating different DAGs depending on your needs at a given moment.
The issue is that you want to make sure that the same DAG is loaded by the
scheduler and the executor. For example, if the scheduler loads up the DAG
and sees a task called ""random_task_012345"", it will create an instruction
for the executor to run that task. Then if the executor loads up the DAG
and finds only ""random_task_98765"", it will cause a problem. So
""slowly-changing"" means the DAG can change, but do it on a timescale that
won't cause conflicts when you're trying to run it.

One last note -- the web interface implicitly assumes that the DAG remains
constant over time. You may see odd behavior with mutating DAGs.

On Sat, Aug 27, 2016 at 8:04 PM Lance Norskog <lance.norskog@gmail.com>
wrote:

> Yes! We use tables to generate some of our DAGs and it works well.  What's
> most important is to avoid block-copying code among DAGs.
>
> On Sat, Aug 27, 2016 at 12:05 PM, Vincent Poulain <
> vincent.poulain@tinyclues.com> wrote:
>
> > Hello,
> > First, thank you for your great job. Airflow made my job easier :)
> >
> > Is a dynamic generation of DAG a good practice ? As WePay explained
> there :
> > https://wecode.wepay.com/posts/airflow-wepay.
> >
> > In part of a DAG, I have *n* ECS tasks need to be launched daily, each
> one
> > with specific *params*. *n & params* depending on configuration which is
> > daily updated.
> > For now, I have just one task fetching this configuration and internally
> > broadcast ECS task.
> > I am wondering if Airflow would be suitable to generate DAG dynamically
> > (depending on configuration) in order to split this task into those
> > multiple ECS task (using ECSOperator, for instance).
> >
> > Thank you for your tips,
> >
>
>
>
> --
> Lance Norskog
> lance.norskog@gmail.com
> Redwood City, CA
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<CADsgxrG7E65uQoL9Z0SGkqsAgP348pQ8VfV30yBMcBwaVXTg=w@mail.gmail.com>,Jeremiah Lowin <jlo...@apache.org>,0,0
33,34,"Hi,

I'll be proceeding with the repo migration tomorrow as planned, getting
things started in the morning. From previous experiences migrating repos on
Github, the transition should be smooth where Github handles all of the
redirection. Even Travis and other Github ecosystem tools should just work.

Chris, I assume the merge button will be greyed on PRs, what's the new PR
acceptance flow like?

Thanks,

Max

On Tue, May 3, 2016 at 4:28 PM, Siddharth Anand <sanand@apache.org> wrote:

> Committers/Maintainers :
> If you have an open item on
> https://cwiki.apache.org/confluence/display/AIRFLOW/Migrating+to+Apache,
> please send status/blockers on your items :
> Specifically:
>    - Max (repo migration)
>    - Jeremiah (travis migration)
>
>    - I know you are waiting on Max
>
>    - Bolke
>
>    - GH issue-to-Jira migration - it looks like we decided to handle this
> in a lazy fashion, so maybe we can close this?
> -s

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201605.mbox/<CAHEEp7WZLifgEn7CH8xi7idtjd6ytmhNoN9W0+rdByrjdk-bhQ@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
42,43,"Hi,


why would you want these services to be long-running? Why not write
integration tests against clean instances of each service or clean
instances with data fixtures on top?
Travis offers spinning up a variety of services (
https://docs.travis-ci.com/user/database-setup/) so does wercker (
http://devcenter.wercker.com/docs/services).
These services are docker containers run off of pre-build Docker images.


Best,

Georg


On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com> wrote:

> Hi Chris,
>
> Didn’t they offer a dedicated Travis thing as well? And can we setup long
> running services (ie. Hadoop, KDC, Presto, MySQL, etc) in the Jenkins
> environment, because that would be sufficient I guess? To my knowledge what
> Apache offers is more suited for unit tests (ie. the hadoop-qa runs) than
> integration tests.
>
> What are your thoughts?
>
> Bolke
>
> > Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <criccomini@apache.org>
> het volgende geschreven:
> >
> > Hey Bolke,
> >
> > Thanks for stepping up. The ""traditional"" Apache infra is Jenkins with a
> > pool of machines that they provide. That might or might not be
> satisfactory
> > for us (it's certainly antiquated technology). If we decide we don't like
> > it, I think that's OK, as long as we move forward knowing that any other
> > testing solution can disappear at any time.
> >
> > My 2c. :)
> >
> > Cheers,
> > Chris
> >
> > On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <
> > dan.davydov@airbnb.com.invalid> wrote:
> >
> >> This is extremely generous of you! I do agree with the approach of
> trying
> >> to get funding from Apache and having shared resources (e.g. so that we
> >> don't depend on any one company or individual for the uptime of the
> >> integration environment, plus so we would have public cloud integration
> >> potentially).
> >>
> >> On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com>
> wrote:
> >>
> >>> Hi,
> >>>
> >>> I have been thinking about an integration test environment. Aside from
> >> any
> >>> technical requirements we need a place to do it. I am willing to offer
> a
> >>> place in Lab env I am running or to fund an environment in AWS/GCloud
> if
> >>> Apache cannot make these kind of resources available.
> >>>
> >>> If running in our Lab there is virtually no restriction what we could
> do,
> >>> however I will hand select people who have access to this environment.
> I
> >>> will also hold ultimate power to remove access from anyone. I even
> might
> >>> ask for a confirmation that you will behave when using our property
> >> (don’t
> >>> worry won’t cover it with legal wording). This is a IAAS service so we
> >> need
> >>> to cover the things we need ourselves, but the upside is we can and it
> is
> >>> free. We could setup a Gitlab instance that mirrors from Apache a kicks
> >> off
> >>> runners to do testing. Downside 1) it might not be entirely Apache
> like.
> >>> Sorry cant help that. 2) there is no guaranteed up time 3) I might need
> >> to
> >>> remove it in the future e.g. when I change jobs for example :). 4) No
> >>> public cloud integration, it’s a private stack after all.
> >>>
> >>> I can also fund on AWS/GCloud. Again, I probably want to have ultimate
> >>> power on access to this environment - it’s my company’s money on the
> line
> >>> after all. Major downside to this is that it is dependent on and
> limited
> >> by
> >>> the budget I can make available. Upside is that it is not company
> >> property.
> >>> Also I personally have less exposure to public cloud environments due
> to
> >>> company restrictions.
> >>>
> >>> Are there any other options? Any thoughts?
> >>>
> >>> Bolke
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<CAK5UC3BWHsj8m_nAC9ToB7CWxqZxRinZBZAWAH7wGNzo96EeGg@mail.gmail.com>,Georg Walther <georg.walt...@markovian.com>,0,1
72,73,"My previous post did not make it through, here it goes again.

I agree with Dan, it is reasonable to aim for supporting python 2.4.


Begin forwarded message:

> I just have two comments / questions (I'm playing devils advocate  
> here).
>
> 1. It'd be nice if there was a comment at the beginning of the block  
> =20
> saying why the try blocks need to be nested (for the uninformed ones  
> =20
> like me).
>
> 2. What should be the policy with respect to particular (or minimum)  
> =20
> version numbers for the dependencies?  Is there a general Apache =20
> guideline with respect to this or is this handled on a per project =20
> basis?  The reason for my question is that at some point, addressing  
> =20
> all the corner cases (e.g., backward compatibility and others)  
> becomes =20=
> an obstacle for easy code maintenance.
>
> - Julio
>
>
>
>
> On Jan 8, 2009, at 11:51 AM, Daniel Mercer wrote:
>
>> Howdy,
>>
>> Prior to 2.5, to get try/except/finally behavior you need to nest =20
>> two try blocks =97 one of the so-called =91python warts=92. Here is  
>> a =20=
>
>> patch to client.py that ought to let it work with earlier pythons.
>>
>> Dan M.
>>
>> Index: client.py
>> = 
>> 3D 
>> = 
>> 3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
> = 
> 3D 
> = 
> 3D 
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>> --- client.py   (revision 732752)
>> +++ client.py   (working copy)
>> @@ -162,21 +162,22 @@
>>                sys.exit(-1)
>>        args =3D map(lambda x: eval(x), sys.argv[1:])
>>        try:
>> -               res =3D f(*args)
>> -               if (os.getenv(""USE_HTML_TABLES"")):
>> -                       try:
>> -                               makeHTMLTable(res)
>> -                       except:
>> +               try:
>> +                       res =3D f(*args)
>> +                       if (os.getenv(""USE_HTML_TABLES"")):
>> +                               try:
>> +                                       makeHTMLTable(res)
>> +                               except:
>> +                                       pprint(res)
>> +                       else:
>>                                pprint(res)
>> -               else:
>> -                       pprint(res)
>> -       except TashiException, e:
>> -               print e.msg
>> -               exitCode =3D e.errno
>> -       except TypeError, e:
>> -               print e
>> -               print ""\t"" + getFunctionInfo(function)
>> -               exitCode =3D -1
>> +               except TashiException, e:
>> +                       print e.msg
>> +                       exitCode =3D e.errno
>> +               except TypeError, e:
>> +                       print e
>> +                       print ""\t"" + getFunctionInfo(function)
>> +                       exitCode =3D -1
>>        finally:
>>                client._transport.close()
>>        sys.exit(exitCode)
>>
>>
>>
>> On 1/8/09 7:34 AM, ""Ryan, Michael P"" <michael.p.ryan@intel.com>  
>> wrote:
>>
>> I did a little more reading on the finally clause and it appears =20
>> that it only started working in it's current form on python2.5.
>> I installed CentOS 5.2 and I was able to get the same error message  
>> =20=
>
>> with python 2.4.3, but once I installed 2.5.4, the messages went  
>> away.
>> Can you try running the following and report the version numbers?
>>
>> $ /usr/bin/env python
>> $ /usr/bin/python
>>
>> If either reports 2.4.3, I think you only need to change the #! at  
>> =20
>> the top of the file (client.py) to point to python2.5
>>
>> - Michael
>>
>> From: Ryan, Michael P [mailto:michael.p.ryan@intel.com]
>> Sent: Wednesday, January 07, 2009 5:54 PM
>> To: tashi-user@incubator.apache.org
>> Cc: KunWang
>> Subject: RE: [question] getting tashi run smoothly
>>
>> Those with_statement imports are unneccessary.  I just committed a  
>> =20
>> fix that removes them.  Just for reference, I am using Python 2.5.2.
>> Please try using trunk instead of the imported code (tashi/trunk =20
>> instead of tashi/import/tashi-intel-r399).
>> I'm not sure why ""finally"" would be considered a syntax error.  I'm  
>> =20=
>
>> assuming you didn't modify the code once you checked it out.
>> I'm currently downloading and installing CentOS 5 to test on and I  
>> =20
>> will let you know if I am able to reproduce the error.
>> The only other things I can think of checking are to revert to the  
>> =20
>> older version of python now that the with imports are removed and  
>> to =20=
>
>> make sure you have identical files:
>>
>> $ md5sum ./trunk/src/tashi/client/client.py ./import/tashi-intel-=20
>> r399/src/tashi/client/client.py
>> de302c54d3236e468ba7c2e82d6ceafe  ./trunk/src/tashi/client/client.py
>> 7e9eabb435e989272a7a4d620f53f6ba  ./import/tashi-intel-r399/src/=20
>> tashi/client/client.py
>>
>> - Michael
>>
>> From: KunWang [mailto:kwang@wayne.edu]
>> Sent: Tuesday, January 06, 2009 7:37 PM
>> To: Ryan, Michael P
>> Cc: tashi-user@incubator.apache.org
>> Subject: RE: [question] getting tashi run smoothly
>>
>> Hello,
>>
>> Thanks for your help.
>>
>> I am using Xen on  CentOS 5.0 .
>>
>> The default version of python is 2.4.3.
>> Following is the log when I was trying to do =93make=94 under tashi- 
>> =20=
>
>> intel-r399 directory.
>> [root@mvm tashi-intel-r399]# make
>> Building tashi.services...
>>  File ""./build.py"", line 3
>>    from __future__ import with_statement
>> SyntaxError: future feature with_statement is not defined
>> make: *** [src/tashi/services] Error 1
>>
>> Then I installed newer version of Python(Python 2.5.4)
>> The error happened in different place.
>> [root@ mvm tashi-intel-r399]# make
>> Building tashi.services...
>> Generating Python code for 'services.thrift'...
>> Copying generated code to 'tashi.services' package...
>> Generatign Python code for 'messagingthrift'...
>> Copying generated code to 'tashi.messaging.messagingthrift'  
>> package...
>> Generating client symlinks...
>>  File ""../src/tashi/client/client.py"", line 163
>>    finally:
>>          ^
>> SyntaxError: invalid syntax
>> make: *** [bin/getInstances] Error 1
>>
>> Does tashi require particular version of Python?
>> By the way, thrift is available on my machine.
>>
>> Thanks again,
>> Kun
>>
>>
>> From: Ryan, Michael P [mailto:michael.p.ryan@intel.com]
>> Sent: Monday, January 05, 2009 11:28 AM
>> To: KunWang
>> Cc: tashi-user@incubator.apache.org
>> Subject: RE: [question] get tashi run smoothly
>>
>>
>> Using migration in Tashi is dependent on using the migration =20
>> provided by the VMM underneath it.  Are you using Qemu/KVM or Xen?   
>> =20=
>
>> If you are using Qemu/KVM, you need to be aware of some of the =20
>> limitations inherent in that VMM's implementation of live migration  
>> =20=
>
>> (the disk image has to be served from the same file server, etc.).
>>
>>
>>
>> The Makefile exists to reorganize the source tree to be usable --  
>> as =20=
>
>> you stated, python is not a compiled language.  It also uses thrift  
>> =20=
>
>> to dynamically create some python code.  I suspect you would see =20
>> many errors if you didn't have thrift installed.  Thrift is another  
>> =20=
>
>> Apache Incubator project (http://incubator.apache.org/thrift/).
>>
>>
>>
>> I'm glad to see you're trying to use Tashi and I'm glad to help you  
>> =20=
>
>> however I can.  If you need more specific directions, we're working  
>> =20=
>
>> on some, but in the mean time, please send any error messages you =20
>> are getting so that I can respond to those individually.  The only  
>> =20
>> request I have is that you send questions like this to =
> tashi-user@incubator.apache.org=20
>> , which is CC'd on this message.  Thanks.
>>
>>
>>
>> - Michael
>>
>> From: KunWang [mailto:kwang@wayne.edu]
>> Sent: Friday, January 02, 2009 11:29 AM
>> To: Ryan, Michael P
>> Subject: [question] get tashi run smoothly
>> Dear Michael,
>>
>> I watched your talk on =
> http://blogs.intel.com/research/2008/10/cloud_computing_in_pittsburgh.php=20=
>
>> .
>> =46rom your talk, I find you are already  using tashi for  
>> migration. =20=
>
>> However, I encountered some problem
>> when I was trying to use tashi to establish cloud computing =20
>> environment in my lab.
>>
>> I checked out my source code from
>> http://incubator.apache.org/tashi/source_code.html
>>
>> I followed this tutorial  to use tashi
>> http://opencirrus.intel-research.net/index.php/Using_Tashi
>>
>> I found there are several  Makefiles in the source tree.  Except  
>> for =20=
>
>> the utilities, many error happened when
>> I was trying to use GNU make to =93compile=94 the source code.
>> Since Python is an interpretive language, I found the Makefile =20
>> didn=92t do too much concrete job. I can=92t find
>> enough documents on that website on installing tashi.
>>
>> I am very interested in tashi. Could you please give me some advice  
>> =20=
>
>> to get tashi run smoothly like you?
>> Thanks in advance.
>>
>> Regards,
>> Kun
>>
>>
>
>


",http://mail-archives.apache.org/mod_mbox/incubator-tashi-user/200901.mbox/%3c2DFE78A0-5ED3-45B7-8794-989F81390D38@andrew.cmu.edu%3e,Julio Lopez <jclopez+ta...@andrew.cmu.edu>,0,0
0,0,"Github user johnament commented on a diff in the pull request:      https://github.com/apache/incubator-usergrid/pull/112#discussion_r21156485        --- Diff: stack/corepersistence/queryindex/src/main/java/org/apache/usergrid/persistence/index/impl/IndexingUtils.java ---     @@ -151,69 +151,99 @@ public static XContentBuilder createDoubleStringIndexMapping(                            .startObject( type )           +                    // don't store source object in ES      --- End diff --          Wouldn't it be faster (perf wise) to use inline strings here for mapping instead of the builder?   --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well. If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA. ---",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201412.mbox/raw/%3c20141202125155.89A8C9B89F6@tyr.zones.apache.org%3e,johnament <...@git.apache.org>,0,0
92,93,"Actually it's not really possible, by design. Airflow makes it such that
your script can only define one DAG shape for a single DAG object that has
a unique corresponding `dag_id`. You cannot really express the use case
where your DAG would have a different shape over time (or given a DagRun
payload) using a single DAG object.

Allowing this would not only break the semantic of the name DAG (making a
DAG object many different DAGs based on context) and bring much more
complexity that may be hard to comprehend or simply visualize from a UI
perspective. For example, if you look at the ""tree view"", you can imagine
that such a [useful] view wouldn't work in the context of a DAG that
changes a lot from run to run.

Knowing this, here are some options around heterogenous DAGs shapes:
* For slow changing DAGs where a few tasks may be added or removed over
time, you can generally manage that by being careful around
start_date/end_date of the task, and perhaps populating some historical
states when needed (backfill with or without mark_success depending on your
use case). The typical way to approach DAG shape change can involve pausing
the DAG, setting up the right start date, altering state if/where needed,
and unpausing the DAG.
* using templates or PythonOperator, you can force tasks to not run, just
succeed based on conditions, basically skipping tasks depending on
arbitrary criteria. The DAG shape is the same, but tasks are instructed to
skip and succeed based on context
* if each run is very heterogenous across runs, we recommend that you
instantiate different ""singleton"" DAGs with a different `dag_id` using
`schedule_interval='@once'`, each dag_id is expected to run a single time
and can have a distinct shape
* for a major break in shape over time, where the shape is homogenous
before a big change, then there's a major change, then it's homogenous
again, you may want to keep the before and after DAGs around as 2 different
objects, with their respective start_date/end_date/dag_id that do no
overlap. This use either DAGs when backfilling and apply the proper logic
to the right date range.

So essentially the constraint is that a DAG is a single Directed Acyclic
Graph, not a collection or DAGs that depend on input parameter (that's
logical given the object's name). You can easily build a DAG factory as a
function that can spit out different DAG objects based on params, but it's
a constraint that each has a unique `dag_id`.

Note that it could be interesting to have the notion of a ""DAG Family"",
that could represent a set of DAG that have something in common (for
example, if they are generated from the same DAG Factory). Unfortunately
introducing a new entity (DAGFamily) may represent a significant amount of
work. It's also unclear how introducing this notion would help beyond what
we get from simple conventions like prefixing the dag_id with something
that represents the DAG family.

Max

On Tue, May 30, 2017 at 7:30 AM, Scott Halgrim <scott.halgrim@zapier.com>
wrote:

> I think so. It’s not completely clear what you want to do with those
> different tasks but you should be able to create those tasks with a factory
> method. We have a subdag whose tasks vary depending on how many tables it
> finds in our database (one task per table).
>
> Scott
>
> On May 30, 2017, 7:21 AM -0700, Leroy Julien <julien.leroy@heig-vd.ch>,
> wrote:
> > Hi,
> >
> > I would like to know if it’s possible to make a DAG with a variable
> number of tasks depending on a parameter given to the 'trigger_dag -c’
> command.
> >
> > Thanks
> > Julien
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CAHEEp7WvHYQzSEZ=389BNouKst9NriXN_WPioJem7X2CcfJ0OQ@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
190,191,"german corepages
----------------

                 Key: JSPWIKI-339
                 URL: https://issues.apache.org/jira/browse/JSPWIKI-339
             Project: JSPWiki
          Issue Type: New Feature
          Components: Localization
    Affects Versions: 2.7.x
            Reporter: Florian Holeczek
            Priority: Minor


patch for adding german corepages to the trunk

-- 
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.


",http://mail-archives.apache.org/mod_mbox/jspwiki-dev/200808.mbox/<1337707723.1218808664332.JavaMail.jira@brutus>,"""Florian Holeczek (JIRA)"" <j...@apache.org>",0,0
219,220,"I using oliophp to stress a machine (16Core) as web server with two other DB
nodes ( a master_slave cluster, master using a SATA disk while slave using a
SSD disk)
When #concurrent users scaling  from 9K 10K 11K 12K 13K 14K 15K 16K the
throughput increasing and then decreasing. It seems that there are some
bottleneck here.
*User
**Throughput(Ops)
* *14000* *1843.955*  *13000* *1849.213*  *12000* *1842.368*  *11000* *
1859.053*  *10000* *1969.393*  *9000* *1810.323* My ramp time is 300s while
steady time is 600s and the rampdown is 60s
The client start up:
    Time between starts (ms) :1
    Start simultaneously: No
    Start agents in parallel: No
See my attachment run.xml

But my profiling data shows that the CPU( Highest is about 80%~90% when
#concurrent user is 10000, softirq% is about 14% with *4tx and 4rx* queues )
/ Networks Bandwidth(70% of 1Gb) /Memory Usage/Disk are not the bottleneck.
The Apache error log is very clean with no exception and error. At the same
time I have disabled the static images serving (Just disable all *<img* tag
in the HTML)
>From the pictures in
http://docs.google.com/present/view?id=df7282nf_30x8gwmrch&autoStart=true ,
when 9K concurrent user, the response time is steady enough, when 10K, there
is pulse lasting 600sec (what happen?) and down to very small enough in the
last 300sec.
I want to know what cause the strange pulse when concurrent users reach 10K?

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200909.mbox/%3cbf99758a0909240203p7d2219e0m2c2fa43c7493c56f@mail.gmail.com%3e,Mingfan Lu <mingfan...@gmail.com>,0,0
269,270,"Yes. In-scope delimiters refers to delimiters, largely terminating delimiters, which are the
terminators and separators that are surrounding and related to a given element or model-group
in DFDL, based on DFDL's scoping rules for properties, and based on the nesting of elements
and model-groups in the schema.

In your schema, for ""field"" element, both the comma and newline are in-scope delimiters.

Some people will call this the  ""in-scope terminating markup"" but I just searched the DFDL
spec and did not find the term ""markup"" used in this way, which is good. I've never liked
referring to delimiters as ""markup"".

One clarification perhaps: if an element has a terminator and length kind delimited, The surrounding
group's separator is still considered to be in-scope and must be escaped. DFDL didn't have
to be defined this way, we could have gone with a rule where a terminator is the only in-scope
markup if specified, but that was not the decision. Even if an element has a terminator, the
enclosing model-group's separator/terminator are still considered to be in-scope.

E.g., consider this unusual example:

<sequence dfdl:terminator=""#"" dfdl:separator=""$"" dfdl:separatorPosition=""postfix"">
   <!-- we have both a terminator above, AND a postfix separator -->
   <element name=""foo"" type=""xs:string"" dfdl:terminator=""%""/> <!-- and another terminator
-->
</sequence>

For the ""foo"" element, the in-scope terminating delimiters include %, $ , and #. DFDL specifies
that the ""foo"" element must be terminated by a ""%"", but the escape-scheme rules indicate that
if the ""foo"" content contains any of %, $, or # that those characters are protected via an
escape scheme.






________________________________
From: Costello, Roger L. <costello@mitre.org>
Sent: Monday, November 18, 2019 6:52 AM
To: users@daffodil.apache.org <users@daffodil.apache.org>
Subject: Is there such a thing as ""in-scope delimiters""?


Hi Folks,



Is there such a thing as in-scope delimiters?



At the field element in the below DFDL schema, what are the in-scope delimiters? Comma and
newline?



Notice that the field element references a block escapeScheme, which specifies that the double
quote symbol is used to escape a block of text. If a field’s value is escaped (via double
quotes), then what delimiters are escaped? All in-scope delimiters – comma and newline?
 /Roger



<xs:annotation>
    <xs:appinfo source=""http://www.ogf.org/dfdl/"">
        <dfdl:defineEscapeScheme name='Quotes'>
            <dfdl:escapeScheme escapeKind='escapeBlock'
                escapeBlockStart='""'
                escapeBlockEnd='""'
                escapeEscapeCharacter='""'
                extraEscapedCharacters=''
                generateEscapeBlock='whenNeeded'/>
        </dfdl:defineEscapeScheme>
        <dfdl:format ref=""default-dfdl-properties""/>
    </xs:appinfo>
</xs:annotation>

<xs:element name=""csv"">
    <xs:complexType>
        <xs:sequence>
            <xs:sequence dfdl:separator=""%NL;"" dfdl:separatorPosition=""infix"">
                <xs:element name=""record"" maxOccurs=""unbounded"">
                    <xs:complexType>
                        <xs:sequence dfdl:separator="","" dfdl:separatorPosition=""infix"">
                            <xs:element name=""field"" maxOccurs=""unbounded"" type=""xs:string""
                                dfdl:escapeSchemeRef=""Quotes""
                                dfdl:occursCountKind=""implicit"">
                            </xs:element>
                        </xs:sequence>
                    </xs:complexType>
                </xs:element>
            </xs:sequence>
        </xs:sequence>
    </xs:complexType>
</xs:element>




",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201911.mbox/%3cSN6PR1501MB21585A529D82A88C229922D4D34D0@SN6PR1501MB2158.namprd15.prod.outlook.com%3e,"""Beckerle, Mike"" <mbecke...@tresys.com>",0,0
43,44,"
On 30/09/2011, at 4:21 AM, Eric Kolotyluk wrote:

> OK, I was finally able to get this project to build :-)

Glad to hear it, and sorry it took so long :)

> 
> 1. There seems to be a bug in the compile plugin where it is succeeding
>   when it should fail.
> 2. Trying to update AssemblyInfo.cs file is problematic with respect to
>   source control. The plugin should not update this file, rather it
>   should update one in the target directory. If there really is some
>   reason to update files that are likely under source control, then
>   the compile plugin needs to interact with the defined SCM.


Can you make sure separate issues are created for these in JIRA? I wasn't able to find an
existing report of them.

- Brett

--
Brett Porter
brett@apache.org
http://brettporter.wordpress.com/
http://au.linkedin.com/in/brettporter





",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201109.mbox/%3cEC133868-4E74-4E14-B297-44AE0064D731@apache.org%3e,Brett Porter <br...@apache.org>,1,0
181,182,"Hi,

Thank you very much for the initiative Gunnar.

I will take a look into it and starting filling up the gaps about IOTA.

On Thu, Feb 9, 2017 at 8:47 AM, Gunnar Tapper <tapper.gunnar@gmail.com>
wrote:

> Hi,
>
> I've created a first version of the iota Contributor Guide:
> https://cwiki.apache.org/confluence/display/IOTA/iota+Contributor+Guide
>
> For release 0.1, I suggest that we focus on the build chapters since being
> able to build the source is typically a requirement to make a release
> public.
>
> --
> Thanks,
>
> Gunnar
> *If you think you can you can, if you think you can't you're right.*
>



-- 
Barbara Gomes
Computer Engineer
San Jose, CA

",http://mail-archives.apache.org/mod_mbox/iota-dev/201702.mbox/<CANMFQO5ypPc0Dt9a58XGXsAQZtYUb1_hHQd7d+fYu=covp+D3A@mail.gmail.com>,Barbara Malta Gomes <barbaramaltago...@gmail.com>,0,1
303,304,"Hi Mark,

My DFDL tutorial shows how to use DFDL to process text files and binary files. My tutorial
on processing binary files shows how to use DFDL schema to process Windows EXE files, byte
by byte, bit by bit.

http://www.xfront.com/DFDL/DFDL-part2.pptx

/Roger


From: Mark Webb <elihusmails@gmail.com>
Sent: Thursday, June 25, 2020 8:20 AM
To: users@daffodil.apache.org
Subject: [EXT] looking for an example that parses data at the bit level

I'm looking for a tutorial that walks me through developing a DFDL schema that processes a
file bit-by-bit.  I haven't found anything like this and I'm hoping someone in the community
may know where I can find an example/tutorial.

Thanks,
Mark

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/202006.mbox/%3cDM6PR09MB3337D8EE12EB72F5CAB14932C8920@DM6PR09MB3337.namprd09.prod.outlook.com%3e,Roger L Costello <coste...@mitre.org>,0,1
5,5,"     [ https://issues.apache.org/jira/browse/USERGRID-1193?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Michael Russo updated USERGRID-1193: ------------------------------------     Sprint: Usergrid 37  (was: Usergrid 36)  > [SPIKE] Evaluate code to reduce GC pressure > ------------------------------------------- > >                 Key: USERGRID-1193 >                 URL: https://issues.apache.org/jira/browse/USERGRID-1193 >             Project: Usergrid >          Issue Type: Story >            Reporter: Jeffrey  >            Assignee: Mike Dunker >             Fix For: 2.1.2 > > > Review the code paths and look at areas where we can improve the code (low hanging fruit, obvious things) which reduce the GC pressure.    -- This message was sent by Atlassian JIRA (v6.3.4#6332)",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201603.mbox/%3cJIRA.12927353.1452024548000.196209.1456937478378@Atlassian.JIRA%3e,"""Michael Russo (JIRA)"" <j...@apache.org>",0,0
113,114,"We encountered the same kind of problem with the scheduler that stopped
doing its job even after rebooting. I thought changing the start date or
the state of a task instance might be to blame but I've never been able to
pinpoint the problem either.

We are using celery and docker if it helps.

Le sam. 25 mars 2017 à 01:53, Bolke de Bruin <bdbruin@gmail.com> a écrit :

> We are running *without* num runs for over a year (and never have). It is
> a very elusive issue which has not been reproducible.
>
> I like more info on this but it needs to be very elaborate even to the
> point of access to the system exposing the behavior.
>
> Bolke
>
> Sent from my iPhone
>
> > On 24 Mar 2017, at 16:04, Vijay Ramesh <vijay@change.org> wrote:
> >
> > We literally have a cron job that restarts the scheduler every 30 min.
> Num
> > runs didn't work consistently in rc4, sometimes it would restart itself
> and
> > sometimes we'd end up with a few zombie scheduler processes and things
> > would get stuck. Also running locally, without celery.
> >
> >> On Mar 24, 2017 16:02, <lrohde@quartethealth.com> wrote:
> >>
> >> We have max runs set and still hit this. Our solution is dumber:
> >> monitoring log output, and kill the scheduler if it stops emitting.
> Works
> >> like a charm.
> >>
> >>> On Mar 24, 2017, at 5:50 PM, F. Hakan Koklu <fhakan.koklu@gmail.com>
> >> wrote:
> >>>
> >>> Some solutions to this problem is restarting the scheduler frequently
> or
> >>> some sort of monitoring on the scheduler. We have set up a dag that
> pings
> >>> cronitor <https://cronitor.io/> (a dead man's snitch type of service)
> >> every
> >>> 10 minutes and the snitch pages you when the scheduler dies and does
> not
> >>> send a ping to it.
> >>>
> >>> On Fri, Mar 24, 2017 at 1:49 PM, Andrew Phillips <
> aphillips@qrmedia.com>
> >>> wrote:
> >>>
> >>>> We use celery and run into it from time to time.
> >>>>>
> >>>>
> >>>> Bang goes my theory ;-) At least, assuming it's the same underlying
> >>>> cause...
> >>>>
> >>>> Regards
> >>>>
> >>>> ap
> >>>>
> >>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CAH053HR05xfs4QegwcjsYb_Bw2k3hAWxUF4w3OzpfHNtws6i9A@mail.gmail.com>,Gael Magnan <gaelmag...@gmail.com>,0,0
288,289,"
     [ https://issues.apache.org/jira/browse/KAFKA-135?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Jun Rao updated KAFKA-135:
--------------------------

       Resolution: Fixed
    Fix Version/s: 0.7
           Status: Resolved  (was: Patch Available)

Thanks for the patch, Pierre-Yves. Just committed this.

> the ruby kafka gem is not functional
> ------------------------------------
>
>                 Key: KAFKA-135
>                 URL: https://issues.apache.org/jira/browse/KAFKA-135
>             Project: Kafka
>          Issue Type: Bug
>          Components: clients
>    Affects Versions: 0.6, 0.7
>            Reporter: Pierre-Yves Ritschard
>             Fix For: 0.7
>
>         Attachments: 0001-Fix-gem-building.patch
>
>
> The gem spec is missing a file declaration, the resulting gem is thus unusable

--
This message is automatically generated by JIRA.
For more information on JIRA, see: http://www.atlassian.com/software/jira

        

",http://mail-archives.apache.org/mod_mbox/kafka-dev/201109.mbox/<496743450.13529.1317059006665.JavaMail.tomcat@hel.zones.apache.org>,"""Jun Rao (JIRA)"" <j...@apache.org>",0,0
170,171,"Hello

On Fri, Feb 6, 2009 at 8:10 AM, Akara Sucharitakul
<Akara.Sucharitakul@sun.com> wrote:
> Hi William,
>
> What is the number of attendees added during the steady state?

That would be 64888 * 0.0948 = 6151

EventDetail (success count)	64888
% EventDetail views where attendee added	9.48

Complete report is attached!

>
> In the summary report, under miscellaneous stats, there's the ""% EventDetail
> views where attendee added"" field. Use this percentage point and calculate
> the added attendees from the success count of EventDetail (first table -
> ""Operation Mix."")
>
> Thanks,
> -Akara
>
> William Voorsluys wrote:
>>
>> Hello,
>>
>> Here are some details about a 1 hour run that shows when the bursts
>> happen.
>> The tools I ran were ""mpstat -P ALL 5; vmstat 5; iostat -x 5"".
>> However, in this for some reason the Detail Results page was not
>> generated for this run. Thus, the graph attached belongs to another
>> run, which was identical but different tools were running.
>> System 'william2' is the web server (Ubuntu server 8.04 64-bit, 2
>> CPUs, 3GB RAM).
>> System 'mysql1' runs only the database (Debian Etch 32-bit, 4 CPUS, 1GB
>> RAM).
>> Both the filestore and the database files are on NFS.
>>
>> Cheers,
>>
>> William
>>
>>
>>
>> On Wed, Feb 4, 2009 at 4:45 AM, Shanti Subramanyam
>> <Shanti.Subramanyam@sun.com> wrote:
>>>
>>> I have noticed some bursts at 15-20 mins, but I think these are specific
>>> to
>>> our deployment (nfs issues) and we're working on tuning those - we
>>> haven't
>>> identified any Olio issue that will cause this behavior (at least not yet
>>> !)
>>> I assume you're running with a local filestore ? I'd be happy to take a
>>> look
>>> at one of your run outputs (assuming you collect vmstat, iostat etc.)
>>>
>>> Shanti
>>>
>>> William Voorsluys wrote:
>>>>
>>>> Hello,
>>>>
>>>> What is the minimum time I should run Olio's PHP workload to be sure
>>>> my system really supports a certain amount of concurrent users? I've
>>>> been running for 30 minutes on steady state and all metrics pass, but
>>>> extending the run to 1 hour make things fail. It seems that at 25
>>>> minutes there's a workload burst that destabilize my system.
>>>> Is there a documentation that describes workload characteristics, such
>>>> as when bursts take place?
>>>>
>>>> Thanks,
>>>>
>>>> William
>>>>
>>
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>>      System parameters on server william2
>>
>> #
>> # /etc/sysctl.conf - Configuration file for setting system variables
>> # See sysctl.conf (5) for information.
>> #
>>
>> #kernel.domainname = example.com
>>
>> # the following stops low-level messages on console
>> kernel.printk = 4 4 1 7
>>
>> # enable /proc/$pid/maps privacy so that memory relocations are not
>> # visible to other users.  (Added in kernel 2.6.22.)
>> kernel.maps_protect = 1
>>
>> # Increase inotify availability
>> fs.inotify.max_user_watches = 524288
>>
>> # protect bottom 64k of memory from mmap to prevent NULL-dereference
>> # attacks against potential future kernel security vulnerabilities.
>> # (Added in kernel 2.6.23.)
>> vm.mmap_min_addr = 65536
>>
>> ##############################################################3
>> # Functions previously found in netbase
>> #
>>
>> # Comment the next two lines to disable Spoof protection (reverse-path
>> filter)
>> # Turn on Source Address Verification in all interfaces to
>> # prevent some spoofing attacks
>> net.ipv4.conf.default.rp_filter=1
>> net.ipv4.conf.all.rp_filter=1
>>
>> # Uncomment the next line to enable TCP/IP SYN cookies
>> # This disables TCP Window Scaling (http://lkml.org/lkml/2008/2/5/167)
>> #net.ipv4.tcp_syncookies=1
>>
>> # Uncomment the next line to enable packet forwarding for IPv4
>> #net.ipv4.ip_forward=1
>>
>> # Uncomment the next line to enable packet forwarding for IPv6
>> #net.ipv6.ip_forward=1
>>
>>
>> ###################################################################
>> # Additional settings - these settings can improve the network
>> # security of the host and prevent against some network attacks
>> # including spoofing attacks and man in the middle attacks through
>> # redirection. Some network environments, however, require that these
>> # settings are disabled so review and enable them as needed.
>> #
>> # Ignore ICMP broadcasts
>> #net/ipv4/icmp_echo_ignore_broadcasts = 1
>> #
>> # Ignore bogus ICMP errors
>> #net/ipv4/icmp_ignore_bogus_error_responses = 1
>> # # Do not accept ICMP redirects (prevent MITM attacks)
>> #net/ipv4/conf/all/accept_redirects = 0
>> # _or_
>> # Accept ICMP redirects only for gateways listed in our default
>> # gateway list (enabled by default)
>> # net/ipv4/conf/all/secure_redirects = 1
>> #
>> # Do not send ICMP redirects (we are not a router)
>> #net/ipv4/conf/all/send_redirects = 0
>> #
>> # Do not accept IP source route packets (we are not a router)
>> #net/ipv4/conf/all/accept_source_route = 0
>> #
>> # Log Martian Packets
>> #net/ipv4/conf/all/log_martians = 1
>> #
>> # Always defragment packets
>> #net/ipv4/ip_always_defrag = 1
>>
>>
>>
>>      Processor info for server william2
>>
>>
>>
>> processor       : 0
>> cpu family      : 6
>> model           : 23
>> model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz
>> cpu MHz         : 2327.498
>> cache size      : 6144 KB
>> cpu cores       : 1
>> cpuid level     : 10
>> cache_alignment : 64
>> processor       : 0
>> cpu family      : 6
>> model           : 23
>> model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz
>> cpu MHz         : 2327.498
>> cache size      : 6144 KB
>> cpu cores       : 1
>> cpuid level     : 10
>> cache_alignment : 64
>>
>>
>>
>>      Memory info for server william2
>>
>>
>>
>> MemTotal:      3139292 kB
>> MemFree:       1072572 kB
>> SwapCached:          0 kB
>> SwapTotal:      409616 kB
>> SwapFree:       409548 kB
>>
>>
>>
>>      Kernel on server william2
>>
>>
>>
>> Linux 2.6.24-23-xen #1 SMP Thu Nov 27 20:14:09 UTC 2008 x86_64
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>>
>>
>>      System parameters on server mysql1
>>
>>
>>
>> #
>> # /etc/sysctl.conf - Configuration file for setting system variables
>> # See sysctl.conf (5) for information.
>> #
>>
>> #kernel.domainname = example.com
>> #net/ipv4/icmp_echo_ignore_broadcasts=1
>>
>> # Uncomment the following to stop low-level messages on console
>> #kernel.printk = 4 4 1 7
>>
>> ##############################################################3
>> # Functions previously found in netbase
>> #
>>
>> # Uncomment the next line to enable Spoof protection (reverse-path filter)
>> #net.ipv4.conf.default.rp_filter=1
>>
>> # Uncomment the next line to enable TCP/IP SYN cookies
>> #net.ipv4.tcp_syncookies=1
>>
>> # Uncomment the next line to enable packet forwarding for IPv4
>> #net.ipv4.conf.default.forwarding=1
>>
>> # Uncomment the next line to enable packet forwarding for IPv6
>> #net.ipv6.conf.default.forwarding=1
>>
>>
>>
>>      Processor info for server mysql1
>>
>>
>>
>> processor       : 0
>> cpu family      : 6
>> model           : 23
>> model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz
>> cpu MHz         : 2327.498
>> cache size      : 6144 KB
>> cpuid level     : 10
>> processor       : 1
>> cpu family      : 6
>> model           : 23
>> model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz
>> cpu MHz         : 2327.498
>> cache size      : 6144 KB
>> cpuid level     : 10
>> processor       : 2
>> cpu family      : 6
>> model           : 23
>> model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz
>> cpu MHz         : 2327.498
>> cache size      : 6144 KB
>> cpuid level     : 10
>> processor       : 3
>> cpu family      : 6
>> model           : 23
>> model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz
>> cpu MHz         : 2327.498
>> cache size      : 6144 KB
>> cpuid level     : 10
>>
>>
>>
>>      Memory info for server mysql1
>>
>>
>>
>> MemTotal:      1048688 kB
>> MemFree:        499280 kB
>> SwapCached:          0 kB
>> SwapTotal:      522072 kB
>> SwapFree:       522072 kB
>>
>>
>>
>>      Kernel on server mysql1
>>
>>
>>
>> Linux 2.6.18.8.xs5.0.0.10.439 #1 SMP Wed Aug 6 23:55:12 UTC 2008 i686
>>
>>
>
>



-- 
William Voorsluys

williamvoor.googlepages.com

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200902.mbox/%3c4264c9500902051532j68e354c6o68bea12876fc8668@mail.gmail.com%3e,William Voorsluys <williamv...@gmail.com>,0,0
169,170,"Hi,

How to set --master, --deploy-mode, --driver-class-path and --driver-java-options through
Apache Livy?
I want to set the master, spark deploy-mode, driver-class-path and driver-java-options for
the Spark job when the job is triggered through Apache Livy without having to restart the
Livy server when these settings change. How to do this since there are no direct options to
do this in Livy?
I understand these are used to set the parameters for Client mode. I want to set these when
I spawn a job in client mode.

Also when I try to set ""spark.master"" through REST through ""conf"" param, it is not being set.
This is the JSON I'm sending to the Livy API.
{
                ""file"": ""/user/livy/spark-examples.jar"",
                ""conf"" : {
                                ""spark.master"": ""yarn"",
                                ""spark.submit.deployMode"": ""cluster""
                },
                ""args"": [""2""],
                ""className"": ""org.apache.spark.examples.SparkPi""
}

Thanks and Regards,
Sarthak

Privileged/Confidential Information may be contained in this message and is intended only
for the use of the addressee.
If you are not the addressee, or person responsible for delivering it to the addressee, you
should not copy or deliver
this to anyone else. If you receive this message by mistake, please delete the message from
any/all computer/s and
notify the sender immediately by reply Email. We appreciate your assistance in preserving
the confidentiality of our
correspondence. Any information in this message that does not relate to the official business
of the organization
shall be understood as neither given nor endorsed by it. Please advise immediately if you
or your employer
does not consent to Internet Email for messages of this kind. Thank you. (HCID0411)

",http://mail-archives.apache.org/mod_mbox/incubator-livy-user/201805.mbox/%3cF2AD9F4B093BD64C9083BFF1AAECE5E06688CA@HCAZINEXCH2%3e,Sarthak Singhal <Sarthak.Sing...@hitachiconsulting.com>,0,0
102,103,"Hey all,

The airflow meetup recording is now available here:

  https://wepayinc.app.box.com/s/hf1chwmthuet29ux2a83f5quc8o5q18k

A lot of good conversation on how Airflow is run, how different companies
are using it, future roadmap work, etc.

Cheers,
Chris

On Tue, Jun 14, 2016 at 6:39 PM, Chris Riccomini <criccomini@apache.org>
wrote:

> Hey all,
>
> The Airflow meetup tonight in SF will be streamed here:
>
> https://wepay.zoom.us/j/4152006713
>
> A recording will be posted later.
>
> Cheers,
> Chris
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CABYbY7f+AsuLA1=ViP11w13ir70HSgNGiq+XqQcPWDTgax1Efg@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,0
271,272,"
Hi All,

When will the Java Implementation of Olio will be released?

Also, will there be possibly implementations that use HBase in the future? 

I am currently studying the viability of using a completely distributed file systems (HBase
+ HDFS) for Web 2.0 applications.



Thanks,
Harold


      

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200905.mbox/%3c945489.72599.qm@web51005.mail.re2.yahoo.com%3e,Harold Lim <rold...@yahoo.com>,0,0
242,243,"Hi Folks,

If we want to describe a text data format, then we specify that the data has a text representation,
i.e.,

representation=""text""

To describe a binary data format we do this:

representation=""binary""

The ""representation"" terminology seems reasonable to me.

Next, we want to describe the fundamental (atomic) units of the representation. For the text
representation the units are characters. For the binary representation the units may be either
bytes or bits. It seems to me, that ""units"" is the appropriate term for this, i.e.,

units=""characters""
units=""bytes""
units=""bits""

Sadly, that is not the term that DFDL uses. Instead, it uses ""lengthUnits"" as the term, i.e.,

lengthUnits=""characters""
lengthUnits=""bytes""
lengthUnits=""bits""

Why? That terminology makes no sense to me. Can you give a rationale for why the term is ""lengthUnits""
and not ""units""?

/Roger

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/202001.mbox/%3cBL0PR0901MB3124C2779B7E9920D409CCF1C83C0@BL0PR0901MB3124.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
139,140,"I meant the API -- will check the wiki now. Thanks!

On Wed, Feb 8, 2017 at 8:33 AM Bolke de Bruin <bdbruin@gmail.com> wrote:

> On this proposal? No, not yet. Just popped in my mind yesterday. API there
> is a bit on the wiki.
>
> > On 8 Feb 2017, at 14:31, Jeremiah Lowin <jlowin@apache.org> wrote:
> >
> > Makes a lot of sense. At the NY meetup there was considerable interest in
> > using the API (and quite a few hacks around exposing the CLI!) -- is
> there
> > more complete documentation anywhere?
> >
> > Thanks Bolke
> >
> > On Wed, Feb 8, 2017 at 1:36 AM Bolke de Bruin <bdbruin@gmail.com> wrote:
> >
> >> Hi All,
> >>
> >> Now that we have an API in place. I would like to propose a new state
> for
> >> tasks named “WAITING_ON_CALLBACK”. Currently, we have tasks that have a
> >> kind of polling mechanism (ie. Sensors) that wait for an action to
> happen
> >> and check if that action happened by regularly polling a particular
> >> backend. This will always use a slot from one of the workers and could
> >> starve an airflow cluster for resources. What if a callback to Airflow
> >> could happen that task to change its status by calling a callback
> mechanism
> >> without taking up a worker slot. A timeout could (should) be associated
> >> with the required callback so that the task can fail if required. So a
> bit
> >> more visual:
> >>
> >>
> >> Task X from DAG Z  does some work and sets “WAITING_ON_CALLBACK” -> API
> >> post to /dags/Z/dag_runs/20170101T00:00:00/tasks/X with payload “set
> status
> >> to SUCCESS”
> >>
> >> DAG Z happily continues.
> >>
> >> Or
> >>
> >> Task X from DAG Z sets “WAITING_ON_CALLBACK” with timeout of 300s ->
> time
> >> passes -> scheduler sets task to FAILED.
> >>
> >>
> >> Any thoughts?
> >>
> >> - Bolke
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CADsgxrGOa=cs8QwT2m8M_gi4V0ybFztDAq=fVbg1uC_V12Dn1Q@mail.gmail.com>,Jeremiah Lowin <jlo...@apache.org>,0,0
90,91,"
----- ""Christoph Karner"" <lechris48@yahoo.de> wrote:

> > --
> > Joe Ocaba
> > ----- ""Christoph Karner"" de> wrote:
> >
> > > hello!
> > >
> > > im experimenting with npanday 1.3-incubating and ran into a few
> > > problems:
> > >  * the visual studio add-in starts succesfully, yet there are no
> > > npanday entries
> > > in the solution/project context menu
> > > (add-in installed with installer)
> > >  * when i try to create a project from command line, maven doesnt
> seem
> > > to find
> > > the archetype
> > >     as it fails complaining that there is no pom.xml in the
> directory
> > > (the
> > > repository is in my .m2 though)
> > >
> > > my configuration
> > > maven 3
> > > visual studio 2010 (german)
> >
> > thanks for taking the time in testing out 1.3 Chris. Unfortunately
> at the
> >moment NPanday only supports Visual Studios English Version, there
> are already
> >issues created for German support as well as Italian and Spanish. We
> do not have
> >
> >enough contributors that know the language. We would gladly accept
> patches for
> >this so that NPanday can work on other languages as well.
> >
> 
> 
> thanks for your help!
> 
> i thought it would be something like that, did not find any issues
> though.
> what would i have to do to add german language support myself?


You would need to build the source code and in \dotnet\assemblies\NPanday.VisualStudio.Addin\Connect.cs
you need to modify launchNPandayBuildSystem function there is a loop that checks for the specific
Control Caption in the running VS
 i.e. if (control.Caption.Equals(""C&onfiguration Manager...""))

you can include the german equivalent for this. 


> > for the creating of the project what command did you use?
> 
> i used
> 
> 
>   mvn archetype:generate
> -DarchetypeArtifactId=maven-archetype-dotnet-simple
> -DarchetypeGroupId=npanday
> -DarchetypeVersion=1.3-incubating
> 
> 
> as stated in ""Creating a simple project"". i dont see anything wrong
> with it.
> also tested it on my linux machine by now and it worked like a
> charm...
> the machine where it doesnt work runs windows xp by the way.
> also i never used maven on windows before, so it could be that im
> overlooking
> something.

NPanday was originally developed under the windows XP OS, so it should work fine. 
You can check out the docs here http://incubator.apache.org/npanday/docs/1.3-incubating/index.html
 
> regards,chris

",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201102.mbox/%3c648435296.12288.1296689669367.JavaMail.root@cell.g2ix.net%3e,Josimpson Ocaba <joc...@maestrodev.com>,0,1
58,59,"Hi Max,

Sounds good. Couple of things: 

* Can I suggest using the v1-8-test branch as the branch to be used for preparing the rc?
If we hit RC then move it over to v1-8-stable? V1-8-test already had some fixes in that should
land in 1.8.2 and the RC should be tagged in the stable branch. That also reduces to amount
of merge conflicts probably as many have been merged. Where did you branch off from? Anyways,
see also the release management thing on the wiki.

Blocker(!)
* In the backfills we can loose tasks to execute due to a task setting its own state to NONE
if concurrency limits are reached, this makes them fall outside of the scope the backfill
is managing hence they will not be executed (https://issues.apache.org/jira/browse/AIRFLOW-1294
<https://issues.apache.org/jira/browse/AIRFLOW-1294>). Setting itself to NONE should
probably be “CONCURRENCY_REACHED” (new state). I have marked it as a blocker as we had
multiple people hitting the issue, but I need 1-2 days to get a patch. Feel free to downgrade
to critical if you like :).

Cheers
Bolke


> On 8 Jun 2017, at 02:35, Maxime Beauchemin <maximebeauchemin@gmail.com> wrote:
> 
> What a pleasant, mind numbing afternoon doing some release management
> 
> Notes:
> * Added a warning that the package name has changed on Pypi
> <https://pypi.python.org/pypi/airflow>
> * Removed references to my name here
> <https://github.com/apache/incubator-airflow/pull/2352> and merged
> * Addressed John D. Ament's concerns here
> <https://github.com/apache/incubator-airflow/pull/2354>, please review!
> * ""footable"" appears to have been removed, not a problem anymore
> * that `airflow-jira is a god send! thanks Bolke.
> * reviewed list of Airbnb's production cherries and flagged those as `Fix
> Version == 1.8.2`
> * Started branch v1-8-2.rc1 and started picking cherries using
> `airflow-jira compare 1.8.2`
> 
> I'll finish going through picking everything that targeted 1.8.2 that does
> not create merge conflict.
> 
> If there's anything flagged as ""blocker"" that generates merge conflict,
> I'll go case by case about it.
> 
> Soon after, I should be able to announce 1.8.2 RC1, hopefully sometime
> tomorrow or Friday.
> 
> Let me know if there's anything else I'm missing that I should consider.
> 
> Cheers!
> 
> Max


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201706.mbox/<05DAC1DE-5622-4AE1-85B1-F1C8C561E0D5@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,1
155,156,"The link in the file RELEASE_NOTES.txt - http://incubator.apache.org/hcatalog/docs/r0.1.0/install.html
doesn't work. I guess this will start working at/after the release..

Other than that +1

(Downloaded the tar ball, and ran unit tests).

On Jul 12, 2011, at 5:05 PM, Ashutosh Chauhan wrote:

>  Hi,
> 
>    I have created a candidate build for HCatalog 0.1.0-incubating.
> This is the initial release of HCatalog.
> 
>    Keys used to sign the release are available at
> http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.1/KEYS?revision=1145809&view=markup.
> 
>    Please download, test, and try it out:
> 
>    http://people.apache.org/~hashutosh/hcatalog-0.1.0-incubating-candidate-1/
> 
>    The release, md5 signature, gpg signature, and rat report can all
> be found at the above address.
> 
>    Should we release this? Vote closes on Thursday, July 15th.
> 
>    Thanks,
>   Ashutosh


",http://mail-archives.apache.org/mod_mbox/incubator-hcatalog-user/201107.mbox/%3cCF6818F4-4825-4CB2-9E3B-E9AA0791FEBF@hortonworks.com%3e,Devaraj Das <d...@hortonworks.com>,1,1
249,250,"Ideally the CLI and WebUI should both access an API that handles authentication and authorization.
This would resolve both issues. However, the UI already allows for authentication and to a
lesser extent authorization. Thus allowing this from the UI (which we already do for Celery)
is not a big change.

- Bolke


> Op 7 jul. 2016, om 11:01 heeft Alexander Alten-Lorenz <wget.null@gmail.com> het
volgende geschreven:
> 
> Sounds good, but on the other hand I'm with Maxime. Given that the task can be triggered
per CLI, the functionality is available but needs a local login. When the ""run"" button now
would be available for everyone who has access to the UI, I can imagine that would cause some
serious load issues in a production environment, especially with SLA based workflow setups.

> On the other hand, when the ""run"" button with a local executor would queue the task in
a control queue (like ""external triggered"") a admin could finally mark them as ""approved"".

> 
> --alex
> 
>> On Jul 7, 2016, at 12:12 AM, Jeremiah Lowin <jlowin@apache.org> wrote:
>> 
>> Perhaps it's a good chance to revisit the functionality. Right now the UI
>> ""run"" button actually runs the task via CeleryExecutor. Perhaps instead (or
>> just when using a non-Celery executor) it should queue the task and let the
>> Scheduler pick it up. I guess in that case it would just be sugar for
>> marking a TI as QUEUED. Just a thought.
>> 
>> On Wed, Jul 6, 2016 at 2:54 AM Maxime Beauchemin <maximebeauchemin@gmail.com>
>> wrote:
>> 
>>> Hi,
>>> 
>>> The problem is that a web server isn't the right place to run an airflow
>>> task. From the context of the web request scope we have to somehow pass a
>>> message to an external executor to run the task. For LocalExecutor to work
>>> the web server would have to start a LocalExecutor as a sub process and
>>> that doesn't sound like a great idea...
>>> 
>>> Max
>>> 
>>> On Tue, Jul 5, 2016 at 11:22 AM, Jason Chen <chingchien.chen@gmail.com>
>>> wrote:
>>> 
>>>> Hi Airflow team,
>>>> I am using the ""LocalExecutor"" and it works very well to run the
>>> workflow
>>>> I setup.
>>>> 
>>>> I noticed that, from the UI, it can trigger a task to run.
>>>> However, I got the error ""Only works with the CeleryExecutor, sorry "".
>>>> I can ssh into airflow node and run the command line from there.
>>>> However, it would be nice to just run it from airflow UI.
>>>> Is it possible to do that (with ""LocalExecutor"") or it's a future feature
>>>> to consider ?
>>>> 
>>>> Thanks.
>>>> Jason
>>>> 
>>> 
> 


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201607.mbox/<F35CF8E9-E86E-42DC-B757-D5E52975BB2B@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,0
57,58,"I just wrote an answer on stack overflow.

Thanks,

Max

On Wed, Sep 21, 2016 at 3:47 AM, Jiacai Liu <jiacai2050@gmail.com> wrote:

> http://stackoverflow.com/questions/39612488/airflow-
> triggle-dag-execution-date-is-the-next-day-why
>
>
> Don't know it is right to ask question in this email list, If I go wrong,
> please let me know.
>
> Thanks.
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201609.mbox/<CAHEEp7XKGGvsXL84426PARWXvJzmSp6nU6M-YbCMEFjB0cJjWQ@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
245,246,"Benj,

RequestContext.getCurrentInstance().addPartialTarget(component);

-----Original Message-----
From: Benj Fayle [mailto:bfayle@maketechnologies.com]
Sent: Monday, April 02, 2007 2:16 PM
To: adffaces-user@incubator.apache.org
Subject: RE: PPR Issue


In ADF you could do force partial page rendering from your backing bean
programmatically use:

AdfFacesContext.getCurrentInstance().addPartialTarget(getContentPanel())
;

In this case getContentPanel() was a control reference in the backing
bean that had a binding from the page.

I'm not sure what the equivalent Trinidad class is to AdfFacesContext.

Benj

-----Original Message-----
From: Chris Gibbons [mailto:cgibbons@solutionstream.com] 
Sent: Friday, March 30, 2007 3:33 PM
To: adffaces-user@incubator.apache.org
Subject: PPR Issue

Hi,

 

   I have a page that has two text boxes and a string of text.  The text
string needs to reflect the values from the inputText boxes.  I have
autoSubmit=true, and have partialListeners on my outputText set to id's
of the inputText boxes, and an update does trigger.  When I debug into
my code, the value that is backing the outputText does get updated, but
the text on the webpage doesn't update with this new value.  Now what?
I'm stuck, and getting rather frustrated with PPR in Trinidad, any and
all help would be greatly appreciated.

 

Chris



",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200704.mbox/%3c5B930A45C354E84395A194D500589ED80DBE3A@jaxmsx02.nemours.org%3e,"""William Hoover"" <whoo...@nemours.org>",0,0
107,108,"On Tue, Dec 31, 2013 at 12:24 AM, Naresh Yadav <nyadav.ait@gmail.com> wrote:
> Hi tim,
>
> list of tags is not small, can be really big so i cannot use negate the
> tags approach...Other approach you said is using subfields how to do that
> in blur...My thought on this was to introduce new column Tags which will
> store sorted all tags of that row...So for cases where i need exact match
> then will query on *Tags* column and case where i need partial match of
> tags then will use Tag column..

Your approach is largely the same as the subcolumn approach.
Subcolumns would just allow you to do it without storing the original
value multiple times.  I actually haven't used them, but I reckon it'd
look something like:

ColumnDefinition tags = new ColumnDefinition(""fam"",""tag"",null, true,
""text"",null);
ColumnDefinition tagsExact = new
ColumnDefinition(""fam"",""tag"",""exact"",""false,""string"",null);

and querying:
A) fam.tag:Tag1

B) fam.tag.exact:Tag1

Thanks,
--tim

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201401.mbox/%3cCAG_bHoxXCzed0WWKYeDa9=603u5kRJnFyrKu=5X=4eeyDn6g4w@mail.gmail.com%3e,Tim Williams <william...@gmail.com>,1,0
198,199,"The specification says this about ES:

Used in whitespace separated lists when empty string is one of the values.

Recall that in XML an attribute's value may be delimited by either double or single quotes.

So, are these two whitespace-separated lists equivalent?

                ""A B %ES; C D""
                'A B """" C D'

/Roger

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201909.mbox/%3cBL0PR0901MB3124CFD1F5F5505FCE195DBEC88C0@BL0PR0901MB3124.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
207,208,"
    [ https://issues.apache.org/jira/browse/PARQUET-124?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14193313#comment-14193313
] 

Chris Albright edited comment on PARQUET-124 at 11/1/14 6:35 PM:
-----------------------------------------------------------------

[~rdblue], I was not able to find a way compare to the path prefixes without using String#startsWith.
It does seem very unlikely that there would ever exist a situation where the footers come
from a different filesystem than the root, since the footers are read out of the root to begin
with :) 

I thought of comparing the scheme as well, but I was not able to make that work since the
scheme was missing from the root path URI. If its important, I've got some other ideas, but
I'm not sure its within scope of this issue. I'll go ahead and create a new one and work off
of that.

Pull request is updated, and passing in Travis.


was (Author: chrisalbright):
[~rdblue], I was not able to find a way to the path prefixes without converting using String#startsWith.
It does seem very unlikely that there would ever exist a situation where the footers come
from a different filesystem than the root, since the footers are read out of the root to begin
with :) 

I thought of comparing the scheme as well, but I was not able to make that work since the
scheme was missing from the root path URI. If its important, I've got some other ideas, but
I'm not sure its within scope of this issue. I'll go ahead and create a new one and work off
of that.

Pull request is updated, and passing in Travis.

> parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException
> --------------------------------------------------------------------------------------------
>
>                 Key: PARQUET-124
>                 URL: https://issues.apache.org/jira/browse/PARQUET-124
>             Project: Parquet
>          Issue Type: Bug
>          Components: parquet-mr
>    Affects Versions: parquet-mr_1.6.0, 1.6.0rc2
>            Reporter: Chris Albright
>            Priority: Minor
>         Attachments: PARQUET-124-test
>
>
> I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example),
and in the process of updating the library versions, am getting the warning below.
> The version of Parquet-Hadoop in the original example is 1.0.0. I am using 1.6.0rc3
> The ParquetFileWriter.mergeFooters(Path, List<Footer>) method is performing a check
to ensure the footers are all for files in the output directory. The output directory is supplied
by ParquetFileWriter.writeMetadataFile; in 1.0.0, the output path was converted to a fully
qualified output path before the call to mergeFooters, but in 1.6.0rc[2,3] that conversion
happens after the call to mergeFooters. Because of this, the check within merge footers is
failing (the URI for the footers starts with file:, but not the URI for the root path does
not)
> Here is the warning message and stacktrace.
> Oct 30, 2014 9:11:31 PM WARNING: parquet.hadoop.ParquetOutputCommitter: could not write
summary file for /tmp/1414728690018-0/output
> parquet.io.ParquetEncodingException: file:/tmp/1414728690018-0/output/part-r-00000.parquet
invalid: all the files must be contained in the root /tmp/1414728690018-0/output
> 	at parquet.hadoop.ParquetFileWriter.mergeFooters(ParquetFileWriter.java:422)
> 	at parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:398)
> 	at parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:50)
> 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:936)
> 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:832)
> 	at com.zenfractal.SparkParquetExample$.main(SparkParquetExample.scala:72)
> 	at com.zenfractal.SparkParquetExample.main(SparkParquetExample.scala)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",http://mail-archives.apache.org/mod_mbox/parquet-dev/201411.mbox/<JIRA.12751879.1414730028000.391263.1414866993634@Atlassian.JIRA>,"""Chris Albright (JIRA)"" <j...@apache.org>",0,0
27,28,"Simon/Gary,
We can inherit from any skin in Trinidad already.
You use the <extends> element in the trinidad-skins.xml file and the 
value that goes in there is the skin-id of the skin
you want to extend..
I'll add a demo if there isn't one already.
Thanks,
Jeanne

Simon Lessard wrote:
> Hello Gary
>
> We plan to add inheritance from any skin, not just from simple in the
> future, so you'll be able to override just what you want.
>
>
> Regards,
>
> ~ Simon
>
> On 3/30/07, Matt Cooper <matt.faces@gmail.com> wrote:
>>
>> It would be very cool to have a page or set of pages in the demo project
>> that served as a skin generator where the user could create a skin 
>> using a
>> GUI and not have to know anything about CSS.  If anyone is looking for a
>> project, this would be a very cool one to donate.  ;-)
>>
>> On 3/30/07, Gary VanMatre <gvanmatre@comcast.net> wrote:
>> >
>> > >From: Jeanne Waldman <jeanne.waldman@oracle.com>
>> > >
>> > > You should be able to create an html file from the 
>> skin-selectors.xml
>> > > file by running
>> > > 'mvn site'.
>> > > - Jeanne
>> > >
>> >
>> > I really like this global skinning concept but it would be nice if 
>> there
>> > was a way to just override a few things in a skin. From what I
>> understand,
>> > it's a all of nothing type of approach (speaking from the ADF
>> 10.1.3.2side) and only an option for the simple skin. I'd like to be 
>> able
>> to pick an
>> > existing skin but only override a couple of things.
>> >
>> >
>> > This would be very handy for the CSS challenged. Is this possible in
>> > Trinidad?
>> >
>> >
>> > Gary
>> >
>> >
>> > > Simon Lessard wrote:
>> > > > There's two places:
>> > > >
>> > > > For the full list, if you downloaded the source files, in
>> > > > /trinidad/src/site/xdoc/skin-selectors.xml
>> > > > For the old list (from ADF Faces):
>> > > >
>> > >
>> >
>> http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/j

>>
>> > > sf/doc/skin-selectors.html
>> > > >
>> > > >
>> > > >
>> > > > Regards,
>> > > >
>> > > > ~ Simon
>> > > >
>> > > > On 3/30/07, Chris Hane wrote:
>> > > >>
>> > > >>
>> > > >> I've started to edit a custom skin for Trinidad. I thought I saw
>> > > >> somewhere a tag list for each component. And of course I can't

>> find
>> > it
>> > > >> now that I'm looking for it.
>> > > >>
>> > > >> Could someone point me to the page that lists the elements for

>> each
>> > > >> component that can be skinned?
>> > > >>
>> > > >> Thanks,
>> > > >> Chris....
>> > > >>
>> > > >
>>
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200704.mbox/%3c4611326C.50905@oracle.com%3e,Jeanne Waldman <jeanne.wald...@oracle.com>,0,0
174,175,"Hi Steve,

I see that my defaults.dfdl.xsd file has these:

fillByte=""f""
textPadKind=""none""

Will those produce the behavior you describe?

/Roger


-----Original Message-----
From: Steve Lawrence <slawrence@apache.org> 
Sent: Tuesday, June 25, 2019 2:34 PM
To: users@daffodil.apache.org
Subject: [EXT] Re: 0,100 --> parse --> 100 --> unparse --> 100ff ... Huh?

There are no defaults value for these properties. And actually, dfdl:textNumberPadKind isn't
a thing, it should be dfdl:textPadKind. But if textPadKind=""none"" then no padding will be
added.

But another option is that dfdl:fillByte is being used to fill those extra bytes. Perhaps
dfdl:fillByte=""f""?

- Steve

On 6/25/19 2:28 PM, Costello, Roger L. wrote:
> Hi Steve,
> 
>>  I would guess that you have: 
>>  textNumberPadKind=""padChar"",
>>  textNumberPadCharacter=""f"", and
>>  textNumberJustification=""left""
> 
> Actually, I looked at my defaults.dfdl.xsd file and it doesn't mention any of those properties.
If those properties are not specified, do they default to the values you list?
> 
> /Roger
> 
> -----Original Message-----
> From: Steve Lawrence <slawrence@apache.org>
> Sent: Tuesday, June 25, 2019 2:14 PM
> To: users@daffodil.apache.org
> Subject: [EXT] Re: 0,100 --> parse --> 100 --> 100ff ... Huh?
> 
> This is a good example of the difference between # and 0 pattern characters when unparsing.
> 
> With the pattern ""0,000"", the value ""100"" will be padded with zero's and so will unparse
to ""0,100"", which matches the expected length of 5.
> 
> However, with the pattern ""#,###"", the value ""100"" will unparse to 
> ""100""--no comma is needed and it will not zero pad. But your test1 
> element is defined as having a length of 5 and the the unparsed value 
> has a length of 3. In this case, Daffodil uses textNumberPadKind and 
> related properties (textNumberPadCharacter, textNumberJustification,
> etc.) to pad the unparsed value up to the needed 5 characters.
> 
> So I would guess that you have textNumberPadKind=""padChar"", textNumberPadCharacter=""f"",
and textNumberJustification=""left"". Those three properties will cause Daffodil to add extra
""f"" characters as padding to the right of the string.
> 
> I don't think I would say to never use the '#' character. There are certainly going to
be times where you don't want extra padding characters, like in some delimited formats where
numbers do not have explicit lengths.
> 
> - Steve
> 
> On 6/25/19 1:32 PM, Costello, Roger L. wrote:
>> Hello DFDL community,
>>
>> My input file has this:
>>
>> 0,100
>>
>> 0,100
>>
>> My DFDL schema is this:
>>
>> <xs:elementname=""input"">
>> <xs:complexType>
>> <xs:sequencedfdl:separator=""%NL;""dfdl:separatorPosition=""infix"">
>> <xs:elementname=""test1""type=""xs:unsignedInt""
>>                  dfdl:length=""5""dfdl:lengthKind=""explicit""
>>                  dfdl:textNumberCheckPolicy=""strict""
>>                  dfdl:textNumberPattern=""#,###""/> 
>> <xs:elementname=""test2""type=""xs:unsignedInt""
>>                  dfdl:length=""5""dfdl:lengthKind=""explicit""
>>                  dfdl:textNumberCheckPolicy=""strict""
>>                  dfdl:textNumberPattern=""0,000""/> </xs:sequence> 
>> </xs:complexType> </xs:element>
>>
>> The output of parsing is this:
>>
>> <input>
>> <test1>100</test1>
>> <test2>100</test2>
>> </input>
>>
>> The output of unparsing is this:
>>
>> 100ff
>> 0,100
>>
>> Huh?
>>
>> Why am I getting 100ff?
>>
>> I think the lesson learned is never use the pound (#) symbol in 
>> dfdl:textNumberPattern. Do you agree?
>>
>> /Roger
>>
> 


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201906.mbox/%3cCH2PR09MB40230C6AF568F2A7D7E64598C8E30@CH2PR09MB4023.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
114,115,"Chad Huneycutt wrote:
> I am trying to get started with tashi following the ""setting tashi up 
> on a single test machine"" instructions, but the nodemanager is failing 
> to start complaining that it is unable to load VM info from 
> /var/tmp/nm.dat.  How does that file get created and initialized?
>
Hi Chad,

It gets initialized when the nodemanager starts up. I've changed the 
code in my test/deployment repository to not throw the exception, but I 
haven't committed it back to svn yet.

Can you make sure that, even though it threw an exception, it is in fact 
running?

Greetings,
Michael.

",http://mail-archives.apache.org/mod_mbox/incubator-tashi-user/201103.mbox/%3c4D7A54B6.9020106@cmu.edu%3e,Michael Stroucken <...@cmu.edu>,0,0
21,22,"See <https://builds.apache.org/job/provisionr-master/93/>  ------------------------------------------ [...truncated 4846 lines...] INFO: ------------------------------------------------------------------------ Aug 22, 2013 1:58:20 AM org.apache.maven.cli.event.ExecutionEventLogger logStats INFO: Total time: 3:31.988s Aug 22, 2013 1:58:20 AM org.apache.maven.cli.event.ExecutionEventLogger logStats INFO: Finished at: Thu Aug 22 01:58:20 UTC 2013 Aug 22, 2013 1:58:20 AM org.apache.maven.cli.event.ExecutionEventLogger logStats INFO: Final Memory: 55M/297M [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-rundeck/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT.pomAug 22, 2013 1:58:20 AM org.apache.maven.cli.event.ExecutionEventLogger sessionEnded INFO: ------------------------------------------------------------------------  [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/target/provisionr-rundeck-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-rundeck/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/target/provisionr-rundeck-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-rundeck/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/target/classes/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-rundeck/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon-tests/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon-tests/target/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon-tests/target/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-console/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/provisionr-console-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-console/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/provisionr-console-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-console/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/classes/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-console/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/classes/org.apache.provisionr.console.cfg> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-console/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack-tests/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack-tests/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack-tests/target/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack-tests/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack-tests/target/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack-tests/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly-tests/0.5.0-incubating-SNAPSHOT/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/target/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly-tests/0.5.0-incubating-SNAPSHOT/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/target/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly-tests/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly-tests/0.5.0-incubating-SNAPSHOT/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/branding/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-branding/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-branding/0.5.0-incubating-SNAPSHOT/provisionr-branding-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/branding/target/provisionr-branding-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-branding/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-branding/0.5.0-incubating-SNAPSHOT/provisionr-branding-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/branding/target/provisionr-branding-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-branding/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-branding/0.5.0-incubating-SNAPSHOT/provisionr-branding-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-core/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/provisionr-core-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-core/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/provisionr-core-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-core/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/classes/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-core/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/classes/org.apache.provisionr.cfg> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-core/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/classes/org.apache.felix.fileinstall-templates.cfg> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-core/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-fileinstall.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/commands/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-commands/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-commands/0.5.0-incubating-SNAPSHOT/provisionr-commands-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/commands/target/provisionr-commands-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-commands/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-commands/0.5.0-incubating-SNAPSHOT/provisionr-commands-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/commands/target/provisionr-commands-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-commands/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-commands/0.5.0-incubating-SNAPSHOT/provisionr-commands-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/target/provisionr-assembly-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/target/provisionr-assembly-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/target/provisionr-0.5.0-incubating-SNAPSHOT.tar.gz> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-assembly/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT.tar.gz [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/commands/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-karaf-commands/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/commands/target/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-karaf-commands/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/commands/target/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-karaf-commands/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/provisionr-examples-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/provisionr-examples-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.prov
sionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/org/apache/provisionr/examples/templates/cdh3.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-cdh3.template [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/org/apache/provisionr/examples/templates/cdh4.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-cdh4.template [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/org/apache/provisionr/examples/templates/jenkins.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-examples/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-jenkins.template [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/explorer/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-karaf-web-explorer/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/explorer/target/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.war> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-karaf-web-explorer/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.war [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/explorer/target/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-karaf-web-explorer/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/classes/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/classes/org.apache.provisionr.cloudstack.cfg> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-cloudstack/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/api/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-api/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-api/0.5.0-incubating-SNAPSHOT/provisionr-api-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/api/target/provisionr-api-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-api/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-api/0.5.0-incubating-SNAPSHOT/provisionr-api-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/api/target/provisionr-api-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-api/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-api/0.5.0-incubating-SNAPSHOT/provisionr-api-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/database/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-database/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/database/target/activiti-database-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-database/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/database/target/activiti-database-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$activiti-database/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-aggregator/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/provisionr-aggregator-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/provisionr-amazon-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/provisionr-amazon-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/classes/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/classes/org.apache.provisionr.amazon.cfg> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-amazon/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-test-support/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/target/provisionr-test-support-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-test-support/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/target/provisionr-test-support-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-test-support/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/target/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-test-support/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-features/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/target/provisionr-features-0.5.0-incubating-SNAPSHOT.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-features/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/target/provisionr-features-0.5.0-incubating-SNAPSHOT-sources.jar> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-features/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/target/features.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-features/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/parent/pom.xml> to /home/hudson/hudson/jobs/provisionr-master/modules/org.apache.provisionr$provisionr-parent/builds/2013-08-22_01-54-44/archive/org.apache.provisionr/provisionr-parent/0.5.0-incubating-SNAPSHOT/provisionr-parent-0.5.0-incubating-SNAPSHOT.pom Waiting for Jenkins to finish collecting data channel stopped Archiving artifacts Maven RedeployPublisher use remote ubuntu3 maven settings from : /home/jenkins/.m2/settings.xml [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact activiti-database-0.5.0-incubating-SNAPSHOT.jar Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 0.3 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20130822.020138-43.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20130822.020138-43.jar (539 KB at 2603.1 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20130822.020138-43.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20130822.020138-43.pom (3 KB at 26.2 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml (309 B at 1.6 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 10.1 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml (309 B at 2.1 KB/sec) Deploying the main artifact activiti-database-0.5.0-incubating-SNAPSHOT-sources.jar Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20130822.020138-43-sources.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20130822.020138-43-sources.jar (6 KB at 82.6 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 9.4 KB/sec) [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.jar Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 1.6 KB/sec) Uploading: https://reposi
ory.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20130822.020145-43.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20130822.020145-43.jar (48 KB at 394.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20130822.020145-43.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20130822.020145-43.pom (7 KB at 73.6 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml (315 B at 4.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 8.7 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml (315 B at 2.2 KB/sec) Deploying the main artifact activiti-karaf-commands-0.5.0-incubating-SNAPSHOT-sources.jar Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20130822.020145-43-sources.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20130822.020145-43-sources.jar (35 KB at 240.6 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 10.5 KB/sec) [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.war Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 3.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-20130822.020147-42.war Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-20130822.020147-42.war (1884 KB at 330.5 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-20130822.020147-42.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-20130822.020147-42.pom (15 KB at 84.1 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/maven-metadata.xml (319 B at 1.9 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 7.6 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/maven-metadata.xml (319 B at 2.2 KB/sec) Deploying the main artifact activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT-sources.jar Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-20130822.020147-42-sources.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-20130822.020147-42-sources.jar (5 KB at 48.5 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 12.3 KB/sec) [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact provisionr-aggregator-0.5.0-incubating-SNAPSHOT.pom Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (643 B at 2.9 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/provisionr-aggregator-0.5.0-incubating-20130822.020203-42.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/provisionr-aggregator-0.5.0-incubating-20130822.020203-42.pom (4 KB at 38.0 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/maven-metadata.xml (313 B at 1.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (643 B at 3.7 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-aggregator/maven-metadata.xml (313 B at 1.3 KB/sec) [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact provisionr-amazon-0.5.0-incubating-SNAPSHOT.jar Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 5.7 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42.jar (92 KB at 126.3 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42.pom (9 KB at 67.9 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/maven-metadata.xml (309 B at 2.8 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 7.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/maven-metadata.xml (309 B at 2.4 KB/sec) Deploying the main artifact provisionr-amazon-0.5.0-incubating-SNAPSHOT-sources.jar Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42-sources.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42-sources.jar (70 KB at 534.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 10.1 KB/sec) Deploying the main artifact provisionr-amazon-0.5.0-incubating-SNAPSHOT-features.xml Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42-features.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42-features.xml (2 KB at 18.9 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 9.9 KB/sec) Deploying the main artifact provisionr-amazon-0.5.0-incubating-SNAPSHOT-defaults.cfg Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42-defaults.cfg Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-20130822.020213-42-defaults.cfg (905 B at 8.5 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 11.8 KB/sec) [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.jar Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 0.1 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-20130822.020225-42.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-20130822.020225-42.jar (13 KB at 128.9 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-20130822.020225-42.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-20130822.020225-42.pom (5 KB at 37.3 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/maven-metadata.xml (437 B at 0.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/provisionr-amazon-tests/maven-metadata.xml (437 B at 1.0 KB/sec) ERROR: Failed to deploy metadata: Could not transfer metadata org.apache.provisionr:provisionr-amazon-tests:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond org.apache.maven.artifact.deployer.ArtifactDeploymentException: Failed to deploy metadata: Could not transfer metadata org.apache.provisionr:provisionr-amazon-tests:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond 	at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:143) 	at hudson.maven.reporters.MavenArtifactRecord.deploy(MavenArtifactRecord.java:190) 	at hudson.maven.RedeployPublisher.perform(RedeployPublisher.java:176) 	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:782) 	at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:754) 	at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.post2(MavenModuleSetBuild.java:1007) 	at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:707) 	at hudson.model.Run.execute(Run.java:1628) 	at hudson.maven
MavenModuleSetBuild.run(MavenModuleSetBuild.java:506) 	at hudson.model.ResourceController.execute(ResourceController.java:88) 	at hudson.model.Executor.run(Executor.java:247) Caused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy metadata: Could not transfer metadata org.apache.provisionr:provisionr-amazon-tests:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond 	at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:372) 	at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:269) 	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy(DefaultRepositorySystem.java:413) 	at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:139) 	... 11 more Caused by: org.eclipse.aether.transfer.MetadataTransferException: Could not transfer metadata org.apache.provisionr:provisionr-amazon-tests:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$5.wrap(WagonRepositoryConnector.java:995) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$5.wrap(WagonRepositoryConnector.java:983) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$PutTask.run(WagonRepositoryConnector.java:895) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector.put(WagonRepositoryConnector.java:530) 	at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:366) 	... 14 more Caused by: org.apache.maven.wagon.TransferFailedException: The target server failed to respond 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:580) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:524) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:505) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:485) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$PutTask.run(WagonRepositoryConnector.java:871) 	... 16 more Caused by: org.apache.http.NoHttpResponseException: The target server failed to respond 	at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:95) 	at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:62) 	at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:254) 	at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:289) 	at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:252) 	at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:191) 	at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:300) 	at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:127) 	at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:715) 	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:520) 	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906) 	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:746) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:574) 	... 20 more [INFO] Deployment failed after 1 min 7 sec Build step 'Deploy artifacts to Maven repository' changed build result to FAILURE ",https://mail-archives.apache.org/mod_mbox/provisionr-dev/201308.mbox/raw/%3C437393710.205.1377136993025.JavaMail.hudson%40aegis%3E,Apache Jenkins Server  <jenk...@builds.apache.org>,0,0
37,38,"You can just write a function that receives the schedule_interval and
prepares and return a dag object. Call it twice with 2 different
schedule_intervals.

You should also be able to copy.deepcopy the dag object and alter the
schedule_interval of the copy, but that's not as clean.

Max

On Sun, Jun 5, 2016 at 11:51 AM, Jeremiah Lowin <jlowin@apache.org> wrote:

> It would be useful to have a DAG.copy() method, then you could duplicate it
> and change the schedule interval of the copy. I can't remember if there is
> a copy() method though -- but I think theres a subdag method which
> effectively does the same thing.
>
> On Sun, Jun 5, 2016 at 2:17 PM <hilaviz@gmail.com> wrote:
>
> > Hi,
> > Is it possible to define a DAG with two different scheduling intervals?
> > I have a DAG with 3 tasks defined in it, the schedule interval is
> > ""@weekly"" but I also want to run it every month, without duplicate the
> > entire DAG/code.
> > Thanks
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CAHEEp7UFwKfw9i9JFQky5fGoeJ9x7=QcZEJLMdv+yqrg1b3Ywg@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
41,42,"Please see inline comments.

On Sun, October 23, 2011 5:10 pm, Michael Stroucken wrote:
> Kushal Dalmia wrote:
>
>> Hi,
>>
>>
>> We have setup Tashi on a local machine and are running the node manager
>>  and cluster manager on the same machine. When we use the client to
>> create a VM, no VM gets instantiated. We do not see any errors from the
>> client, however the NodeManager(running in DEBUG = 1 mode) shows us the
>> following log:
>>
>>
> Hi Kushal,
>
>
> I have to assume you are running from the latest SVN trunk if I see
> messages related to vgscratch here.
>
> Keep in mind that there is some debug output put in /tmp by the
> nodemanager currently (like /tmp/20037.{out|err}). It should have something
> there, because I see that the nodemanager tries to create VMs.

My /tmp/<pid>.err file looks like this:
/etc/qemu-ifup.1: could not launch network script
qemu-system-x86_64: -net
tap,ifname=tashi4.0,vlan=1,script=/etc/qemu-ifup.1: Device 'tap' could not
be initialized

The /etc/qemu-ifup.1 script looks like this:
#!/bin/sh

/sbin/ifconfig $1 0.0.0.0 up
/usr/sbin/brctl addif mybridge $1
exit 0


>
> Does the /etc/qemu-ifup.1 script exist?

Yes.

> Does the disk image exist?

Yes.

> Is the name of your emulator really /usr/local/bin/qemu-system-x86_64?

Yes.

>
>
> Also, do you know what prints those messages about leaking file
> descriptors? Probably not important, but I'd like to clean that up.
>

Not really sure what prints that. Will let you know if I find something.

Thanks,
Kushal

> I haven't looked at the configuration file yet, please check for the
> above issues first.
>
> Greetings,
> Michael.
>
>
>



",http://mail-archives.apache.org/mod_mbox/incubator-tashi-user/201110.mbox/%3c61b1ae7275f9f559c25e4856aac736c7.squirrel@webmail.andrew.cmu.edu%3e,"""Kushal Dalmia"" <kdal...@andrew.cmu.edu>",0,0
109,110,"Hi, Shailendra,
I save the data in the PE like this:

private List<Event> T_lineitem = new ArrayList<Event>();

private long count = 0;

And in the onEvent(Event event){
count=count+1;
T_lineitem .add(event)
}

I find the count works correct in each same PE, but T_lineitem stores all
the data in every PE.
That means the list T_lineitem  are shared for every PE.
I don't know what is the problem.

Thank you!
Dingyu Yang


2012/10/3 Shailendra Mishra <shailendrah@gmail.com>

> I am assuming you are planning on doing windowed joins - all you need
> to do is keep save the window state and on each insert to the state
> data strucuture check if the window has expired. If the window does
> expire then compute the join and output it. Now this logic works only
> for tumbling windows for sliding windows you have to work harder,
> however the logic is kinda similar. - Shailendra
>
>
> On Wed, Oct 3, 2012 at 1:24 AM, 杨定裕 <yangdingyu@gmail.com> wrote:
> > I have read the paper of S4: Distributed Stream Computing Platform.
> > There is a example of Joining: Click-through rate. Two data tables
> RawServe
> > and RawClick are joined according 'serve' column.
> > The data with same serve in two tables are sent to the same PE. The data
> are
> > streaming to the PE and joined.
> >  I have a question :
> > While there are new tuples sent to the PE, PE has no previous data and
> > previous data in the PE are discarded after streaming processing.
> > As I know,the data is not stored in the PE.
> > So how can I join the data in the PE?
> >
>

",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201210.mbox/%3cCACDs3rA+nNA9zeNEU_kgrivy3hMsptp0YM6QR4eLEQ+ct5-LgA@mail.gmail.com%3e,杨定裕 <yangdin...@gmail.com>,0,0
22,23,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9781248
  
    change OPTIONS to SPARK_SHELL_OPTS!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

",http://mail-archives.apache.org/mod_mbox/spark-dev/201402.mbox/<20140217024948.3A35B8279EC@tyr.zones.apache.org>,aarondav <...@git.apache.org>,0,0
115,116,"That is the expected behavior. What happened is two worker processes each
grabbed a request of TaskB1, but one of them noticed, and left it to the
other one.

There are a handful of reasons it might be showing up in your logs.

Which version of Airflow are you running? Is your scheduler set to restart
periodically? Are you running more than one scheduler?

On Sat, May 20, 2017 at 6:53 PM Jason Chen <chingchien.chen@gmail.com>
wrote:

> Hi Airflow team,
>
>
> I am using airflow with celery (2 nodes; i.e., two AWS instances)
> My dag looks like below (the python dag name is task_ABC.py). Note in the
> dag python file, I setup ""max_active_runs=1""
>
>
>            /---------> TaskB1 -----------> TaskC1---------\
> TaskA -----------> TaskB2  ----------> TaskC2----------> TaskD
>            \----------> TaskB3  -----------> TaskC3--------/
>
> So, After TaskA; it runs TaskB1, TaskB2 and TaskB3 simultaneously.  TaskB1,
> B2 and B3 are running same shell-script (TaskB.sh) with different input
> arguments. It drops ""Another instance is running, skipping"" warning for
> TaskB1 and TaskB3 (as the log below). It did not drop same warning in
> TaskB2, I think it's because TaskB2 is running in different celery node (I
> have two celery nodes).
> If I manually make TaskB1 as successful, TaskB3 can proceed
>
> The following is the log. Any idea to handle this ?
> Thanks.
>
> -Jason
>
> ========= Log of TaskB1 ============
>
> [2017-05-20 23:09:47,270] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-20 23:09:49,017] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-20 23:09:49,165] {models.py:1196} INFO -
>
> --------------------------------------------------------------------------------
> Starting attempt 1 of 2
>
> --------------------------------------------------------------------------------
>
> [2017-05-20 23:09:49,182] {models.py:1219} INFO - Executing
> <Task(PythonOperator): TaskB1> on 2017-05-20 03:40:00
> [2017-05-20 23:09:49,214] {task_ABC.py:185} INFO -
> /mycode/process/gfs0p25/TaskB.sh 2017052012 <(201)%20705-2012> rain
> [2017-05-21 00:09:56,054] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-21 00:09:59,759] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-21 00:10:00,008] {models.py:1146} WARNING - Another instance
> is running, skipping.
>
>
> ========= Log of TaskB3 ============
>
> [2017-05-20 23:09:44,660] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-20 23:09:46,047] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-20 23:09:46,205] {models.py:1196} INFO -
>
> --------------------------------------------------------------------------------
> Starting attempt 1 of 2
>
> --------------------------------------------------------------------------------
>
> [2017-05-20 23:09:46,224] {models.py:1219} INFO - Executing
> <Task(PythonOperator): TaskB3> on 2017-05-20 03:40:00
> [2017-05-20 23:09:46,257] {best_weather-BLEND-v1-1-0.py:245} INFO -
> /mycode/process/gfs0p25/TaskB.sh 2017052012 <(201)%20705-2012> snow
> [2017-05-21 00:09:48,029] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-21 00:09:49,080] {models.py:154} INFO - Filling up the DagBag
> from /code/task_ABC.py
> [2017-05-21 00:09:49,156] {models.py:1146} WARNING - Another instance
> is running, skipping.
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CANcybpAjb+ZaJSn+AFatz5K2Ttn7GK5UEnRJSeyeXJHD5VWdNQ@mail.gmail.com>,George Leslie-Waksman <geo...@cloverhealth.com.INVALID>,0,0
69,70,"Hanneli,

We hit the data bricks api from Airflow. The connection info for our data
bricks instance is pretty simple:

conn type: http
host: https://<cloud_id>.cloud.databricks.com/
login: <your username>
password: <your password>

Knowing what error you get would be helpful! Also if you provide the code
your using for your hook.

Cheers,
Matt

On January 18, 2017 at 4:33:07 PM, Hanneli Tavante (hannelita@gmail.com)
wrote:

Hi! Could anybody help me?
I am trying to use the HTTPHook to connect with Databricks. (Airflow 1.8;
Databricks API 2.0 and Python 2.7.6 )
Is there any example with that? It looks like I can't authenticate. I
created an HTTP connection on 'Connections', inserted my credentials and
pointed to the port 443, but I did not succeed.
Thanks!

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CALzpcTgUcPqy2++n9qOi955v_e8YStTNFxqKoVwkasBuGSF1JA@mail.gmail.com>,Matt Hickman <matt.hick...@gmail.com>,0,0
24,25,"Hello,

I,m hadoop cluster admin in MGID company.

We try to use gobblin to ingest from kafka to hdfs.

We have 4 kafka clusters (not confluent, but we use confluent schema 
registry) and our application write to kafka in avro.

Our problem is convert avro to parquet befor write to hdfs.
For this we use converter 
converter.classes=org.apache.gobblin.converter.avro.AvroToJsonStringConverter,org.apache.gobblin.converter.json.JsonStringToJsonIntermediateConverter,org.apache.gobblin.converter.parquet.JsonIntermediateToParquetGroupConverter

But we have an error at sturtup our job
In test, use standalone mode

java.lang.IllegalStateException: This is not a JSON Array. at 
com.google.gson.JsonElement.getAsJsonArray(JsonElement.java:106) at 
org.apache.gobblin.converter.json.JsonStringToJsonIntermediateConverter.convertSchema(JsonStringToJsonIntermediateConverter.java:71)

at 
org.apache.gobblin.converter.json.JsonStringToJsonIntermediateConverter.convertSchema(JsonStringToJsonIntermediateConverter.java:48)

at

Maybe you know how to solve this problem?

Our test config

|kafka.brokers=kafka-node:9092 
#kafka.schema.registry.class=org.apache.gobblin.source.extractor.extract.kafka.ConfluentKafkaSchemaRegistry

kafka.deserializer.type=CONFLUENT_AVRO 
kafka.schema.registry.url=http://kafka-node:8081 
source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaDeserializerSource 
extract.namespace=org.apache.gobblin.extract.kafka 
converter.classes=""org.apache.gobblin.converter.json.JsonStringToJsonIntermediateConverter,org.apache.gobblin.converter.parquet.JsonIntermediateToParquetGroupConverter""

extract.namespace=org.apache.gobblin.extract.converter 
writer.builder.class=org.apache.gobblin.writer.ParquetDataWriterBuilderwriter.destination.type=HDFS

writer.output.format=PARQUET writer.file.path.type=tablename 
topic.name=test_hdfs topic.whitelist=test_hdfs 
data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher 
Regards, Roman Tarasov! |


",http://mail-archives.apache.org/mod_mbox/incubator-gobblin-user/201902.mbox/%3cc62c0ace-656a-2b40-4b52-68d2532e16d7@mgid.com%3e,"""roman.tarasov@mgid.com"" <roman.tara...@mgid.com>",0,0
119,120,"depends_on_past is looking at previous task instance which sounds the same
as ""latestonly"" but the difference becomes apparent if you look at this
example.

Let's say you have a dag, scheduled to run every day and it has been
failing for the past 3 days. The whole purpose of that dag is to populate
snapshot table or do a daily backup.  If you use depends on past, you would
have to rerun all missed runs or mark them as successful eventually doing
useless work (3 daily snapshots or backups for the same data).

LatestOnly allows you to bypass missed runs and just do it once for most
recent instance.

Another difference, depends on past is tricky if you use BranchOperator
because some branches may not run one day and run another - it will really
mess up your logic.

On Mon, Mar 20, 2017 at 12:45 PM, Ruslan Dautkhanov <dautkhanov@gmail.com>
wrote:

> Thanks Boris. It does make sense.
> Although how it's different from depends_on_past task-level parameter?
> In both cases, a task will be skipped if there is another TI of this task
> is still running (from a previous dagrun), right?
>
>
> Thanks,
> Ruslan
>
>
> On Sat, Mar 18, 2017 at 7:11 PM, Boris Tyukin <boris@boristyukin.com>
> wrote:
>
> > you would just chain them - there is an example that came with airflow
> 1.8
> > https://github.com/apache/incubator-airflow/blob/master/
> > airflow/example_dags/example_latest_only.py
> >
> > so in your case, instead of dummy operator, you would use your Oracle
> > operator.
> >
> > Does it make sense?
> >
> >
> > On Sat, Mar 18, 2017 at 7:12 PM, Ruslan Dautkhanov <dautkhanov@gmail.com
> >
> > wrote:
> >
> > > Is there is a way to combine scheduling behavior operators  (like this
> > > LatestOnlyOperator)
> > > with a functional operator (like Oracle_Operator)? I was thinking
> > multiple
> > > inheritance would do,like
> > >
> > > > class Oracle_LatestOnly_Operator (Oracle_Operator,
> LatestOnlyOperator):
> > > > ...
> > >
> > > I might be overthinking this and there could be a simpler way?
> > > Sorry, I am still learning Airflow concepts...
> > >
> > > Thanks.
> > >
> > >
> > >
> > > --
> > > Ruslan Dautkhanov
> > >
> > > On Sat, Mar 18, 2017 at 2:15 PM, Boris Tyukin <boris@boristyukin.com>
> > > wrote:
> > >
> > > > Thanks George for that feature!
> > > >
> > > > sure, just created a jira on this
> > > > https://issues.apache.org/jira/browse/AIRFLOW-1008
> > > >
> > > >
> > > > On Sat, Mar 18, 2017 at 12:05 PM, siddharth anand <sanand@apache.org
> >
> > > > wrote:
> > > >
> > > > > Thx Boris . Credit goes to George (gwax) for the implementation of
> > the
> > > > > LatestOnlyOperator.
> > > > >
> > > > > Boris,
> > > > > Can you describe what you mean in a Jira?
> > > > > -s
> > > > >
> > > > > On Fri, Mar 17, 2017 at 6:02 PM, Boris Tyukin <
> boris@boristyukin.com
> > >
> > > > > wrote:
> > > > >
> > > > > > this is nice indeed along with the new catchup option
> > > > > > https://airflow.incubator.apache.org/scheduler.html#
> > > > backfill-and-catchup
> > > > > >
> > > > > > Thanks Sid and Ben for adding these new options!
> > > > > >
> > > > > > for a complete picture, it would be nice to force only one dag
> run
> > at
> > > > the
> > > > > > time.
> > > > > >
> > > > > > On Fri, Mar 17, 2017 at 7:33 PM, siddharth anand <
> > sanand@apache.org>
> > > > > > wrote:
> > > > > >
> > > > > > > With the Apache Airflow 1.8 release imminent, you may want
to
> try
> > > out
> > > > > the
> > > > > > >
> > > > > > > *LatestOnlyOperator.*
> > > > > > >
> > > > > > > If you want your DAG to only run on the most recent scheduled
> > slot,
> > > > > > > regardless of backlog, this operator will skip running
> downstream
> > > > tasks
> > > > > > for
> > > > > > > all DAG Runs prior to the current time slot.
> > > > > > >
> > > > > > > For example, I might have a DAG that takes a DB snapshot
once a
> > > day.
> > > > It
> > > > > > > might be that I paused that DAG for 2 weeks or that I had
set
> the
> > > > start
> > > > > > > date to a fixed data 2 weeks in the past. When I enable
my
> DAG, I
> > > > don't
> > > > > > > want it to run 14 days' worth of snapshots for the current
> state
> > of
> > > > the
> > > > > > DB
> > > > > > > -- that's unnecessary work.
> > > > > > >
> > > > > > > The LatestOnlyOperator avoids that work.
> > > > > > >
> > > > > > > https://github.com/apache/incubator-airflow/commit/
> > > > > > > edf033be65b575f44aa221d5d0ec9ecb6b32c67a
> > > > > > >
> > > > > > > With it, you can simply use
> > > > > > > latest_only = LatestOnlyOperator(task_id='latest_only',
> dag=dag)
> > > > > > >
> > > > > > > instead of
> > > > > > > def skip_to_current_job(ds, **kwargs):
> > > > > > >     now = datetime.now()
> > > > > > >     left_window = kwargs['dag'].following_
> > > > schedule(kwargs['execution_
> > > > > > > date'])
> > > > > > >     right_window = kwargs['dag'].following_
> schedule(left_window)
> > > > > > >     logging.info(('Left Window {}, Now {}, Right Window
> > > > > > > {}').format(left_window,now,right_window))
> > > > > > >     if not now <= right_window:
> > > > > > >         logging.info('Not latest execution, skipping
> > downstream.')
> > > > > > >         return False
> > > > > > >     return True
> > > > > > >
> > > > > > > short_circuit = ShortCircuitOperator(
> > > > > > >   task_id         = 'short_circuit_if_not_current_job',
> > > > > > >   provide_context = True,
> > > > > > >   python_callable = skip_to_current_job,
> > > > > > >   dag             = dag
> > > > > > > )
> > > > > > >
> > > > > > > -s
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CANRT7T3RLceFYB9icAq7LiytTPJUq_20P6dMnn16CTaJ=2W94g@mail.gmail.com>,Boris Tyukin <bo...@boristyukin.com>,0,0
171,172,"Hi,

Is it possible to switch off skinning altogether?  If so, will turning off
the skinning prevent the wrapping of various components in SPANs and the
download of a ~140k CSS file?

Cheers,

Chris.

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200702.mbox/%3cb6b1d0ad0702130747q185969dfu94dece28f65ecc3e@mail.gmail.com%3e,"""Chris Lowe"" <chris.lowe...@gmail.com>",0,0
172,173,"It should be available if all the infrastructure works as
it should.

-- Adam


On 2/11/07, Steve Vangasse <steve@boardshop.co.uk> wrote:
> Thanks Adam. That was a quick response for a Sunday! Will that be
> available at:
>
> http://people.apache.org/maven-snapshot-repository/org/apache/myfaces/tr
> inidad
>
> tomorrow, or do I have to build from source control.
>
>
> Steve Vangasse
>
> www.boardshop.co.uk
>
> 0870 0600 688
>
>
> -----Original Message-----
> From: Adam Winer [mailto:awiner@gmail.com]
> Sent: 11 February 2007 17:56
> To: adffaces-user@incubator.apache.org
> Subject: Re: Skinning tr:panelTip
>
> Steve,
>
> I've rectified this just now.  panelTip has been brought into the modern
> era with a real, non-UINode renderer (makes it twice as fast, FWIW), and
> three skinning selectors:
>   af|panelTip
>   af|panelTip::label
>   af|panelTip::content
>
> ... and pretty much the obvious meaning for each.  We probably should
> also add an optional skinnable icon, which'd bring this back to parity
> with the old ADF Faces panelTip.
>
> Cheers,
> Adam
>
>
> On 2/11/07, Steve Vangasse <steve@boardshop.co.uk> wrote:
> > Hello,
> > I've been looking through the Trinidad code for a way to skin the
> > panelTip component. So far the best I have found are OraTipText
> > OraTipLabel which only skin the text and the label, not the
> > surrounding panel. I've resorted to using inlineStyle but I would much
>
> > prefer to keep the styling in the skin file. Does anyone know how this
>
> > can be done?
> >
> > Thanks,
> >
> > Steve Vangasse
> >
> > www.shopformat.com
> >
>
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200702.mbox/%3c6dac79b90702111407n36ed0620n25fa3697689dc997@mail.gmail.com%3e,"""Adam Winer"" <awi...@gmail.com>",0,0
117,118,"Oh, thats my bad.
I copied the logs directly. I was trying to simplify the names.
Consider this:
airflow.utils.AirflowException: DAG
[Pipeline_1]
could not be found in /usr/local/airflow/dags/pipeline.py

On Fri, Apr 29, 2016 at 12:19 PM, Jeremiah Lowin <jlowin@gmail.com> wrote:

> That error message usually means that an error took place inside Airflow
> before the task ran -- maybe something with setting up the task? The task's
> state is NONE, meaning it never even started, but the executor is reporting
> that it successfully sent the command to start the task (SUCCESS)... the
> culprit is some failure in between.
>
> The error message seems to say that the DAG itself couldn't be loaded from
> the .py file:
>
> airflow.utils.AirflowException: DAG
> [Pipeline_DEVTEST_CDB_DEVTEST_00_B10C8DBE1CFA89C1F274B]
> could not be found in /usr/local/airflow/dags/pipeline.py
>
> However a DAG with such a complicated name isn't referenced in the example
> code (just ""Pipeline"" + i). My guess is that the DAG id is being generated
> in a non-deterministic or time-based way, and therefore the run command
> can't find it once the generation criteria change. But hard to say without
> more detail.
>
>
>
> On Fri, Apr 29, 2016 at 3:11 PM Bolke de Bruin <bdbruin@gmail.com> wrote:
>
> > I would really like to know what the use case is for a depends_on_past on
> > the *task* level.  What past are you trying to depend on?
> >
> > What I am currently assuming from just reading the example and replying
> on
> > my phone is that the depends_on_past prevents execution. Have t4 and t5
> > ever run?
> >
> > Bolke
> >
> > Sent from my iPhone
> >
> > > On 29 apr. 2016, at 21:01, Chris Riccomini <criccomini@apache.org>
> > wrote:
> > >
> > > @Bolke/@Jeremiah, do you guys think this is related? Full thread is
> here:
> > >
> https://groups.google.com/forum/?pli=1#!topic/airbnb_airflow/y7wt3I24Rmw
> > >
> > >> On Fri, Apr 29, 2016 at 11:57 AM, Chris Riccomini <chrisr@wepay.com>
> > wrote:
> > >>
> > >> Please subscribe to the dev@ mailing list. Sorry to make you jump
> > through
> > >> hoops--I know it's annoying--but it's for a good cause. ;)
> > >>
> > >> This looks like a bug. I'm wondering if it's related to
> > >> https://issues.apache.org/jira/browse/AIRFLOW-20. Perhaps the
> backfill
> > is
> > >> causing a mis-alignment between the dag runs, and depends_on_past
> logic
> > >> isn't seeing the prior execution?
> > >>
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201604.mbox/<CADrzvViM37CPyU+nc4=UkY4hXU3MF=N-V31ZDEJrpHQcQWRREA@mail.gmail.com>,harish singh <harish.sing...@gmail.com>,0,1
131,132,"> 6. Went to UI/Connections but couldn't find my connector/hook

The experts on the list will hopefully set me right I'm my understanding 
is incorrect, but from what I can see the list of hook types available 
*in the UI* is fixed:

https://github.com/apache/incubator-airflow/blob/f360414774f1ecb2e0f4e53ebdd623c7435f9a78/airflow/www/views.py#L2424

Regards

ap

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201609.mbox/<e5efb2d467d9b3f7aa41bda557720146@qrmedia.com>,Andrew Phillips <andr...@apache.org>,0,0
118,119,"Hello, I am attempting to mavenize a .NET project we have and am getting
this build error when attempting to use the auto-generated pom file the
Npanday VS plugin made:

NPANDAY-1005-0001: Error copying dependency
VBIDE:VBIDE:com_reference:{0002E157-0000-0000-C000-000000000046}-5.3-0:5.3.0.0:compile
File
C:\Users\MyUser\.m2\repository\VBIDE\VBIDE\5.3.0.0\VBIDE-5.3.0.0-{0002E157-0000-0000-C000-000000000046}-5.3-0.dll
does not exist
Downloading:
http://m2.myCompany.net/nexus/content/groups/public//VBIDE/VBIDE/5.3.0.0/VBIDE-5.3.0.0-{0002E157-0000-0000-C000-000000000046}-5.3-0.com_reference

Unable to find resource
'VBIDE:VBIDE:com_reference:{0002E157-0000-0000-C000-000000000046}-5.3-0:5.3.0.0'
in repository central (http://central)

NPANDAY-181-121:  Problem in resolving assembly:
VBIDE:VBIDE:com_reference:{0002E157-0000-0000-C000-000000000046}-5.3-0:5.3.0.0:compile,
Message = Unable to download the artifact from any repository

This appears to be related to a dependency in the pom file:
<dependency>
      <groupId>VBIDE</groupId>
      <artifactId>VBIDE</artifactId>
      <version>5.3.0.0</version>
      <type>com_reference</type>
      <classifier>{0002E157-0000-0000-C000-000000000046}-5.3-0</classifier>
    </dependency>

So it seems that basically my company's repository does not have this VBIDE
artifact, no surpise there I suppose, this is the first time anyone in our
company has attempted to mavenize a .net project.  However I am not able to
find another repository where VBIDE exists (http://mvnrepository.com/ does
not appear to know what VBIDE is)

It is strange that the pom is advising that I need it but I am able to
compile the project in Visual Studio.
-- 
*Kent Johnston**
**Developer*

*Lixar I.T. Inc.**
*T: 613.785.3805
F: 613.722.5297
kjohnston@lixar.com
www.lixar.com

",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201106.mbox/%3cBANLkTinaHeE6ftViRf1UxzDJq=OFL8cJEw@mail.gmail.com%3e,Kent Johnston <kjohns...@lixar.com>,0,0
250,251,"That's as good a place as any. You are also welcome to tweet your slides
out as well (and mention @ApacheAirflow) so we can retweet via the Apache
Airflow twitter account.

-s

On Thu, Nov 17, 2016 at 2:04 PM, Rob Froetscher <rfroetscher@lumoslabs.com>
wrote:

> Thanks, I've added the slides to the links page. I didn't see a specific
> page for that meetup for slides in general.
>
> On Thu, Nov 17, 2016 at 11:39 AM, siddharth anand <sanand@apache.org>
> wrote:
>
> > Rob,
> > Wiki Access granted.
> > -s
> >
> > On Thu, Nov 17, 2016 at 10:49 AM, Rob Froetscher <
> > rfroetscher@lumoslabs.com>
> > wrote:
> >
> > > Here are our slides:
> > > https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYd
> > > Ohq_np90VlbVRc/edit?usp=sharing
> > >
> > > I don't think I have permissions to edit the wiki
> > >
> > > On Thu, Nov 17, 2016 at 10:41 AM, Chris Riccomini <
> criccomini@apache.org
> > >
> > > wrote:
> > >
> > > > We'll be posting the video recording shortly. IT is working on it. :)
> > > >
> > > > Will post link on the meetup and mailing list.
> > > >
> > > > On Thu, Nov 17, 2016 at 10:12 AM, Siddharth Anand
> > > > <sanand@agari.com.invalid> wrote:
> > > > > Chris and WePayEng,
> > > > > Thanks for hosting another great Airflow meet-up.
> > > > >
> > > > > Can all of the speakers post their slides online and add links to
> > those
> > > > > talks in response to this email (and also on our Wiki)?
> > > > >
> > > > > -s
> > > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201611.mbox/<CANLtMicW=8=8Kr3b6Vd0g+ae+uQH7LDz4BPXmjErxAJjyusy2g@mail.gmail.com>,siddharth anand <san...@apache.org>,0,1
29,30,"Hi Bruno,

my comment is (hopefully) related to the looping problem earlier
mentioned in your test, for further info please take a look into the
issue, where I just added further infos see:
https://issues.apache.org/jira/browse/ODFTOOLKIT-388?focusedCommentId=13969480&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13969480

PS: FOR EDITING ODF
To easily edited the content.xml of a file I suggest to use JEdit, see
www.jedit.org using the Archive plugin
http://plugins.jedit.org/plugins/?Archive (after pressing open file,
select an ODF and before pressing OPEN button, open the plugin dialog
and select a file within the ODF zip).
After that I use the http://plugins.jedit.org/plugins/?XML to indent the
XML. (the latter can be mapped to a short-cut making work faster).
The big advantage of this approach you may edit and save the embedded
XML without unzip and zip all the time!

Regards,
Svante

Am 15.04.2014 14:08, schrieb Bruno Girin:
> Hi Svante,
>
> I'm happy to help with this. However, I'm not sure how that relates to my
> problem: I am just trying to read a spreadsheet's content by iterating over
> tables, rows and cells in that spreadsheet not knowing how many of those
> items I have when I open the file.
>
> From Nick's comment, it looks like my code is not iterating over the
> spreadsheet properly so I would welcome any suggestion as to how I should
> do that.
>
> In short, what I'm trying to do is this:
>
> open spreadsheet
> for table : spreadsheet {
>   for row : table {
>     for cell : row {
>       store the value in a completely separate structure
>     }
>   }
> }
>
>
> Cheers,
>
> Bruno
>
>
>
> On 15 April 2014 12:45, Svante Schubert <svante.schubert@gmail.com> wrote:
>
>> I am still working on a patch for fixing the performance problem for
>> spreadsheets. But again this will come a little later this year with a
>> major contribution and major refactoring is required on my side before
>> submittable.
>> Basically the idea for fix is to separate any functionality for altering
>> cells into two basic functions to avoid code redundancy:
>>
>> One function will be selecting the range of cells to be altered (might
>> be a single cell, row, column, full table or sub-rectangle). The
>> altering might be any arbitrary change to be applied on the range's
>> column/row/cells (e.g. ""draw borders around the selected range"" or for
>> instance ""alter the styles from those cells containing content/format"").
>> This change will be covered by second function (or class with a certain
>> method/interface a only JDK 8 support Lambda functions, calling a
>> function with a function as parameter).
>> By this split, I could reuse the selection part, which is quite
>> difficult with all the repeated/coverage of columns/rows and cells.
>> Note: The second function will be called for every column/row/cells
>> within the given range. Seems to have quite a good performance in my
>> current tests..
>>
>> Best regard,
>> Svante
>>
>> Am 15.04.2014 13:22, schrieb Bruno Girin:
>>> Hi Nick,
>>>
>>>
>>> On 15 April 2014 11:49, Nicholas Evans <nick.evans@inology.nl> wrote:
>>>
>>>> Dear Bruno,
>>>>
>>>> I have tried out your test code and cannot reproduce the exception that
>> you
>>>> get.
>>>>
>>> I don't seem to be able to reproduce it either when running it in the ODF
>>> toolkit copy taken from SVN this morning but it hangs instead. This is
>> the
>>> behaviour I was seeing when I wrote the first implementation using
>>> v0.5-incubating from the Maven repositories; moving to v0.6-incubating
>>> using the .jar directly seemed to fix the hanging problem but triggered
>> the
>>> exception. Obviously what might have happened is that by doing that I
>>> introduced an interfering dependency.
>>>
>>> Is there any plan to make ODF Toolkit available in the Maven repositories
>>> again so that client projects can just reference the Maven repo?
>>>
>>>
>>>
>>>> I can load the spreadsheet in without a problem, and can also query the
>>>> spreadsheet as expected.
>>>>
>>>> I couldn't get your test to pass because it seems to take a long time to
>>>> run.  Methods like getRowCount() can return much higher values than you
>>>> expect (on your test code it returns 1048576 for me), and
>> getRowByIndex()
>>>> is a very slow method for large numbers of rows.
>>>>
>>> Right, so what is the best way to iterate over all rows in a table and
>> all
>>> cells in a row? This is a very simple spreadsheet with 1 table, 1 row
>> and 3
>>> cells in the first row.
>>>
>>> If getRowCount() and getRowByIndex() are unreliable and slow, should they
>>> be deprecated or at least identified as not safe for general use?
>>>
>>>
>>>
>>>> Are you running this code in a clean project without other dependencies
>>>> which might be interfering?  If not perhaps you could try this?
>>>>
>>> This is very possible as my project also uses Apache POI so there may be
>>> some dependencies that interfere. As explained above, I'm currently using
>>> the 0.6 jar files direct but would prefer to just reference the project
>> in
>>> a Maven repo to let Maven sort out dependencies.
>>>
>>> Cheers,
>>>
>>> Bruno
>>>
>>


",http://mail-archives.apache.org/mod_mbox/incubator-odf-users/201404.mbox/%3c534D258B.10100@gmail.com%3e,Svante Schubert <svante.schub...@gmail.com>,0,0
160,161,"did you read the trinidad wiki on seam ?



On 3/7/07, Juan Giovanolli <juan.giovanolli@santexgroup.com> wrote:
>
> I found that the problem posted befote was for duplication of jars
> Now, after correct that , I get this problem when I start my jboss 4.0.5
> server:
>
> [code]
> 18:36:26,343 INFO  [Server] Starting JBoss (MX MicroKernel)...
> 18:36:26,343 INFO  [Server] Release ID: JBoss [Zion] 4.0.5.GA (build:
> CVSTag=Branch_4_0 date=200610162339)
> 18:36:26,359 INFO  [Server] Home Dir: C:\jboss-4.0.5.GA
> 18:36:26,359 INFO  [Server] Home URL: file:/C:/jboss-4.0.5.GA/
> 18:36:26,359 INFO  [Server] Patch URL: null
> 18:36:26,359 INFO  [Server] Server Name: default
> 18:36:26,359 INFO  [Server] Server Home Dir:
> C:\jboss-4.0.5.GA\server\default
> 18:36:26,359 INFO  [Server] Server Home URL:
> file:/C:/jboss-4.0.5.GA/server/default/
> 18:36:26,359 INFO  [Server] Server Log Dir:
> C:\jboss-4.0.5.GA\server\default\log
> 18:36:26,359 INFO  [Server] Server Temp Dir:
> C:\jboss-4.0.5.GA\server\default\tmp
> 18:36:26,359 INFO  [Server] Root Deployment Filename: jboss-service.xml
> 18:36:26,734 INFO  [ServerInfo] Java version: 1.5.0_06,Sun Microsystems Inc.
> 18:36:26,734 INFO  [ServerInfo] Java VM: Java HotSpot(TM) Client VM
> 1.5.0_06-b05,Sun Microsystems Inc.
> 18:36:26,734 INFO  [ServerInfo] OS-System: Windows XP 5.1,x86
> 18:36:30,359 INFO  [Server] Core system initialized
> 18:36:31,781 INFO  [Log4jService$URLWatchTimerTask] Configuring from URL:
> resource:log4j.xml
> 18:36:33,140 INFO  [SocketServerInvoker] Invoker started for locator:
> InvokerLocator [socket://192.168.250.29:3873/]
> 18:36:33,687 INFO  [AspectDeployer] Deployed AOP:
> file:/C:/jboss-4.0.5.GA/server/default/deploy/ejb3-interceptors-aop.xml
> 18:36:35,234 INFO  [AspectDeployer] Deployed AOP:
> file:/C:/jboss-4.0.5.GA/server/default/deploy/jboss-portal.sar/portal-aop.xm
> l
> 18:36:40,734 INFO  [WebService] Using RMI server codebase:
> http://Santex29:8083/
> 18:36:42,906 INFO  [Embedded] Catalina naming disabled
> 18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set
> in the classpath. Will load the default rule set.
> 18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set
> in the classpath. Will load the default rule set.
> 18:36:43,375 INFO  [Http11BaseProtocol] Initializing Coyote HTTP/1.1 on
> http-0.0.0.0-8080
> 18:36:43,375 INFO  [Catalina] Initialization processed in 391 ms
> 18:36:43,375 INFO  [StandardService] Starting service jboss.web
> 18:36:43,375 INFO  [StandardEngine] Starting Servlet Engine: Apache
> Tomcat/5.5.20
> 18:36:43,421 INFO  [StandardHost] XML validation disabled
> 18:36:43,453 INFO  [Catalina] Server startup in 78 ms
> 18:36:43,593 INFO  [TomcatDeployer] deploy, ctxPath=/portal-cms,
> warUrl=.../tmp/deploy/tmp51467portal-cms-exp.war/
> 18:36:44,000 INFO  [WebappLoader] Dual registration of jndi stream handler:
> factory already defined
> 18:36:44,156 ERROR [[/portal-cms]] Error configuring application listener of
> class org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl
> java.lang.ClassNotFoundException:
> org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl
>         at
> org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav
> a:1355)
>         at
> org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav
> a:1201)
>         at
> org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:
> 3711)
>         at
> org.apache.catalina.core.StandardContext.start(StandardContext.java:4211)
>         at
> org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:7
> 59)
>         at
> org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)
>         at
> org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)
>         at
> org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at
> org.apache.catalina.core.StandardContext.init(StandardContext.java:5052)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)
>         at
> org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at
> org.jboss.web.tomcat.tc5.TomcatDeployer.performDeployInternal(TomcatDeployer
> .java:297)
>         at
> org.jboss.web.tomcat.tc5.TomcatDeployer.performDeploy(TomcatDeployer.java:10
> 3)
>         at
> org.jboss.web.AbstractWebDeployer.start(AbstractWebDeployer.java:371)
>         at org.jboss.web.WebModule.startModule(WebModule.java:83)
>         at org.jboss.web.WebModule.startService(WebModule.java:61)
>         at
> org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.
> java:289)
>         at
> org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp
> ort.java:245)
>         at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at
> org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav
> a:978)
>         at $Proxy0.start(Unknown Source)
>         at
> org.jboss.system.ServiceController.start(ServiceController.java:417)
>         at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy98.start(Unknown Source)
>         at
> org.jboss.web.AbstractWebContainer.start(AbstractWebContainer.java:466)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at
> org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java
> :133)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe
> rationInterceptor.java:142)
>         at
> org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9
> 7)
>         at
> org.jboss.system.InterceptorServiceMBeanSupport.invokeNext(InterceptorServic
> eMBeanSupport.java:238)
>         at
> org.jboss.ws.integration.jboss.DeployerInterceptor.start(DeployerInterceptor
> .java:92)
>         at
> org.jboss.deployment.SubDeployerInterceptorSupport$XMBeanInterceptor.start(S
> ubDeployerInterceptorSupport.java:188)
>         at
> org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja
> va:95)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy99.start(Unknown Source)
>         at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)
>         at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)
>         at
> org.jboss.deployment.MainDeployer.addDeployer(MainDeployer.java:368)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at
> org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java
> :133)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe
> rationInterceptor.java:142)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy89.addDeployer(Unknown Source)
>         at org.jboss.web.tomcat.tc5.Tomcat5.startService(Tomcat5.java:506)
>         at
> org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.
> java:289)
>         at
> org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp
> ort.java:245)
>         at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at
> org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java
> :133)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe
> rationInterceptor.java:142)
>         at
> org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9
> 7)
>         at
> org.jboss.deployment.SubDeployerInterceptor.invokeNext(SubDeployerIntercepto
> r.java:124)
>         at
> org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja
> va:109)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at
> org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav
> a:978)
>         at $Proxy0.start(Unknown Source)
>         at
> org.jboss.system.ServiceController.start(ServiceController.java:417)
>         at
> org.jboss.system.ServiceController.start(ServiceController.java:435)
>         at
> org.jboss.system.ServiceController.start(ServiceController.java:435)
>         at
> org.jboss.system.ServiceController.start(ServiceController.java:435)
>         at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy4.start(Unknown Source)
>         at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)
>         at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)
>         at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)
>         at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)
>         at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at
> org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java
> :133)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe
> rationInterceptor.java:142)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy6.deploy(Unknown Source)
>         at
> org.jboss.deployment.scanner.URLDeploymentScanner.deploy(URLDeploymentScanne
> r.java:421)
>         at
> org.jboss.deployment.scanner.URLDeploymentScanner.scan(URLDeploymentScanner.
> java:634)
>         at
> org.jboss.deployment.scanner.AbstractDeploymentScanner$ScannerThread.doScan(
> AbstractDeploymentScanner.java:263)
>         at
> org.jboss.deployment.scanner.AbstractDeploymentScanner.startService(Abstract
> DeploymentScanner.java:336)
>         at
> org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.
> java:289)
>         at
> org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp
> ort.java:245)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at
> org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav
> a:978)
>         at $Proxy0.start(Unknown Source)
>         at
> org.jboss.system.ServiceController.start(ServiceController.java:417)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy4.start(Unknown Source)
>         at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)
>         at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)
>         at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)
>         at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)
>         at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:766)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39
> )
>         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
> .java:25)
>         at java.lang.reflect.Method.invoke(Method.java:585)
>         at
> org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java
> :155)
>         at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)
>         at
> org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java
> :133)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe
> rationInterceptor.java:142)
>         at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
>         at
> org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26
> 4)
>         at
> org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
>         at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)
>         at $Proxy5.deploy(Unknown Source)
>         at org.jboss.system.server.ServerImpl.doStart(ServerImpl.java:482)
>         at org.jboss.system.server.ServerImpl.start(ServerImpl.java:362)
>         at org.jboss.Main.boot(Main.java:200)
>         at org.jboss.Main$1.run(Main.java:490)
>         at java.lang.Thread.run(Thread.java:595)
> [/code]
>
> should I copy Trinidad into default/lib path of jboss???
>
> Thanks in advance
>
>
>
> Juan Giovanolli
> System Developer
> C�rdoba � Argentina
>
>
>


-- 
Matthias Wessendorf
http://tinyurl.com/fmywh

further stuff:
blog: http://jroller.com/page/mwessendorf
mail: mwessendorf-at-gmail-dot-com
",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200703.mbox/%3c71235db40703071348u35b8b8e0k299b027cdb844b6a@mail.gmail.com%3e,"""Matthias Wessendorf"" <mat...@apache.org>",1,1
13,13,"bernhardriegler opened a new pull request #632: fix typo URL: https://github.com/apache/usergrid/pull/632          ---------------------------------------------------------------- This is an automated message from the Apache Git Service. To respond to the message, please log on to GitHub and use the URL above to go to the specific comment.   For queries about this service, please contact Infrastructure at: users@infra.apache.org   With regards, Apache Git Services",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201912.mbox/raw/%3C157590122405.17818.17363436252179212303.gitbox%40gitbox.apache.org%3E,GitBox <...@apache.org>,0,0
78,79,"Hi Airflow devs,

I have a question when running backfill in Airflow.

Right now I have Task A > Task B running daily and I want to backfill them
for the past ten days. While I noticed that they did not run in order,
which means May 26th Task A and Task B ran before May 25th. I tried setting
depends_on_past but it failed. I wonder if there is anyway to run backfill
sequentially?

Best regards,

Jerry

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CAJn56HLzsBAs8Y7zbH_dsjNBnjHNU1gnet1qjmk7=wAhcW3N4g@mail.gmail.com>,Jerry Zhou <jz...@oanda.com>,0,0
133,134,"Hello,

If you want a skinnable icon where tr:icon name=""arrow"", for example, 
then you would create this definition in your skin file:

.AFArrowIcon:alias {
  content:url(/skins/purple/images/next.png); width:11px; height: 15px; 
}

- Jeanne

Meyer, Stefan wrote:

>I want ro display skin specific images. Can I add new icons and display
>them with tr:icon? How?
>
>  
>


",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200701.mbox/%3c45A52660.7000005@oracle.com%3e,Jeanne Waldman <jeanne.wald...@oracle.com>,0,0
263,264,"
    [ https://issues.apache.org/jira/browse/LENS-9?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14227360#comment-14227360
] 

Rajat Khandelwal edited comment on LENS-9 at 11/27/14 7:54 AM:
---------------------------------------------------------------

What is the accepted norm for javadoc formatting? Because my IDE is converting 
{code}
/** The num cols. */
{code}
to
{code}
 /**
   * The num cols.	
   */
{code}

And 
{code}
  /**
     * Instantiates a new lens row writer.
     *
     * @param out
     *          the out
     * @param encoding
     *          the encoding
     * @param tmpPath
     *          the tmp path
     * @param extn
     *          the extn
     */
{code}
to 
{code}
 /**
     * Instantiates a new lens row writer.
     *
     * @param out      the out
     * @param encoding the encoding
     * @param tmpPath  the tmp path
     * @param extn     the extn
     */
{code}

cc: [~suma.shivaprasad]



was (Author: prongs):
What is the accepted norm for javadoc formatting? Because my IDE is converting 
{code}
/** The num cols. */
{code}
to
{code}
 /**
   * The num cols.	
   */
{code}

And 
{code}
  /**
     * Instantiates a new lens row writer.
     *
     * @param out
     *          the out
     * @param encoding
     *          the encoding
     * @param tmpPath
     *          the tmp path
     * @param extn
     *          the extn
     */
{code}
to 
{code}
 /**
     * Instantiates a new lens row writer.
     *
     * @param out      the out
     * @param encoding the encoding
     * @param tmpPath  the tmp path
     * @param extn     the extn
     */
{code}



> Checkstyle/Findbugs violations in code
> --------------------------------------
>
>                 Key: LENS-9
>                 URL: https://issues.apache.org/jira/browse/LENS-9
>             Project: Apache Lens
>          Issue Type: Improvement
>          Components: build, test
>            Reporter: Srikanth Sundarrajan
>            Priority: Minor
>             Fix For: 2.0
>
>
> There are a few checkstyle violations in code. It would help to eliminate at least the
ones that are potentially critical. It would in general makes things better if all of them
are addressed and we enable failOnValidation. Also it looks like findbugs isn't enabled.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",http://mail-archives.apache.org/mod_mbox/lens-dev/201411.mbox/<JIRA.12755258.1415941881000.33141.1417074912618@Atlassian.JIRA>,"""Rajat Khandelwal (JIRA)"" <j...@apache.org>",0,0
83,84,"
    [ https://issues.apache.org/jira/browse/KAFKA-231?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13178590#comment-13178590
] 

Neha Narkhede commented on KAFKA-231:
-------------------------------------

+1
                
> avoid logging stacktrace directly
> ---------------------------------
>
>                 Key: KAFKA-231
>                 URL: https://issues.apache.org/jira/browse/KAFKA-231
>             Project: Kafka
>          Issue Type: Improvement
>          Components: core
>            Reporter: Jun Rao
>            Assignee: Jun Rao
>             Fix For: 0.7.1
>
>         Attachments: KAFKA-231.patch
>
>
> There are several places where we log the stacktrace directly. This can be avoided by
using the proper log4j method.

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa
For more information on JIRA, see: http://www.atlassian.com/software/jira

        

",http://mail-archives.apache.org/mod_mbox/kafka-dev/201201.mbox/<1576933030.126.1325554821227.JavaMail.tomcat@hel.zones.apache.org>,"""Neha Narkhede (Commented) (JIRA)"" <j...@apache.org>",0,0
36,37,"Meeting minutes:
https://cwiki.apache.org/confluence/display/GOBBLIN/5th+Apache+Gobblin+Community+Monthly+Meetup


Regards,
Abhishek

On Tue, Aug 15, 2017 at 11:50 AM, Abhishek Tiwari <abti@apache.org> wrote:

> Hi all,
>
> Friendly reminder: Our fifth edition of monthly Apache Gobblin video
> conference is tomorrow - 8/16 Wednesday (9:00 - 10:00am PT).
>
> Unless specified, we will have an interactive open agenda with a mix of:
> - Meet and greet
> - Quick overview of any new major features
> - Community discussion (issues that anyone is facing / feature asks / etc)
>
> Please join us at: https://bluejeans.com/741314307/
>
> Regards,
> Abhishek
>

",http://mail-archives.apache.org/mod_mbox/incubator-gobblin-user/201708.mbox/%3cCAFjTMnwU73g-biVEfanLwgXSbSE8TX+K1g9jTZ5WTyFidfihiQ@mail.gmail.com%3e,Abhishek Tiwari <a...@apache.org>,0,1
302,303,"I am not that familiar with Xen.  Maybe Michael S can answer from
here.  Where do you see hostid->none error?  I you could just copy and
paste your messages to the screen maybe I can help.  It has been
awhile since I have encountered these error so I might not remember
exactly what to do.  However, if you show me exactly what is printed
to the screen, it may job my memory.

Richard


On Fri, Apr 29, 2011 at 2:06 AM, hari narayanan <hari.zlatan@gmail.com> wrote:
> Sorry for the trouble ... We created the Vm based on the hostname found in
> /etc/hosts. After creating the vm, we still get hostid->none error...
> gethosts at the CM lists our laptop, but primitive.py says ""Failed to
> schedule or activate vm"" ... vncviewer works for domU images in xen... so,
> if we add the host (laptop) , then how can we vncview into it considering
> its dom0?
>
> On Thu, Apr 28, 2011 at 5:14 PM, Richard Gass <richardgass@gmail.com> wrote:
>>
>> Tashi doesn't use thrift anymore.  We switched to rpyc.  Install that.
>>
>> If you didn't have ipython installed, when you start the CM or NM, you
>> would not have an active prompt, just log messages.  If you restart
>> now, you should get an ipython prompt.  Can you show the output of
>> data.baseDataObject.hosts now?
>>
>> Richard
>>
>>
>> On Fri, Apr 29, 2011 at 12:10 AM, hari narayanan <hari.zlatan@gmail.com>
>> wrote:
>> > No, but we installed it just now ... we dont have thrift installed ....
>> > should we get that also ??
>> >
>> > And we always start CM as u said ....
>> >
>> >
>> >
>> > On Thu, Apr 28, 2011 at 4:49 PM, Richard Gass <richardgass@gmail.com>
>> > wrote:
>> >>
>> >> Do you have ipython installed?
>> >>
>> >> On Thu, Apr 28, 2011 at 11:38 PM, hari narayanan
>> >> <hari.zlatan@gmail.com>
>> >> wrote:
>> >> > We dont get output for data.baseDataObject.hosts in CM .... but, we
>> >> > get
>> >> > the
>> >> > laptop name in tashiclient. getHosts with state Normal...
>> >> >
>> >> > On Thu, Apr 28, 2011 at 4:32 PM, Richard Gass <richardgass@gmail.com>
>> >> > wrote:
>> >> >>
>> >> >> Send me the following information.  on the CM
>> >> >> ""data.baseDataObject.hosts"" or from the tashi client ""tashi
>> >> >> gethosts""
>> >> >>
>> >> >>
>> >> >> On Thu, Apr 28, 2011 at 11:22 PM, hari narayanan
>> >> >> <hari.zlatan@gmail.com>
>> >> >> wrote:
>> >> >> > Hi,
>> >> >> >
>> >> >> > we are able to start the vm image using xen command... but,
it
>> >> >> > doesnt
>> >> >> > work
>> >> >> > when we try with tashi ... Still get the same error ""no
>> >> >> > hostid-none""
>> >> >> > ...
>> >> >> > We
>> >> >> > tried to change the CM and NM config files to make sure that
>> >> >> > hostid
>> >> >> > value is
>> >> >> > equal to our actual laptop's hostname ""akshay"" ... and also
,we
>> >> >> > are
>> >> >> > able
>> >> >> > to
>> >> >> > ping the address and hostname of the laptop ... what could
be the
>> >> >> > prob?
>> >> >> >
>> >> >> > On Tue, Apr 26, 2011 at 10:16 PM, Akshay Sheth
>> >> >> > <aks.sheth88@gmail.com>
>> >> >> > wrote:
>> >> >> >>
>> >> >> >> Hey Micheal,
>> >> >> >>
>> >> >> >> I was planning to use Sqlite3 but when I write the insert
queries
>> >> >> >> that
>> >> >> >> the
>> >> >> >> tables hosts and networks dont exist. How do I fix this?
Also
>> >> >> >> when I
>> >> >> >> put
>> >> >> >> data.baseDataObject.getHosts on CM I dont get any output.
Also
>> >> >> >> eventually I
>> >> >> >> get no hostId for the VM. What could be wrong?
>> >> >> >>
>> >> >> >> Thanks
>> >> >> >> Akshay
>> >> >> >>
>> >> >> >> On Tue, Apr 26, 2011 at 9:55 PM, hari narayanan
>> >> >> >> <hari.zlatan@gmail.com>
>> >> >> >> wrote:
>> >> >> >>>
>> >> >> >>> Hi,
>> >> >> >>>
>> >> >> >>> we have managed to setup internet in xen and downloaded
tashi
>> >> >> >>> also
>> >> >> >>> ...
>> >> >> >>> now, we get a different error  in node manager...
failed to load
>> >> >> >>> vminfo
>> >> >> >>> /var/tmp/nm.data ... and no vm information found in
>> >> >> >>> /var/tmp/vmcontrolQemu
>> >> >> >>> ... When we used vmcspecificcall, we get the same
no hostd none
>> >> >> >>> error
>> >> >> >>>
>> >> >> >>> Thanks,
>> >> >> >>> Hari
>> >> >> >>>
>> >> >> >>> On Tue, Apr 26, 2011 at 6:50 PM, Michael Stroucken
<mxs@cmu.edu>
>> >> >> >>> wrote:
>> >> >> >>>>
>> >> >> >>>> hari narayanan wrote:
>> >> >> >>>>>
>> >> >> >>>>> Sorry ... We couldnt get it work . so, we
moved to Xen ....
>> >> >> >>>>
>> >> >> >>>> Hi Hari,
>> >> >> >>>>
>> >> >> >>>> I used to use Xen until I had too many problems
with booting
>> >> >> >>>> kernels
>> >> >> >>>> inside the VM image. But with kvm I'm using the
same network
>> >> >> >>>> setup
>> >> >> >>>> as
>> >> >> >>>> I did
>> >> >> >>>> with Xen, except for having to add the qemu-ifup
files that I
>> >> >> >>>> mentioned
>> >> >> >>>> before.
>> >> >> >>>>
>> >> >> >>>>
>> >> >> >>>> Greetings,
>> >> >> >>>> Michael.
>> >> >> >>>
>> >> >> >>
>> >> >> >
>> >> >> >
>> >> >>
>> >> >>
>> >> >>
>> >> >> --
>> >> >> Richard Gass
>> >> >
>> >> >
>> >>
>> >>
>> >>
>> >> --
>> >> Richard Gass
>> >
>> >
>>
>>
>>
>> --
>> Richard Gass
>
>



-- 
Richard Gass

",http://mail-archives.apache.org/mod_mbox/incubator-tashi-user/201104.mbox/%3cBANLkTime=_heJeyK5yBVwdJ7w8oE4rWY=A@mail.gmail.com%3e,Richard Gass <richardg...@gmail.com>,0,0
135,136,"Just a reminder that the second Podling report is imminent.  After
three monthly reports (this is our second), we move to quarterly
reporting.  Anyone want to volunteer?

http://wiki.apache.org/incubator/June2016

-Jakob

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201605.mbox/<CADiKvVvXTdgzwKaCE_oijH7cE-pUNBFFdriCC58_5PBYnduS-g@mail.gmail.com>,Jakob Homan <jgho...@gmail.com>,1,1
59,60,"Hi 


What version of npanday are you using? The groupid were changed to apache.org.npanday.plugins
can you also check your local repository if there is a plugin that would match this version
if not we have release with a repository packaged you can use that. 


Hope that helps, 


Joe 

----- ""Cihan"" <ctozan@yahoo.com> wrote: 


Hi 

I'm trying to build a .net project with npanday.its giving this error. 
Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories
[local (C:\Documents and Settings\U053797.KFS\.m2\repository), central (http://repo1.maven.org/maven2)]:
Plugin not found in any plugin repository -> [Help 1] 

in my repository, maven-aspx-plugin jar and pom files are present and versions are correct.

what is the reason for this error? 


Thanks 


my pom file:<? 
< 
< 
< 
< 
< 
< 
</ 
< 
< 
< 
< 
< 
< 
< 
< 
< 
< 
< 
< 
< 
</ 
</ 
</ 
</ 
</xmlversion=""1.0""encoding=""utf-8""?>projectxmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""xmlns:xsd=""http://www.w3.org/2001/XMLSchema""xmlns=""http://maven.apache.org/POM/4.0.0"">parent>artifactId>Solution1-parent</artifactId>groupId>KFS.Solution1</groupId>version>1.0-SNAPSHOT</version>relativePath>../pom.xml</relativePath>parent>modelVersion>4.0.0</modelVersion>artifactId>WebSite</artifactId>packaging>asp</packaging>name>KFS.Solution1
: WebSite</name>build>sourceDirectory>.</sourceDirectory>plugins>plugin>groupId>org.apache.npanday.plugins</groupId>
artifactId>maven-aspx-plugin</artifactId>extensions>true</extensions>configuration>frameworkVersion>3.5</frameworkVersion>configuration>plugin>plugins>build>project>




full error text:------------------------------------------------------------------ 
Executing Maven 
Pom File: D:\My Documents\Visual Studio 2008\Projects\Solution1\pom.xml 
Goal: compile 
Arguments: compile 
NPanday Command: C:\Documents and Settings\U053797.KFS\Desktop\apache-maven-3.0.3-bin\apache-maven-3.0.3\bin\mvn.bat
compile 

------------------------------------------------------------------ 

[INFO] Scanning for projects... 
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for KFS.Solution1:WebSite:asp:1.0-SNAPSHOT

[WARNING] 'build.plugins.plugin.version' for org.apache.npanday.plugins:maven-aspx-plugin
is missing. @ line 16, column 15 
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability
of your build. 
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed
projects. 
[WARNING] 
[INFO] ------------------------------------------------------------------------ 
[INFO] Reactor Build Order: 
[INFO] 
[INFO] KFS.Solution1 : Solution1-parent 
[INFO] KFS.Solution1 : WebSite 
[INFO] 
[INFO] ------------------------------------------------------------------------ 
[INFO] Building KFS.Solution1 : Solution1-parent 1.0-SNAPSHOT 
[INFO] ------------------------------------------------------------------------ 
[INFO] 
[INFO] ------------------------------------------------------------------------ 
[INFO] Building KFS.Solution1 : WebSite 1.0-SNAPSHOT 
[INFO] ------------------------------------------------------------------------ 
[INFO] ------------------------------------------------------------------------ 
[INFO] Reactor Summary: 
[INFO] 
[INFO] KFS.Solution1 : Solution1-parent .................. SUCCESS [0.015s] 
[INFO] KFS.Solution1 : WebSite ........................... FAILURE [0.187s] 
[INFO] ------------------------------------------------------------------------ 
[INFO] BUILD FAILURE 
[INFO] ------------------------------------------------------------------------ 
[INFO] Total time: 7.652s 
[INFO] Finished at: Fri Aug 05 09:55:22 EEST 2011 
[INFO] Final Memory: 3M/15M 
[INFO] ------------------------------------------------------------------------ 
[ERROR] Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories
[local (C:\Documents and Settings\U053797.KFS\.m2\repository), central (http://repo1.maven.org/maven2)]:
Plugin not found in any plugin repository -> [Help 1] 
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. 
[ERROR] Re-run Maven using the -X switch to enable full debug logging. 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following
articles: 
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginVersionResolutionException

NPanday Execution Failed!, with exit code: 1 

",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201108.mbox/%3c1992149918.5662.1312531459480.JavaMail.root@cell.g2ix.net%3e,Josimpson Ocaba <joc...@g2ix.net>,0,1
130,131,"Hi,
I am using Olio java with glassfish 3.0 and configure my database
my.cfg following the values with Olio java distribution. However, I
encounter a problem of Broken pipe when the workloads are addevent or
addPerson. I set the <threadstart> to 100. This problem happens
randomly (>50%) when a benchmark test begins. And then I have to
manually restart the glassfish server to solve the issue. Could anyone
help on this? Thanks.

The error log is
<logger>com.sun.faban.driver.engine.TimeThread.13</logger>
  <level>WARNING</level>
  <class>com.sun.faban.driver.engine.AgentThread</class>
  <method>logError</method>
  <thread>34</thread>
  <message>UIDriverAgent[0].13.doAddPerson: Broken pipe</message>
  <exception>
    <message>java.net.SocketException: Broken pipe</message>
    <frame>
      <class>java.net.SocketOutputStream</class>
      <method>socketWrite0</method>
    </frame>
    <frame>
      <class>java.net.SocketOutputStream</class>
      <method>socketWrite</method>
      <line>92</line>
    </frame>
    <frame>
      <class>java.net.SocketOutputStream</class>
      <method>write</method>
      <line>136</line>
    </frame>
    <frame>
      <class>com.sun.faban.driver.transport.util.TimedOutputStream</class>

- Hao

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/201010.mbox/%3cAANLkTi=vWJ7wpt8xZgWND9Nge_c+PAC0sqGjt=FFO8B6@mail.gmail.com%3e,hao zhang <julius.haozh...@gmail.com>,0,0
17,17,"Andrei Savu created PROVISIONR-5: ------------------------------------               Summary: Write a ""Quick Start Guide"" page                  Key: PROVISIONR-5                  URL: https://issues.apache.org/jira/browse/PROVISIONR-5              Project: Provisionr           Issue Type: Documentation           Components: Documentation     Affects Versions: 0.5.0-incubating             Reporter: Andrei Savu   A page focused on how to get stuff done with Provisionr - how to bootstrap cluster & setup services (by using other tools for configuration).   -- This message is automatically generated by JIRA. If you think it was sent incorrectly, please contact your JIRA administrators For more information on JIRA, see: http://www.atlassian.com/software/jira",https://mail-archives.apache.org/mod_mbox/provisionr-dev/201303.mbox/raw/%3CJIRA.12638106.1363820557526.23293.1363820596473%40arcas%3E,"""Andrei Savu (JIRA)"" <j...@apache.org>",1,0
204,205,"I am trying to get started with tashi following the ""setting tashi up on 
a single test machine"" instructions, but the nodemanager is failing to 
start complaining that it is unable to load VM info from 
/var/tmp/nm.dat.  How does that file get created and initialized?


",http://mail-archives.apache.org/mod_mbox/incubator-tashi-user/201103.mbox/%3c4D7A5350.4040403@cc.gatech.edu%3e,Chad Huneycutt <ch...@cc.gatech.edu>,0,0
64,65,"To elaborate on my last message, the EXE specification says this:

All image files that import symbols, including virtually all executable (EXE) files, have
an .idata section. A typical file layout for the import information follows:

Directory Table

Null Directory Entry

DLL1 Import Lookup Table

Null

DLL2 Import Lookup Table

Null

DLL3 Import Lookup Table

Null

Hint-Name Table

Note the use of the word ""typical."" Apparently my EXE is not typical as it has 6 Import Lookup
Tables.

/Roger

-----Original Message-----
From: Costello, Roger L. <costello@mitre.org> 
Sent: Sunday, March 3, 2019 3:17 PM
To: users@daffodil.apache.org
Subject: Re: Need help informing Daffodil that we're finished with this field and it's time
to build the next field

Hi Steve,

The EXE specification is silent on the number of occurrences of the Import_Lookup_Table. All
I know is that once we see the first Hint_Name_Table entry (a 2-byte address, followed by
a null-terminated name, followed by an optional null) then we know that there are no more
Import_Lookup_Tables. Is there a way to express (for Import_Lookup_Table):

	There are no more occurrences
	once we encounter a 2-byte
	address, followed by a null-terminated
	string, followed by an optional
	null.

Is it possible to express that?

/Roger

-----Original Message-----
From: Steve Lawrence <slawrence@apache.org>
Sent: Sunday, March 3, 2019 11:25 AM
To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org>
Subject: [EXT] Re: Need help informing Daffodil that we're finished with this field and it's
time to build the next field

I think we need more information, specifically how do you know what is the last Import_Lookup_Table?

Within an Import_Lookup_Table you say that the Lookup_Table_Entry ends with the last entry
is all nulls, but that doesn't tell us where the last Import_Lookup_Table is. Is it when there's
a special value of a Lookup_Table_Entry? Or maybe when the Import_Lookup_table has a no Lookup_Table_Entires
and just has the terminating Null? Something else?

You'll likely need to add a discriminator somewhere, but we'd need more information to know
what that discriminator should be.

- Steve

On 3/3/19 7:55 AM, Costello, Roger L. wrote:
> Hello DFDL community,
> 
> In the Windows EXE file format there is one or more 
> Import_Lookup_Tables followed by a Hint_Name_Table. I am struggling 
> with how to inform Daffodil, ""Hey Daffodil, the input is finished with 
> the Import_Lookup_Tables, now it's time to build the Hint_Name_Table.""
> I am hoping you can show me how to inform Daffodil of this.
> 
> Each Import_Lookup_Table consists of one of more 32-bit fields, 
> terminated by a 32-bit field containing all nulls.
> 
> The Hint_Name_Table consists of one of more entries; each entry 
> consists of a 2-byte address, followed by a null-terminated name, 
> followed by an optional null (to align to a 2-byte boundary).
> 
> Here is a graphic that illustrates the Import_Lookup_Tables followed 
> by the
> Hint_Name_Table:
> 
> Here is the relevant portion of my DFDL schema:
> 
> <xs:elementname=""idata_Section"">
> <xs:complexType>
> <xs:sequence>>
> <xs:elementref=""Import_Lookup_Table""
>                          maxOccurs=""unbounded""/> 
> <xs:elementref=""Hint_Name_Table""/>
> </xs:sequence>
> </xs:complexType>
> </xs:element>
> 
> <xs:elementname=""Import_Lookup_Table"">
> <xs:complexType>
> <xs:sequence>
> <xs:elementref=""Lookup_Table_Entry""
>                          maxOccurs=""unbounded""/> 
> <xs:elementname=""Null_Lookup_Table_Entry""
>                          type=""xs:hexBinary""
>                          dfdl:lengthKind=""explicit""
>                          dfdl:length=""4""
>                          dfdl:lengthUnits=""bytes""> <xs:annotation> 
> <xs:appinfosource=""http://www.ogf.org/dfdl/"">
> <dfdl:assert>
>                              { . eq xs:hexBinary(""00000000"") } 
> </dfdl:assert> </xs:appinfo> </xs:annotation> </xs:element> 
> </xs:sequence> </xs:complexType> </xs:element>
> 
> How do I inform Daffodil that the input has finished with the Import_Lookup_Tables?
> 
> /Roger
> 


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201903.mbox/%3cSN6PR0901MB235148E528E84F487319D7C6C8700@SN6PR0901MB2351.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
296,297,"Hmmm, the apache mailing list won't let my current email post a reply. 
Trying again.

-Akara

-------- Original Message --------
Subject: 	Re: Parsing Olio runtimeStats
Date: 	Wed, 05 May 2010 14:17:35 -0700
From: 	Akara Sucharitakul <akara.sucharitakul@oracle.com>
Reply-To: 	akara.sucharitakul@oracle.com
Organization: 	Oracle
To: 	olio-user@incubator.apache.org
CC: 	akara.sucharitakul <Akara.Sucharitakul@sun.com>, Shanti Subramanyam 
<shanti.subramanyam@gmail.com>
References: 
<s2s89c38a6f1005051212z7cb84f71o589fa41c7ca8704d@mail.gmail.com>



Let me try to address this below:

On 05/05/10 12:12, Vasileios Kontorinis wrote:
> Akara and Shanti hi,
>     I am parsing the runtimeStats from the driver.log file and I run 
> into this interesting issue.
> In the output for _steady state_ I get ""-"" for the response time when 
> there are no successful operations since the last time runtimeStats 
> were printed.
> This can happen for two reasons:
> 1) The interval for printing the runtimeStats is small  (in my case 
> 5secs) and some operations, especially the ones that take long and 
> have small frequency in matrix (add event, add user) just never happened.
Yes, if the number of successful operations in that period is 0, most 
calculations will be a divide by 0. That's why it is printing a '-'.
> 2) There are a bunch of requests going on and none of them is 
> successful. This can happen when there is no sufficient memory in the 
> machine. In that case there is lots of swapping, the cpu goes to 100% 
> utilization and all the operations time-out.
>
> Is there any small change I can do to distinguish between the two 
> cases? Maybe printing a small ""t"" when there are many requests timing 
> out, so that I can distinguish between the two cases? Any ideas are 
> welcome.
There is the error count in the runtime stats that tell you about error 
cases. This would also include timeouts. But the problem is not that 
simple. The real problem is not the operations timing out but rather the 
operations waiting that would time out. And  we cannot distinguish 
between these waiting operations and other operations waiting just with 
slow response time.
>
> One more thing. I use the variableLoad setting and in order  to know 
> how many users are simulated while the benchmark run, I  parse log.xml 
> looking for ""Active threads: "". The problem is that log.xml is big 
> especially when many requests timeout. Is it easy to also log the 
> changes of active threads in the drive.log file ??

Please look into log configuration in $FABAN/config/logging.properties. 
You should be able to make certain loggers log to a particular file. I 
don't have the detail off  the  top of my head.

-Akara


",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/201005.mbox/%3c4BE1E133.2070708@Sun.COM%3e,Akara Sucharitakul <Akara.Sucharita...@Sun.COM>,0,0
213,214,"Hi Sergio,

dotnet-executable-config is not really a packaging, but rather only a 
dependency type. This means, there is no lifecycle bound by default: 
http://www.npanday.org/docs/1.2/guide/maven/project-types.html

You can use the build-helper-maven-plugin with the goal attach-artifact, 
to package a config together with its executable...
http://mojo.codehaus.org/build-helper-maven-plugin/usage.html Section 
""Attach additional artifacts to your project"".

I think npanday will resolve the config together with the exe. If it 
doesn't, you add an extra dependency with the same group, name and 
version plus <type>dotnet-executable-config</type>

The Exe-Pom could look like this...

hope that helps,
-Lars

<?xml version=""1.0"" encoding=""utf-8""?>

<project xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
xmlns:xsd=""http://www.w3.org/2001/XMLSchema""
xmlns=""http://maven.apache.org/POM/4.0.0"">

   <modelVersion>4.0.0</modelVersion>

   <groupId>TestGroup</groupId>

   <artifactId>TestArtifactName</artifactId>

   <packaging>dotnet-executable</packaging>

   <name>Executable-pom</name>

   <version>1.0-SNAPSHOT</version>

   <build>
     <plugins>
       <plugin>
         <groupId>org.codehaus.mojo</groupId>
         <artifactId>build-helper-maven-plugin</artifactId>
         <version>1.5</version>
         <executions>
           <execution>
             <id>attach-artifacts</id>
             <phase>package</phase>
             <goals>
               <goal>attach-artifact</goal>
             </goals>
             <configuration>
               <artifacts>
                 <artifact>
                   <file>pathtobin/bin-name.exe.config</file>
                   <type>dotnet-executable-config</type>
                 </artifact>
               </artifacts>
             </configuration>
           </execution>
         </executions>
       </plugin>
     </plugins>
   </build>

</project>





Am 12.10.10 01:12, schrieb Sergio Rupena:
>
>
> I am trying to create a pom which allows me to bundle my 'app.config'
> file together with my application. The documentation (see
> http://www.npanday.org/docs/1.2/guide/maven/project-types.html
> <http://www.npanday.org/docs/1.2/guide/maven/project-types.html>  )
> suggests that this should be possible using the dotnet-executable-config
> packaging type.
>
>
>
> Using the normal maven-compile plugin this should be doable using the
> following pom:
>
>
>
> <?xml version=""1.0"" encoding=""utf-8""?>
>
> <project xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
> xmlns:xsd=""http://www.w3.org/2001/XMLSchema""
> xmlns=""http://maven.apache.org/POM/4.0.0"">
>
>    <modelVersion>4.0.0</modelVersion>
>
>    <groupId>TestGroup</groupId>
>
>    <artifactId>TestArtifactName.Config</artifactId>
>
>    <packaging>dotnet-executable-config</packaging>
>
>    <name>configuration file pom</name>
>
>    <version>1.0-SNAPSHOT</version>
>
> </project>
>
>
>
> But this results in an error:
>
>
>
> [INFO] Scanning for projects...
>
> [INFO]
> ------------------------------------------------------------------------
>
> [INFO] Building configuration file pom
>
> [INFO]    task-segment: [install]
>
> [INFO]
> ------------------------------------------------------------------------
>
> [INFO]
> ------------------------------------------------------------------------
>
> [ERROR] BUILD ERROR
>
> [INFO]
> ------------------------------------------------------------------------
>
> [INFO] Cannot find lifecycle mapping for packaging:
> 'dotnet-executable-config'.
>
> Component descriptor cannot be found in the component repository:
> org.apache.maven.lifecycle.mapping.LifecycleMappingdotnet-executable-con
> fig.
>
> [INFO]
> ------------------------------------------------------------------------
>
> [INFO] For more information, run Maven with the -e switch
>
> [INFO]
> ------------------------------------------------------------------------
>
> [INFO] Total time:<  1 second
>
> [INFO] Finished at: Tue Oct 12 01:10:08 CEST 2010
>
> [INFO] Final Memory: 1M/15M
>
>
>
> I am using npanday 1.2.1
>
>
>
> Any help would be appreciated,
>
>
>
> /joe
>
>
>    


",http://mail-archives.apache.org/mod_mbox/incubator-npanday-users/201010.mbox/%3c4CB4009D.8070100@lcorneliussen.de%3e,Lars Corneliussen ...@lcorneliussen.de>,1,0
224,225,"Oops, I meant what is tempParams['extend']?  Is
it a JS function?  If so, how did it get into tempParams?

-- Adam


On 2/15/07, Adam Winer <awiner@gmail.com> wrote:
> What is tempParams['source'] here?  A DOM node, etc.?
>
> -- Adam
>
>
> On 2/15/07, Christopher Cudennec <SmutjeJim@gmx.net> wrote:
> > Hi... I'm getting closer to integrating trinidad :).
> >
> > My next problem (also already posted some weeks ago) is a JS error that
> > only occurs in Firefox. IE does not have any problems. I get this
> > exception after submitting the very first page of my app (login form):
> >
> > Fehler: uncaught exception: [Exception... ""Node was not found""  code:
> > ""8"" nsresult: ""0x80530008 (NS_ERROR_DOM_NOT_FOUND_ERR)""  location:
> > ""http://localhost:8080/op/adf/jsLibs/Common11-m7.js Line: 4106""]
> >
> > Debugging revealed that tempParams contains to values: ""source"" (the
> > login button) and ""extend"" (a function ?!). Trying to remove the second
> > value causes the exception.
> >
> > 4103 if(isDOM)
> > 4104 {
> > 4105 for(var paramName in tempParams)
> > 4106 form.removeChild(tempParams[paramName]);
> > 4107 }
> > 4108 }
> >
> >
> > Sounds familiar or any suggestions?
> >
> > Christopher
> >
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200702.mbox/%3c6dac79b90702151636o1cf91f8ck53e1610d0f420c4f@mail.gmail.com%3e,"""Adam Winer"" <awi...@gmail.com>",0,0
150,151,"IMHO, a DAG run without a start date is non-sensical but is not enforced
 That said, our UI allows for the manual creation of DAG Runs without a
start date as shown in the images below:


   - https://www.dropbox.com/s/3sxcqh04eztpl7p/Screenshot%
   202017-02-22%2016.00.40.png?dl=0
   <https://www.dropbox.com/s/3sxcqh04eztpl7p/Screenshot%202017-02-22%2016.00.40.png?dl=0>
   - https://www.dropbox.com/s/4q6rr9dwghag1yy/Screenshot%
   202017-02-22%2016.02.22.png?dl=0
   <https://www.dropbox.com/s/4q6rr9dwghag1yy/Screenshot%202017-02-22%2016.02.22.png?dl=0>


On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin <
maximebeauchemin@gmail.com> wrote:

> Our database may have edge cases that could be associated with running any
> previous version that may or may not have been part of an official release.
>
> Let's see if anyone else reports the issue. If no one does, one option is
> to release 1.8.0 as is with a comment in the release notes, and have a
> future official minor apache release 1.8.1 that would fix these minor
> issues that are not deal breaker.
>
> @bolke, I'm curious, how long does it take you to go through one release
> cycle? Oh, and do you have a documented step by step process for releasing?
> I'd like to add the Pypi part to this doc and add committers that are
> interested to have rights on the project on Pypi.
>
> Max
>
> On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com> wrote:
>
> > So it is a database integrity issue? Afaik a start_date should always be
> > set for a DagRun (create_dagrun) does so  I didn't check the code though.
> >
> > Sent from my iPhone
> >
> > > On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.INVALID>
> > wrote:
> > >
> > > Should clarify this occurs when a dagrun does not have a start date,
> not
> > a
> > > dag (which makes it even less likely to happen). I don't think this is
> a
> > > blocker for releasing.
> > >
> > >> On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <dan.davydov@airbnb.com>
> > wrote:
> > >>
> > >> I rolled this out in our prod and the webservers failed to load due to
> > >> this commit:
> > >>
> > >> [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag
> > >> 7c94d81c390881643f94d5e3d7d6fb351a445b72
> > >>
> > >> This fixed it:
> > >> -                            </a> <span id=""statuses_info""
> > >> class=""glyphicon glyphicon-info-sign"" aria-hidden=""true"" title=""Start
> > Date:
> > >> {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}""></span>
> > >> +                            </a> <span id=""statuses_info""
> > >> class=""glyphicon glyphicon-info-sign"" aria-hidden=""true""></span>
> > >>
> > >> This is caused by assuming that all DAGs have start dates set, so a
> > broken
> > >> DAG will take down the whole UI. Not sure if we want to make this a
> > blocker
> > >> for the release or not, I'm guessing for most deployments this would
> > occur
> > >> pretty rarely. I'll submit a PR to fix it soon.
> > >>
> > >>
> > >>
> > >> On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <
> criccomini@apache.org
> > >
> > >> wrote:
> > >>
> > >>> Ack that the vote has already passed, but belated +1 (binding)
> > >>>
> > >>> On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <bdbruin@gmail.com>
> > >>> wrote:
> > >>>
> > >>>> IPMC Voting can be found here:
> > >>>>
> > >>>> http://mail-archives.apache.org/mod_mbox/incubator-general/
> > >>> 201702.mbox/%
> > >>>> 3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <
> > >>>> http://mail-archives.apache.org/mod_mbox/incubator-general/
> > >>> 201702.mbox/%
> > >>>> 3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E>
> > >>>>
> > >>>> Kind regards,
> > >>>> Bolke
> > >>>>
> > >>>>> On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com>
> wrote:
> > >>>>>
> > >>>>> Hello,
> > >>>>>
> > >>>>> Apache Airflow (incubating) 1.8.0 (based on RC4) has been accepted.
> > >>>>>
> > >>>>> 9 “+1” votes received:
> > >>>>>
> > >>>>> - Maxime Beauchemin (binding)
> > >>>>> - Arthur Wiedmer (binding)
> > >>>>> - Dan Davydov (binding)
> > >>>>> - Jeremiah Lowin (binding)
> > >>>>> - Siddharth Anand (binding)
> > >>>>> - Alex van Boxel (binding)
> > >>>>> - Bolke de Bruin (binding)
> > >>>>>
> > >>>>> - Jayesh Senjaliya (non-binding)
> > >>>>> - Yi (non-binding)
> > >>>>>
> > >>>>> Vote thread (start):
> > >>>>> http://mail-archives.apache.org/mod_mbox/incubator-
> > >>>> airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-
> > >>>> 6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.
> > >>>> org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-
> > >>> 092E-48D2-AA0F-
> > >>>> 15F44376A8FF@gmail.com%3E>
> > >>>>>
> > >>>>> Next steps:
> > >>>>> 1) will start the voting process at the IPMC mailinglist. I
do
> expect
> > >>>> some changes to be required mostly in documentation maybe a license
> > here
> > >>>> and there. So, we might end up with changes to stable. As long
as
> > these
> > >>> are
> > >>>> not (significant) code changes I will not re-raise the vote.
> > >>>>> 2) Only after the positive voting on the IPMC and finalisation
I
> will
> > >>>> rebrand the RC to Release.
> > >>>>> 3) I will upload it to the incubator release page, then the
tar
> ball
> > >>>> needs to propagate to the mirrors.
> > >>>>> 4) Update the website (can someone volunteer please?)
> > >>>>> 5) Finally, I will ask Maxime to upload it to pypi. It seems
we can
> > >>> keep
> > >>>> the apache branding as lib cloud is doing this as well (
> > >>>> https://libcloud.apache.org/downloads.html#pypi-package <
> > >>>> https://libcloud.apache.org/downloads.html#pypi-package>).
> > >>>>>
> > >>>>> Jippie!
> > >>>>>
> > >>>>> Bolke
> > >>>>
> > >>>>
> > >>>
> > >>
> > >>
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CANLtMicNtNTysxT+xRmdW2Jry2VMJXuMmwx146PkPDu356utOg@mail.gmail.com>,siddharth anand <san...@apache.org>,0,1
256,257,"Hello everbody!

I'd like use a basic template feature with trinidad and like to inform
me about the actual situation of this issue. I've found a statement of
Matthias Wessendorf, he states that one should use facelets. However,
facelets isn't a standard. Furthermore, I couldn't find any examples
with trinidad and my migrated JSF examples didn't work (""illegal
component hierarchy detected, expected UIXCommand but found another type
of component instead.""). I reckon, it might be a problem with nested
forms, however, this would be another entry...
In my opinion, this is more than one reason do not move to facelets. Are
there any other options? Did somebody get Tiles working with trinidad?
Is there any roadmap for this issue?

Many thanks

Kuno


",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200703.mbox/%3c1174464220.6253.30.camel@linws5181.bkw-fmb.ch%3e,Kuno Baeriswyl <kuno.baeris...@bkw-fmb.ch>,0,0
38,39,"About the database error: starting from scratch also gives the same error:

Fresh install. Delete airflow.db sqllite db. And the : airflow initdb

same error as above.

On Thu, Jan 26, 2017 at 10:12 AM Alex Van Boxel <alex@vanboxel.be> wrote:

> Not directly one I can share. I'll spend some time looking at it. I'll try
> to create a unittest of it.
>
> On Thu, Jan 26, 2017 at 9:47 AM Bolke de Bruin <bdbruin@gmail.com> wrote:
>
> Do you have a example dag for this? That makes it easier to work on.
>
> Bolke
>
> Sent from my iPhone
>
> > On 26 Jan 2017, at 08:36, Alex Van Boxel <alex@vanboxel.be> wrote:
> >
> > Another thing that I noticed (but observed it in beta 2 as well). Is the
> > following:
> >
> > - The following trigger should not fire.
> > --- Trigger rule is ONE_SUCCESS
> > --- upstream: UP_FOR_RETRY + SKIPPED
> >  => task get's triggered
> >  => resulting SKIPPED
> >  => DAG marked success, with half of the Tasks never scheduled
> >
> > - UP_FOR_RETRY is propagated downstream (actually resulting in failure
> > described above)
> > --- *Does this make sense* ?!
> >
> > Both are a problem, and certainly the combination of both. I'll see if I
> > can spend some time investigating this.
> >
> >
> >
> >> On Thu, Jan 26, 2017 at 7:34 AM Bolke de Bruin <bdbruin@gmail.com>
> wrote:
> >>
> >> Mmm that is due to the reverting of one changes to the db. Need to look
> >> into that how to fix it.
> >>
> >> Sent from my iPhone
> >>
> >>> On 26 Jan 2017, at 00:51, Alex Van Boxel <alex@vanboxel.be> wrote:
> >>>
> >>> I do seem to have a problem upgrading the MySQL database with the last
> >>> commit:
> >>>
> >>>
> >>> 2017-01-25T23:41:55.662572654Z
> >>>
> >>
> /usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/messaging.py:69:
> >>> UserWarning: Revision 1a5a9e6bf2b5 referenced from 1a5a9e6bf2b5 ->
> >>> 127d2bf2dfa7 (head), Add dag_id/state index on dag_run table is not
> >> present
> >>> 2017-01-25T23:41:55.662613670Z   warnings.warn(msg)
> >>> 2017-01-25T23:41:55.664560884Z Traceback (most recent call last):
> >>> 2017-01-25T23:41:55.664582238Z   File ""/usr/local/bin/airflow"", line 4,
> >> in
> >>> <module>
> >>> 2017-01-25T23:41:55.664758457Z
> >>>
> >>
> __import__('pkg_resources').run_script('airflow==1.8.0b1+apache.incubating',
> >>> 'airflow')
> >>> 2017-01-25T23:41:55.664776553Z   File
> >>> ""/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py"",
> line
> >>> 739, in run_script
> >>> 2017-01-25T23:41:55.664937644Z
> >>> self.require(requires)[0].run_script(script_name, ns)
> >>> 2017-01-25T23:41:55.664952001Z   File
> >>> ""/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py"",
> line
> >>> 1494, in run_script
> >>> 2017-01-25T23:41:55.665072840Z     exec(code, namespace, namespace)
> >>> 2017-01-25T23:41:55.665086884Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/EGG-INFO/scripts/airflow"",
> >>> line 28, in <module>
> >>> 2017-01-25T23:41:55.665211755Z     args.func(args)
> >>> 2017-01-25T23:41:55.665225157Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/bin/cli.py"",
> >>> line 931, in upgradedb
> >>> 2017-01-25T23:41:55.665326691Z     db_utils.upgradedb()
> >>> 2017-01-25T23:41:55.665346979Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/utils/db.py"",
> >>> line 292, in upgradedb
> >>> 2017-01-25T23:41:55.665449297Z     command.upgrade(config, 'heads')
> >>> 2017-01-25T23:41:55.665462333Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/command.py"",
> >>> line 174, in upgrade
> >>> 2017-01-25T23:41:55.665521466Z     script.run_env()
> >>> 2017-01-25T23:41:55.665527232Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/base.py"",
> >>> line 416, in run_env2017-01-25T23:41:55.665638879Z
> >>> util.load_python_file(self.dir, 'env.py')
> >>> 2017-01-25T23:41:55.665656437Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/pyfiles.py"",
> >>> line 93, in load_python_file
> >>> 2017-01-25T23:41:55.665682760Z     module = load_module_py(module_id,
> >> path)
> >>> 2017-01-25T23:41:55.665699257Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/compat.py"",
> >>> line 79, in load_module_py
> >>> 2017-01-25T23:41:55.665725036Z     mod = imp.load_source(module_id,
> path,
> >>> fp)
> >>> 2017-01-25T23:41:55.665751433Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/migrations/env.py"",
> >>> line 88, in <module>
> >>> 2017-01-25T23:41:55.665778219Z     run_migrations_online()
> >>> 2017-01-25T23:41:55.665787820Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/migrations/env.py"",
> >>> line 83, in run_migrations_online
> >>> 2017-01-25T23:41:55.665811353Z     context.run_migrations()
> >>> 2017-01-25T23:41:55.665818933Z   File ""<string>"", line 8, in
> >> run_migrations
> >>> 2017-01-25T23:41:55.666707547Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/runtime/environment.py"",
> >>> line 797, in run_migrations
> >>> 2017-01-25T23:41:55.666839945Z
> >> self.get_context().run_migrations(**kw)
> >>> 2017-01-25T23:41:55.666859336Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/runtime/migration.py"",
> >>> line 305, in run_migrations
> >>> 2017-01-25T23:41:55.666970791Z     for step in
> self._migrations_fn(heads,
> >>> self):
> >>> 2017-01-25T23:41:55.666992265Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/command.py"",
> >>> line 163, in upgrade
> >>> 2017-01-25T23:41:55.667021804Z     return
> script._upgrade_revs(revision,
> >>> rev)
> >>> 2017-01-25T23:41:55.667032496Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/base.py"",
> >>> line 329, in _upgrade_revs
> >>> 2017-01-25T23:41:55.667144295Z     revs = list(revs)
> >>> 2017-01-25T23:41:55.667157407Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
> >>> line 641, in _iterate_revisions
> >>> 2017-01-25T23:41:55.667302027Z     requested_lowers =
> >>> self.get_revisions(lower)
> >>> 2017-01-25T23:41:55.667315913Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
> >>> line 298, in get_revisions
> >>> 2017-01-25T23:41:55.667390590Z     return
> >> sum([self.get_revisions(id_elem)
> >>> for id_elem in id_], ())
> >>> 2017-01-25T23:41:55.667411867Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
> >>> line 300, in get_revisions
> >>> 2017-01-25T23:41:55.667419226Z     resolved_id, branch_label =
> >>> self._resolve_revision_number(id_)
> >>> 2017-01-25T23:41:55.667434110Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
> >>> line 433, in _resolve_revision_number
> >>> 2017-01-25T23:41:55.667576273Z     self._revision_map
> >>> 2017-01-25T23:41:55.667590641Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/langhelpers.py"",
> >>> line 240, in __get__
> >>> 2017-01-25T23:41:55.667648561Z     obj.__dict__[self.__name__] =
> result =
> >>> self.fget(obj)
> >>> 2017-01-25T23:41:55.667658598Z   File
> >>>
> >>
> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
> >>> line 151, in _revision_map
> >>> 2017-01-25T23:41:55.667680311Z     down_revision = map_[downrev]
> >>> 2017-01-25T23:41:55.667746364Z KeyError: '1a5a9e6bf2b5'
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>> On Wed, Jan 25, 2017 at 11:15 PM Bolke de Bruin <bdbruin@gmail.com>
> >> wrote:
> >>>>
> >>>> Hi All,
> >>>>
> >>>> I have made the THIRD beta of Airflow 1.8.0 available at:
> >>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/ <
> >>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/> , public
> >> keys
> >>>> are available at
> >>>> https://dist.apache.org/repos/dist/release/incubator/airflow/ <
> >>>> https://dist.apache.org/repos/dist/release/incubator/airflow/> .
It
> is
> >>>> tagged with a local version “apache.incubating” so it allows upgrading
> >> from
> >>>> earlier releases. This beta is available for testing in a more
> >> production
> >>>> like setting (acceptance environment?).
> >>>>
> >>>> I would like to encourage everyone  to try it out, to report back any
> >>>> issues so we get to a rock solid release of 1.8.0. When reporting
> >> issues a
> >>>> test case or even a fix is highly appreciated.
> >>>>
> >>>> Issues cleared:
> >>>>
> >>>> * Manual trigger not working
> >>>> * Performance issues with MySQL
> >>>> * Postgres auto-commit support left over to the driver
> >>>> * Poison pill taken while task has exited
> >>>> * Keep cgroups optional
> >>>> * Funcsigs pinned to 1.0.0
> >>>>
> >>>> Issue(s) remaining (blocker for RC):
> >>>> * Cgroups not py3 compatible
> >>>>
> >>>> If all goes well we should have a Release Candidate on Feb 2. Thanks
> for
> >>>> reporting issues and keep on testing please :). Moving towards RC I
> >> tend to
> >>>> like small bug fixes only. When we mark RC (do we need to vote on
> this?)
> >>>> the procedure becomes even more strict. Please remember that the FINAL
> >>>> release is dependent on a vote on the IPMC mailinglist.
> >>>>
> >>>> Cheers
> >>>> Bolke
> >>>
> >>> --
> >>> _/
> >>> _/ Alex Van Boxel
> >>
> > --
> >  _/
> > _/ Alex Van Boxel
>
> --
>   _/
> _/ Alex Van Boxel
>
-- 
  _/
_/ Alex Van Boxel

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CALCMntvu56Ly+yVxkE27EjLJ1bT=_wHUq0Hx_34XK_CAd8YG_w@mail.gmail.com>,Alex Van Boxel <a...@vanboxel.be>,0,1
93,94,"Is this reproducible? Could you collect and share the zookeeper logs?

-Flavio

On Sep 5, 2012, at 3:52 PM, Davide Simoncelli wrote:

> Hello,
> 
> I'm trying to running an application on a cluster with 10 nodes. There is also an adapter
cluster with only one nodes.
> What I noticed is that the node in the adapter cluster sends events and the node on it
is running (the top command shows that the java process is using the CPU).
> The other 10 nodes (all of them) don't receive anything and the java process on each
node doesn't even use the CPU. After a while the following exception is thrown:
> 
> [ZkClient-EventThread-27-localhost:2181] ERROR o.a.s4.comm.topology.ClustersFromZK -
Zookeeper session expired, possibly due to a network partition for cluster [cluster1_adapter].
This node is considered as dead by Zookeeper. Proceeding to stop this node.
> 
> There is no error when clusters are created and nodes are started. Also the status command
shows the following output that let me to assume everything is ok:
> App Status
> ----------------------------------------------------------------------------------------------------------------------------------
>        Name              Cluster                                                  URI
                                           
> ----------------------------------------------------------------------------------------------------------------------------------
> testAppAdapter    cluster1_adapter  file:/home/s4-piper/testApp/build/libs/testAppAdapter.s4r
                          
>     testApp                 cluster1      file:/tmp/testApp.s4r                     
                                            
> ----------------------------------------------------------------------------------------------------------------------------------
> 
> 
> Cluster Status
> ----------------------------------------------------------------------------------------------------------------------------------
>                                                                                    Active
nodes                                  
>        Name                App           Tasks   --------------------------------------------------------------------------------
>                                                   Number    Task id                 
       Host                         Port    
> ----------------------------------------------------------------------------------------------------------------------------------
>  cluster1_adapter   testAppAdapter    1         1        Task-0                  computer1
                  13000   
>      cluster1           testApp                 10        10       Task-6           
      computer2                   12006   
>                                                              Task-7                 
computer4                   12007   
>                                                              Task-4                 
computer6                   12004   
>                                                              Task-5                 
computer7                   12005   
>                                                              Task-2                 
computer9                   12002   
>                                                              Task-3                 
computer11                  12003   
>                                                              Task-0                 
computer17                  12000   
>                                                              Task-1                 
computer18                  12001   
>                                                              Task-9                 
computer23                  12009   
>                                                              Task-8                 
computer37                  12008   
> ----------------------------------------------------------------------------------------------------------------------------------
> 
> 
> 
> Stream Status
> ----------------------------------------------------------------------------------------------------------------------------------
>        Name                               Producers                                 
            Consumers                       
> ----------------------------------------------------------------------------------------------------------------------------------
> RawlData                             cluster1_adapter(testAppAdapter)               
            cluster1(testApp)                  
> ----------------------------------------------------------------------------------------------------------------------------------
> 
> Could you help me?
> 
> Thank you
> 
> Regards
> 
> - Davide


",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201209.mbox/%3cE1270171-4B93-494C-BBA1-2276D6A8690F@yahoo-inc.com%3e,Flavio Junqueira <...@yahoo-inc.com>,0,0
307,308,"Samba,

I am not sure what you are trying, but I use tr:table with facelets.
works fine.

On 9/8/06, Samba <saasira@gmail.com> wrote:
> Hi! All,
> Can any one tell me why <af:table> is not working with Facelets?
> I'm getting Access Denied message, for the  table.
> is there any work around ?
>
> thanks in advance,
> Samba.
> --
> Vuntaanu...
> Samba.
>
>


-- 
Matthias Wessendorf

further stuff:
blog: http://jroller.com/page/mwessendorf
mail: mwessendorf-at-gmail-dot-com

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200609.mbox/%3c71235db40609072317s53371e49s7b4d3021b91f3ab6@mail.gmail.com%3e,"""Matthias Wessendorf"" <mat...@apache.org>",1,0
235,236,"Okay, that seems to edit or modify some scripts, but which file should I do this? Inside Taverna
server directory or in the java client?
Please explain more 

Thanks  

-----Original Message-----
From: alaninmcr [mailto:alaninmcr@googlemail.com] 
Sent: Thursday, March 26, 2015 9:03 PM
To: users@taverna.incubator.apache.org
Subject: Re: A few questions before choosing Taverna for our project

On 26/03/2015 19:45, Ahmad Aburomman wrote:
> Dear Stian,
>
> I'm working on workflow and I ran it perfectly (tomcat6 and RESTful), 
> I got the output without provenance bundle

Before you started the run, did you set generate-provenance to true?

Also, you need to make sure you are not sending the outputs to a Baclava file.

See
http://dev.mygrid.org.uk/wiki/display/tav250/REST+API#RESTAPI-Resource:/runs/{id}/generate-provenance

> I don't know how to configure Taverna server to enable provenance 
> data, can you guide me please?

It is not part of the server configuration. It is set for each run.

> Kind regards
>
> Ahmad

Alan


",http://mail-archives.apache.org/mod_mbox/incubator-taverna-users/201503.mbox/%3c016c01d0687e$5eafc550$1c0f4ff0$@rai.usc.es%3e,"""Ahmad Aburomman"" <ahmad.aburom...@rai.usc.es>",0,0
63,64,"This is fixed. This was due to an issue with Travis’ cache: it can corrupt the cache and
therefore
untarring fails. Now hive is re-downloaded in case this happens.

- Bolke

> Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org> het volgende
geschreven:
> 
> We have a few non-deterministic unit test failures that are affecting many
> -- but not all -- PRs. I believe they are being ignored as ""unrelated"" but
> they have the potential to mask real issues and should be addressed.
> Unfortunately they're out of my expertise so I'm going to list the ones
> I've identified and hope someone smarter than me can see if they can help!
> 
> In particular, we have a number of simple PR's that should obviously have
> no problems (typos, readme edits, etc.) that are nonetheless failing tests,
> causing frustration for all. Here is one from just this morning:
> https://github.com/apache/incubator-airflow/pull/1705/files
> 
> Thanks in advance!
> 
> 1. Python 3 Mysql (this one is pretty common), due to not being able to
> find ""beeline"" which I believe is related to Hive. This is the error:
> 
> ======================================================================
> 
> ERROR: test_mysql_to_hive_partition (tests.TransferTests)
> 
> ----------------------------------------------------------------------
> 
> Traceback (most recent call last):
> 
>  File ""/home/travis/build/apache/incubator-airflow/tests/operators/operators.py"",
> line 208, in test_mysql_to_hive_partition
> 
>    t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/models.py"",
> line 2350, in run
> 
>    force=force,)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/utils/db.py"",
> line 54, in wrapper
> 
>    result = func(*args, **kwargs)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/models.py"",
> line 1388, in run
> 
>    result = task_copy.execute(context=context)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/operators/mysql_to_hive.py"",
> line 131, in execute
> 
>    recreate=self.recreate)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/hooks/hive_hooks.py"",
> line 322, in load_file
> 
>    self.run_cli(hql)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/hooks/hive_hooks.py"",
> line 212, in run_cli
> 
>    cwd=tmp_dir)
> 
>  File ""/opt/python/3.4.2/lib/python3.4/subprocess.py"", line 858, in __init__
> 
>    restore_signals, start_new_session)
> 
>  File ""/opt/python/3.4.2/lib/python3.4/subprocess.py"", line 1456, in
> _execute_child
> 
>    raise child_exception_type(errno_num, err_msg)
> 
> nose.proxy.FileNotFoundError: [Errno 2] No such file or directory: 'beeline'
> 
> 
> 2. Python 3 Postgres (this one is really infrequent):
> 
> ======================================================================
> 
> FAIL: Test that ignore_first_depends_on_past doesn't affect results
> 
> ----------------------------------------------------------------------
> 
> Traceback (most recent call last):
> 
>  File ""/home/travis/build/apache/incubator-airflow/tests/jobs.py"",
> line 349, in test_dagrun_deadlock_ignore_depends_on_past
> 
>    run_kwargs=dict(ignore_first_depends_on_past=True))
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/utils/db.py"",
> line 54, in wrapper
> 
>    result = func(*args, **kwargs)
> 
>  File ""/home/travis/build/apache/incubator-airflow/tests/jobs.py"",
> line 221, in evaluate_dagrun
> 
>    self.assertEqual(ti.state, expected_state)
> 
> nose.proxy.AssertionError: None != 'success'
> 
> 3. Mysql (py2 and py3, infrequent). This appears to happen when the
> SLA code is called wiht mysql. Bizarrely, this doesn't appear to
> actually raise an error in the test -- it just prints a logging error.
> It must be trapped somewhere.
> 
> ERROR [airflow.jobs.SchedulerJob] Boolean value of this clause is not defined
> 
> Traceback (most recent call last):
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/jobs.py"",
> line 667, in _do_dags
> 
>    self.manage_slas(dag)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/utils/db.py"",
> line 53, in wrapper
> 
>    result = func(*args, **kwargs)
> 
>  File ""/home/travis/build/apache/incubator-airflow/airflow/jobs.py"",
> line 301, in manage_slas
> 
>    .all()
> 
>  File ""/home/travis/build/apache/incubator-airflow/.tox/py34-cdh-airflow_backend_mysql/lib/python3.4/site-packages/sqlalchemy/sql/elements.py"",
> line 2760, in __bool__
> 
>    raise TypeError(""Boolean value of this clause is not defined"")
> 
> TypeError: Boolean value of this clause is not defined


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<0F685D21-3BFB-4FFE-8691-E3F7FC213A0B@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,0
168,169,"I was able to track down what happens to some more details:

When a user sends a request to root (""/""). router has a rule to
redirect it to events:index.
In the events_controller where index is handled, the request is
checked to see whether it is a request by a logged in user (by
checking the session id).
If it is a request by a logged in user, a redirect is done to /home
(redirect_to(home_path)). This is where thin outputs 302 status with
""You are being redirected"" message. From what I read about rails
redirects this behavior seems correct
(http://rails.rubyonrails.org/classes/ActionController/Base.html#M000468).
I am not sure why it has not occurred before to anybody when doing
testing (may be my environment is still not exactly as expected to
be).
Anyways, what I did to fix this was enable setFollowRedirects in http
client at the OlioDriver (http.setFollowRedirects(true)). I am not
sure this is the perfect solution. But so far it is working for me.

Thanks
-sahan

On Thu, Dec 23, 2010 at 4:02 AM, Amanda Waite <amandarwaite@gmail.com> wrote:
> It's likely to be a Rails routing issue, I had a few random occurrences of
> this but it always went away when I investigated. Take a look at
> http://www.tutorialspoint.com/ruby-on-rails-2.1/rails-routes.htm for an
> insight into how routing works. You should be able to figure out whey the
> redirect is happening on your setup.
>
> Mandy
>
> On Thu, Dec 23, 2010 at 4:11 AM, Sahan Gamage <sahans@gmail.com> wrote:
>>
>> Thanks for the quick reply.
>> Here are some details of the system:
>> OS: Ubuntu 9.04
>> Ruby : 1.8.7
>> Rails : 2.3.5
>> proxy : nginx 0.6.35
>>
>> I did some more digging into the problem and I logged the entire html
>> page when the exception occurs (doEventDetail) in UIDriver.java. It
>> seems like server sends a redirection message (<html><body>You are
>> being <a href=""http://10.0.100.176/home"">redirected</a>.</body></html>)
>> and that's why it fails to scrape an event from the html.
>>
>> I am looking into this one
>> If you have seen this before and know a quick fix pls let me know.
>>
>> Thanks a lot
>> -sahan
>>
>> On Wed, Dec 22, 2010 at 3:52 AM, Amanda Waite <amandarwaite@gmail.com>
>> wrote:
>> > It means that the driver was unable to scrape an Event ID from the
>> > HomePage
>> > which itself suggests that the HomePage wasn't rendered correctly. What
>> > version of Ruby? What version of Rails? What OS? Is there anything in
>> > the
>> > logs? If not and you are running in production you might want to run in
>> > development mode and run a smaill test and observe what happens when
>> > EventDetail fails.
>> >
>> > We still only support Ruby 1.8.7 and Rails 2.3.5
>> >
>> > Thanks
>> >
>> > Mandy
>> >
>> > On Wed, Dec 22, 2010 at 4:19 AM, Sahan Gamage <sahans@gmail.com> wrote:
>> >>
>> >> Hi all,
>> >>
>> >> I configured faban/olio to run a performance test with Ruby version of
>> >> Olio.
>> >>
>> >> When I run the performance test I get this warning (and the
>> >> IOException) in the faban Run Log:
>> >>
>> >> UIDriverAgent[0].1.doEventDetail: In event detail and select event is
>> >> null
>> >>
>> >> Trace:
>> >>
>> >> org.apache.olio.workload.driver.UIDriver        doEventDetail   656
>> >> sun.reflect.GeneratedMethodAccessor6    invoke
>> >> sun.reflect.DelegatingMethodAccessorImpl        invoke  25
>> >> java.lang.reflect.Method        invoke  597
>> >> com.sun.faban.driver.engine.TimeThread  doRun   169
>> >> com.sun.faban.driver.engine.AgentThread run     202
>> >>
>> >> Once the test is completed I see that lot of (39 out of 91)
>> >> ""EventDetail"" operations are failed  (this is a small test run - so
>> >> the numbers are small)
>> >>
>> >> What might be the cause of this problem ?
>> >> How does this affect the results, in the sense of throughput and
>> >> response
>> >> time ?
>> >>
>> >> Thanks in advance
>> >> -sahan
>> >
>> >
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/201012.mbox/%3cAANLkTi=wrpQD8tCnZ1e2M_xTjW2Xtfdu1vo3rdV3GiJo@mail.gmail.com%3e,Sahan Gamage <sah...@gmail.com>,0,0
212,213,"Hi,
I set the 2 PEs(logic is very simple, just compare the values) threads is 1,200,300,400, the
execution time is 400s,161s,163s,171s, why I increase threads but the time get slower?

How I can improve the speed?


/Sky

",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201306.mbox/%3c387873ABC9A55249A6FD608D4DFCEA3516702EE4@ESGSCMB101.ericsson.se%3e,Sky Zhao <sky.z...@ericsson.com>,0,0
253,254,"Hi Qi,

I don't see anything suspicious in the log. Could you try out the
https://github.com/gearpump/gearpump-java-example/tree/master/src/main/java/kafka2kafka
example
to see whether it's a framework bug ?

""group-id"" is set to ""gearpump"" if not configured by user. If you want to
configure ""group-id"", you may create KafkaSource like

*Properties properties = new Properties();*
*properties.put(""group-id"", ""my-group"");*
*properties.put(""zookeeper.servers"", ""localhost:2181"");*
*KafkaSource source = new KafkaSource(""topic"", properties, storageFactory);*


On Tue, May 3, 2016 at 3:13 PM 舒琦 <shuqi@eefung.com> wrote:

> Hi Manu,
>
>
> Could you also help me to check the log in the attachment.
>
>
> How can I specify a group id when using Kafka Source, now I just set “
> group.id=XXX” in UserConfig.
>
>
> Thanks.
>
>
> ————————
> 舒琦
> 地址：长沙市岳麓区文轩路27号麓谷企业广场A4栋1单元6F
> 网址：http://www.eefung.com
> 微博：http://weibo.com/eefung
> 邮编：410013
> 电话：400-677-0986
> 传真：0731-88519609
>
>  原始邮件
> *发件人:* 舒琦<shuqi@eefung.com>
> *收件人:* user<user@gearpump.incubator.apache.org>
> *发送时间:* 2016年5月3日(周二) 14:19
> *主题:* Re: Questions About Kafka Source
>
> Hi Manu,
>
>
> Gearpump: 0.7.6_2.11
>
> Kafka: 0.8.2.1_2.10.
>
>
> Thanks.
>
> ————————
> 舒琦
> 地址：长沙市岳麓区文轩路27号麓谷企业广场A4栋1单元6F
> 网址：http://www.eefung.com
> 微博：http://weibo.com/eefung
> 邮编：410013
> 电话：400-677-0986
> 传真：0731-88519609
>
>  原始邮件
> *发件人:* Manu Zhang<owenzhang1990@gmail.com>
> *收件人:* user<user@gearpump.incubator.apache.org>
> *发送时间:* 2016年5月3日(周二) 14:13
> *主题:* Re: Questions About Kafka Source
>
> Hi Qi,
>
> Your code looks right. Which gearpump version and kafka version have you
> used ?
>
>
>
> On Tue, May 3, 2016 at 1:37 PM 舒琦 <shuqi@eefung.com> wrote:
>
>> Hi Manu,
>>
>>
>> Thanks for your help.
>>
>>
>> I used the kafka-console-consumer with the same zks and topic, and it can
>> consume messages. There is still lots of messages in that topic.
>>
>>
>> Belowing is the function I used to get Kafka Soruce, could you please
>> help to check if it is ok, thanks.
>>
>>
>>
>>
>> ————————
>> 舒琦
>> 地址：长沙市岳麓区文轩路27号麓谷企业广场A4栋1单元6F
>> 网址：http://www.eefung.com
>> 微博：http://weibo.com/eefung
>> 邮编：410013
>> 电话：400-677-0986
>> 传真：0731-88519609
>>
>>  原始邮件
>> *发件人:* Manu Zhang<owenzhang1990@gmail.com>
>> *收件人:* user<user@gearpump.incubator.apache.org>
>> *发送时间:* 2016年5月3日(周二) 12:40
>> *主题:* Re: Questions About Kafka Source
>>
>> Hi Qi,
>>
>> Neither the red ballon nor the message receive message throughput means
>> any message has been consumed by KafkaSource. Those are messages source
>> send to itself to trigger next Task execution. The metrics is a bit
>> confusing and I think we need to fix this.
>>
>> Yes, both zookeeper servers and kafka brokers configs are comma-separated
>> list strings. One way to check whether your configurations is correct it to
>> consume from the topic using kafka-console-consumer. This also makes sure
>> the topic has data to consume.
>>
>> Hope this helps.
>>
>> Thanks,
>> Manu
>>
>> On Tue, May 3, 2016 at 12:07 PM 舒琦 <shuqi@eefung.com> wrote:
>>
>>> Hi,
>>>
>>>
>>> I constructed a DAG as show blowing, “kafka source”consumes messages
>>> from kafka topic “webs”, its metrics shows  that it consumes lots of
>>> messages, but actually there is no messages handled and I also can’t find
>>> active group under topic “webs”, the log is ok too.
>>>
>>>
>>> I just wonder the properties of kafka for zks and brokers, if there is a
>>> list of zookeeper servers, should I use comma to separate? just like below:
>>>
>>>
>>> zks=zk1:3181,zk2:3181,zk3:3181
>>>
>>> brokers=kfk1:9096,kfk2:9096,kfk3:9096
>>>
>>>
>>> Thanks for your help.
>>>
>>>
>>>
>>>
>>> ————————
>>> Qi Shu
>>>
>>

",http://mail-archives.apache.org/mod_mbox/incubator-gearpump-user/201605.mbox/%3cCABT57mbRA=ShBc7ezHb3f3HxmSe5WCd0rHETg5LfmrnPhGM8=A@mail.gmail.com%3e,Manu Zhang <owenzhang1...@gmail.com>,0,0
233,234,"FYI,
I've just hit a major bug in the release candidate related to ""clear task""
behavior.

I've been running airflow in both stage and prod since yesterday on rc5 and
have reproduced this in both environments. I will file a JIRA for this
tonight, but wanted to send a note over email as well.

In my example, I have a 2 task DAG. For a given DAG run that has completed
successfully, if I
1) clear task2 (leaf task in this case), the previously-successful DAG Run
goes back to Running, requeues, and executes the task successfully. The DAG
Run the returns from Running to Success.
2) clear task1 (root task in this case), the previously-successful DAG Run
goes back to Running, DOES NOT requeue or execute the task at all. The DAG
Run the returns from Running to Success though it never ran the task.

1) is expected and previous behavior. 2) is a regression.

The only workaround is to use the CLI to run the task cleared. Here are
some images :
*After Clearing the Tasks*
https://www.dropbox.com/s/wmuxt0krwx6wurr/Screenshot%202017-03-14%2014.09.34.png?dl=0

*After DAG Runs return to Success*
https://www.dropbox.com/s/qop933rzgdzchpd/Screenshot%202017-03-14%2014.09.49.png?dl=0

This is a major regression because it will force everyone to use the CLI
for things that they would normally use the UI for.

-s


-s


On Tue, Mar 14, 2017 at 1:32 PM, Daniel Huang <dxhuang@gmail.com> wrote:

> +1 (non-binding)!
>
> On Tue, Mar 14, 2017 at 11:35 AM, siddharth anand <sanand@apache.org>
> wrote:
>
> > +1 (binding)
> >
> >
> > On Tue, Mar 14, 2017 at 8:42 AM, Maxime Beauchemin <
> > maximebeauchemin@gmail.com> wrote:
> >
> > > +1 (binding)
> > >
> > > On Tue, Mar 14, 2017 at 3:59 AM, Alex Van Boxel <alex@vanboxel.be>
> > wrote:
> > >
> > > > +1 (binding)
> > > >
> > > > Note: we had to revert all our ONE_SUCCESS with ALL_SUCCESS trigger
> > rules
> > > > where the parent nodes where joining with a SKIP. But I can of should
> > > have
> > > > known this was coming. Apart of that I had a successful run last
> night.
> > > >
> > > >
> > > > On Tue, Mar 14, 2017 at 1:37 AM siddharth anand <sanand@apache.org>
> > > wrote:
> > > >
> > > > I'm going to deploy this to staging now. Fab work Bolke!
> > > > -s
> > > >
> > > > On Mon, Mar 13, 2017 at 2:16 PM, Dan Davydov <dan.davydov@airbnb.com
> .
> > > > invalid
> > > > > wrote:
> > > >
> > > > > I'll test this on staging as soon as I get a chance (the testing
is
> > > > > non-blocking on the rc5). Bolke very much in particular :).
> > > > >
> > > > > On Mon, Mar 13, 2017 at 10:46 AM, Jeremiah Lowin <
> jlowin@apache.org>
> > > > > wrote:
> > > > >
> > > > > > +1 (binding) extremely impressed by the work and diligence all
> > > > > contributors
> > > > > > have put in to getting these blockers fixed, Bolke in particular.
> > > > > >
> > > > > > On Mon, Mar 13, 2017 at 1:07 AM Arthur Wiedmer <
> arthur@apache.org>
> > > > > wrote:
> > > > > >
> > > > > > > +1 (binding)
> > > > > > >
> > > > > > > Thanks again for steering us through Bolke.
> > > > > > >
> > > > > > > Best,
> > > > > > > Arthur
> > > > > > >
> > > > > > > On Sun, Mar 12, 2017 at 9:59 PM, Bolke de Bruin <
> > bdbruin@gmail.com
> > > >
> > > > > > wrote:
> > > > > > >
> > > > > > > > Dear All,
> > > > > > > >
> > > > > > > > Finally, I have been able to make the FIFTH RELEASE
CANDIDATE
> > of
> > > > > > Airflow
> > > > > > > > 1.8.0 available at: https://dist.apache.org/repos/
> > > > > > > > dist/dev/incubator/airflow/ <https://dist.apache.org/
> > > > > > > > repos/dist/dev/incubator/airflow/> , public keys
are
> available
> > > at
> > > > > > > > https://dist.apache.org/repos/dist/release/incubator/
> airflow/
> > <
> > > > > > > > https://dist.apache.org/repos/dist/release/incubator/
> airflow/>
> > .
> > > > It
> > > > > is
> > > > > > > > tagged with a local version “apache.incubating”
so it allows
> > > > > upgrading
> > > > > > > from
> > > > > > > > earlier releases.
> > > > > > > >
> > > > > > > > Issues fixed since rc4:
> > > > > > > >
> > > > > > > > [AIRFLOW-900] Double trigger should not kill original
task
> > > instance
> > > > > > > > [AIRFLOW-900] Fixes bugs in LocalTaskJob for double
run
> > > protection
> > > > > > > > [AIRFLOW-932] Do not mark tasks removed when backfilling
> > > > > > > > [AIRFLOW-961] run onkill when SIGTERMed
> > > > > > > > [AIRFLOW-910] Use parallel task execution for backfills
> > > > > > > > [AIRFLOW-967] Wrap strings in native for py2 ldap
> compatibility
> > > > > > > > [AIRFLOW-941] Use defined parameters for psycopg2
> > > > > > > > [AIRFLOW-719] Prevent DAGs from ending prematurely
> > > > > > > > [AIRFLOW-938] Use test for True in task_stats queries
> > > > > > > > [AIRFLOW-937] Improve performance of task_stats
> > > > > > > > [AIRFLOW-933] use ast.literal_eval rather eval because
> > > > > ast.literal_eval
> > > > > > > > does not execute input.
> > > > > > > > [AIRFLOW-919] Running tasks with no start date shouldn't
> break
> > a
> > > > DAGs
> > > > > > UI
> > > > > > > > [AIRFLOW-897] Prevent dagruns from failing with unfinished
> > tasks
> > > > > > > > [AIRFLOW-861] make pickle_info endpoint be login_required
> > > > > > > > [AIRFLOW-853] use utf8 encoding for stdout line decode
> > > > > > > > [AIRFLOW-856] Make sure execution date is set for
local
> client
> > > > > > > > [AIRFLOW-830][AIRFLOW-829][AIRFLOW-88] Reduce Travis
log
> > > verbosity
> > > > > > > > [AIRFLOW-794] Access DAGS_FOLDER and SQL_ALCHEMY_CONN
> > exclusively
> > > > > from
> > > > > > > > settings
> > > > > > > > [AIRFLOW-694] Fix config behaviour for empty envvar
> > > > > > > > [AIRFLOW-365] Set dag.fileloc explicitly and use for
Code
> view
> > > > > > > > [AIRFLOW-931] Do not set QUEUED in TaskInstances
> > > > > > > > [AIRFLOW-899] Tasks in SCHEDULED state should be white
in the
> > UI
> > > > > > instead
> > > > > > > > of black
> > > > > > > > [AIRFLOW-895] Address Apache release incompliancies
> > > > > > > > [AIRFLOW-893][AIRFLOW-510] Fix crashing webservers
when a
> > dagrun
> > > > has
> > > > > no
> > > > > > > > start date
> > > > > > > > [AIRFLOW-793] Enable compressed loading in S3ToHiveTransfer
> > > > > > > > [AIRFLOW-863] Example DAGs should have recent start
dates
> > > > > > > > [AIRFLOW-869] Refactor mark success functionality
> > > > > > > > [AIRFLOW-856] Make sure execution date is set for
local
> client
> > > > > > > > [AIRFLOW-814] Fix Presto*CheckOperator.__init__
> > > > > > > > [AIRFLOW-844] Fix cgroups directory creation
> > > > > > > >
> > > > > > > > No known issues anymore.
> > > > > > > >
> > > > > > > > I would also like to raise a VOTE for releasing 1.8.0
based
> on
> > > > > release
> > > > > > > > candidate 5, i.e. just renaming release candidate
5 to 1.8.0
> > > > release.
> > > > > > > >
> > > > > > > > Please respond to this email by:
> > > > > > > >
> > > > > > > > +1,0,-1 with *binding* if you are a PMC member or
> *non-binding*
> > > if
> > > > > you
> > > > > > > are
> > > > > > > > not.
> > > > > > > >
> > > > > > > > Thanks!
> > > > > > > > Bolke
> > > > > > > >
> > > > > > > > My VOTE: +1 (binding)
> > > > > > >
> > > > > >
> > > > >
> > > >
> > > > --
> > > >   _/
> > > > _/ Alex Van Boxel
> > > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CANLtMifC-AH63z=iD3=O-UxFM6vQ_uG=4B7jED_CyUqFNkFGFg@mail.gmail.com>,siddharth anand <san...@apache.org>,0,1
99,100,"mysql_hook uses MySQLdb. Just see if you are not hit by this issue:

http://stackoverflow.com/questions/6383310/python-mysqldb-library-not-loaded-libmysqlclient-18-dylib



On Mon, Jun 20, 2016 at 5:58 AM, Msr Msr <msrmaillist@gmail.com> wrote:

> Hi,
>
> I have installed Airflow on Mac and trying to use  MySqlOperator.
>
> It is giving below error message.
>
> ------------------
>
> mms-MacBook-Pro:~ mm$ airflow initdb
>
> [2016-06-19 17:21:06,794] {__init__.py:36} INFO - Using executor
> SequentialExecutor
>
> DB: sqlite:////Users/mm/airflow/airflow.db
>
> [2016-06-19 17:21:07,345] {db.py:222} INFO - Creating tables
>
> INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
>
> INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
>
> ERROR [airflow.models.DagBag] Failed to import:
> /Users/mm/airflow/dags/S3test.py
>
> Traceback (most recent call last):
>
>   File ""/Users/mm/anaconda/lib/python2.7/site-packages/airflow/models.py"",
> line 247, in process_file
>
>     m = imp.load_source(mod_name, filepath)
>
>   File ""/Users/mm/airflow/dags/S3test.py"", line 4, in <module>
>
>     from airflow.operators import MySqlOperator
>
> ImportError: cannot import name MySqlOperator
>
> -------------------
>
>
> pip install airflow[mysql] always gives Requirements met
>
>
> Could someone please suggest what could be the reason for this error and
> how to resolve it
>
>
> Thanks,
>
> msr
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CAGeMxTdpshVhSJfC_kWsnshSG6HYqTube43UzjG38YueqNgGqg@mail.gmail.com>,Sumit Maheshwari <sumeet.ma...@gmail.com>,0,0
126,127,"+1 on this.

We are using Airflow as a cron replacement, and we have biweekly jobs and
monthly jobs as well.
It would be really useful to be able to configure it such that dags run on
the start_date and the timestamps corresponds to it.

On Tue, Jul 19, 2016 at 10:49 AM, Tyrone Hinderson <thinderson@reonomy.com>
wrote:

> I'm aware that a DAG scheduled to start at time X with interval Y will
> first run at time X + Y. The documentation describes this:
>
> ""Note that if you run a DAG on a schedule_interval of one day, the run
> stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59. In other
> words, the job instance is started once the period it covers has ended.""
>
> I'd like to know if this behavior is configurable? There may be a
> particular way of thinking about business processes that fits this pattern;
> however, seeing last week's date on a weekly job that ran today confuses my
> team, and I'd love to use a flag that makes
> 1. DAGs run on the start_date
> 2. DagRun timestamps correspond with the intended actual run date/time.
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201607.mbox/<CAKMdtFcA9ccUv1r5LtN09jqCKJVRJzkTU+p0i=cytZfpi7H9XA@mail.gmail.com>,Joy Gao <j...@wepay.com>,0,0
308,309,"On 12/13/06, Gary VanMatre <gvanmatre@apache.org> wrote:
> I've been experimenting with using Shale Clay for template composition using
> Trinidad components.
>
> Anyway, it requires a couple classes that handle all of the special method
> binding events.
>
> http://svn.apache.org/viewvc/shale/sandbox/shale-clay-trinidad/src/main/java/org/apache/shale/clay/component/chain/trinidad/PropertyListenerCommand.java?view=markup
>
> This class has a dependency with commons chains.  It also requires some xml
> configs.  The clay configs are generated form the TLD's using a shale maven
> plugin and we also have a commons chains config.
>
> I was thinking that instead of adding yet another dependencies in Trinidad
> to provide native support like you have for Facelets, what would you think
> about a maven architecture type that would generate a base project with the
> exta bits?

So, you mean a new archetype?  Sounds excellent.

-- Adam

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200612.mbox/%3c6dac79b90612130936j3873b5fja8d0f730adeb5ca6@mail.gmail.com%3e,"""Adam Winer"" <awi...@gmail.com>",0,0
47,48,"We hosted several metopes here at Blue Apron.  I will bring it up to our
administrative team and give an update.  Mid-january is probably a good
target.

- Joe

On Thu, Dec 15, 2016 at 5:18 PM, Luke Ptz <lukeptzcode@gmail.com> wrote:

> Cool to see the interest is there! I unfortunately can't offer a space for
> a meetup, can anyone else? If not could always be informal/meet in a public
> setting
>
> On Wed, Dec 14, 2016 at 7:08 PM, Andrew Phillips <andrewp@apache.org>
> wrote:
>
> > We at Blue Apron would be very interested.
> >>
> >
> > Same here.
> >
> > ap
> >
>



-- 
*Joe Napolitano *| Sr. Data Engineer
www.blueapron.com | 5 Crosby Street, New York, NY 10013

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<CAPp6otLDvGifZtaTo6DHRNajKOWXTd89f0BZZhDoq=vcuZu9YQ@mail.gmail.com>,Joseph Napolitano <joseph.napolit...@blueapron.com.INVALID>,0,1
167,168,"deft-dev@incubator.apache.org --> awf-dev@incubator.apache.org
deft-users@incubator.apache.org --> awf-users@incubator.apache.org
deft-commits@incubator.apache.org --> awf-commits@incubator.apache.org

Thanks,
Matt

",http://mail-archives.apache.org/mod_mbox/incubator-awf-users/201202.mbox/%3cCAE9L6G30jx=PhXWEt4hTgTm83qTCF42vNGM=KD2qhBKd4Rugvg@mail.gmail.com%3e,Matt Benson <mben...@apache.org>,1,1
125,126,"Hey Brian,

Try going here to create the JIRA:

https://issues.apache.org/jira/secure/CreateIssue!default.jspa

Cheers,
Chris

On Tue, May 31, 2016 at 9:33 AM, Brian Candler <b.candler@pobox.com> wrote:

> Trying to follow the quickstart at
> http://pythonhosted.org/airflow/start.html (inside a fresh virtualenv,
> under ubuntu 14.04)
>
> I get the following error:
>
> (venv)brian@cfprov:~/airflow$ airflow initdb
> [2016-05-31 17:02:25,939] {__init__.py:36} INFO - Using executor
> SequentialExecutor
> [2016-05-31 17:02:26,049] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2016-05-31 17:02:26,114] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> DB: sqlite:////home/brian/airflow/airflow.db
> [2016-05-31 17:02:26,361] {db.py:222} INFO - Creating tables
> INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
> INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
> INFO  [alembic.runtime.migration] Running upgrade  -> e3a246e0dc1, current
> schema
> INFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 ->
> 1507a7289a2f, create is_encrypted
> /home/brian/airflow/venv/local/lib/python2.7/site-packages/alembic/util/messaging.py:69:
> UserWarning: Skipping unsupported ALTER for creation of implicit constraint
>   warnings.warn(msg)
> INFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f ->
> 13eb55f81627, maintain history for compatibility with earlier migrations
> INFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 ->
> 338e90f54d61, More logging into task_isntance
> INFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 ->
> 52d714495f0, job_id indices
> INFO  [alembic.runtime.migration] Running upgrade 52d714495f0 ->
> 502898887f84, Adding extra to Log
> INFO  [alembic.runtime.migration] Running upgrade 502898887f84 ->
> 1b38cef5b76e, add dagrun
> INFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e ->
> 2e541a1dcfed, task_duration
> INFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed ->
> 40e67319e3a9, dagrun_config
> INFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 ->
> 561833c1c74b, add password column to user
> INFO  [alembic.runtime.migration] Running upgrade 561833c1c74b ->
> 4446e08588, dagrun start end
> INFO  [alembic.runtime.migration] Running upgrade 4446e08588 ->
> bbc73705a13e, Add notification_sent column to sla_miss
> INFO  [alembic.runtime.migration] Running upgrade bbc73705a13e ->
> bba5a7cfc896, Add a column to track the encryption state of the 'Extra'
> field in connection
> INFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 ->
> 1968acfc09e3, add is_encrypted column to variable table
> INFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 ->
> 2e82aab8ef20, rename user table
> ERROR [airflow.models.DagBag] Failed to import:
> /home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/example_dags/example_twitter_dag.py
> Traceback (most recent call last):
>   File
> ""/home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/models.py"",
> line 247, in process_file
>     m = imp.load_source(mod_name, filepath)
>   File
> ""/home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/example_dags/example_twitter_dag.py"",
> line 26, in <module>
>     from airflow.operators import BashOperator, HiveOperator,
> PythonOperator
> ImportError: cannot import name HiveOperator
> Done.
> (venv)brian@cfprov:~/airflow$
>
> Airflow *does* appear to work (the web UI comes up), but I also get the
> same error logged for the execution of the example_bash_operator code:
>
> [2016-05-31 17:04:16,451] {models.py:154} INFO - Filling up the DagBag
> from /home/brian/airflow/dags/example_dags/example_bash_operator.py
> [2016-05-31 17:04:16,452] {models.py:250} ERROR - Failed to import:
> /home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/example_dags/example_twitter_dag.py
> Traceback (most recent call last):
>   File
> ""/home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/models.py"",
> line 247, in process_file
>     m = imp.load_source(mod_name, filepath)
>   File
> ""/home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/example_dags/example_twitter_dag.py"",
> line 26, in <module>
>     from airflow.operators import BashOperator, HiveOperator,
> PythonOperator
> ImportError: cannot import name HiveOperator
>
>
> If I try it from the command line:
>
> >>> from airflow.operators.hive_operator import HiveOperator
> [2016-05-31 17:24:59,316] {__init__.py:36} INFO - Using executor
> SequentialExecutor
> [2016-05-31 17:24:59,427] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2016-05-31 17:24:59,484] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File
> ""/home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/operators/hive_operator.py"",
> line 4, in <module>
>     from airflow.hooks import HiveCliHook
> ImportError: cannot import name HiveCliHook
>
> And again:
>
> >>> from airflow.hooks.hive_hooks import HiveCliHook
> [2016-05-31 17:27:11,716] {__init__.py:36} INFO - Using executor
> SequentialExecutor
> [2016-05-31 17:27:11,824] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2016-05-31 17:27:11,872] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File
> ""/home/brian/airflow/venv/local/lib/python2.7/site-packages/airflow/hooks/hive_hooks.py"",
> line 19, in <module>
>     import unicodecsv as csv
> ImportError: No module named unicodecsv
>
> Aha. This error has been hidden by magic.After ""pip install unicodecsv"" it
> seems happy:
>
> >>> from airflow.operators import HiveOperator
> [2016-05-31 17:28:15,828] {__init__.py:36} INFO - Using executor
> SequentialExecutor
> [2016-05-31 17:28:15,936] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2016-05-31 17:28:15,984] {driver.py:120} INFO - Generating grammar tables
> from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> >>>
>
> So: I think the ""airflow"" package is missing a dependency on ""unicodecsv"".
>
> Regards,
>
> Brian Candler.
>
> P.S. Side problem: I wanted to create an issue on JIRA but couldn't see
> how. I have an Apache JIRA account (username ""candlerb""). Under the
> ""Create"" menu it offers me two choices:
>
> * New JIRA Project     # really, I can do this?!
>
> * Create Service Desk Request     # whatever that is
>
> But all I wanted to do was create an issue. Am I missing something
> obvious? Or are only project developers allowed to raise issues?
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201605.mbox/<CABYbY7cnPPeFOgLrD1JigFWb6_6+n_+2O+BhuwYeH98aKkR02w@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
153,154,"Cool, would you have remote joining setup (hangout?, adobe?) or recording
for this for the folks not in NYC ?

Thanks for hosting !

On Fri, Jan 20, 2017 at 10:37 AM, Joseph Napolitano <
joseph.napolitano@blueapron.com.invalid> wrote:

> Hi all!
>
> I want to officially announce a Meetup for Airflow in NYC!  I'm looking
> forward to meeting other community members to share knowledge and network.
>
> We may create an official Meetup page, but in the meantime please signup
> here:
> https://docs.google.com/spreadsheets/d/1WmfgZeExSVdLf-
> u1uh3IleeHy8QTwaJ4BkkSkVM-X1E/edit?usp=sharing
>
> I have a confirmed date of February 1st @ 6:30 at Blue Apron's
> headquarters.
>
> In Summary:
> Date: Feb 1st
> Time 6:30 - 9pm EST
> Location: 40 W 23rd St. New York, NY 10010
> https://www.google.com/maps/place/40+W+23rd+St,+New+York,+
> NY+10010/@40.7420885,-73.9938457,17z/data=!3m1!4b1!4m5!
> 3m4!1s0x89c259a46471d2a1:0xc2517d92b1b68bba!8m2!3d40.
> 7420845!4d-73.9916517?hl=en
>
> We're on the 5th floor.  You need to check in with security in the building
> lobby, and again when you reach the fifth floor to get a name tag.
>
> Food & drink will be provided!
>
> Let me know if you would like to present.  We'd love to hear about your
> architecture and war stories.  We will have a large projector and PA system
> setup.
>
> Sorry about the short notice, but it took a while to get approved over the
> holidays and new year.  If we can't generate enough interest we can
> certainly push it back a month.
>
> Thanks, and Bon Appétite!
>
> --
> *Joe Napolitano *| Sr. Data Engineer
> www.blueapron.com | 5 Crosby Street, New York, NY 10013
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CAEOTQt8T8Z0x49RP91jV8tLW_7S=NWXHEEsX8=iY_Y3WgacOiA@mail.gmail.com>,Jacky <jhsonl...@gmail.com>,0,1
44,45,"Awesome, thanks Jeremiah!

On Fri, Jan 20, 2017 at 8:20 AM, Jeremiah Lowin <jlowin@apache.org> wrote:

> Hi Laura,
>
> The error is raised if an unused argument is passed to BaseOperator --
> basically if there is anything in either args or kwargs. The original issue
> was that in a number of cases arguments were misspelled or misused by
> Operator subclasses and instead of raising an error, they were just passed
> up the inheritance chain and finally (silently) absorbed by BaseOperator,
> so there was no warning.
>
> I think a workaround should be straightforward -- when you call
> super().__init__ for the BaseOperator, just pass arguments explicitly
> rather than with args/kwargs, or (alternatively), pop arguments out of
> kwargs when you use them ahead of calling that __init__.
>
> On Thu, Jan 19, 2017 at 10:23 AM Laura Lorenz <llorenz@industrydive.com>
> wrote:
>
> > Hi! Is there a way to determine the rationale behind deprecation
> warnings?
> > In particular I'm interested in the following:
> >
> >
> > /Users/llorenz/Envs/fileflow/lib/python2.7/site-packages/
> airflow/models.py:1719:
> > > PendingDeprecationWarning: Invalid arguments were passed to
> > > DivePythonOperator. Support for passing such arguments will be dropped
> in
> > > Airflow 2.0. Invalid arguments were:
> > >
> > > *args: ()
> > >
> > > **kwargs: {'data_dependencies': {'something': 'write_a_file'}}
> > >
> > >   category=PendingDeprecationWarning
> > >
> >
> > Our home grown plugin fileflow depends on this capability so I'd like to
> > get more information about how it will be changing to see if I can
> > anticipate a workaround to support airflow 2.0.
> >
> > Thanks!
> >
> > Laura
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CAJQxwiEfj7f9-=g8oNO+L_a91_MOvzN9PFDTgDL+rDEngy5FQg@mail.gmail.com>,Laura Lorenz <llor...@industrydive.com>,0,0
70,71,"thanks Ben for the explanation. Is there a Jira for this or do you want to
me open one? I think it is pretty important thing as all public talks
mentioned tasks generation programmatically (and dynamically) as a one of
the main features of Airflow. If we cannot see what was really generated in
the past and get to every task even if it does not exist anymore, it does
not complete this feature.

Also I am concerned at this point (very) that a lot of things require
restart of airflow scheduler and webserver - it does not look like a good
strategy to me. I realize most of this happening because of python caching
but as an end user, I do not really care :)

I will be also looking at Luigi and Oozie (leaving letter for last because
I get dizzy by looking at its xml). My use case is to generate tasks every
day for hundreds of tables and some table will come and go.

On Sat, Oct 15, 2016 at 3:01 PM, Ben Tallman <ben@apigee.com> wrote:

> That is part of it. In this case, we aren't planning to store the contents
> of the DagBag, as it was when the DagRun was created (that was the pickling
> stuff that is deprecated), but it solves HALF of the problem. It allows us
> to begin at least drawing the graph as it was when it was run. Storing the
> DagBag Dag would begin to solve your problem as well.
>
> I would dearly love to have tasks generated at schedule time (not during
> the run), not every time the dag file is evaluated (every 3 minutes or so).
>
> There is disagreement as to the best way to handle this, however based on
> conversations that I've heard and participated in, the current preferred
> solution is to head down the path of a ""git time machine"". However that
> doesn't actually solve the problem that we see. Basically, we want to have
> the evaluation of the dag python file interogate outside systems to
> generate the tasks and have them run. The problem with the git time machine
> solution is that those outside systems are not static. They change over
> time. In the past, an effort was made to pickle the dag, and run from that,
> but pickling has it's own issues.
>
> To be clear, at the time, I think the goal of the pickling was to
> distribute the dag to distributed workers, not freeze it in time. I think
> that storing the pickled dag in the dagrun could probably solve this, but
> it is a major issue/change. It is one that I am beginning to work on for us
> though.
>
>
> Thanks,
> Ben
>
> *--*
> *ben tallman* | *apigee
> <http://t.sidekickopen06.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nM
> JW7t5XZs4WJfgqW4WJj7n3MP7VWW3LqXLC56dWRRf2H8CkP02?t=http%3A%
> 2F%2Fwww.apigee.com%2F&si=5141814536306688&pi=e558dca3-
> da0a-4d9f-c1b3-6cb9174fcb5e>*
>  | m: +1.503.680.5709 | o: +1.503.608.7552 | twitter @anonymousmanage
> <http://t.sidekickopen06.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nM
> JW7t5XZs4WJfgqW4WJj7n3MP7VWW3LqXLC56dWRRf2H8CkP02?t=http%3A%
> 2F%2Ftwitter.com%2Fanonymousmanage&si=5141814536306688&pi=e558dca3-
> da0a-4d9f-c1b3-6cb9174fcb5e>
>  @apigee
> <http://t.sidekickopen06.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nM
> JW7t5XZs4WJfgqW4WJj7n3MP7VWW3LqXLC56dWRRf2H8CkP02?t=https%
> 3A%2F%2Ftwitter.com%2Fapigee&si=5141814536306688&pi=
> e558dca3-da0a-4d9f-c1b3-6cb9174fcb5e>
> <http://t.sidekickopen06.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nM
> JW7t5XZs4WJfgqW4WJj7n3MP7VWW3LqXLC56dWRRf2H8CkP02?t=http%3A%
> 2F%2Fadapt.apigee.com%2F&si=5141814536306688&pi=e558dca3-
> da0a-4d9f-c1b3-6cb9174fcb5e>
>
> On Sat, Oct 15, 2016 at 11:35 AM, Boris Tyukin <boris@boristyukin.com>
> wrote:
>
> > Hi Ben,
> >
> > is it to address the issue I just described yesterday ""Issue with
> > Dynamically created tasks in a DAG""?
> >
> > I was hoping someone can confirm this as a bug and if there is a JIRA to
> > address that - otherwise I would be happy to open one. To me it is a
> pretty
> > major issue and a very misleading one especially because Airflow's key
> > feature is to generate/update DAGs programmatically
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<CANRT7T3OBZbNwcCebr18=OEHY18rOUoRWVLXY2RKmLTdNqV68w@mail.gmail.com>,Boris Tyukin <bo...@boristyukin.com>,0,0
182,183,"Hello DFDL community,

For initiators:

  *   Some data formats identify the start of data by preceding the data with a symbol
  *   DFDL calls that symbol an ""initiator""
     *   Other people call it a ""tag"" or a ""label"".

For terminators:

  *   Some data formats identify the end of data by following the data with a symbol
  *   DFDL calls that symbol a ""terminator""
     *   Other people call it ???

What do other people call terminators?

/Roger

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201909.mbox/%3cBL0PR0901MB312466DE05E2510084F98FCCC8850@BL0PR0901MB3124.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
258,259,"
     [ https://issues.apache.org/jira/browse/KAFKA-46?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Neha Narkhede reassigned KAFKA-46:
----------------------------------

    Assignee: Neha Narkhede
    
> Commit thread, ReplicaFetcherThread for intra-cluster replication
> -----------------------------------------------------------------
>
>                 Key: KAFKA-46
>                 URL: https://issues.apache.org/jira/browse/KAFKA-46
>             Project: Kafka
>          Issue Type: Bug
>            Reporter: Jun Rao
>            Assignee: Neha Narkhede
>
> We need to implement the commit thread at the leader and the fetcher thread at the follower
for replication the data from the leader.

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa
For more information on JIRA, see: http://www.atlassian.com/software/jira

        

",http://mail-archives.apache.org/mod_mbox/kafka-dev/201111.mbox/<20782639.3804.1320531651548.JavaMail.tomcat@hel.zones.apache.org>,"""Neha Narkhede (Assigned) (JIRA)"" <j...@apache.org>",0,0
96,97,"Someone pointed me at ""behavior testing"" for python using the behave
package:

https://pythonhosted.org/behave/tutorial.html


Basically, in natural language, you specify test cases, which correspond to
agile user stories.
It's as close to TDD as you can get.

In more elaborate scenarios, you can mix small datasets, return values etc.
with the test case description itself,
so it reads like a user story, annotated with data examples:

Scenario: some scenario  Given a set of specific users     | name
| department  |     | Barry     | Beer Cans   |     | Pudey     |
Silly Walks |     | Two-Lumps | Silly Walks |
 When we count the number of people in each department Then we will
find two people in ""Silly Walks""  But we will find one person in ""Beer
Cans""

I'm wondering whether behavior testing could be suitable for testing
workflows.
In the considerations, I've talked about files that an instrumented hook in
a test flow could pick up.
Maybe wiring them into the text of the test case a la this behavior testing
can make things a lot more readable.

Anyone having experience with behavior testing who can shine a bright light
on how this works out in practice
and if/how this contributes to the bottom line of higher quality?

Rgds,

Gerard




On Wed, May 10, 2017 at 8:27 AM, Gerard Toonstra <gtoonstra@gmail.com>
wrote:

> Hi Laura,
>
> Yes, testing hooks and operators is about the basic behavior of those, so
> you look for infrastructural issues. The idea is to have sound,
> robust components as a result of that testing that behave as you'd predict
> in all circumstances. This would also consider issues like returning values
> from
> operators that end up in xcom values for example, ways how operators
> interact with other operators and errors in connecting to systems,
> errors raised from operators, etc...  I'm not 100% sure about the scope
> yet if it's needed to physically connect to other systems, because you
> could claim it's the responsibility of the library you're using to make
> sure it can do this.
>
> For testing workflow, I think it suffices to instrument hooks to return
> data from flat files or throw exceptions. So it's a test script of some kind
> that decides and tests what happens in specific situations of data
> availability / exceptions / system availability. Inspecting the database can
> help with that. It really helps to reduce the dependency of a CI suite to
> your other infrastructure, because if one system is down, you may
> potentially not be able to deploy software for hotfixes, etc...  Even more
> importantly, the effort to load data into all your systems to prepare for
> the workflow testing is quite large and dealing with simpler ways to pass
> data around significantly helps reduce that testing effort. If testing
> is ""too difficult"" and takes too much effort, you'll see cases occur where
> it's bypassed because of delivery pressure. So the idea is to make it
> as easy as possible. Maybe it can also do checks on the queries that
> operators send to other systems to confirm the jinja templating works ok.
>
> ( eventually, many test suites around the world set up like this
> contribute to the quality of airflow itself ).
>
> For business testing, I'm suggesting to make it part of the actual DAG you
> run in production and run these checks on a daily basis, so it's not
> a test suite you run against it when everything finished, it's a check
> after each significant operation to confirm your code did what you expected
> it to do. The definition of how to check for that is vague though, because
> this is highly contextual and you don't necessarily know if your code
> runs until you deploy. So it may be best to recognize another level here:
>
> 1 Ensure that your business code runs (""it compiles""). So there should be
> at least one test to confirm queries do run?  Unless there is sufficient
>    abstraction from within the operator that it only deals with
> parametrization, in which case it may not be necessary.
>    I strongly believe in being as rigorous as you can in input validation
> and again, I think it's easier to run many cases using flat files rather
> than setting
>    up target systems in such a state that you get the right check done,
> thus it's mostly about effort again. 3rd party API's don't even allow you to
>     set these up, unless it's been done by some excellent developers with
> special id's.
>    But yeah, 3rd party API's always give surprises and that has to be
> resolved as you go along.
>
> 2. The rest is just running final checks in production daily in your
> regular DAG. Assuming that the underlying infrastructure/platform code is
> ok,
>    your components are robust, you did all the input validation checks
> that are necessary, your code compiles, you're mostly dealing with
>    potentially really weird values (but not falling outside the current
> validation boundaries), data volumes, dropped records, etc... which you can
>    correlate somehow with associated data or compare against history.
> Datadog for example offers a monitoring service where you can check with
>    a SARIMA model if the calculated avg margin/invoice has an expected
> value. BA's often use a number of checks to validate the quality of data
>    for a given day before they begin and I'm referring to such checks done
> automatically on a daily basis.
>
> Rgds,
>
> Gerard
>
>
> On Tue, May 9, 2017 at 9:46 PM, Arthur Wiedmer <arthur.wiedmer@gmail.com>
> wrote:
>
>> Hi,
>>
>> I would love to see if we can contribute some of the work we have done
>> internally at Airbnb to support some testing of DAGs. We have a long ways
>> to go though :)
>>
>> Best,
>> Arthur
>>
>> On Tue, May 9, 2017 at 12:34 PM, Sam Elamin <hussam.elamin@gmail.com>
>> wrote:
>>
>> > Thanks Gerard and Laura, I have created an email thread as agreed in the
>> > call so lets take the discussion there. If anyone else is interested in
>> > helping us build this library please do get in touch!
>> >
>> > On Tue, May 9, 2017 at 5:40 PM, Laura Lorenz <llorenz@industrydive.com>
>> > wrote:
>> >
>> > > Good points @Gerard. I think the distinctions you make between
>> different
>> > > testing considerations could help us focus our efforts. Here's my 2
>> cents
>> > > in the buckets you describe; I'm wondering if any of these use cases
>> > align
>> > > with anyone else and can help narrow our scope, and if I understood
>> you
>> > > right @Gerard:
>> > >
>> > > Regarding platform code: For our own platform code (ie custom
>> Operators
>> > and
>> > > Hooks), we have our CI platform running unittests on their
>> construction
>> > > and, in the case of hooks, integration tests on connectivity. The
>> latter
>> > > involves us setting up test integration services (i.e. a test MySQL
>> > > process) which we start up as docker containers and we flip our
>> airflow's
>> > > configuration to point at them during testing using environment
>> > variables.
>> > > It seems from a browse on airflow's testing that operators and hooks
>> are
>> > > mostly unittested, with the integrations mocked or skipped (ie
>> > > https://github.com/apache/incubator-airflow/blob/master/
>> > > tests/contrib/hooks/test_jira_hook.py#L40-L41
>> > > or
>> > > https://github.com/apache/incubator-airflow/blob/master/
>> > > tests/contrib/hooks/test_sqoop_hook.py#L123-L125).
>> > > If the hook is using some other, well tested library to actually
>> > establish
>> > > the connection, the case can probably be made here that the custom
>> > operator
>> > > and hook authors don't need integration tests, so since the normal
>> > unittest
>> > > library is enough to handle these that might not need to be in scope
>> for
>> > a
>> > > new testing library to describe.
>> > >
>> > > Regarding data manipulation functions of the business code:
>> > > For us, we run tests on each operator in each DAG on CI, seeded with
>> test
>> > > input data, asserted against known output data, all of which we have
>> > > compiled over time to represent different edge cases we expect or have
>> > > seen. So this is a test at the level of the operator as described in a
>> > > given DAG. Because we only describe edge cases we have seen or can
>> > predict,
>> > > its a very reactive way to handle testing at this level.
>> > >
>> > > If I understand your idea right, another way to test (or at least,
>> > surface
>> > > errors) at this level is, given you have a DAG that is resilient
>> against
>> > > arbitrary data failures, your DAG should include a validation
>> task/report
>> > > at its end or a test suite should run daily against the production
>> error
>> > > log for that DAG that surfaces errors your business code encountered
>> on
>> > > production data. I think this is really interesting and reminds me of
>> an
>> > > airflow video I saw once (can't remember who gave the talk) on a DAG
>> > whose
>> > > last task self-reported error counts and rows lost. If implemented as
>> a
>> > > test suite you would run against production this might be a direction
>> we
>> > > would want a testing library to go into.
>> > >
>> > > Regarding the workflow correctness of the business code:
>> > > What we set out to do on our side was a hybrid version of your item 1
>> > and 2
>> > > which we call ""end-to-end tests"": to call a whole DAG against 'real'
>> > > existing systems (though really they are test docker containers of the
>> > > processes we need (MySQL and Neo4J specifically) that we use
>> environment
>> > > variables to switch our airflow to use when instantiating hooks etc),
>> > > seeded with test input files for services that are hard to set up
>> (i.e.
>> > > third party APIs we ingest data from). Since the whole DAG is seeded
>> with
>> > > known input data, this gives us a way to compare the last output of a
>> DAG
>> > > to a known file, so that if any workflow changes OR business logic in
>> the
>> > > middle affected the final output, we would know as part of our test
>> suite
>> > > instead of when production breaks. In other words, a way to test a
>> > > regression of the whole DAG. So this is the framework we were thinking
>> > > needed to be created, and is a direction we could go with a testing
>> > library
>> > > as well.
>> > >
>> > > This doesn't get to your point of determining what workflow was used,
>> > which
>> > > is interesting, just not a use case we have encountered yet (we only
>> have
>> > > deterministic DAGs). In my mind in this case we would want a testing
>> > suite
>> > > to be able to more or less turn some DAGs ""on"" against seeded input
>> data
>> > > and mocked or test integration services, let a scheduler go at it, and
>> > then
>> > > check the metadata database for what workflow happened (and, if we had
>> > test
>> > > integration services, maybe also check the output against the known
>> > output
>> > > for the seeded input). I can definitely see your suggestion of
>> developing
>> > > instrumentation to inspect a followed workflow as a useful addition a
>> > > testing library could include.
>> > >
>> > > To some degree our end-to-end DAG tests overlaps in our workflow with
>> > your
>> > > point 3 (UAT environment), but we've found that more useful to test if
>> > > ""wild data"" causes uncaught exceptions or any integration errors with
>> > > difficult-to-mock third party services, not DAG level logic
>> regressions,
>> > > since the input data is unknown and thus we can't compare to a known
>> > output
>> > > in this case, depending instead on a fallible human QA or just
>> accepting
>> > > that the DAG running with no exceptions as passing UAT.
>> > >
>> > > Laura
>> > >
>> > > On Tue, May 9, 2017 at 2:15 AM, Gerard Toonstra <gtoonstra@gmail.com>
>> > > wrote:
>> > >
>> > > > Very interesting video. I was unable to take part. I watched only
>> part
>> > of
>> > > > it for now.
>> > > > Let us know where the discussion is being moved to.
>> > > >
>> > > > The confluence does indeed seem to be the place to put final
>> > conclusions
>> > > > and thoughts.
>> > > >
>> > > > For airflow, I like to make a distinction between ""platform"" and
>> > > ""business""
>> > > > code. The platform code are
>> > > > the hooks and operators and provide the capabilities of what your
>> ETL
>> > > > system can do. You'll test this
>> > > > code with a lot of thoroughness, such that each component behaves
>> how
>> > > you'd
>> > > > expect, judging from
>> > > > the constructor interface. Any abstractions in there (like copying
>> > files
>> > > to
>> > > > GCS) should be kept as hidden
>> > > > as possible (retries, etc).
>> > > >
>> > > > The ""business"" code is what runs on a daily basis. This can be
>> divided
>> > in
>> > > > another two concerns
>> > > > for testing:
>> > > >
>> > > > 1 The workflow, the code between the data manipulation functions
>> that
>> > > > decides which operators get called
>> > > > 2 The data manipulation function.
>> > > >
>> > > >
>> > > > I think it's good practice to run tests on ""2"" on a daily basis and
>> not
>> > > > just once on CI. The reason is that there
>> > > > are too many unforeseen circumstances where data can get into a bad
>> > > state.
>> > > > So such tests shouldn't run
>> > > > once on a highly controlled environment like CI, but run daily in
a
>> > less
>> > > > predictable environment like production,
>> > > > where all kind of weird things can happen, but you'll be able to
>> catch
>> > > with
>> > > > proper checks in place. Even if the checks
>> > > > are too rigorous, you can skip them and improve on them, so that it
>> > fits
>> > > > what goes on in your environment
>> > > > to your best ability.
>> > > >
>> > > >
>> > > > Which mostly leaves testing workflow correctness and platform code.
>> > What
>> > > I
>> > > > had intended to do was;
>> > > >
>> > > > 1. Test the platform code against real existing systems (or maybe
>> > docker
>> > > > containers), to test their behavior
>> > > >     in success and failure conditions.
>> > > > 2. Create workflow scripts for testing the workflow; this probably
>> > > requires
>> > > > some specific changes in hooks,
>> > > >    which wouldn't call out to other systems, but would just pick up
>> > small
>> > > > files you prepare from a testing repo
>> > > >    and pass them around. The test script could also simulate
>> > > > unavailability, etc.
>> > > >    This relieves you of a huge responsibility of setting up systems,
>> > > docker
>> > > > containers and load that with data.
>> > > >     Airflow sets up pretty quickly as a docker container and you can
>> > also
>> > > > start up a sample database with that.
>> > > >     Afterwards, from a test script, you can check which workflow was
>> > > > followed by inspecting the database,
>> > > >    so develop some instrumentation for that.
>> > > > 3. Test the data manipulation in a UAT environment, mirrorring the
>> runs
>> > > in
>> > > > production to some extent.
>> > > >     That would be a place to verify if the data comes out correctly
>> and
>> > > > also show people what kind of
>> > > >    monitoring is in place to double-check that.
>> > > >
>> > > >
>> > > > On Tue, May 9, 2017 at 1:14 AM, Arnie Salazar <
>> asalazar@riotgames.com>
>> > > > wrote:
>> > > >
>> > > > > Scratch that. I see the whole video now.
>> > > > >
>> > > > > On Mon, May 8, 2017 at 3:33 PM Arnie Salazar <
>> asalazar@riotgames.com
>> > >
>> > > > > wrote:
>> > > > >
>> > > > > > Thanks Sam!
>> > > > > >
>> > > > > > Is there a part 2 to the video? If not, can you post the
""next
>> > steps""
>> > > > > > notes you took whenever you have a chance?
>> > > > > >
>> > > > > > Cheers,
>> > > > > > Arnie
>> > > > > >
>> > > > > > On Mon, May 8, 2017 at 3:08 PM Sam Elamin <
>> hussam.elamin@gmail.com
>> > >
>> > > > > wrote:
>> > > > > >
>> > > > > >> Hi Folks
>> > > > > >>
>> > > > > >> For those of you who missed it, you can catch the discussion
>> from
>> > > the
>> > > > > link
>> > > > > >> on this tweet <https://twitter.com/samelamin/status/
>> > > > 861703888298225670>
>> > > > > >>
>> > > > > >> Please do share and feel free to get involved as the
more
>> feedback
>> > > we
>> > > > > get
>> > > > > >> the better the library we create is :)
>> > > > > >>
>> > > > > >> Regards
>> > > > > >> Sam
>> > > > > >>
>> > > > > >> On Mon, May 8, 2017 at 9:43 PM, Sam Elamin <
>> > hussam.elamin@gmail.com
>> > > >
>> > > > > >> wrote:
>> > > > > >>
>> > > > > >> > Bit late notice but the call is happening today
at 9 15 utc
>> so
>> > in
>> > > > > about
>> > > > > >> >  30 mins or so
>> > > > > >> >
>> > > > > >> > It will be recorded but if anyone would like to
join in on
>> the
>> > > > > >> discussion
>> > > > > >> > the hangout link is https://hangouts.google.com/hangouts/_/
>> > > > > >> > mbkr6xassnahjjonpuvrirxbnae
>> > > > > >> >
>> > > > > >> > Regards
>> > > > > >> > Sam
>> > > > > >> >
>> > > > > >> > On Fri, 5 May 2017 at 21:35, Ali Uz <aliuz1@gmail.com>
>> wrote:
>> > > > > >> >
>> > > > > >> >> I am also very interested in seeing how this
turns out. Even
>> > > though
>> > > > > we
>> > > > > >> >> don't have a testing framework in-place on
the project I am
>> > > working
>> > > > > >> on, I
>> > > > > >> >> would very much like to contribute to some
general framework
>> > for
>> > > > > >> testing
>> > > > > >> >> DAGs.
>> > > > > >> >>
>> > > > > >> >> As of now we are just implementing dummy tasks
that test our
>> > > actual
>> > > > > >> tasks
>> > > > > >> >> and verify if the given input produces the
expected output.
>> > > Nothing
>> > > > > >> crazy
>> > > > > >> >> and certainly not flexible in the long run.
>> > > > > >> >>
>> > > > > >> >>
>> > > > > >> >> On Fri, 5 May 2017 at 22:59, Sam Elamin <
>> > hussam.elamin@gmail.com
>> > > >
>> > > > > >> wrote:
>> > > > > >> >>
>> > > > > >> >> > Haha yes Scott you are in!
>> > > > > >> >> > On Fri, 5 May 2017 at 20:07, Scott Halgrim
<
>> > > > > scott.halgrim@zapier.com
>> > > > > >> >
>> > > > > >> >> > wrote:
>> > > > > >> >> >
>> > > > > >> >> > > Sounds A+ to me. By “both of you”
did you include me? My
>> > > first
>> > > > > >> >> response
>> > > > > >> >> > > was just to your email address.
>> > > > > >> >> > >
>> > > > > >> >> > > On May 5, 2017, 11:58 AM -0700, Sam
Elamin <
>> > > > > >> hussam.elamin@gmail.com>,
>> > > > > >> >> > > wrote:
>> > > > > >> >> > > > Ok sounds great folks
>> > > > > >> >> > > >
>> > > > > >> >> > > > Thanks for the detailed response
laura! I'll invite
>> both
>> > of
>> > > > you
>> > > > > >> to
>> > > > > >> >> the
>> > > > > >> >> > > > group if you are happy and we
can schedule a call for
>> > next
>> > > > > week?
>> > > > > >> >> > > >
>> > > > > >> >> > > > How does that sound?
>> > > > > >> >> > > > On Fri, 5 May 2017 at 17:41,
Laura Lorenz <
>> > > > > >> llorenz@industrydive.com
>> > > > > >> >> >
>> > > > > >> >> > > wrote:
>> > > > > >> >> > > >
>> > > > > >> >> > > > > We do! We developed our
own little in-house DAG test
>> > > > > framework
>> > > > > >> >> which
>> > > > > >> >> > we
>> > > > > >> >> > > > > could share insights on/would
love to hear what
>> other
>> > > folks
>> > > > > >> are up
>> > > > > >> >> > to.
>> > > > > >> >> > > > > Basically we use mock a
DAG's input data, use the
>> > > > BackfillJob
>> > > > > >> API
>> > > > > >> >> > > directly
>> > > > > >> >> > > > > to call a DAG in a test,
and compare its outputs to
>> the
>> > > > > >> intended
>> > > > > >> >> > result
>> > > > > >> >> > > > > given the inputs. We use
docker/docker-compose to
>> > manage
>> > > > > >> services,
>> > > > > >> >> > and
>> > > > > >> >> > > > > split our dev and test
stack locally so that the
>> tests
>> > > have
>> > > > > >> their
>> > > > > >> >> own
>> > > > > >> >> > > > > scheduler and metadata
database and so that our CI
>> tool
>> > > > knows
>> > > > > >> how
>> > > > > >> >> to
>> > > > > >> >> > > > > construct the test stack
as well.
>> > > > > >> >> > > > >
>> > > > > >> >> > > > > We co-opted the BackfillJob
API for our own purposes
>> > > here,
>> > > > > but
>> > > > > >> it
>> > > > > >> >> > > seemed
>> > > > > >> >> > > > > overly complicated and
fragile to start and interact
>> > with
>> > > > our
>> > > > > >> own
>> > > > > >> >> > > > > in-test-process executor
like we saw in a few of the
>> > > tests
>> > > > in
>> > > > > >> the
>> > > > > >> >> > > Airflow
>> > > > > >> >> > > > > test suite. So I'd be really
interested on finding a
>> > way
>> > > to
>> > > > > >> >> > streamline
>> > > > > >> >> > > how
>> > > > > >> >> > > > > to describe a test executor
for both the Airflow
>> test
>> > > suite
>> > > > > and
>> > > > > >> >> > > people's
>> > > > > >> >> > > > > own DAG testing and make
that a first class type of
>> > API.
>> > > > > >> >> > > > >
>> > > > > >> >> > > > > Laura
>> > > > > >> >> > > > >
>> > > > > >> >> > > > > On Fri, May 5, 2017 at
11:46 AM, Sam Elamin <
>> > > > > >> >> hussam.elamin@gmail.com
>> > > > > >> >> > > > > wrote:
>> > > > > >> >> > > > >
>> > > > > >> >> > > > > > Hi All
>> > > > > >> >> > > > > >
>> > > > > >> >> > > > > > A few people in the
Spark community are
>> interested in
>> > > > > >> writing a
>> > > > > >> >> > > testing
>> > > > > >> >> > > > > > library for Airflow.
We would love anyone who uses
>> > > > Airflow
>> > > > > >> >> heavily
>> > > > > >> >> > in
>> > > > > >> >> > > > > > production to be involved
>> > > > > >> >> > > > > >
>> > > > > >> >> > > > > > At the moment (AFAIK)
testing your DAGs is a bit
>> of a
>> > > > pain,
>> > > > > >> >> > > especially if
>> > > > > >> >> > > > > > you want to run them
in a CI server
>> > > > > >> >> > > > > >
>> > > > > >> >> > > > > > Is anyone interested
in being involved in the
>> > > discussion?
>> > > > > >> >> > > > > >
>> > > > > >> >> > > > > > Kind Regards
>> > > > > >> >> > > > > > Sam
>> > > > > >> >> > > > > >
>> > > > > >> >> > > > >
>> > > > > >> >> > >
>> > > > > >> >> >
>> > > > > >> >>
>> > > > > >> >
>> > > > > >>
>> > > > > >
>> > > > >
>> > > >
>> > >
>> >
>>
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CAM5819Snsuu=3mWYycVC_iy89STJLpqHHDp-xCCX3Zd2yksCNw@mail.gmail.com>,Gerard Toonstra <gtoons...@gmail.com>,0,1
234,235,"-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello,

The Apache Taverna team are happy to announce the release of Taverna
Parent 1 incubating and Taverna Language 0.15.0 incubating.

Apache Taverna Language is a set of APIs for workflow definitions
(SCUFL2) and workflow inputs/outputs/run (DataBundle), as consumed and
produced by the Apache Taverna workflow system. The API includes
support for working with Research Object Bundles, and loading/saving
Taverna workflows in different formats.

The release artifacts are downloadable from:

https://www.apache.org/dyn/closer.cgi/incubator/taverna/

Maven JAR artifacts are available from:

https://repository.apache.org/content/repositories/releases/org/apache/t
averna/

Release notes are available from:

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=1231832
2&version=12332247

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=1231832
2&version=12332246

More details on Apache Taverna can be found at:
http://taverna.incubator.apache.org/

We would like to thank all contributors who made this release possible.

Thanks,

The Apache Taverna team
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVy2qXAAoJEPK45GBX+Cy5TmIIAKrz/kVO1FAkRGsg760y4aJp
+2W0262HCW+sdMkJauJAjyW7+W8LTBoBr1GquPYEE1wl1KVz6H63J4VFxuUdOyMn
0ujH4Y9Nqw8H76enm+hGba8rUxymfIcXEzWy6fBq7G2ho3rzosEcW+Mco6+Md3up
J0fY1mS7+WYjf8X6vrRQDWeX5eWt4tte2kVAFUQeIDrsSj6zc+fJ8sYOylRfHfx1
3tnfpDjBA3QNfmui5WDok/LX/LqI8cPOwyyA9bu8jp+CSlAEB38TBGGPa47+5ZK6
jty9SqMOkOwUwFz2I2aKnYiaD3CErUJqWqPWnT+oGdwi7WfnHqi+2IRrSwfkjbM=
=0LG4
-----END PGP SIGNATURE-----

",http://mail-archives.apache.org/mod_mbox/incubator-taverna-users/201508.mbox/%3c55CB6A97.2080204@manchester.ac.uk%3e,Ian Dunlop <ian.dun...@manchester.ac.uk>,0,1
278,279,"Dear David.

Pretty much make sense to me!
Thanks for the reply.

Best regards
Dominic

2018-07-17 22:39 GMT+09:00 david.breitgand@gmail.com <
david.breitgand@gmail.com>:

> Hi Dominic,
>
> Lean OpenWhisk is not supposed to run on the IoT devices such as sensors
> and actuators directly. It's supposed to run on a Gateway node that
> controls the sensors and actuators connected to it. Think AWS GreenGrass,
> Azure Functions on IoT Edge. This is the same use case. The data from a
> sensor, say a temperature sensor reading, will be sent to the Gateway via
> MQTT or HTTP or whatever and there will be a provider at the Gateway (say,
> an MQTT feed, which is outside of the OW core and this proposal) that can
> trigger an action on a trigger previously created via a feed action for
> this type of feed.
>
> This proposal is strictly about making OW a better fit for small Gateway
> form factors.
>
> It's true that there are some other tools we need to provide to make
> OW@Edge a feasible option for developers, but they are outside of the
> core and this specific proposal and merit a separate discussion.
>
> Cheers.
>
> -- david
>
> On 2018/07/16 11:40:35, Dominic Kim <style9595@gmail.com> wrote:
> > Dear David.
> >
> > This is an awesome idea!!
> >
> > Is this to control IoT devices programmatically?
> > If yes, there would be many different types of IoT devices especially in
> > terms of their capabilities such as lighting sensors, thermometer, robot
> > cleaner, and so on.
> >
> > Then do you have anything in mind to take care of heterogeneous sets of
> > edge nodes?
> > There is a possibility that some actions should only run on thermometers,
> > while the others should run on lighting sensors.
> >
> > If you are trying to install ""one-for-all"" OpenWhisk cluster rather than
> > having separate OpenWhisk clusters for each device types, how will you
> > manage heterogenous container pools and properly assign relevant actions
> to
> > them?
> >
> >
> > Best regards,
> > Dominic
> >
> >
> > 2018-07-16 20:24 GMT+09:00 Markus Thoemmes <markus.thoemmes@de.ibm.com>:
> >
> > > Hi David,
> > >
> > > please send your PR for sure! IIRC we made the Loadbalancer pluggable
> > > specifically for this use-case. Sounds like a great addition to our
> > > possible deployment topologies.
> > >
> > > Shameless plug: Would you review the architecture I proposed here:
> > > https://lists.apache.org/thread.html/29289006d190b2c68451f7625c13bb
> > > 8020cc8e9928db66f1b0def18e@%3Cdev.openwhisk.apache.org%3E
> > >
> > > In theory, this could make your proposal even leaner in the future.
> Don't
> > > hear me say though we should hold this back, we can absolutely go
> forward
> > > with your implementation first. Just wanted to quickly verify this
> use-case
> > > will also work with what we might plan for in the future.
> > >
> > > Cheers,
> > > Markus
> > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/openwhisk-dev/201808.mbox/<CAFEpjOqa5tpMf5xsZSPfqrAP1TFCrMzRaOMm8+YAHmEryVLOow@mail.gmail.com>,Dominic Kim <style9...@gmail.com>,0,1
145,146,"Nice! so if I submit a streaming query over REST can I get the results back
over REST or Websocket?

On Tue, Nov 28, 2017 at 10:29 PM, Saisai Shao <sai.sai.shao@gmail.com>
wrote:

> Livy doesn't add any restriction on how user uses Spark API, so of course
> Structured Streaming is supported.
>
> 2017-11-29 14:21 GMT+08:00 kant kodali <kanth909@gmail.com>:
>
>> Hi All,
>>
>> Does Apache Livy support Spark Structured Streaming 2.2.0? If so, any
>> examples please? preferably in Java.
>>
>> Thanks,
>> kant
>>
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-livy-user/201711.mbox/%3cCA+iiNx8supJ=rj4_k4yjcyAvgGG-LY-7Uy3==kyLGni0r0q0Aw@mail.gmail.com%3e,kant kodali <kanth...@gmail.com>,0,0
247,248,"Good job, guys.  ;)


On Tue, Jul 30, 2013 at 10:33 AM, Andrew Phillips <aphillips@qrmedia.com>wrote:

> I'll check with Ninefold...
>>>
>>
>> Could we avoid worrying about logo copyrights with a textual list?  This
>> seems like a low-value activity and we should not dress up the Apache
>> Wiki like a NASCAR.
>>
>
> I'm using text for now - works for me. Logos make it look a bit fancier
> but I'd certainly make them smaller and add text too.
>
> But I'll stop now on suspicion of a bike-shedding risk ;-)
>
> ap
>

",http://mail-archives.apache.org/mod_mbox/jclouds-dev/201307.mbox/<CAMxpo7j1iRh3GuybhFJ3GDxDnck2+rDr9MLHfnvwX+MFKzANBA@mail.gmail.com>,Becca Wood <silky...@apache.org>,0,0
122,123,"The messages started indicating a permissions issue after I upgraded
Geocoder.php.  Geocoder.php was set to rwx for only the root user, changing
to all users fixes what I saw...

On Thu, Oct 8, 2009 at 2:11 PM, Akara Sucharitakul <
Akara.Sucharitakul@sun.com> wrote:

> Can you please elaborate what permission issue has caused it? Thanks.
>
> -Akara
>
> Joshua Schnee wrote:
>
>> By using the included Geocoder.php I was able to determine that there was
>> a permissions issue with the file itself.  After correcting it, there errors
>> go away.
>>
>> Thanks for the help,
>>
>>
>> On Wed, Oct 7, 2009 at 11:36 AM, Akara Sucharitakul <
>> Akara.Sucharitakul@sun.com <mailto:Akara.Sucharitakul@sun.com>> wrote:
>>
>>    Turns out the file I attached was before saving the changes. Please
>>    use this one instead. Thanks.
>>
>>    -Akara
>>
>>
>>    Akara Sucharitakul wrote:
>>
>>        This looks good to me (as I suspected). Based on this and the
>>        previous error you've got, the problem tends to be in
>>        classes/Geocoder.php line 33. Also the single quote on the php
>>        may not have allowed for proper formatting.
>>
>>        I've made some very minor changes to Geocoder.php, just to make
>>        sure the messages come out right. Can you please try drop it in
>>        and see whether the $stream shows up in the message? If it
>>        doesn't, we need to check what curl_string is doing. It's also a
>>        function in this file.
>>
>>        Thanks,
>>        -Akara
>>
>>        Joshua Schnee wrote:
>>
>>            So here's the results from my manual test.  I'm still seeing
>>            these errors in my httpd error_log, but the manual request
>>            looks OK to me.  Am I missing something?
>>
>>            URL request:
>>
>> http://192.168.1.5:8080/geocoder/geocode?street=186+Qgt+Blvd&city=avhtapshc&state=IG&zip=62221&
>>            <
>> http://192.168.1.5:8080/geocoder/geocode?street=186+Qgt+Blvd&city=avhtapshc&state=IG&zip=62221&
>> >
>>            <
>> http://192.168.1.5:8080/geocoder/geocode?street=186+Qgt+Blvd&city=avhtapshc&state=IG&zip=62221&
>>            <
>> http://192.168.1.5:8080/geocoder/geocode?street=186+Qgt+Blvd&city=avhtapshc&state=IG&zip=62221&
>> >>
>>
>>
>>            <?xml version=""1.0"" ?>
>>            <ResultSet xmlns=""urn:yahoo:maps""
>>            xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
>>            xsi:schemaLocation=""urn:yahoo:maps
>>            http://api.local.yahoo.com/MapsService/V1/GeocodeResponse.xsd
>> "">
>>            <Result precision=""address"">
>>            <Latitude>
>>            33.0000
>>            </Latitude><Longitude>
>>            -177.0000
>>            </Longitude><Address>
>>            186 Qgt Blvd
>>            </Address><City>
>>            avhtapshc
>>            </City><State>
>>            IG
>>            </State><zip>
>>            62221
>>            </zip><Country>
>>            USA
>>
>>            </Country>
>>            </Result>
>>            </ResultSet>
>>
>>
>>            BTW, I've since needed to change my ipaddress, so here's my
>>            updated geocoder entry in config.php.
>>            $olioconfig['geocoderURL'] =
>>            'http://192.168.1.5:8080/geocoder/geocode'; //Geocoder URL
>>
>>            Thanks,
>>
>>            On Mon, Oct 5, 2009 at 9:32 PM, Akara Sucharitakul
>>            <Akara.Sucharitakul@sun.com
>>            <mailto:Akara.Sucharitakul@sun.com>
>>            <mailto:Akara.Sucharitakul@sun.com
>>            <mailto:Akara.Sucharitakul@sun.com>>> wrote:
>>
>>               Josh,
>>
>>               Here's the request URL generation from the php code.
>>
>>                      $url =
>>            Web20::$config['geocoderURL'].'?appid=gsd5f&street='.
>>                              $this->street.'&city='.$this->city.
>>                              '&state='.$this->state.'&zip='.$this0->zip;
>>
>>               Try making a request based on this get request string.
>>            The fields
>>               are in the exception you sent. (186 Qgt Blvd...)
>>
>>               Let me know what comes out in your case. You can use the
>>            browser to
>>               make that request. Just do a view page source on the
>>            response to see
>>               the xml. Thanks.
>>
>>               -Akara
>>
>>
>>
>>                Joshua Schnee wrote:
>>
>>                   Hi,
>>
>>                   I'm wondering if there is a good way to test the
>>            tomcat/geocoder
>>                   setup.  Both my SUT and client seem to be able to
>>            reach the
>>                   geocoder but I'm currently seeing the following
>>            errors in my
>>                   httpd error_log.
>>
>>                   /[Thu Oct 01 19:30:49 2009] [error] [client
>> 10.20.52.194]
>>                   exception 'Exception' with message 'Did not find xml
>>            part in:
>>                   $stream' in
>>            /var/www/html/oliophp/classes/Geocoder.php:36\nStack
>>                   trace:\n#0
>>
>> /var/www/html/oliophp/public_html/addEventResult.php(52):
>>                   Geocoder->__construct('186+Qgt+Blvd', 'avhtapshct',
>> 'IG',
>>                   '62221')\n#1 {main}
>>                   /
>>                   In the past, this meant that my geocoder wasn't up or
>>            configured
>>                   appropriately in the config.php.  I am trying to run
>>            it on the
>>                   same system as the one under test so my config.php
>>            has the
>>                   following line :
>>                   $olioconfig['geocoderURL'] =
>>                   'http://10.20.52.197:8080/geocoder/geocode';
>>            //Geocoder URL
>>
>>                   This url gives the following output for both the SUT
>>            and the
>>                   client driver.
>>
>>                   SUT:
>>                   33.0000 -177.0000
>>                   null
>>                   null null null USA
>>
>>                   Client:
>>                    <?xml version=""1.0"" ?>
>>                   - <ResultSet xmlns=""urn:yahoo:maps""
>>                   xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
>>                   xsi:schemaLocation=""urn:yahoo:maps
>>
>> http://api.local.yahoo.com/MapsService/V1/GeocodeResponse.xsd"">
>>                   - <Result precision=""address"">
>>                    <Latitude>33.0000</Latitude>
>>                    <Longitude>-177.0000</Longitude>
>>                    <Address>null</Address>
>>                    <City>null</City>
>>                    <State>null</State>
>>                    <zip>null</zip>
>>                    <Country>USA</Country>
>>                    </Result>
>>                    </ResultSet>
>>                   --         SUT IP : 10.20.52.197
>>                   Client IP : 10.20.52.194
>>
>>                   Ideas?
>>                   -Josh
>>
>>
>>
>>
>>
>>            --            -Josh
>>
>>
>>
>>
>>
>>
>> --
>> -Josh
>>
>>
>


-- 
-Josh

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200910.mbox/%3c8207aa2d0910081227n184fe8b6l16cadb75a84fc6@mail.gmail.com%3e,Joshua Schnee <jpsch...@gmail.com>,0,0
281,282,"
     [ https://issues.apache.org/jira/browse/JSPWIKI-329?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Florian Holeczek updated JSPWIKI-329:
-------------------------------------

        Fix Version/s:     (was: 2.7.x)
    Affects Version/s: 2.7.x

> wrong version history grouping on page info tab
> -----------------------------------------------
>
>                 Key: JSPWIKI-329
>                 URL: https://issues.apache.org/jira/browse/JSPWIKI-329
>             Project: JSPWiki
>          Issue Type: Bug
>          Components: Default template
>    Affects Versions: 2.6.3, 2.7.x
>            Reporter: Florian Holeczek
>            Priority: Minor
>         Attachments: JSPWIKI-329.png
>
>
> The version history is grouped by 20 entries per page. However, the grouping should follow
the sorting order. At the moment, it starts at version-1, which results e.g. in only displayed
version on page 1 if there are 20 versions available (see attached screenshot).

-- 
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.


",http://mail-archives.apache.org/mod_mbox/jspwiki-dev/200808.mbox/<1020148494.1217581712831.JavaMail.jira@brutus>,"""Florian Holeczek (JIRA)"" <j...@apache.org>",0,0
300,301,"
     [ https://issues.apache.org/jira/browse/AMBARI-2985?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Mahadev konar resolved AMBARI-2985.
-----------------------------------

    Resolution: Fixed

Committed.
                
> Remove stack 1.3 and 1.2 from the Stack Options.
> ------------------------------------------------
>
>                 Key: AMBARI-2985
>                 URL: https://issues.apache.org/jira/browse/AMBARI-2985
>             Project: Ambari
>          Issue Type: Bug
>    Affects Versions: 1.4.0
>            Reporter: Mahadev konar
>            Assignee: Mahadev konar
>             Fix For: 1.4.0
>
>         Attachments: AMBARI-2985.patch
>
>
> Remove stack 1.3 and 1.2 from the Stack Options.

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators
For more information on JIRA, see: http://www.atlassian.com/software/jira

",http://mail-archives.apache.org/mod_mbox/ambari-dev/201308.mbox/<JIRA.12664974.1377151223672.15479.1377152691942@arcas>,"""Mahadev konar (JIRA)"" <j...@apache.org>",1,0
163,164,"Hi,

I have a MySQL table, which will be stored some static information. The information could
be different for different airflow runs, so I hope to use python code to initialize it whenever
airflow starts.


Where is the best place to put such code ?


Is the class DagBag's __init__() a good candidate ?


Please advise.


Thanks.


#############################################

class DagBag(LoggingMixin):
    """"""
    A dagbag is a collection of dags, parsed out of a folder tree and has high
    level configuration settings, like what database to use as a backend and
    what executor to use to fire off tasks. This makes it easier to run
    distinct environments for say production and development, tests, or for
    different teams or security profiles. What would have been system level
    settings are now dagbag level so that one system can run multiple,
    independent settings sets.

    :param dag_folder: the folder to scan to find DAGs
    :type dag_folder: str
    :param executor: the executor to use when executing task instances
        in this DagBag
    :param include_examples: whether to include the examples that ship
        with airflow or not
    :type include_examples: bool
    :param sync_to_db: whether to sync the properties of the DAGs to
        the metadata DB while finding them, typically should be done
        by the scheduler job only
    :type sync_to_db: bool
    """"""
    def __init__(
            self,
            dag_folder=None,
            executor=DEFAULT_EXECUTOR,
            include_examples=configuration.getboolean('core', 'LOAD_EXAMPLES'),
            sync_to_db=False):

        dag_folder = dag_folder or DAGS_FOLDER
        self.logger.info(""Filling up the DagBag from {}"".format(dag_folder))
        self.dag_folder = dag_folder
        self.dags = {}
        self.sync_to_db = sync_to_db
        self.file_last_changed = {}
        self.executor = executor
        self.import_errors = {}
        if include_examples:
            example_dag_folder = os.path.join(

...

#############################


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201611.mbox/<DM5PR20MB133898FBD5F2E5757BB1D7EFB9A10@DM5PR20MB1338.namprd20.prod.outlook.com>,Michael Gong <go...@hotmail.com>,0,0
236,237,"Hi all,

I am very new to Airflow. I am looking for a particular feature in airflow,
but I could not find out whether airflow supports it.

What I am looking for is, executor pooling. i.e. assigning a set of jobs to
a particular pool of executors.

Is this supported in airflow, or would airflow submit jobs to a common pool
of executors?

Best regards

Niranda Perera
Research Assistant
Dept of CSE, University of Moratuwa
niranda.17@cse.mrt.ac.lk
+94 71 554 8430
https://lk.linkedin.com/in/niranda

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CA+1xF3oECsw3F6o+mJ-qKhOLpQMOxk3ef746Etoh1a=kpnG7VA@mail.gmail.com>,Niranda Perera <niranda...@cse.mrt.ac.lk>,0,0
192,193,"Sorry all, the PDF is too large to accept on the mailing list.
I've tried compressing it, let's see if this works.
Brian

>
> On 19 December 2016 at 10:51, Brian Spector <brian.spector@miracl.com>
> wrote:
>
>> Hi, enclosed is a draft of the Apache Milagro Ecosystem paper that has
>> been circulating publicly on the Milagro Slack channel, apologies for not
>> depositing on dev@ first, we're still learning the Apache Way.
>> Go Yamamoto of NTT has suggested another form of bootstrapping trust
>> between two Milagro Ecosystem components in an IoT setting by using 'burned
>> in' public parameters.
>> Go, I would assume that in a scenario that endpoint A that wishes to
>> communicate with endpoint B would create an AES key, and encapsulate that
>> key with ID of endpoint B and then send over the AES key to endpoint B?
>> Perhaps you can elaborate on your solution for bootstrapping trust so we
>> have 2 methods, a) blockchain and b) IBE public parameters.
>> I include the original Latex files for completeness and will send the PDF
>> in another email (as combined they go over file size limits).
>> Thanks
>> Brian
>> --
>>
>>

",http://mail-archives.apache.org/mod_mbox/milagro-dev/201612.mbox/<CAP5eBiuh-73ak0hk2t7ejXvft2gwoKVjaOqyeFLs8fndn7GeHg@mail.gmail.com>,Brian Spector <brian.spec...@miracl.com>,1,0
81,82,"binarySeconds only works for xs:dateTime elements, which means that the
resulting infoset will also have a day/month/year/hour/minute/second
parts. There's no way around that. Note that the dfdl:calendarPattern
does not describe the format of the infoset, but is used to describe the
format of input calendar textual data. The infoset will always be
YYYY-mm-dd hh:mm:ss.

If you wanted only the year, you would need to parse the field as a full
dateTime, and then use inputValueCalc to extract just the year. For example:

  <xs:element name=""root"">
    <xs:complexType>
      <xs:sequence>
        <xs:element name=""dateTime"" type=""xs:dateTime""
          dfdl:lengthKind=""explicit""
          dfdl:lengthUnits=""bytes""
          dfdl:length=""4""
          dfdl:binaryCalendarRep=""binarySeconds""
          dfdl:binaryCalendarEpoch=""1970-01-01T00:00:00"" />
        <xs:element name=""year"" type=""xs:int""
          dfdl:inputValueCalc=""{ fn:year-from-dateTime(../dateTime) }"" />
      </xs:sequence>
    </xs:complexType>
  </xs:element>



On 10/1/19 8:38 AM, Costello, Roger L. wrote:
> Hello DFDL community,
> 
> My binary input file contains the number of seconds since epoch for the start of a year.
> 
> For example, the number of seconds since epoch for the start of 1998 is: 883612800
> 
> I want the XML to just show the year (not month, day, and time).
> 
> For example, I want the XML to show this:
> 
> <Date>1998</Date>
> 
> I thought this would do the job:
> 
> <xs:element name=""Date"" type=""xs:dateTime""
>     dfdl:lengthKind=""explicit"" 
>     dfdl:length=""4"" 
>     dfdl:binaryCalendarRep=""binarySeconds"" 
>     dfdl:calendarPattern=""yyyy"" 
>     dfdl:calendarPatternKind=""explicit""
>     dfdl:lengthUnits=""bytes"" 
>     dfdl:binaryCalendarEpoch=""1970-01-01T00:00:00"" />
> 
> But apparently that's not correct.
> 
> What is the correct way to do it, please?
> 
> /Roger 
> 


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201910.mbox/%3c2b622895-c41b-8168-cb59-6e3a1951a09e@gmail.com%3e,Steve Lawrence <stephen.d.lawre...@gmail.com>,0,0
161,162,"That looks good, thanks!

On 5/17/06, Benj Fayle <bfayle@maketechnologies.com> wrote:
> I used something like the code shown below. If you changed the
> setRowfocus method to use a regular expression you may get the effect
> you want.
>
> /**
>  * @author bfayle
>  *
>  */
> public class ExplorerTabPanelBean {
>         private List treeModelData;
>         private TabMenuModel menuModel;
>
>         public TabMenuModel getMenuModel() {
>                 return menuModel;
>         }
>
>         public void setMenuModel(TabMenuModel menuModel) {
>                 this.menuModel = menuModel;
>         }
>
>         public void setTreeModelData(List treeModelData) throws
> IntrospectionException {
>                 this.treeModelData = treeModelData;
>             menuModel = new TabMenuModel(treeModelData);
>         }
>
>         public class TabMenuModel extends BaseMenuModel {
>                 Object focusRowKey;
>                 public TabMenuModel(Object arg0) {
>                         super(arg0);
>                 }
>
>                 public void setRowFocus(TabNodeImpl currentTab) {
>                         int index = treeModelData.indexOf(currentTab);
>                         setRowIndex(index);
>                         focusRowKey = getRowKey();
>                 }
>
>                 @Override
>                 public Object getFocusRowKey() {
>                         return focusRowKey;
>                 }
>
>         }
>
> }
>
> -----Original Message-----
> From: Thomas Spiegl [mailto:thomas.spiegl@gmail.com]
> Sent: Friday, May 12, 2006 2:30 AM
> To: adffaces-user@incubator.apache.org
> Subject: menuTabs and MenuModel
>
> I'm using the menuTabs component and I want to set the focus path for
> the MenuModel by using a regular expression. Is there a way to achieve
> this?
>
> Thomas
>


-- 
http://www.irian.at

Your JSF powerhouse -
JSF Consulting, Development and
Courses in English and German

Professional Support for Apache MyFaces

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200605.mbox/%3c18ff88f90605161719t1da17018g5ed6f45757a6bc7@mail.gmail.com%3e,"""Thomas Spiegl"" <thomas.spi...@gmail.com>",0,0
95,96,"Wow, this is actually quite complex owing to the unit tests...

Russell Jurney @rjurney <http://twitter.com/rjurney>
russell.jurney@gmail.com LI <http://linkedin.com/in/russelljurney> FB
<http://facebook.com/jurney> datasyndrome.com

On Thu, Apr 27, 2017 at 9:12 PM, Russell Jurney <russell.jurney@gmail.com>
wrote:

> I created a JIRA: https://issues.apache.org/jira/browse/AIRFLOW-1159
>
> Russell Jurney @rjurney <http://twitter.com/rjurney>
> russell.jurney@gmail.com LI <http://linkedin.com/in/russelljurney> FB
> <http://facebook.com/jurney> datasyndrome.com
>
> On Thu, Apr 27, 2017 at 9:00 PM, Russell Jurney <russell.jurney@gmail.com>
> wrote:
>
>> Ok, after some more poking around I found that this is the docker-py 1.6
>> API, which is pretty old. I'm going to update it to the latest and see if I
>> can make it work, since we are using the newer docker API elsewhere.
>>
>> Russell Jurney @rjurney <http://twitter.com/rjurney>
>> russell.jurney@gmail.com LI <http://linkedin.com/in/russelljurney> FB
>> <http://facebook.com/jurney> datasyndrome.com
>>
>> On Thu, Apr 27, 2017 at 8:51 PM, Russell Jurney <russell.jurney@gmail.com
>> > wrote:
>>
>>> I am trying to use DockerOperator and I get this error:
>>>
>>>     from docker import Client, tls
>>> ImportError: cannot import name 'Client'
>>>
>>>
>>> There is no requirements.txt, so I can't figure out what docker API this
>>> is, but it hasn't existed in the official docker Python module for ...
>>> ever? I can't figure out how this is supposed to work.
>>>
>>> I'm happy to fix it because... I have to, but can anyone comment on what
>>> is going on here? What module and what version is this referring to? It
>>> does not seem to be https://github.com/docker/docker-py, I even checked
>>> the 1.x line and that doesn't match either.
>>>
>>> Russell Jurney @rjurney <http://twitter.com/rjurney>
>>> russell.jurney@gmail.com LI <http://linkedin.com/in/russelljurney> FB
>>> <http://facebook.com/jurney> datasyndrome.com
>>>
>>
>>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201704.mbox/<CANSvDjpwLWBo8uW+MsZTEjtrnFSaQKuMGn+DrCvwswAOf9+5XQ@mail.gmail.com>,Russell Jurney <russell.jur...@gmail.com>,0,0
159,160,"i am doing the  application in single system.so i am not using zoo
keeper.also i am not getting any errors.in xml i have 4 PE config,one
dispatcher with 3 partitioner  listing and 3 partitioner config specifying
the stream name and keys

",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201207.mbox/%3cCABmtsn=WgJbdXyksj8NQOo5wHyAMPxVgUKu=D1X2UoR53c64YA@mail.gmail.com%3e,Raghavendar TS <raghav280...@gmail.com>,0,0
208,209,"I have a problem either with 1.1.7 and 1.0.14.

First, I had to manually add to the adf-faces jar the af.taglib.xml
file from the sources, because there was only yhe afh.taglib.xml
one... strange thing. Anyway I use the latest sources from apache to
build the adf-faces jars.

Now, I have a test page that seems to work, <h:*> <af:*> <afh:*> and
<t:*> tags get replaced in my output by the generated HTML, but my
<f:*> ones does not. I have <f:facelet>s in my output page. What's
wrong?

This is the libraries I set for my project:

adf-faces-api-11-m7-SNAPSHOT.jar
adf-faces-impl-11-m7-SNAPSHOT.jar
myfaces-api-1.1.4-SNAPSHOT.jar
myfaces-impl-1.1.4-SNAPSHOT.jar
tomahawk-1.1.4-SNAPSHOT.jar
jsf-facelets-1.1.7.jar
el-api.jar
el-ri.jar
commons-beanutils-1.7.0.jar
commons-codec-1.3.jar
commons-collections-3.1.jar
commons-digester-1.6.jar
commons-el-1.0.jar
commons-fileupload-1.0.jar
commons-lang-2.1.jar
commons-logging-1.0.4.jar
commons-validator-1.1.4.jar
jakarta-oro-2.0.7.jar
jstl-1.1.0.jar

In my facelets page, I declared the namespaces this way:

<html
    xmlns=""http://www.w3.org/1999/xhtml""
    xmlns:ui=""http://java.sun.com/jsf/facelets""
    xmlns:f=""http://java.sun.com/jsf""
    xmlns:h=""http://java.sun.com/jsf/html""
    xmlns:t=""http://myfaces.apache.org/tomahawk""
    xmlns:af=""http://myfaces.apache.org/adf/faces""
    xmlns:afh=""http://myfaces.apache.org/adf/faces/html"">

What's wrong?


2006/5/31, Cosma Colanicchia <cosmacol@gmail.com>:
> 2006/5/31, Frank Felix Debatin <ffd@gmx.net>:
> > > I'm was starting with the latest release, 1.1.7, but I'll
> > > follow your hint and try 1.0.14 first.
> >
> > I was actually hoping you start with 1.1.x to see if you
> > find the same problems ;-)
>
> Ok, I promise that, if I can work out without problems 1.0.14, I'll
> try switch to 1.1.7 :-)
>
> > > So,  the only required
> > > libraries are those for tomahawk and for the sandbox,
> > right?
> >
> > The facelets page mentions a tomahawk taglib contribution.
> > I'm just using the ADF component, so I can't tell.
>
> I'll probably leave the tomahawk and sandbox in the future, it seems
> that ADF Faces has a rather complete feature set..
>
> > Frank Felix
> >
> >
>

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200605.mbox/%3c467251f60605310409t61c7f3dap7d1cc2b3be1a50d8@mail.gmail.com%3e,"""Cosma Colanicchia"" <cosma...@gmail.com>",0,0
149,150,"Hi Airflow team,


I am using airflow with celery (2 nodes; i.e., two AWS instances)
My dag looks like below (the python dag name is task_ABC.py). Note in the
dag python file, I setup ""max_active_runs=1""


           /---------> TaskB1 -----------> TaskC1---------\
TaskA -----------> TaskB2  ----------> TaskC2----------> TaskD
           \----------> TaskB3  -----------> TaskC3--------/

So, After TaskA; it runs TaskB1, TaskB2 and TaskB3 simultaneously.  TaskB1,
B2 and B3 are running same shell-script (TaskB.sh) with different input
arguments. It drops ""Another instance is running, skipping"" warning for
TaskB1 and TaskB3 (as the log below). It did not drop same warning in
TaskB2, I think it's because TaskB2 is running in different celery node (I
have two celery nodes).
If I manually make TaskB1 as successful, TaskB3 can proceed

The following is the log. Any idea to handle this ?
Thanks.

-Jason

========= Log of TaskB1 ============

[2017-05-20 23:09:47,270] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-20 23:09:49,017] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-20 23:09:49,165] {models.py:1196} INFO -
--------------------------------------------------------------------------------
Starting attempt 1 of 2
--------------------------------------------------------------------------------

[2017-05-20 23:09:49,182] {models.py:1219} INFO - Executing
<Task(PythonOperator): TaskB1> on 2017-05-20 03:40:00
[2017-05-20 23:09:49,214] {task_ABC.py:185} INFO -
/mycode/process/gfs0p25/TaskB.sh 2017052012 rain
[2017-05-21 00:09:56,054] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-21 00:09:59,759] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-21 00:10:00,008] {models.py:1146} WARNING - Another instance
is running, skipping.


========= Log of TaskB3 ============

[2017-05-20 23:09:44,660] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-20 23:09:46,047] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-20 23:09:46,205] {models.py:1196} INFO -
--------------------------------------------------------------------------------
Starting attempt 1 of 2
--------------------------------------------------------------------------------

[2017-05-20 23:09:46,224] {models.py:1219} INFO - Executing
<Task(PythonOperator): TaskB3> on 2017-05-20 03:40:00
[2017-05-20 23:09:46,257] {best_weather-BLEND-v1-1-0.py:245} INFO -
/mycode/process/gfs0p25/TaskB.sh 2017052012 snow
[2017-05-21 00:09:48,029] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-21 00:09:49,080] {models.py:154} INFO - Filling up the DagBag
from /code/task_ABC.py
[2017-05-21 00:09:49,156] {models.py:1146} WARNING - Another instance
is running, skipping.

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CAPdFGJuCrctFCX+0f2gC=TRKrr6aP8hQKHjYq-DX0=Edpb9vvg@mail.gmail.com>,Jason Chen <chingchien.c...@gmail.com>,0,0
297,298,"Hi Gunnar,

I have noticed that when *assembly *runs the test, in some cases, some of
the tests fails.

I will need sometime to investigate why it does not work 100% when assembly
calls it.

For now, you can run the tests separately:

>> sbt
>> projetc fey-core
>> test

and for now add the following line to the fey project in
 project/Build.scala

test in assembly := {}


it should look like:
[image: Inline image 1]

Please, let me know if that works for you

Regards

On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <
barbaramaltagomes@gmail.com> wrote:

> HI Gunnar,
>
> Thanks for the info.
>
> I will try to reproduce the issue on CentOS.
>
> If you re-run the tests, do you still get the SAME failed tests or it
> fails but in different tests?
>
> Regards
>
> On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com>
> wrote:
>
>> Hi Barbara:
>>
>> I'm on an x86 box with:
>>
>> [gunnar@localhost ~]$ lsb_release -a
>> LSB Version:    :base-4.0-ia32:base-4.0-noarc
>> h:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-
>> 4.0-noarch:printing-4.0-ia32:printing-4.0-noarch
>> Distributor ID: CentOS
>> Description:    CentOS release 6.8 (Final)
>> Release:        6.8
>> Codename:       Final
>>
>> It may be too weak for what's needed here. I can spin up a AWS micro
>> instance once I know what OS to install.
>>
>> Thanks,
>>
>> Gunnar
>>
>> On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <
>> barbaramaltagomes@gmail.com> wrote:
>>
>>> Hi Gunnar,
>>>
>>> Which Operational System are you using?
>>> *sbt *picks the order of the tests execution and it might be the
>>> problem on your machine.
>>>
>>> As you can see in the picture bellow, I pulled the repo from github and
>>> run the tests and all of them succeeded
>>>
>>> [image: Inline image 1]
>>>
>>> I'm using Mac OS
>>>
>>> About the *ERROR *messages during the tests, those are fey logs caused
>>> intentionally by the tests.
>>>
>>> Also, try to run the tests separated:
>>>
>>> >> sbt
>>> >> project fey-core
>>> >> test
>>>
>>> Regards,
>>>
>>> On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com> wrote:
>>>
>>>> Thanks for pointing this out will take a look.
>>>> -Tony
>>>>
>>>>
>>>> On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com>
>>>> wrote:
>>>>
>>>> Hi,
>>>>
>>>> I verified the required versions and then I ran assembly of the
>>>> fey-core project. 5 failures.
>>>>
>>>> [info] - should result in creating a global Performer child actor with
>>>> the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/G
>>>> LOBAL-TEST'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFO
>>>> RMER-SCHEDULER'
>>>> [info] - should result in one global actor created for orchestration
>>>> [info] - should result in right number of running actors
>>>> [info] Stopping performer inside ensemble
>>>> [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8]
>>>> [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer
>>>> PERFORMER-SCHEDULER
>>>> org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER
>>>>         at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(
>>>> Ensemble.scala:60)
>>>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>>>         at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29
>>>> )
>>>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(Death
>>>> Watch.scala:44)
>>>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
>>>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.j
>>>> ava:260)
>>>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(For
>>>> kJoinPool.java:1339)
>>>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPoo
>>>> l.java:1979)
>>>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinW
>>>> orkerThread.java:107)
>>>>
>>>> [info] - should Send stop message to monitor
>>>> [info] Stopping ensemble
>>>> [info] - should Send stop message to monitor
>>>> [info] - should result in no orchestration running
>>>> [info] - should not affect global performer
>>>> [info] Stopping global performer
>>>> [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7]
>>>> [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer
>>>> GLOBAL-TEST
>>>> org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer
>>>> GLOBAL-TEST
>>>>         at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.apply
>>>> OrElse(GlobalPerformer.scala:49)
>>>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>>>         at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerf
>>>> ormer.scala:28)
>>>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(Death
>>>> Watch.scala:44)
>>>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>>>         at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestAct
>>>> orRef.scala:60)
>>>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>>>         at akka.testkit.CallingThreadDispatcher.process$1(CallingThread
>>>> Dispatcher.scala:250)
>>>>         at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadD
>>>> ispatcher.scala:283)
>>>>         at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingT
>>>> hreadDispatcher.scala:191)
>>>>         at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch
>>>> .scala:147)
>>>>         at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)
>>>>         at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:40
>>>> 2)
>>>>         at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$Fa
>>>> ultHandling$$finishTerminate(FaultHandling.scala:213)
>>>>         at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandli
>>>> ng.scala:172)
>>>>         at akka.actor.ActorCell.terminate(ActorCell.scala:374)
>>>>         at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)
>>>>         at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)
>>>>         at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala
>>>> :282)
>>>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)
>>>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.j
>>>> ava:260)
>>>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(For
>>>> kJoinPool.java:1339)
>>>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPoo
>>>> l.java:1979)
>>>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinW
>>>> orkerThread.java:107)
>>>>
>>>> [info] - should result in restart the orchestration
>>>> [info] - should all previous actors restarted
>>>> [info] Stopping orchestration
>>>> [info] - should result in empty global
>>>> [info] EnsembleSpec:
>>>> [info] Creating a simple Ensemble MY-ENSEMBLE-0005
>>>> [info] - should result in creation of Ensemble actor '
>>>> akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'
>>>> [info] - should result in sending START to monitor actor
>>>> [info] - should result in creation of Performer 'TEST-0004'
>>>> [info] - should result in Empty state variable Ensemble.connectors
>>>> [info] - should result in one entry added to state variable
>>>> Ensemble.performer
>>>> [info] - should result in one right entry to state variable
>>>> Ensemble.performers_metadata
>>>> [info] - should result in two paths added to
>>>> IdentifyFeyActors.actorsPath
>>>> [info] Sending Ensemble.STOP_PERFORMERS to Ensemble
>>>> [info] - should result in Terminate message of actor 'TEST-0004' and
>>>> throw RestartEnsemble Exception
>>>> [info] - should result in Performer 'TEST-0004' restarted
>>>> [info] - should result in two paths added to
>>>> IdentifyFeyActors.actorsPath
>>>> [info] Sending PoisonPill to Ensemble
>>>> [info] - should result in termination of actor 'MY-ENSEMBLE-0005'
>>>> [info] - should result in sending TERMINATE to monitor actor
>>>> [info] - should result in termination of ensemble and performer
>>>> [info] - should result in empty IdentifyFeyActors.actorsPath
>>>> [info] creating more detailed Ensemble
>>>> [info] - should result in creation of Ensemble actor
>>>> [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'
>>>> [info] - should result in creation of Performer 'PERFORMER-PARAMS'
>>>> [info] - should create connection PERFORMER-SCHEDULER ->
>>>> PERFORMER-PARAMS
>>>> [info] - should create 'PERFORMER-SCHEDULER' with schedule time equal
>>>> to 200ms
>>>> [info] - should create 'PERFORMER-SCHEDULER' with connection to
>>>> 'PERFORMER-PARAMS'
>>>> [info] - should create 'PERFORMER-PARAMS' with no connections
>>>> [info] - should create 'PERFORMER-PARAMS' with specified params
>>>> [info] 'PERFORMER-SCHEDULER'
>>>> [info] - should produce 5 messages in 1 seconds
>>>> [info] - should produce 10 messages in 2 seconds
>>>> [info] 'PERFORMER-PARAMS'
>>>> [info] - should process 5 messages in 1 seconds
>>>> [info] - should produce 10 messages in 2 seconds
>>>> [info] Stopping any Performer that belongs to the Ensemble
>>>> [info] - should force restart of entire Ensemble
>>>> [info] - should result in sending STOP - RESTART to monitor actor
>>>> [info] - should keep ensemble actorRef when restarted
>>>> [info] - should stop and start the performer with a new reference
>>>> [info] Restarting an Ensemble
>>>> [info] - should Consuming left messages on Process
>>>> [info] - should Cleanup TestProbs
>>>> [info] Redefining TestProbe for performers
>>>> [info] - should start receiving messages
>>>> [info] Sending PoisonPill to detailed Ensemble
>>>> [info] - should result in termination of Ensemble
>>>> [info] - should result in empty IdentifyFeyActors.actorsPath
>>>> [info] creating Ensemble with Backoff performer
>>>> [info] - should result in creation of Ensemble actor
>>>> [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'
>>>> [info] - should result in creation of Performer 'PERFORMER-PARAMS'
>>>> [info] - should create 'PERFORMER-PARAMS' with backoff time equal to 1
>>>> second
>>>> [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal to
>>>> true
>>>> [info] Performer with backoff enabled
>>>> [info] - should not process messages during the backoff period
>>>> [info] Performer with autoScale
>>>> [info] - should result in router and routees created
>>>> [info] IdentifyFeyActorsSpec:
>>>> [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>>>> [info] - should result in one path added to IdentifyFeyActors.actorsPath
>>>> [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'
>>>> [info] Creating a new actor in the system and sending
>>>> IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>>>> [info] - should result in two paths added to
>>>> IdentifyFeyActors.actorsPath
>>>> [info] - should result in matching paths
>>>> [info] Stopping previous added actor and sending
>>>> IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>>>> [info] - should result in going back to have just one path added to
>>>> IdentifyFeyActors.actorsPath
>>>> [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'
>>>> [info] WatchServiceReceiverSpec:
>>>> [info] Creating WatchServiceReceiver
>>>> [info] - should process initial files in the JSON repository
>>>> [info] Start a Thread with WatchServiceReceiver
>>>> [info] - should Start Thread
>>>> [info] Start watching directory
>>>> [info] - should Starting receiving CREATED event
>>>> [info] - should Starting receiving UPDATE event
>>>> [info] processJson
>>>> [info] - should log to warn level when json has invalid schema
>>>> [info] interrupt watchservice
>>>> [info] - should interrupt thread
>>>> [info] FeyCoreSpec:
>>>> [info] Creating FeyCore
>>>> [info] - should result in creating a child actor with the name
>>>> 'FEY_IDENTIFIER'
>>>> [info] - should result in sending START message to Monitor actor
>>>> [info] Sending FeyCore.START to FeyCore
>>>> [info] - should result in creating a child actor with the name
>>>> 'JSON_RECEIVER'
>>>> [info] - should result in starting FeyWatchService Thread
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to
>>>> FeyCore
>>>> [info] - should result in creating an Orchestration child actor with
>>>> the name 'TEST-ACTOR'
>>>> [info] - should result in creating an Ensemble child actor with the
>>>> name 'TEST-ACTOR/MY-ENSEMBLE-0001'
>>>> [info] - should result in creating an Ensemble child actor with the
>>>> name 'TEST-ACTOR/MY-ENSEMBLE-0002'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'
>>>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
>>>> with key 'TEST-ACTOR'
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command to
>>>> FeyCore
>>>> [info] - should result in creating a new Performer child actor with the
>>>> name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command and
>>>> DELETE ensemble to FeyCore
>>>> [info] - should result in termination of Ensemble with the name
>>>> 'TEST-ACTOR/MY-ENSEMBLE-0001'
>>>> [info] - should result in termination of Performer with the name
>>>> 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'
>>>> [info] - should result in termination of Performer with the name
>>>> 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command and
>>>> same Timestamp to FeyCore
>>>> [info] - should result in logging a 'not recreated' message at Warn
>>>> [info] Sending FeyCore.JSON_TREE to FeyCore
>>>> [info] - should result in logging a 6 path messages at Info
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command to
>>>> FeyCore
>>>> [info] - should result in termination of Orchestration with the name
>>>> 'TEST-ACTOR'
>>>> [info] - should result in sending TERMINATE message to Monitor actor
>>>> [info] - should result in termination of Ensemble with the name
>>>> 'TEST-ACTOR/MY-ENSEMBLE-0002'
>>>> [info] - should result in termination of Performer with the name
>>>> 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'
>>>> [info] - should result in removing key 'TEST-ACTOR' at
>>>> FEY_CACHE.activeOrchestrations
>>>> [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore
>>>> [info] - should result in termination of 'TEST-ORCH-2' *** FAILED ***
>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2""
>>>> -> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-
>>>> 2#-2041630067])) had size 2 instead of expected size 1
>>>> (FeyCoreSpec.scala:144)
>>>> [info] - should result in sending Terminate message to Monitor actor
>>>> *** FAILED ***
>>>> [info]   java.lang.AssertionError: assertion failed: timeout (1 second)
>>>> during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TE
>>>> RMINATE
>>>> [info]   at scala.Predef$.assert(Predef.scala:170)
>>>> [info]   at akka.testkit.TestKitBase$class
>>>> .expectMsgClass_internal(TestKit.scala:435)
>>>> [info]   at akka.testkit.TestKitBase$class
>>>> .expectMsgClass(TestKit.scala:431)
>>>> [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)
>>>> [info]   at org.apache.iota.fey.FeyCoreSpe
>>>> c$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)
>>>> [info]   at org.apache.iota.fey.FeyCoreSpe
>>>> c$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)
>>>> [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>>>> [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>>>> [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
>>>> [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
>>>> [info]   ...
>>>> [info] - should result in empty FEY_CACHE.activeOrchestrations ***
>>>> FAILED ***
>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2""
>>>> -> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-
>>>> 2#-2041630067])) was not empty (FeyCoreSpec.scala:153)
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to
>>>> FeyCore of a GenericReceiverActor
>>>> [info] - should result in creating an Orchestration child actor with
>>>> the name 'RECEIVER_ORCHESTRATION'
>>>> [info] - should result in creating an Ensemble child actor with the
>>>> name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'
>>>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
>>>> with key 'RECEIVER_ORCHESTRATION'
>>>> [info] Sending PROCESS message to the Receiver Performer
>>>> [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore
>>>> [info] - should result in creating an Orchestration child actor with
>>>> the name 'RECEIVED-BY-ACTOR-RECEIVER'
>>>> [info] - should result in creating an Ensemble child actor with the
>>>> name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'
>>>> [info] - should result in creating an Ensemble child actor with the
>>>> name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'
>>>> [info] - should result in one new entry to
>>>> FEY_CACHE.activeOrchestrations with key 'RECEIVED-BY-ACTOR-RECEIVER' ***
>>>> FAILED ***
>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null),
>>>> ""RECEIVED-BY-ACTOR-RECEIVER"" -> (213263914979,Actor[akka://FEY
>>>> -MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]),
>>>> ""TEST-ORCH-2"" -> (213263914979,Actor[akka://FEY
>>>> -TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]),
>>>> ""RECEIVER_ORCHESTRATION"" -> (591997890,Actor[akka://FEY-TE
>>>> ST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 4
>>>> instead of expected size 2 (FeyCoreSpec.scala:200)
>>>> [info] Sending PROCESS message to the Receiver Performer with command
>>>> DELETE
>>>> [info] - should STOP running orchestration
>>>> [info] - should result in one entry in FEY_CACHE.activeOrchestrations
>>>> *** FAILED ***
>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2""
>>>> -> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-
>>>> 2#-2041630067]), ""RECEIVER_ORCHESTRATION"" -> (591997890,Actor[
>>>> akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had
>>>> size 3 instead of expected size 1 (FeyCoreSpec.scala:213)
>>>> [info] Sending PROCESS message to Receiver with checkpoint enabled
>>>> [info] - should Save received JSON to checkpoint dir
>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND GLOBAL
>>>> performer command to FeyCore
>>>> [info] - should result in creating an Orchestration child actor with
>>>> the name 'GLOBAL-PERFORMER'
>>>> [info] - should result in creating an Ensemble child actor with the
>>>> name 'GLOBAL-PERFORMER/ENS-GLOBAL'
>>>> [info] - should result in creating a global Performer child actor with
>>>> the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'
>>>> [info] - should result in creating a Performer child actor with the
>>>> name 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'
>>>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
>>>> with key 'GLOBAL-PERFORMER'
>>>> [info] - should result in one global actor created for orchestration
>>>> [info] - should result in globa metadata add to table
>>>> [info] - should result in right running actors
>>>> [info] Stopping Global actor
>>>> [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3]
>>>> [akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer
>>>> GLOBAL-TEST
>>>> org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer
>>>> GLOBAL-TEST
>>>>         at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.apply
>>>> OrElse(GlobalPerformer.scala:49)
>>>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>>>         at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerf
>>>> ormer.scala:28)
>>>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(Death
>>>> Watch.scala:44)
>>>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
>>>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.j
>>>> ava:260)
>>>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(For
>>>> kJoinPool.java:1339)
>>>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPoo
>>>> l.java:1979)
>>>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinW
>>>> orkerThread.java:107)
>>>>
>>>> [info] - should result in sending logging error
>>>> [info] - should result in orchestration restarted
>>>> [info] - should all previous actors restarted
>>>> [info] Stopping orchestration with global performer
>>>> [info] - should result in sending TERMINATE message to Monitor actor
>>>> [info] - should result in no global actors for orchestration
>>>> [info] Stopping FeyCore
>>>> [info] - should result in sending STOP message to Monitor actor
>>>> [info] BaseAkkaSpec:
>>>> [info] JsonReceiverSpec:
>>>> [info] Executing validJson in JsonReceiver
>>>> [info] - should return false when json schema is not right
>>>> [info] - should log message to Error
>>>> [info] - should return true when Json schema is valid
>>>> [info] Executing checkForLocation in JsonReceiver
>>>> [info] - should log message at Debug level
>>>> [info] - should download jar dynamically from URL
>>>> [info] Start a Thread with the JSON receiver
>>>> [info] - should Start Thread
>>>> [info] - should execute execute() method inside run
>>>> [info] Interrupting the receiver Thread
>>>> [info] - should Throw Interrupted exception
>>>> [info] - should execute exceptionOnRun method
>>>> [info] FeyGenericActorSpec:
>>>> [info] Creating a GenericActor with Schedule time defined
>>>> [info] - should result in scheduler started
>>>> [info] - should result in onStart method called
>>>> [info] - should result in START message sent to Monitor
>>>> [info] - should result in one active actor
>>>> [info] Backoff of GenericActor
>>>> [info] - should be zero until the first PROCESS message
>>>> [info] - should change when first PROCESS message was received
>>>> [info] Sending PROCESS message to GenericActor
>>>> [info] - should call processMessage method
>>>> [info] customReceive method
>>>> [info] - should process any non treated message
>>>> [info] Sending PROCESS message to GenericActor
>>>> [info] - should be discarded when backoff is enabled
>>>> [info] - should be processed when backoff has finished
>>>> [info] Calling startBackoff
>>>> [info] - should set endBackoff with time now
>>>> [info] Calling propagateMessage
>>>> [info] - should send message to connectTo actors
>>>> [info] Scheduler component
>>>> [info] - should call execute() method
>>>> [info] Sending EXCEPTION(IllegalArgumentException) message to
>>>> GenericActor
>>>> [info] - should Throw IllegalArgumentException
>>>> [info] - should Result in restart of the actor with sequence of
>>>> Monitoring: STOP -> RESTART -> START
>>>> [info] - should call onStart method
>>>> [info] - should call onRestart method
>>>> [info] - should restart scheduler
>>>> [info] Sending STOP to GenericActor
>>>> [info] - should terminate GenericActor
>>>> [info] - should call onStop method
>>>> [info] - should cancel scheduler
>>>> [info] - should send STOP - TERMINATE message to Monitor
>>>> [info] - should result in no active actors
>>>> [info] Creating GenericActor with schedule anc backoff equal to zero
>>>> [info] - should not start a scheduler
>>>> [info] - should result in one active actor
>>>> [info] - should result in no discarded PROCESS messages
>>>> [info] FeyGenericActorReceiverSpec:
>>>> [info] Creating a GenericActor with Schedule time defined
>>>> [info] - should result in scheduler started
>>>> [info] - should result in onStart method called
>>>> [info] - should result in START message sent to Monitor
>>>> [info] - should result in one active actor
>>>> [info] - should result in normal functioning of GenericActor
>>>> [info] Sending PROCESS message to GenericReceiver
>>>> [info] - should log message to Warn saying that the JSON could not be
>>>> forwarded to FeyCore when JSON is invalid
>>>> [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON to be
>>>> processed has a valid schema
>>>> [info] - should Download jar from location and send
>>>> ORCHESTRATION_RECEIVED to FeyCore when JSON has a location defined
>>>> [info] Scheduler component
>>>> [info] - should call execute() method
>>>> [info] Sending EXCEPTION(IllegalArgumentException) message to
>>>> GenericActor
>>>> [info] - should Throw IllegalArgumentException
>>>> [info] - should Result in restart of the actor with sequence of
>>>> Monitoring: STOP -> RESTART -> START
>>>> [info] - should call onStart method
>>>> [info] - should call onRestart method
>>>> [info] - should restart scheduler
>>>> [info] Sending STOP to GenericActor
>>>> [info] - should terminate GenericActor
>>>> [info] - should call onStop method
>>>> [info] - should cancel scheduler
>>>> [info] - should send STOP - TERMINATE message to Monitor
>>>> [info] - should result in no active actors
>>>>
>>>> CLeaning up[info] Run completed in 44 seconds, 724 milliseconds.
>>>> [info] Total number of tests run: 243
>>>> [info] Suites: completed 12, aborted 0
>>>> [info] Tests: succeeded 238, failed 5, canceled 0, ignored 0, pending 0
>>>> [info] *** 5 TESTS FAILED ***
>>>> [error] Failed tests:
>>>> [error]         org.apache.iota.fey.FeyCoreSpec
>>>> [error] (fey-core/test:test) sbt.TestsFailedException: Tests
>>>> unsuccessful
>>>> [error] Total time: 46 s, completed Feb 19, 2017 12:36:25 AM
>>>>
>>>>
>>>> --
>>>> Thanks,
>>>>
>>>> Gunnar
>>>> *If you think you can you can, if you think you can't you're right.*
>>>>
>>>>
>>>>
>>>
>>>
>>> --
>>> Barbara Gomes
>>> Computer Engineer
>>> San Jose, CA
>>>
>>
>>
>>
>> --
>> Thanks,
>>
>> Gunnar
>> *If you think you can you can, if you think you can't you're right.*
>>
>
>
>
> --
> Barbara Gomes
> Computer Engineer
> San Jose, CA
>



-- 
Barbara Gomes
Computer Engineer
San Jose, CA

",http://mail-archives.apache.org/mod_mbox/iota-dev/201702.mbox/<CANMFQO6tQ9awvC7HDbcNRCUvoqAf1Z=dFM68-qFA+7FdxECe9A@mail.gmail.com>,Barbara Malta Gomes <barbaramaltago...@gmail.com>,0,0
293,294,"Don't know if this helps anyone. When I was trying different deployment
options on the drawing board, one particular config stood out when
co-locating DNs & Shard-servers...

1. Run Data-Nodes & Shard-servers co-located…
2. Make sure all such machines are placed in one rack [Say RackA]…
3. Start Data-Nodes alone {without Shard-servers} in a separate RackB…

With such a config in place, hadoop places the 1st write-copy locally in
RackA. 2nd & 3rd Copies will be placed in RackB.

RackA machines can use better disks like SSDs or SAS15k drives with lower
storage capacity. RackB machines will be commodity/JBOD machines with high
storage capacity [like a 2*4TB 7.2k SATA etc…]

Reads will go via shard-servers and most of the time access
local-but-excellent disk sub-system

Writes will experience bit of latency because of 2 racks but write-load on
shard-servers will be much-less {No 2nd & 3rd copy to be written}

Apologies if this is basic stuff for the community.

--
Ravi

On Mon, Mar 2, 2015 at 11:31 AM, Ravikumar Govindarajan <
ravikumar.govindarajan@gmail.com> wrote:

> Many thanks Aaron…
>
> Ok that sounds about right.  This will greatly depend on your data, how
>> many fields, how many terms, etc.
>
>
> Fields will be around 20-25.. But majority of searches happen on only 5-6
> fields. Term count ~= 2.5 million.
>
> I would recommend using the latest Java7
>> (perhaps Java8 but I haven't tested with it), and use the G1 garbage
>> collector if you plan on running larger heaps
>
>
> We are using latest version of 1.7. Actually we imported around 2TB of
> data as dry-run with just 16GB heap without major GC issues using good old
> CMS. There is no sorting/faceting during searches also. My reluctance stems
> from the fact that I am not quite familiar with G1 :)
>
> I am favouring more for write-thru cache [at-least around 50Gb] rather
> than read-cache because we have a lot of free RAM available & I feel read
> cache is going to use very less RAM. But I am not sure about this. Any
> pointers will greatly help.
>
> I would also recommend that you increase the
>> block cache buffer and file buffer sizes from 8K to 64K
>
>
> This is one issue we faced during dry-run. Marking-up from 8K to 16K
> solved the issue. I thought 32K must be a good fit for us. Will surely
> explore this…
>
> Thanks again for helping out
>
> On Sat, Feb 28, 2015 at 3:04 AM, Aaron McCurry <amccurry@gmail.com> wrote:
>
>> On Fri, Feb 27, 2015 at 1:29 AM, Ravikumar Govindarajan <
>> ravikumar.govindarajan@gmail.com> wrote:
>>
>> > Hi,
>> >
>> > I need a general guidance on number of machines/shards required for our
>> > first blur set-up
>> >
>> > Some data as follows
>> >
>> > 1. Shard-Server Config : 128GB RAM, 16-core dual socket with
>> > hyper-threading. 32 procs
>> > 2. Total dataset size: 10TB. With rep-factor=3, total cluster-size=30TB.
>> > Pre-populated via
>> >     MR or Thrift...
>> > 3. We receive very less queries per minute [600-900 queries]. But the
>> > response times for
>> >     every query must be <=150 ms
>> >
>> > Initially we thought we can create 500 shards each of 20GB size with
>> around
>> > 20 machines.
>> >
>>
>> Ok that sounds about right.  This will greatly depend on your data, how
>> many fields, how many terms, etc.
>>
>>
>> >
>> > Can each shard-server machine with above specs handle 25 shards? Is
>> such a
>> > configuration over-utilized/under-utilized?
>> >
>>
>> Again, it likely depends.  I would recommend using the latest Java7
>> (perhaps Java8 but I haven't tested with it), and use the G1 garbage
>> collector if you plan on running larger heaps.  Whatever is leftover can
>> be
>> allocated to the block cache.  I would also recommend that you increase
>> the
>> block cache buffer and file buffer sizes from 8K to 64K.  This will also
>> decrease heap pressure for the number of entries in the block cache lru
>> map.
>>
>>
>> >
>> > How do folks run in production. Any numbers/pointers will be really
>> helpful
>> >
>>
>> Generally large shard servers (like the ones you are suggesting) with a
>> few
>> controllers (6-12) are typical setups.
>>
>> If I can provide more details I will follow up.
>>
>> Aaron
>>
>>
>> >
>> > --
>> > Ravi
>> >
>>
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201503.mbox/%3cCAGW2whQ9LZWKhrTgN9MHjGjVb9ObP_QWTeKDzhYud2gf1fHWHA@mail.gmail.com%3e,Ravikumar Govindarajan <ravikumar.govindara...@gmail.com>,0,0
23,24,"Hi All,


We have implemented a solution for allowing the exclusion of individual tasks during a DAG
run. However, when writing unit tests for this, we are encountering an issue with MySQL, which
I am hoping someone is able to help us with.


For our solution, we have a new 'TaskExclusion' table in the meta-data. Our unit tests were
run by Travis, not locally.


The code block under test:


class TaskExclusion(Base):
    """"""
 This class is used to define objects that can be used to specify not to
 run a given task in a given dag on a variety of execution date conditions.
 These objects will be stored in the backend database in the task_exclusion
 table.
 Static methods are provided for the creation, removal and investigation of
 these objects.
 """"""

 __tablename__ = ""task_exclusion""

 id = Column(Integer(), primary_key=True)
    dag_id = Column(String(ID_LEN), nullable=False)
    task_id = Column(String(ID_LEN), nullable=False)
    exclusion_type = Column(String(32), nullable=False)
    exclusion_start_date = Column(DateTime, nullable=True)
    exclusion_end_date = Column(DateTime, nullable=True)
    created_by = Column(String(256), nullable=False)
    created_on = Column(DateTime, nullable=False)

    @classmethod
 @provide_session
 def set(
            cls,
            dag_id,
            task_id,
            exclusion_type,
            exclusion_start_date,
            exclusion_end_date,
            created_by,
            session=None):
        """"""
 Add a task exclusion to prevent a task running under certain
 circumstances.
 :param dag_id: The dag_id of the DAG containing the task to exclude
 from execution.
 :param task_id: The task_id of the task to exclude from execution.
 :param exclusion_type: The type of circumstances to exclude the task
 from execution under. See the TaskExclusionType class for more detail.
 :param exclusion_start_date: The execution_date to start excluding on.
 This will be ignored if the exclusion_type is INDEFINITE.
 :param exclusion_end_date: The execution_date to stop excluding on.
 This will be ignored if the exclusion_type is INDEFINITE or
 SINGLE_DATE.
 :param created_by: Who is creating this exclusion. Stored with the
 exclusion record for auditing/debugging purposes.
 :return: None.
 """"""

 session.expunge_all()

        # Set up execution date range correctly
 if exclusion_type == TaskExclusionType.SINGLE_DATE:
            if exclusion_start_date:
                exclusion_end_date = exclusion_start_date
            else:
                raise AirflowException(
                    ""No exclusion_start_date ""
 )
        elif exclusion_type == TaskExclusionType.DATE_RANGE:
            if exclusion_start_date > exclusion_end_date:
                raise AirflowException(
                    ""The exclusion_start_date is after the exclusion_end_date""
 )
        elif exclusion_type == TaskExclusionType.INDEFINITE:
            exclusion_start_date = None
 exclusion_end_date = None
 else:
            raise AirflowException(
                ""The exclusion_type, {}, is not recognised.""
 .format(exclusion_type)
            )

        # remove any duplicate exclusions
 session.query(cls).filter(
            cls.dag_id == dag_id,
            cls.task_id == task_id,
            cls.exclusion_type == exclusion_type,
            cls.exclusion_start_date == exclusion_start_date,
            cls.exclusion_end_date == exclusion_end_date
        ).delete()

        # insert new exclusion
 session.add(TaskExclusion(
            dag_id=dag_id,
            task_id=task_id,
            exclusion_type=exclusion_type,
            exclusion_start_date=exclusion_start_date,
            exclusion_end_date=exclusion_end_date,
            created_by=created_by,
            created_on=datetime.now())
        )

        session.commit()


The unit test:

class TaskExclusionTest(unittest.TestCase):
    def test_set_exclusion(self, session=None):

        session = settings.Session()

        session.expunge_all()

        dag_id = 'test_task_exclude'
 task_id = 'test_task_exclude'
 exec_date = datetime.datetime.now()

        TaskExclusion.set(dag_id=dag_id,
                          task_id=task_id,
                          exclusion_type=TaskExclusionType.SINGLE_DATE,
                          exclusion_start_date=exec_date,
                          exclusion_end_date=exec_date,
                          created_by='airflow')


        exclusion = session.query(TaskExclusion).filter(
                        TaskExclusion.dag_id == dag_id,
                        TaskExclusion.task_id == task_id,
                        TaskExclusion.exclusion_type == TaskExclusionType.SINGLE_DATE,
                        TaskExclusion.exclusion_start_date == exec_date,
                        TaskExclusion.exclusion_end_date == exec_date).first()

        self.assertTrue(exclusion)


The unit test passes for postgreSQL and SQLite but fails for MySQL. I have checked and the
'exclusion' variable contains a TaskExclusion object for postgreSQL and SQLite but is set
to 'None' for MySQL. Any suggestions on what could be causing this would be much appreciated.


Cheers,
Luke Maycock
OLIVER WYMAN
luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com>
www.oliverwyman.com<http://www.oliverwyman.com/>



________________________________
From: siddharth anand <sanand@apache.org>
Sent: 16 November 2016 00:40
To: dev@airflow.incubator.apache.org
Subject: Re: Skip task

If your requirement is to skip a portion of tasks in a DagRun based on some
state encountered while executing that DagRun, that is what
BranchPythonOperator or ShortCircruitOperator (optionally paired with a
Trigger Rule specified on a downstream task) is made for.

These operators take a custom Python callable as a argument. The callable
can check for the existence of data or files that should have been
generated by an external system or an upstream task in the same DAG. The
callables need to return a Boolean value in the case of the
ShortCircruitOperator or a selected choice (i.e. branch to take) as in the
case of the BranchPythonOperator.

If you have 20 tasks that all depend on the presence of 20 different files,
you would need 20 ShortCircruitOperator or BranchPythonOperator tasks each
either sharing a common callable or each with its own callable.

One could argue that these tasks are ""overhead"" because they just encompass
some conditional or control logic and that DAGs should only contain
workhorse tasks (i.e. tasks that do some  work). DAGs with workhorse-only
tasks are more of a pure dataflow approach -- i.e. no control-logic
operators. However, I don't see another option.

In the current system, a callable registered with a ShortCircruitOperator
would check for the presence of a file -- if the file were not available,
then a series of downstream tasks would be skipped in that DAGRun, until a
task with a Trigger_Rule=""all_done"" were encountered, downstream of which,
tasks would no longer be skipped for the DagRun.

I hope this makes sense.

A long time ago, I proposed UI functionality to skip a series of DAG runs
via the UI, because I knew that no data was available for that time range
from an external system. It wanted to essentially specify a ""blackout""
period in terms of a time range that covered multiple DagRuns. My intention
was for backfills to skip those days. It turns out that my company did not
end up having such a requirement, so I dropped the feature request.

If this is what you are asking for, then I am +1. Please implement it and
submit a PR.

On Tue, Nov 15, 2016 at 2:50 AM, Maycock, Luke <
luke.maycock@affiliate.oliverwyman.com> wrote:

> Thank you for taking the time to respond. This is a great approach if you
> know at the time of creating the DAG which tasks you expect to need to
> skip. However, I don't think this is exactly the use case I have. For
> example, I may be expecting a file to arrive in an FTP folder for loading
> into a database but one day it doesn't arrive so I just want to skip that
> task on that day.
>
>
> Our workflows commonly have around 20 of these types of tasks in. I could
> configure all of these tasks in the way you suggested in case I ever need
> to skip one of them. However, I'd prefer not to have to set the tasks up
> this way and instead have the ability just to skip a task on an ad-hoc
> basis. I could then also use this functionality to add the ability to run
> from a certain point in a DAG or to a certain point in the DAG.
>
>
>
> Thanks,
> Luke Maycock
> OLIVER WYMAN
> luke.maycock@affiliate.oliverwyman.com<mailto:luke.
> maycock@affiliate.oliverwyman.com>
> www.oliverwyman.com<http://www.oliverwyman.com/>
>
>
>
> ________________________________
> From: siddharth anand <sanand@apache.org>
> Sent: 14 November 2016 19:48
> To: dev@airflow.incubator.apache.org
> Subject: Re: Skip task
>
> For cases like this, we (Agari) use the following approach :
>
>   1. Create a Variable in the UI of type boolean such as *enable_feature_x*
>   2. Use a ShortCircuitOperator (or BranchPythonOperator) to Skip
>   downstream processing based on the value of *enable_feature_x*
>   3. Assuming that you don't want to skip ALL downstream tasks, you can
>   use a trigger_rule of all_done to resume processing some portion of your
>   downstream DAG after skipping an upstream portion
>
> In other words, there is already a means to achieve what you are asking for
> today. You can change the value of via *enable_feature_x  *the UI. If you'd
> like to enhance the UI to better capture this pattern, pls submit a PR.
> -s
>
> On Thu, Nov 10, 2016 at 1:20 PM, Maycock, Luke <
> luke.maycock@affiliate.oliverwyman.com> wrote:
>
> > Hi Gerard,
> >
> >
> > I see the new status as having a number of uses:
> >
> >  1.  A user can manually set a task to skip in a DAG run via the UI.
> >  2.  We can then make use of this new status to add the following
> > functionality to Airflow:
> >     *   Run a DAG run up to a certain point and have the rest of the
> tasks
> > have the new status.
> >     *  Run a DAG run from a certain task to the end, setting all
> > pre-requisite tasks to have this new status.
> >
> > I am happy to be challenged on the above use cases if there are better
> > ways to achieve the same things.
> >
> > Cheers,
> > Luke Maycock
> > OLIVER WYMAN
> > luke.maycock@affiliate.oliverwyman.com<mailto:luke.
> > maycock@affiliate.oliverwyman.com>
> > www.oliverwyman.com<http://www.oliverwyman.com/>
> >
> >
> >
> > ________________________________
> > From: Gerard Toonstra <gtoonstra@gmail.com>
> > Sent: 09 November 2016 18:08
> > To: dev@airflow.incubator.apache.org
> > Subject: Re: Skip task
> >
> > Hey Luke,
> >
> > Who or what makes the decision to skip processing that task?
> >
> > Rgds,
> >
> > Gerard
> >
> > On Wed, Nov 9, 2016 at 2:39 PM, Maycock, Luke <
> > luke.maycock@affiliate.oliverwyman.com> wrote:
> >
> > > Hi Gerard,
> > >
> > >
> > > Thank you for your quick response.
> > >
> > >
> > > I am not trying to implement this for a specific operator but rather
> > > trying to add it as a feature for any task in any DAG.
> > >
> > >
> > > Given that the skipped states propagate where all directly upstream
> tasks
> > > are skipped, I don't think this is the state we want to use. For the
> > > functionality I'm looking for, I think I'll need to introduce a new
> > status,
> > > maybe 'disabled'.
> > >
> > >
> > > Again, thanks for your response.
> > >
> > >
> > > Cheers,
> > > Luke Maycock
> > > OLIVER WYMAN
> > > luke.maycock@affiliate.oliverwyman.com<mailto:luke.
> > > maycock@affiliate.oliverwyman.com>
> > > www.oliverwyman.com<http://www.oliverwyman.com/>
> > >
> > >
> > >
> > > ________________________________
> > > From: Gerard Toonstra <gtoonstra@gmail.com>
> > > Sent: 08 November 2016 18:19
> > > To: dev@airflow.incubator.apache.org
> > > Subject: Re: Skip task
> > >
> > > Also in 1.7.1.3, there's the ShortCircuitOperator, which can give you
> an
> > > example.
> > >
> > > https://github.com/apache/incubator-airflow/blob/1.7.1.
> > > 3/airflow/operators/python_operator.py
> > >
> > > You'd have to modify this to your needs, but the way it works is that
> if
> > > the condition evaluates to True, none of the
> > > downstream tasks are actually executed, they'd be skipped. The reason
> for
> > > putting them into SKIPPED state is that
> > > the DAG final result would still be SUCCESS and not failed.
> > >
> > > You could copy the operator from there and don't do the full ""for
> loop"",
> > > only pick the tasks immediately downstream
> > > from this operator and skip that. Or... if you need to skip additional
> > > tasks downstream, add a parameter ""num_tasks""
> > > that decide on a halting condition for the for loop.
> > >
> > > I believe that should work. I didn't try that here, but you can test
> that
> > > and see what it does for you.
> > >
> > >
> > > If you want this as a UI capability... for example have a human
> operator
> > > decide on skipping this yes or not, then
> > > maybe the best way forward would be some kind of highly custom plugin
> > with
> > > its own view. In the end, you'd basically
> > > do the same action in the backend, whether the python cond evaluates to
> > > True or the button is clicked.
> > >
> > > In the plugin case though, you'd have to keep the UI and the structure
> of
> > > the DAG in sync and aligned, otherwise
> > > it'd become a mess.... Airflow wasn't really developed for
> workflow/human
> > > interaction, but in workflows where only
> > > automated processes are involved. That doesn't mean that you can't do
> > > anything like that, but it may be costly resource
> > > wise to get this done. For example, on the basis of the BranchOperator,
> > you
> > > could call an external API to verify if a decision
> > > was taken on a case, then follow branch A or B if the decision is there
> > or
> > > put the state back into UP_FOR_RETRY.
> > > At the moment though, there's no programmatic way to reschedule that
> task
> > > to some minutes or hours into the future before
> > > it's looked at again, unless you really dive into airflow, scheduling
> > > semantics (@once vs. other schedules) and how
> > > the scheduler works.
> > >
> > > Rgds,
> > >
> > > Gerard
> > >
> > >
> > >
> > >
> > > On Tue, Nov 8, 2016 at 5:30 PM, Maycock, Luke <
> > > luke.maycock@affiliate.oliverwyman.com> wrote:
> > >
> > > > Hi All,
> > > >
> > > >
> > > > I am using Airflow 1.7.1.3 and have a particular requirement, which I
> > > > don't think is currently supported by Airflow but just wanted to
> check
> > in
> > > > case I was missing something.
> > > >
> > > >
> > > > I occasionally wish to skip a particular task in a given DAG run such
> > > that
> > > > the task does not run for that DAG run. Is this functionality
> available
> > > in
> > > > Airflow?
> > > >
> > > >
> > > > I am aware of the BranchPythonOperator (https://airflow.incubator.
> > > > apache.org/concepts.html#branching) but I don't think believe this
> is
> > > > exactly what I am looking for.
> > > >
> > > >
> > > > I am thinking that a button in the UI alongside the 'Mark Success'
> and
> > > > 'Run' buttons would be appropriate.
> > > >
> > > >
> > > > If the functionality does not exist, does anyone have any suggestions
> > on
> > > > ways to implement this?
> > > >
> > > >
> > > > Cheers,
> > > > Luke Maycock
> > > > OLIVER WYMAN
> > > > luke.maycock@affiliate.oliverwyman.com<mailto:luke.
> > > > maycock@affiliate.oliverwyman.com>
> > > > www.oliverwyman.com<http://www.oliverwyman.com/>
> > > >
> > > >
> > > > ________________________________
> > > > This e-mail and any attachments may be confidential or legally
> > > privileged.
> > > > If you received this message in error or are not the intended
> > recipient,
> > > > you should destroy the e-mail message and any attachments or copies,
> > and
> > > > you are prohibited from retaining, distributing, disclosing or using
> > any
> > > > information contained herein. Please inform us of the erroneous
> > delivery
> > > by
> > > > return e-mail. Thank you for your cooperation.
> > > >
> > >
> > > ________________________________
> > > This e-mail and any attachments may be confidential or legally
> > privileged.
> > > If you received this message in error or are not the intended
> recipient,
> > > you should destroy the e-mail message and any attachments or copies,
> and
> > > you are prohibited from retaining, distributing, disclosing or using
> any
> > > information contained herein. Please inform us of the erroneous
> delivery
> > by
> > > return e-mail. Thank you for your cooperation.
> > >
> >
> > ________________________________
> > This e-mail and any attachments may be confidential or legally
> privileged.
> > If you received this message in error or are not the intended recipient,
> > you should destroy the e-mail message and any attachments or copies, and
> > you are prohibited from retaining, distributing, disclosing or using any
> > information contained herein. Please inform us of the erroneous delivery
> by
> > return e-mail. Thank you for your cooperation.
> >
>
> ________________________________
> This e-mail and any attachments may be confidential or legally privileged.
> If you received this message in error or are not the intended recipient,
> you should destroy the e-mail message and any attachments or copies, and
> you are prohibited from retaining, distributing, disclosing or using any
> information contained herein. Please inform us of the erroneous delivery by
> return e-mail. Thank you for your cooperation.
>

________________________________
This e-mail and any attachments may be confidential or legally privileged. If you received
this message in error or are not the intended recipient, you should destroy the e-mail message
and any attachments or copies, and you are prohibited from retaining, distributing, disclosing
or using any information contained herein. Please inform us of the erroneous delivery by return
e-mail. Thank you for your cooperation.
",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<BY2PR07MB15561B2792AB6FE46E9F732EC9840@BY2PR07MB1556.namprd07.prod.outlook.com>,"""Maycock, Luke"" <luke.mayc...@affiliate.oliverwyman.com>",0,0
231,232,"On 24 June 2016 at 10:37, Ian Dunlop <ianwdunlop@gmail.com> wrote:
> Thanks magnus. I guess we need to add this to the taverna publications list.

+1

Added to http://taverna.staging.apache.org/community/publications

It will be live after the current Taverna Command line RC vote passes
- as otherwise we would expose the new Download pages early.


-- 
Stian Soiland-Reyes
Apache Taverna (incubating), Apache Commons
http://orcid.org/0000-0001-9842-9718

",http://mail-archives.apache.org/mod_mbox/incubator-taverna-users/201606.mbox/%3cCAMBJEmUvdKwyJDXHzbW_VE4oa0zQOKbhJxAa9APr2LrhpiYkgg@mail.gmail.com%3e,Stian Soiland-Reyes <st...@apache.org>,0,0
140,141,"We use the ICU library for parsing numbers based on the
textNumberPattern. This library has this to say about strict parsing of
numbers:


> The following conditions cause a parse failure relative to [lax] mode
> (examples use the pattern ""#,##0.#""):
> 
> * The presence and position of special symbols, including currency, 
> must match the pattern.
> 
>   '+123' fails (there is no plus sign in the pattern)
> 
> * Leading or doubled grouping separators
> 
>   ',123' and '1,,234"" fail
> 
> * Groups of incorrect length when grouping is used
> 
>   '1,23' and '1234,567' fail, but '1234' passes
> 
> * Grouping separators used in numbers followed by exponents
> 
>   '1,234E5' fails, but '1234E5' and '1,234E' pass ('E' is not an 
>    exponent when not followed by a number)

So bsaed on ICU's description of strict, this is the expected behavior.
It doesn't say anything about missing grouping separators causing an
error. Only that if they do exist then they must be in the right spot.

The only thing the DFDL specification mentions regarding strict numbers
is this:

> If 'strict' and dfdl:textNumberRep is 'standard' then the data must 
> follow the pattern with the exceptions that digits 0-9, decimal 
> separator and exponent separator are always recognised and parsed

To me, that reads like the decimal separator should always be required
in strict mode, so this feels like the ICU behavior and the behavior
described in the DFDL specification do not match. And I believe the DFDL
behavior was intended to match match ICU behavior, so it's possible the
DFDL specification needs to be updated.

I've created DAFFODIL-2384 [1] to track this issue.

- Steve

[1] https://issues.apache.org/jira/browse/DAFFODIL-2384

On 8/17/20 11:57 AM, Roger L Costello wrote:
> What is an example of input that will cause parsing the fail due to the grouping separators
when textNumberCheckPolicy=""strict""? Why isn't the below an example, i.e., why is no error
generated with the below example?  
> 
> Why is it that with this input
> 
> 1234
> 
> No error is raised when textNumberCheckPolicy=""strict"" and textNumberPattern=""#,###""
are specified:
> 
> <xs:element name=""SimpleDataFormat"">
>     <xs:complexType>
>         <xs:sequence>
>             <xs:element name=""NumStudents"" type=""xs:nonNegativeInteger"" 
>                 dfdl:textNumberCheckPolicy=""strict""
>                 dfdl:textNumberPattern=""#,###""
>                 dfdl:textStandardGroupingSeparator="",""
>                 dfdl:textStandardDecimalSeparator="".""
>             />
>         </xs:sequence>
>     </xs:complexType>
> </xs:element>
> 


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/202008.mbox/%3c72b4f5e4-b632-7342-b6cc-71472dbd5c8f@apache.org%3e,Steve Lawrence <slawre...@apache.org>,0,0
301,302,"Hi Folks

For those of you who missed it, you can catch the discussion from the link
on this tweet <https://twitter.com/samelamin/status/861703888298225670>

Please do share and feel free to get involved as the more feedback we get
the better the library we create is :)

Regards
Sam

On Mon, May 8, 2017 at 9:43 PM, Sam Elamin <hussam.elamin@gmail.com> wrote:

> Bit late notice but the call is happening today at 9 15 utc so in about
>  30 mins or so
>
> It will be recorded but if anyone would like to join in on the discussion
> the hangout link is https://hangouts.google.com/hangouts/_/
> mbkr6xassnahjjonpuvrirxbnae
>
> Regards
> Sam
>
> On Fri, 5 May 2017 at 21:35, Ali Uz <aliuz1@gmail.com> wrote:
>
>> I am also very interested in seeing how this turns out. Even though we
>> don't have a testing framework in-place on the project I am working on, I
>> would very much like to contribute to some general framework for testing
>> DAGs.
>>
>> As of now we are just implementing dummy tasks that test our actual tasks
>> and verify if the given input produces the expected output. Nothing crazy
>> and certainly not flexible in the long run.
>>
>>
>> On Fri, 5 May 2017 at 22:59, Sam Elamin <hussam.elamin@gmail.com> wrote:
>>
>> > Haha yes Scott you are in!
>> > On Fri, 5 May 2017 at 20:07, Scott Halgrim <scott.halgrim@zapier.com>
>> > wrote:
>> >
>> > > Sounds A+ to me. By “both of you” did you include me? My first
>> response
>> > > was just to your email address.
>> > >
>> > > On May 5, 2017, 11:58 AM -0700, Sam Elamin <hussam.elamin@gmail.com>,
>> > > wrote:
>> > > > Ok sounds great folks
>> > > >
>> > > > Thanks for the detailed response laura! I'll invite both of you to
>> the
>> > > > group if you are happy and we can schedule a call for next week?
>> > > >
>> > > > How does that sound?
>> > > > On Fri, 5 May 2017 at 17:41, Laura Lorenz <llorenz@industrydive.com
>> >
>> > > wrote:
>> > > >
>> > > > > We do! We developed our own little in-house DAG test framework
>> which
>> > we
>> > > > > could share insights on/would love to hear what other folks are
up
>> > to.
>> > > > > Basically we use mock a DAG's input data, use the BackfillJob
API
>> > > directly
>> > > > > to call a DAG in a test, and compare its outputs to the intended
>> > result
>> > > > > given the inputs. We use docker/docker-compose to manage services,
>> > and
>> > > > > split our dev and test stack locally so that the tests have their
>> own
>> > > > > scheduler and metadata database and so that our CI tool knows
how
>> to
>> > > > > construct the test stack as well.
>> > > > >
>> > > > > We co-opted the BackfillJob API for our own purposes here, but
it
>> > > seemed
>> > > > > overly complicated and fragile to start and interact with our
own
>> > > > > in-test-process executor like we saw in a few of the tests in
the
>> > > Airflow
>> > > > > test suite. So I'd be really interested on finding a way to
>> > streamline
>> > > how
>> > > > > to describe a test executor for both the Airflow test suite and
>> > > people's
>> > > > > own DAG testing and make that a first class type of API.
>> > > > >
>> > > > > Laura
>> > > > >
>> > > > > On Fri, May 5, 2017 at 11:46 AM, Sam Elamin <
>> hussam.elamin@gmail.com
>> > > > > wrote:
>> > > > >
>> > > > > > Hi All
>> > > > > >
>> > > > > > A few people in the Spark community are interested in writing
a
>> > > testing
>> > > > > > library for Airflow. We would love anyone who uses Airflow
>> heavily
>> > in
>> > > > > > production to be involved
>> > > > > >
>> > > > > > At the moment (AFAIK) testing your DAGs is a bit of a pain,
>> > > especially if
>> > > > > > you want to run them in a CI server
>> > > > > >
>> > > > > > Is anyone interested in being involved in the discussion?
>> > > > > >
>> > > > > > Kind Regards
>> > > > > > Sam
>> > > > > >
>> > > > >
>> > >
>> >
>>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CAHogpC4W_VMEUO875bMkJeuAeAMu_ZyQda_pPeVk9Dt-79Sjrw@mail.gmail.com>,Sam Elamin <hussam.ela...@gmail.com>,0,1
210,211,"Hi, I'm attempting to create a custom hook (for mongodb) as a plugin but
it's not appearing in the list of connections in the UI.

Steps:
1. Ran Airflow via docker (https://github.com/puckel/docker-airflow)
2. Copied the example from https://pythonhosted.org/airflow/plugins.html
3. Copied the postgres hook, renamed a few fields and ended up with
https://gist.github.com/shin-nien/2d011972d8631c92675d1de58b000168 (it
won't work for mongo yet but I wanted to test that it would be available).
4. Restarted the container and observed pyc in plugins dir was created.
5. Went to UI and noticed the test menus
6. Went to UI/Connections but couldn't find my connector/hook

Any tips?

Thanks

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201609.mbox/<CALQJ+fzwUC1xkB=wJnVdVFw_+gRzuh-aj_tPT-3dS9FQhq3bXw@mail.gmail.com>,Shin Tai <shin.n...@gmail.com>,0,0
290,291,"Ok.. list didnt return anything because there were no tables present.

I have another query.

*blur (default)> create -t test -c 10 -l test*
*blur (default)> mutate test r1 rid1 f1 c1:v1*
*blur (default)> query test **

*1 results found in [223.357074 ms].  Row [1] Record [1] Column [1] Data
(bytes) [8]*
*result# rowid recordid f1.c1*
*0       r1    rid1     v1   *


Until this point it is working fine.

*blur (default)> create -t testable -c 10 -l test*
*blur (default)> mutate testable r2 rid2 f2 c2:f2*
*blur (default)> query testable **


*2 results found in [36.109771 ms].  Row [2] Record [2] Column [2] Data
(bytes) [16]*
*result# rowid recordid      *
*0       r2             f2.c2*
*0             rid2     f2   *
*1       r1             f1.c1*
*1             rid1     v1   *

Now when i am querying table testable why it is returning 2 results (One
row from test table) even though it has only one record?

*blur (default)> disable test*
*blur (default)> remove test*
*blur (default)> query testable **


*2 results found in [43.491442 ms].  Row [2] Record [2] Column [2] Data
(bytes) [16]*
*result# rowid recordid      *
*0       r2             f2.c2*
*0             rid2     f2   *
*1       r1             f1.c1*
*1             rid1     v1  *

Here even after removing test table, it still returns 2 results , one from
test and one from testable tables.

What is wrong here?? Why is this residual data keep coming?

Any solution on this?


Thanks,
Ameya









On Thu, Aug 7, 2014 at 2:35 PM, Aaron McCurry <amccurry@gmail.com> wrote:

> Unfortunately no. The mail lists strip out all attachments. You could copy
> the text from the terminal or find a public image share. Thanks!
>
> Aaron
>
> On Thursday, August 7, 2014, Ameya Aware <ameya.aware@gmail.com> wrote:
>
> > Can i attach it here in mail?
> >
> > Will that work?
> >
> > Thanks,
> > Ameya
> >
> >
> >
> >
> > On Thu, Aug 7, 2014 at 10:54 AM, Tim Williams <williamstw@gmail.com
> > <javascript:;>> wrote:
> >
> > > Hey Ameya,
> > > Your screenshot got stripped by the mail-list software - can you post
> to
> > > some public spot?  The list command doesn't take any parameters so if
> > > you've created a table and list it and it doesn't show, then
> something's
> > > gone really wrong.
> > >
> > > Thanks,
> > > --tim
> > >
> > >
> > > On Thu, Aug 7, 2014 at 9:53 AM, Ameya Aware <ameya.aware@gmail.com
> > <javascript:;>> wrote:
> > >
> > > > Any help on this?
> > > >
> > > >
> > > > Thanks,
> > > > Ameya
> > > >
> > > >
> > > > On Wed, Aug 6, 2014 at 3:08 PM, Ameya Aware <ameya.aware@gmail.com
> > <javascript:;>>
> > > wrote:
> > > >
> > > >> ok sure..
> > > >>
> > > >> When i give list commnad it didnt list the tables.. Do i need to
> pass
> > > any
> > > >> parameters to it? Though ""help"" doesnt say so.
> > > >>
> > > >> Please find snapshot below.
> > > >>
> > > >> [image: Inline image 1]
> > > >>
> > > >>
> > > >> Thanks,
> > > >> Ameya
> > > >>
> > > >>
> > > >> On Wed, Aug 6, 2014 at 2:24 PM, Tim Williams <williamstw@gmail.com
> > <javascript:;>>
> > > >> wrote:
> > > >>
> > > >>> On Wed, Aug 6, 2014 at 2:21 PM, Ameya Aware <ameya.aware@gmail.com
> > <javascript:;>>
> > > >>> wrote:
> > > >>> > Thanks.
> > > >>> >
> > > >>> > Also can we check which all tables exist?
> > > >>>
> > > >>> Sure, the ""list"" command in the shell will list them.  You may
also
> > > >>> wanna give the ""help"" command a spin:)  If you type 'help' and
it
> > > >>> isn't clear what they actually do, let us know so we can add a
more
> > > >>> helpful description of the command.
> > > >>>
> > > >>> --tim
> > > >>>
> > > >>
> > > >>
> > > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201408.mbox/%3cCABrC6qg4pJDazmNY5jqv0-K6id5LOa=QRGzH=h4yJTvaz4AR0w@mail.gmail.com%3e,Ameya Aware <ameya.aw...@gmail.com>,0,0
173,174,"Hi Richard,

I would suggest WorkspaceImpl#createWorkspace(), since this method is 
public rather than protected (SessionImpl#createWorkspace()).

Hope it helps,

Nicolas


Le 13:53 2005-07-06, vous avez écrit:
>Apologies if this is a really dumb question but how can I create multiple 
>workspaces with jackrabbit? Passing a named workspace into the login 
>returns a ""javax.jcr.NoSuchWorkspaceException"", but will create a new 
>default workspace if it dosnt already exist and trawling over the API 
>hasn't helped. The repository.xml file seems to define a workspace 
>template rather than a list of workspaces (at least that's how I read it).
>
>Ah, as I'm typing I can see createWorkspace() on SessionImpl -- I guess 
>I'm supposed to cast an active session object to the jackrabbit 
>SessionImpl and call that?
>
>- Richard


",http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200507.mbox/<6.1.1.1.2.20050706150153.019af7d8@hermes.ulaval.ca>,Nicolas Belisle <Nicolas.Beli...@bibl.ulaval.ca>,0,0
7,7,"Github user asfgit closed the pull request at:      https://github.com/apache/usergrid/pull/554   --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well. If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA. ---",http://mail-archives.apache.org/mod_mbox/usergrid-dev/201608.mbox/raw/%3C20160805211015.01B89EC22C%40git1-us-west.apache.org%3E,asfgit <...@git.apache.org>,0,0
137,138,"The issue is that you have maxOccurs=""unbounded"" on elements which are potentially 0 bits long.

In particular, F_Record and B_Record. Both of those elements have only optional children.
This means that they will never fail to parse. Instead they will succeed in parsing, but consume
0 bits. Because they can occur an unbounded number of times, Daffodil considers this to be
an error, and backtracks (and subsequently throws an unrelated error down the line).

When maxOccurs is finite, then Daffodil will parse the 0 bits a finite number of times before
resuming the parse normally.

The simplest solution to this, is to add an explicit assertion that F_Record and B_Record
are non-empty:

<xs:annotation>
    <xs:appinfo source=""http://www.ogf.org/dfdl/"">
        <dfdl:assert>
          { dfdl:contentLength(.,'bits') gt 0 }
        </dfdl:assert>
    </xs:appinfo>
</xs:annotation>

Attached, you will find a version of pug_records.xsd that takes this approach.

While this is not technically a bug in Daffodil, it really should issue a warning when this
situation arises. I have opened a ticket to that effect: https://issues.apache.org/jira/browse/DAFFODIL-2247

Given the above, you may be wondering why you do not see thousands of empty instances of F_Record
when maxOccurs=""9999"". I believe this is the correct behavior in this case as defined by section
9.4.2.3, but I would need to read the spec very closely to be sure that this is not a bug
in Daffodil.

Regards,
Brandon

________________________________
From: Peter Kostouros <Peter.Kostouros@awta.com.au>
Sent: Wednesday, December 4, 2019 4:52 PM
To: users@daffodil.apache.org <users@daffodil.apache.org>
Subject: RE: Daffodil parsing fails on optional elements when maxOccurs set to ""unbounded"",
passes when set to ""999""


Hi



I have attached a dataset that shows the problem (PUG.IN) as well as its corresponding parsed
output when the schema has set maxOccurs limits on selected optional elements (PUG_999.IN.XML).



The F_LOOP records in file PUG.IN start with “31” and “32A”.





Peter





From: Sloane, Brandon [mailto:bsloane@tresys.com]
Sent: Thursday, 5 December 2019 2:44 AM
To: users@daffodil.apache.org
Subject: Re: Daffodil parsing fails on optional elements when maxOccurs set to ""unbounded"",
passes when set to ""999""



The only thing that stands out to me is that the error you are seeing should be coming from
ControlRecord, which isn't part of the quoted schema. Other then that, I am not sure what
the issue could be (unless your data actually parses more then 999 instances when unbounded
is used).



Do you have example data that you can share which demonstrates the problem?

________________________________

From: Peter Kostouros <Peter.Kostouros@awta.com.au<mailto:Peter.Kostouros@awta.com.au>>
Sent: Wednesday, December 4, 2019 12:35 AM
To: users@daffodil.apache.org<mailto:users@daffodil.apache.org> <users@daffodil.apache.org<mailto:users@daffodil.apache.org>>
Subject: Daffodil parsing fails on optional elements when maxOccurs set to ""unbounded"", passes
when set to ""999""



Hi

I hope someone can point me in the right direction to help me understand behaviour I have
seen with a particular schema when parsing a file.

I have a schema modelled on the NACHA schema files found in the DFDLSchemas/NACHA directory
on github. In my case, with respect to optional (embedded) looping elements:

1.       Parsing is unsuccessful when maxOccurs attribute is set to “unbounded” (Parse
Error: Failed to populate ControlRecord[1]. Cause: Parse Error: Assertion failed: Not Control
Record);

2.       Parsing is successful when maxOccurs is limited to say “999”.

Below is a snippet from the schema referred to above that results in error:

<!-- F LOOP -->

<xs:element dfdl:lengthKind=""implicit"" name=""F_Records"" minOccurs=""0"">

  <xs:complexType>

    <xs:sequence>

      <xs:element dfdl:lengthKind=""implicit"" name=""F_Record"" minOccurs=""0"" maxOccurs=""unbounded"">

        <xs:complexType>

          <xs:sequence>

            <xs:element ref=""F01_Record"" minOccurs=""0"" />

            <xs:element dfdl:lengthKind=""implicit"" name=""F02_Records"" minOccurs=""0"">

              <xs:complexType>

                <xs:sequence>

                  <xs:element ref=""F02_Record"" minOccurs=""0"" maxOccurs=""unbounded"" />

                </xs:sequence>

              </xs:complexType>

            </xs:element>

          </xs:sequence>

        </xs:complexType>

      </xs:element>

    </xs:sequence>

  </xs:complexType>

</xs:element>



I have attached schema files that demonstrate this issue, so I hope someone can advise me
on what I should be correctly doing.

I am using the Daffodil 2.4.0 release as well as the 2.5 snapshot JAVA APIs, and both show
similar behaviour; I have also seen this behaviour when running the files though the daffodil
command line, with the following tunables

""unqualifiedPathStepPolicy"" = ""defaultNamespace""

""suppressSchemaDefinitionWarnings"" = ""multipleChoiceBranches noEmptyDefault""



Peter



This e-mail and any attachment is intended for the party to which it is addressed and may
contain confidential information or be subject to professional privilege. Its transmission
in not intended to place the contents into the public domain. If you have received this e-mail
in error, please notify us immediately and delete the email and all copies. AWTA Ltd does
not warrant that this e-mail is virus or error free. By opening this e-mail and any attachment
the user assumes all responsibility for any loss or damage resulting from such action, whether
or not caused by the negligence of AWTA Ltd. The contents of this e-mail and any attachments
are subject to copyright and may not be reproduced, adapted or transmitted without the prior
written permission of the copyright owner.

This e-mail and any attachment is intended for the party to which it is addressed and may
contain confidential information or be subject to professional privilege. Its transmission
in not intended to place the contents into the public domain. If you have received this e-mail
in error, please notify us immediately and delete the email and all copies. AWTA Ltd does
not warrant that this e-mail is virus or error free. By opening this e-mail and any attachment
the user assumes all responsibility for any loss or damage resulting from such action, whether
or not caused by the negligence of AWTA Ltd. The contents of this e-mail and any attachments
are subject to copyright and may not be reproduced, adapted or transmitted without the prior
written permission of the copyright owner.

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201912.mbox/%3cDM5PR1501MB2008A06238689E381A0B2730D75C0@DM5PR1501MB2008.namprd15.prod.outlook.com%3e,"""Sloane, Brandon"" <bslo...@tresys.com>",0,0
180,181,"it's working as it always has, though this case (when a dag is behind
several runs and needs to catch up) appears to result in an empty Gantt
chart, which is not ideal.

On Mon, Nov 14, 2016 at 12:43 AM Sumit Maheshwari <sumeet.manit@gmail.com>
wrote:

> So we can say that its not broken, right?
>
>
> PS: link to screenshot
>
> https://www.dropbox.com/s/wql05icqgnewl0d/Screenshot%202016-11-14%2006.55.43.png?dl=0
>
>
>
>
> On Mon, Nov 14, 2016 at 12:30 PM, siddharth anand <sanand@apache.org>
> wrote:
>
> > Found the issue.. it seems the gantt charts load from the most recent
> > ""running"" DagRun, which in the example I provided (assuming you can see
> the
> > attached screenshots in my original email), has not currently running
> > tasks, hence the gantt charts look completely bare. If the Gantt chart
> > instead defaulted to the most recent running DagRun with at least one
> task
> > complete, it would be more useful IMHO.
> >
> > -s
> >
> > On Sun, Nov 13, 2016 at 9:53 PM, siddharth anand <sanand@apache.org>
> > wrote:
> >
> >> I've installed airflow on a fresh virtualenv and I still have the issue
> >> above on the gantt chart. Anyone else notice the same. @Sumit, as the
> last
> >> person to merge a gantt chart change, can you repro on a fresh
> virtualenv?
> >>
> >> I notice a single ""/"" character on the page.
> >>
> >> -s
> >>
> >> On Sat, Nov 12, 2016 at 9:25 PM, siddharth anand <sanand@apache.org>
> >> wrote:
> >>
> >>> Gantt chart is broken for me on master.
> >>>
> >>> I think it's due to this merge.
> >>> https://github.com/apache/incubator-airflow/commit/868bc8313
> >>> 7adca0ebfd5780f0dff5a7bfdfaadf9
> >>>
> >>> Why is an end_date needed?
> >>>
> >>> [image: Inline image 1]
> >>>
> >>> This is the tree view:
> >>> [image: Inline image 2]
> >>>
> >>> Sumit, as the merger/committer, can you confirm?
> >>>
> >>> -s
> >>>
> >>
> >>
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201611.mbox/<CANLtMiddPr+dYZzeGkYmDrs1XP7zeK4diX3T4N3kqBGHR2kPnQ@mail.gmail.com>,siddharth anand <san...@apache.org>,0,0
248,249,"+1

I checked the following:

[ X ] Build and Unit Tests Pass
[ X ] Integration Tests Pass
[ X ] ""Incubating"" in References to Project and Distribution File Names
[ X ] Signatures and Hashes Match Keys
[ X ] DISCLAIMER, LICENSE, and NOTICE Files in Source and Binary Release Packages
[ X ] DISCLAIMER, LICENSE, and NOTICE are consistent with ASF and Incubator Policy
[ X ] CHANGELOG included with release distribution
[ X ] All Source Files Have Correct ASF Headers
[ X ] No Binary Files in Source Release Packages

> On Feb 7, 2020, at 4:12 PM, Joshua Poore <poorejc@apache.org> wrote:
> 
> Hi Folks,
>  
> Please VOTE on the Apache Flagon 2.1.0 Release Candidate 01
>  
> About Flagon: http://flagon.incubator.apache.org/ <http://flagon.incubator.apache.org/>
>  
> This {Major/Minor/Patch} release includes :
>  
> 	• Module package-bundler deployment support (include UserALE.js via 'import' &
'require')
> 	• Updated existing example page to include a range of HTML elements
> 	• New example page for including UserALE.js as a module (Webpack example)
> 	• Added support for logging from HTML Forms
> 	• Added SessionId persistence via SessionStorage
> 	• Exposes a wide range of functions to support custom logging with UserALE.js
> 	• Added support for passing auth-headers via log pipeline to back-end
> 	• Added additional log fields: browser type and version, inner width/height (for heatmaps)
>  
> We solved 31 issues: https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12345442&styleName=Text&projectId=12320621&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED_8301b4e9c1c91354ea85ab02c89ec979db077d9a_lin
<https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12345442&styleName=Text&projectId=12320621&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED_8301b4e9c1c91354ea85ab02c89ec979db077d9a_lin>
>  
> Git source tag (7746500): https://github.com/apache/incubator-flagon-useralejs/releases/tag/2.1.0-RC-01
<https://github.com/apache/incubator-flagon-useralejs/releases/tag/2.1.0-RC-01>
>  
> Staging repo: https://dist.apache.org/repos/dist/dev/incubator/flagon/ <https://dist.apache.org/repos/dist/dev/incubator/flagon/>
>  
> Source Release Artifacts: https://dist.apache.org/repos/dist/dev/incubator/flagon/apache-flagon-useralejs-incubating-2.1.0-RC-01/
<https://dist.apache.org/repos/dist/dev/incubator/flagon/apache-flagon-useralejs-incubating-2.1.0-RC-01/>
>  
> PGP release keys (signed using F9374FAE3FCADF6E): https://dist.apache.org/repos/dist/dev/incubator/flagon/KEYS
<https://dist.apache.org/repos/dist/dev/incubator/flagon/KEYS>
>  
> Link to Successful Jenkins Build: https://builds.apache.org/job/useralejs-ci/101/ <https://builds.apache.org/job/useralejs-ci/101/>
>  
> Reference to UserALE.js testing framework to assist in verifying this release: https://cwiki.apache.org/confluence/display/FLAGON/UserALE.js+Testing+Framework
<https://cwiki.apache.org/confluence/display/FLAGON/UserALE.js+Testing+Framework>
>  
> Vote will be open for 72 hours. Please VOTE as follows:
>  
> [ ] +1, let's get it released!!!
> [ ] +/-0, fine, but consider to fix few issues before...
> [ ] -1, nope, because... (and please explain why)
>  
> Along with your VOTE, please indicate testing and checks you've made against build artifacts,
src, and documentation:
>  
> [ ] Build and Unit Tests Pass
> [ ] Integration Tests Pass
> [ ] ""Incubating"" in References to Project and Distribution File Names
> [ ] Signatures and Hashes Match Keys
> [ ] DISCLAIMER, LICENSE, and NOTICE Files in Source and Binary Release Packages
> [ ] DISCLAIMER, LICENSE, and NOTICE are consistent with ASF and Incubator Policy
> [ ] CHANGELOG included with release distribution
> [ ] All Source Files Have Correct ASF Headers
> [ ] No Binary Files in Source Release Packages
>  
> Thank you to everyone that is able to VOTE as well as everyone that contributed to Apache
Flagon 2.1.0


",http://mail-archives.apache.org/mod_mbox/incubator-flagon-user/202002.mbox/%3c43A6EE27-A7E3-4603-8406-4BF0DD2ED96D@apache.org%3e,Joshua Poore <poor...@apache.org>,0,1
97,98,"On 12/15/11 11:09 AM, raghavendhra rahul wrote:
> How to add those classes.
> Also how to run other examples described with s4.
> I am really interested in knowing about s4.

I would really suggest that you follow the manual. You probably forgot 
to checkout tag 0.3. See: http://docs.s4.io/tutorials/getting_started.html

Matthieu

",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201112.mbox/%3c4EE9C9F0.8010100@apache.org%3e,Matthieu Morel <mmo...@apache.org>,0,1
176,177,"Nope, I don't believe anyone has (I didn't validate that, though). Want to
give it a shot? :)

On Mon, Apr 3, 2017 at 6:09 PM, Alex Guziel <alex.guziel@airbnb.com.invalid>
wrote:

> Did we do this?
>
> On Mon, Apr 3, 2017 at 5:53 PM, <johndament@apache.org> wrote:
>
> > Dear podling,
> >
> > This email was sent by an automated system on behalf of the Apache
> > Incubator PMC. It is an initial reminder to give you plenty of time to
> > prepare your quarterly board report.
> >
> > The board meeting is scheduled for Wed, 19 April 2017, 10:30 am PDT.
> > The report for your podling will form a part of the Incubator PMC
> > report. The Incubator PMC requires your report to be submitted 2 weeks
> > before the board meeting, to allow sufficient time for review and
> > submission (Wed, April 05).
> >
> > Please submit your report with sufficient time to allow the Incubator
> > PMC, and subsequently board members to review and digest. Again, the
> > very latest you should submit your report is 2 weeks prior to the board
> > meeting.
> >
> > Thanks,
> >
> > The Apache Incubator PMC
> >
> > Submitting your Report
> >
> > ----------------------
> >
> > Your report should contain the following:
> >
> > *   Your project name
> > *   A brief description of your project, which assumes no knowledge of
> >     the project or necessarily of its field
> > *   A list of the three most important issues to address in the move
> >     towards graduation.
> > *   Any issues that the Incubator PMC or ASF Board might wish/need to be
> >     aware of
> > *   How has the community developed since the last report
> > *   How has the project developed since the last report.
> > *   How does the podling rate their own maturity.
> >
> > This should be appended to the Incubator Wiki page at:
> >
> > https://wiki.apache.org/incubator/April2017
> >
> > Note: This is manually populated. You may need to wait a little before
> > this page is created from a template.
> >
> > Mentors
> > -------
> >
> > Mentors should review reports for their project(s) and sign them off on
> > the Incubator wiki page. Signing off reports shows that you are
> > following the project - projects that are not signed may raise alarms
> > for the Incubator PMC.
> >
> > Incubator PMC
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201704.mbox/<CABYbY7cvNtFWrjoJbu+_opuuh61oK_iYhJQHWBsS9V0dRPdQHA@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
11,11,Github user susthitsoft commented on the issue:      https://github.com/apache/usergrid/pull/574        @michaelarusso @mdunker Is this going to be merged any time soon?,http://mail-archives.apache.org/mod_mbox/usergrid-dev/201807.mbox/raw/%3C20180710121244.0F650DFAF1%40git1-us-west.apache.org%3E,susthitsoft <...@git.apache.org>,0,0
98,99,"Do the workers themselves need to be on Windows, or do you just need work
to happen on a Windows machine?  If the later, consider using the SSH
Execute Operator?

-R

On Tue, May 16, 2017 at 1:43 AM Niranda Perera <niranda.17@cse.mrt.ac.lk>
wrote:

> To clarify the the question more,
>
> I want to run workers on windows, as I have some apps in my workflow which
> require windows. But I can keep my master instance in linux.
>
> I hope this issue of gunicorn not compatible with windows [1] would not be
> a blocker to run the worker?
>
> I also want to monitor the status of the windows worker from the airflow
> master and I hope not having gunicorn, would not break that functionality?
>
> [1]
>
> http://stackoverflow.com/questions/32378494/how-to-run-airflow-on-windows/32378495
>
> Best
> Nira
>
> Best regards
>
> Niranda Perera
> Research Assistant
> Dept of CSE, University of Moratuwa
> niranda.17@cse.mrt.ac.lk
> +94 71 554 8430 <+94%2071%20554%208430>
> https://lk.linkedin.com/in/niranda
>
> On Tue, May 16, 2017 at 12:00 PM, Niranda Perera <niranda.17@cse.mrt.ac.lk
> >
> wrote:
>
> > Hi devs,
> >
> > Does airflow work on the windows environment?
> >
> > Best regards
> >
> > Niranda Perera
> > Research Assistant
> > Dept of CSE, University of Moratuwa
> > niranda.17@cse.mrt.ac.lk
> > +94 71 554 8430 <+94%2071%20554%208430> <+94%2071%20554%208430>
> > https://lk.linkedin.com/in/niranda
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CALXon6Xt191iRxPP_RtuRSKAvghyq1gAg-jwOE_GshAw+R_BJA@mail.gmail.com>,Russell Pierce <russell.s.pie...@gmail.com>,0,0
144,145,"Nice! That really worked with Lighttpd.
Isn't there a solution for more web servers with the same problem?
Tried to set this line:
ENV['""INLINEDIR'] ||= '/export/faban/olio_rails/olioapp/tmp'
into *environment.rb* and *environments/production.rb* files but it didn't
work.

regards,
--
Bruno Guimarães Sousa
www.ifba.edu.br
PONTONET - DGTI - IFBA
Ciência da Computação UFBA
Registered Linux user #465914


On Sat, May 22, 2010 at 3:13 PM, Amanda Waite
<amandarwaite@googlemail.com>wrote:

> Hi Bruno,
>
> I've done lots of testing with Olio Rails with Lighttpd and FCGI. It works
> well once you have everything setup. The error messages you've posted
> suggest that the fastcgi backends aren't running or that they are crashing
> on startup. Are you starting the backends manually or is lighttpd starting
> them for you? Is there anything in the Olio application log files?
>
> One of the biggest issues with the backends is setting up image_science
> ruby_inline to work correctly, I blogged about this here:
> http://blogs.sun.com/mandy/category/Lighttpd
>
> Amanda
>
> 2010/5/22 Bruno Guimarães Sousa <brgsousa@gmail.com>
>
> Hi !
>> Did anyone get olio (rails) working with FCGI?
>>
>> Trying to deploy Olio with lighttpd + ruby 1.8.7 + ruby-fcgi and had no
>> success yet.
>> dispatch.fcgi file is correctly placed and has execution permission.
>>
>> I'm getting *500 - Internal Server Error*
>>
>> LOGS:
>> 2010-05-22 11:32:38: (mod_fastcgi.c.1734) connect failed: Connection
>> refused on unix:/tmp/ruby-rails.socket-4
>> 2010-05-22 11:32:38: (mod_fastcgi.c.3037) backend died; we'll disable it
>> for 1 seconds and send the request to another backend instead: reconnects: 0
>> load: 1
>> 2010-05-22 11:32:39: (mod_fastcgi.c.2582) unexpected end-of-file (perhaps
>> the fastcgi process died): pid: 29184 socket: unix:/tmp/ruby-rails.socket-4
>> 2010-05-22 11:32:39: (mod_fastcgi.c.3367) response not received, request
>> sent: 938 on socket: unix:/tmp/ruby-rails.socket-4 for /dispatch.fcgi?,
>> closing connection
>>
>>
>> The same configuration with another rails app, it DID work.
>>
>> --
>> Bruno Guimarães Sousa
>> www.ifba.edu.br
>> PONTONET - DGTI - IFBA
>> Ciência da Computação UFBA
>> Registered Linux user #465914
>>
>
>

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/201005.mbox/%3cAANLkTikkJmNOHl1yAka6qPHPZByR2442igS2I2ff_JtS@mail.gmail.com%3e,Bruno Guimarães Sousa <brgso...@gmail.com>,0,0
254,255,"Welcome!

a) Yes, that's the correct DFDLGeneralFormat.dfdl.xsd. Though I'm
surprised you need that. That file is distributed in the Daffodil jars,
and when Daffodil compiles a schema it should be able to find it inside
a jar. So you shouldn't need to provide a separate copy of that file
when using Daffodil. If that's not the case, let us know--it's likely a bug.

b) This is the most recent version of the CSV schema. The issue you're
seeing is that the encodingErrorPolicy is set to ""error"" in
csv.dfdl.xsd, but Daffodil does not support this value--we only support
encodingErrorPolicy=""replace"".

Normally encodingErrorPolicy=""error"" should be a schema definition error
with a message like:

  Daffodil does not support encodingErrorPolicy=""error"", use ""replace""
instead""

But we found that there are a lot of schemas out there that already use
""error"", particularly those created by IBM. (Note that IBM DFDL supports
""error"" but not ""replace"", so we are incompatible with each other in
that regard).

Fortunately, most of the time this property doesn't actually matter--it
really only has an affect if you are parsing data that has encoding
errors, which is pretty uncommon. So because this property doesn't
usually matter, but we still want to support all these schemas that
already use ""error"", we decided to just ignore this property and always
treat it as if it have a value of ""replace"". That way we can support
schemas that use either ""error"" or ""replace"". But this does mean we are
ignoring a property when it is ""error"", so we output some warnings just
to make it clear that Daffodil might behave differently than the schema
author intended.

In this case, the schema author set it to ""error"" simple so that it
would work in both IBM DFDL and Daffodil.

So this warning can just be ignored. If you want to get rid of the
warnings and you don't care about portability with IBM DFDL, then the
easy solution is to just change encodingErrorPolicy to ""replace"" and the
warnings will go away.


On 12/28/20 3:16 PM, Don Brutzman wrote:
> Am trying to parse CSV example after installing latest version of 3.0.0.
> 
> * Apache Daffodil Examples
>   https://daffodil.apache.org/examples
> 
> * DFDLSchemas / CSV
>   https://github.com/DFDLSchemas/CSV
> 
> * csv.dfdl.xsd
>  
> https://github.com/DFDLSchemas/CSV/blob/master/src/main/resources/com/tresys/csv/xsd/csv.dfdl.xsd
> 
> 
> Had to go searching for
> 
> * <xs:include
> schemaLocation=""org/apache/daffodil/xsd/DFDLGeneralFormat.dfdl.xsd"" />
> 
> and found a copy at
> 
> *
> https://github.com/apache/incubator-daffodil/blob/master/daffodil-lib/src/main/resources/org/apache/daffodil/xsd/DFDLGeneralFormat.dfdl.xsd
> 
> 
> and also looked around for example data, which matches excerpt on
> examples page:
> 
> *
> https://raw.githubusercontent.com/DFDLSchemas/CSV/master/src/test/resources/com/tresys/csv/data/simpleCSV.csv
> 
> 
> Then, using Ant invocation as follows,
> 
>     <target name=""daffodil.parse.csv""  description=""daffodil.apache.org
> example"">
>         <echo>daffodil parse csv</echo>
>         <exec executable=""daffodil""  dir=""."" vmlauncher=""false"">
>             <arg value=""parse""/>
>             <arg value=""--schema""/>
>             <arg value=""examples/csv/csv.dfdl.xsd""/>
>             <arg value=""--output""/>
>             <arg value=""examples/csv/simpleCSV.parse.xml""/>
>             <arg value=""examples/csv/simpleCSV.csv""/>
>         </exec>
>     </target>
> 
> ... was able to produce attached simpleCSV.parse.xml (attached) which in
> turn matches following and result on HTML page,
> 
> *
> https://github.com/DFDLSchemas/CSV/blob/master/src/test/resources/com/tresys/csv/infosets/simpleCSV.xml
> 
> 
> So that's good.  However am also getting a lot of Daffodil schema warnings:
> 
> daffodil.parse.csv:
> daffodil parse csv
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: title Location line 59 column 16 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: item Location line 66 column 16 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: sequence[1] Location line 54 column 8 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: sequence[1] Location line 58 column 14 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: sequence[1] Location line 65 column 14 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: header Location line 55 column 10 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: record Location line 63 column 10 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> [warning] Schema Definition Warning: dfdl:encodingErrorPolicy=""error"" is
> not yet implemented. The 'replace' value will be used.
> Schema context: element reference ex:file Location line 36 in
> RobodataDFDL/examples/csv/csv.dfdl.xsd
> BUILD SUCCESSFUL (total time: 3 seconds)
> 
> Wondering please:
> 
> a. am i using the correct DFDLGeneralFormat.dfdl.xsd file for this example,
> b. is there a more recent version that is suitable for version 3.0.0 ?
> 
> Thanks for all feedback.
> 
> all the best, Don


",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/202012.mbox/%3cb28972e7-9e6e-6496-23ba-2b02d1998db7@apache.org%3e,Steve Lawrence <slawre...@apache.org>,0,0
151,152,"On Thu, Jul 10, 2014 at 1:00 PM, Ravikumar Govindarajan
<ravikumar.govindarajan@gmail.com> wrote:
> Aaron,
>
> This is a lengthy post. Please bear...
>
> We are looking at Blur slightly differently. No Map-Red ops, No immutable
> RowId data etc... Just plain online-search like regular lucene/SOLR/ES
>
> Our use-case mandates that Documents for a RowId will arrive incrementally.
> We don't have the luxury of dropping the whole-row and re-indexing it, as a
> given Row will have hundreds of thousands of docs...
>
> A single row-id will always be found in one shard, but spread across
> segments. We have modified blur sources on both indexing/search side to
> support this requirement
>
> In other words, we support ADD_RECORDS thrift-op to an existing Row..
>
> We actually are now testing a sharding strategy similar to databases in Blur
>
> 1. Initially we start with lets say 300 shards per table aka base-shards
> 2. Each shard has a fixed size lets say 16 GB. Client will watch for this
>     and spawn a new shard when size exceeds. {An alias-shard in ES terms}
> 3. ZK will hold the Base --> List-of-Alias shards
> 4. A RowId will be allocated a shard that has least number of alias shards.
>     This mapping will never change in the lifetime of a Row
> 5. ADD_RECORDS op will go the latest alias, while DEL/UPDATE will go to
>     all aliases+base shards.
> 6. Once all 300 base-shards have spawned aliases, admins can create new
>     base shards on the cluster. Newer RowIds will auto-allocate to freshly
>     created shards
> 7. Both horizontal & vertical scaling of shards can be supported easily by
>     this approach
>
> Now all these are possible only if the RowId -> Base-Shard mapping is
> maintained externally.

Hi Ravi,
Can you explain how searching across a records in a row works in this
case?  For example, the row query example in the docs[1]?

Thanks,
--tim

[1] - http://incubator.apache.org/blur/docs/0.2.2/data-model.html#row_query

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201407.mbox/%3cCAG_bHow3uHxmqfhJU4QyHBoWEcHo3DSGOms=0bG_PDd=9+rL8g@mail.gmail.com%3e,Tim Williams <william...@gmail.com>,1,0
186,187,"Hi Tim,

I tried subcolumn but it does not seems to be supported...Here is my code :

http://pastebin.com/YQYetczs

Query ""data.tag.exact:Pune"";      DO NOT RETURN RESULTS
Query ""data.tag:Pune"";               RETURN RESULTS

Also i checked with schema command on blur shell, it do not shows subcolumn
exact...

Please have a look at my sample code and suggest changes to make it work..

Thanks
Naresh


On Thu, Jan 2, 2014 at 5:04 PM, Tim Williams <williamstw@gmail.com> wrote:

> On Tue, Dec 31, 2013 at 12:24 AM, Naresh Yadav <nyadav.ait@gmail.com>
> wrote:
> > Hi tim,
> >
> > list of tags is not small, can be really big so i cannot use negate the
> > tags approach...Other approach you said is using subfields how to do that
> > in blur...My thought on this was to introduce new column Tags which will
> > store sorted all tags of that row...So for cases where i need exact match
> > then will query on *Tags* column and case where i need partial match of
> > tags then will use Tag column..
>
> Your approach is largely the same as the subcolumn approach.
> Subcolumns would just allow you to do it without storing the original
> value multiple times.  I actually haven't used them, but I reckon it'd
> look something like:
>
> ColumnDefinition tags = new ColumnDefinition(""fam"",""tag"",null, true,
> ""text"",null);
> ColumnDefinition tagsExact = new
> ColumnDefinition(""fam"",""tag"",""exact"",""false,""string"",null);
>
> and querying:
> A) fam.tag:Tag1
>
> B) fam.tag.exact:Tag1
>
> Thanks,
> --tim
>

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201401.mbox/%3cCAKUJFg36e_KU0qs83wVNH9YXgA25VkD+yZ1t5Ac6tQvJF9yTJQ@mail.gmail.com%3e,Naresh Yadav <nyadav....@gmail.com>,0,0
257,258,"Hello,

JobExecutionEntity is afaik not a regal entity because it maps TemporalType
to a java.sql.* (Timestamp). I think java.sql.* was legal at one point but
not anymore.

The javadoc for Java EE 6 and Java EE 7 says this about TemporalType:
 ""Type used to indicate a specific mapping of java.util.Date or
java.util.Calendar.""

cheers / Karl

",http://mail-archives.apache.org/mod_mbox/incubator-sirona-user/201511.mbox/%3cCAPCnv11bpLDxSwxHG+ozUVcBQFx_AuCUL3=TVMf+7K-VcYhu-A@mail.gmail.com%3e,Karl Kildén <karl.kil...@gmail.com>,0,0
279,280,"<html>
<head></head>
<body>
<div>
<div>
<h4>
<img height=""9px"" width=""9px"" src=""https://avatars0.githubusercontent.com/u/7692062?v=4&s=60""
/>
nathantfrank<span style=""color: ##24229F""> (nathantfrank)</span> 
</h4>
<p>
<small><b style=""color: #900C3F"">[2019-01-02T17:08:09.555Z]</b></small>

Could someone provide some guidance on what I might be missing? I am currently trying to copy
a file locally to an Azure DataLakeStore. Here is the pull file I have put together so far.<pre><code>job.name=GobblinCopy
job.group=Copy
job.description=Test Gobblin job <span class=""keyword"">for</span> copy
<span class=""preprocessor""># job.schedule=<span class=""number"">0</span>
<span class=""number"">0</span>/<span class=""number"">2</span> * * *
?</span>

source.<span class=""keyword"">class</span>=org.apache.gobblin.data.management.copy.CopySource
writer.builder.<span class=""keyword"">class</span>=org.apache.gobblin.data.management.copy.writer.FileAwareInputStreamDataWriterBuilder
extract.<span class=""keyword"">namespace</span>=org.apache.gobblin.copy

source.filebased.uri=file:<span class=""comment"">///</span>

<span class=""preprocessor""># data.publisher.type=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher</span>
<span class=""preprocessor""># data.publisher.final.dir=file:<span class=""comment"">///home/ntfrank/Documents/</span></span>

writer.fs.uri=adl:<span class=""comment"">//[ADLSName].azuredatalakestore.net/</span>

gobblin.dataset.pattern=file:<span class=""comment"">///home/ntfrank/copy_data/</span>
<span class=""preprocessor""># converter.classes=org.apache.gobblin.data.management.copy.converter.UnGzipConverter</span>

fs.AbstractFileSystem.adl.impl=<span class=""string"">""org.apache.hadoop.fs.adl.Adl""</span>
dfs.adls.oauth2.access.token.provider.type=ClientCredential
dfs.adls.oauth2.refresh.url=https:<span class=""comment"">//login.microsoftonline.com/[TenantID]/oauth2/v2.0/token</span>
dfs.adls.oauth2.client.id=[ClientID]
writer.encrypted.dfs.adls.oauth2.credential=[ClientSecret]
<span class=""preprocessor""># encrypt.key.loc=/passphrase/file/path</span>

task.maxretries=<span class=""number"">0</span>
workunit.retry.enabled=<span class=""literal"">false</span></code></pre>
</p>
<hr>
</div>
<div>
<h4>
<img height=""9px"" width=""9px"" src=""https://avatars1.githubusercontent.com/u/2355051?v=4&s=60""
/>
Zhixiong Chen<span style=""color: ##24229F""> (zxcware)</span> 
</h4>
<p>
<small><b style=""color: #900C3F"">[2019-01-02T17:51:33.691Z]</b></small>

<p><span data-link-type=""mention"" data-screen-name=""nathantfrank"" class=""mention"">@nathantfrank</span>
There are some missing required configurations:<br><code>gobblin.dataset.pattern=</code><br><code>data.publisher.type=</code><br><code>data.publisher.final.dir=</code></p><p>For
distcp, we have a template available in module <code>gobblin-runtime/src/main/resources/templates</code>.</p>
</p>
<hr>
</div>
<div>
<h4>
<img height=""9px"" width=""9px"" src=""https://avatars1.githubusercontent.com/u/2355051?v=4&s=60""
/>
Zhixiong Chen<span style=""color: ##24229F""> (zxcware)</span> 
</h4>
<p>
<small><b style=""color: #900C3F"">[2019-01-02T17:51:55.511Z]</b></small>

Are you running the job in your local machine?
</p>
<hr>
</div>
<div>
<h4>
<img height=""9px"" width=""9px"" src=""https://avatars0.githubusercontent.com/u/7692062?v=4&s=60""
/>
nathantfrank<span style=""color: ##24229F""> (nathantfrank)</span> 
</h4>
<p>
<small><b style=""color: #900C3F"">[2019-01-02T19:18:22.573Z]</b></small>

<span data-link-type=""mention"" data-screen-name=""zxcware"" class=""mention"">@zxcware</span>
I am running it locally and in standalone mode
</p>
<hr>
</div>
</div>
</body>

",http://mail-archives.apache.org/mod_mbox/incubator-gobblin-user/201901.mbox/%3c963522361.0.1546459164790.JavaMail.abtiwari@abtiwari-mn2.linkedin.biz%3e,apache.gobb...@gmail.com,0,0
121,122,"Not a specific doc, but probably worth a mention in the README?

BTW my own observation is that unless you have a perfectly python prepared
environment, it never is as simple as pip or setup.py install. Usually
there are packages missing that need to be manually installed.

Thanks,
Thomas


On Fri, Apr 7, 2017 at 1:26 PM, Bolke de Bruin <bdbruin@gmail.com> wrote:

> Python setup.py install
>
> Standard python practice. Hardly worth a specific doc?
>
> Sent from my iPhone
>
> > On 7 Apr 2017, at 20:36, Thomas Weise <thw@apache.org> wrote:
> >
> > Hi,
> >
> > I was looking for instructions on how to build and install Airflow from
> > source and did not find anything in the top level README. I think that
> > would be a good place to mention it, so newcomers know what to do?
> >
> > Let me know if that's of interest, I can create a JIRA and open a PR for
> it?
> >
> > Thanks,
> > Thomas
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201704.mbox/<CA+5xAo0R=i+3Tx6sQokq2KPnxHGeHdARyq3fv06y29Kh+3OArA@mail.gmail.com>,Thomas Weise <...@apache.org>,1,0
292,293,"- Do you intend to pull in metadata only in source about the number of
records? It seems like it but you have mentioned that you pull data D
(whole data?) in source. If so what is the work left for workunits?

Yep I intend to pull the meta data in the Source as that would be required
to create the partition information which will be passed to the WorkUnits.
However there are some sources which does not produce the metadata for the
extracted data like twitter search. Take a look at the use case of twitter
data, the search API of the twitter does not give the meta data for the big
search entries.
https://dev.twitter.com/rest/public/search


-What you mean by keeping things in-memory between source / workunits. That
wont be possible for something like Yarn mode.

I have not made it clear for the yarn use cases, I intent to use
Distributed memory ( Apache Ignite) for the same.

I hope I am clear this time.




On Fri, Sep 8, 2017 at 6:05 AM, Abhishek Tiwari <abti@apache.org> wrote:

> I am not super clear with your use-case:
>
> - Do you intend to pull in metadata only in source about the number of
> records? It seems like it but you have mentioned that you pull data D
> (whole data?) in source. If so what is the work left for workunits?
> - What you mean by keeping things in-memory between source / workunits.
> That wont be possible for something like Yarn mode.
>
> Regards,
> Abhishek
>
> On Wed, Sep 6, 2017 at 5:20 AM, Vicky Kak <vicky.kak@gmail.com> wrote:
>
>> Hey Guys,
>>
>> I have checked in sample code demonstrating the pattern as explained
>> above.
>> https://github.com/dallaybatta/gobblin-examples
>>
>> I am soon going to put the documentation about the same, please note that
>> it is just a quick hack to demonstrate the pattern as explained in the
>> email chain.
>>
>>
>> Regards,
>> Vicky
>>
>>
>>
>> On Tue, Sep 5, 2017 at 6:48 PM, Vicky Kak <vicky.kak@gmail.com> wrote:
>>
>>> I am not able to see this email yet in the email archive here
>>> https://lists.apache.org/list.html?user@gobblin.incubator.apache.org
>>>
>>> Can anyone take a note of it and get it working?
>>>
>>> Thanks,
>>> Vicky
>>>
>>>
>>>
>>> On Wed, Aug 30, 2017 at 4:08 PM, Vicky Kak <vicky.kak@gmail.com> wrote:
>>>
>>>> Hi Guys,
>>>>
>>>> We have got a use case where there is no meta data information about
>>>> the data to be processed in Gobblin. We need to read the whole data chunk
>>>> and then create a partition, I would be interested to know how this is
>>>> being addressed by others. Let me explain it with the sample generic data,
>>>> assume that we have got data D with N records in it. We do the following
>>>> 1) In the Source implementation we pull all the data D using rest API.
>>>> We have got the N records in the Source implementation and we are creating
>>>> n(workunit number)*M( records to be processed by each workunit) = N.
>>>> 2) We are passing the starting id to the workunit via the SourceState.
>>>> 3) Each WorkUnit makes an redundant REST call to fetch the sub set of
>>>> D, starting from the id that is passed from Source to it.
>>>>
>>>> So there are 1 REST call in Source and n REST calls to get the data,
>>>> total of n+1 calls are being made although the data can be fetched by a
>>>> single call in the Source.
>>>>
>>>> What I am thinking is to have the data D in the memory ( it should be
>>>> distributed memory for YARN case) and pass the reference of it to the
>>>> WorkUnits for processing, however would like to know how this is being
>>>> addressed by others. This can be one of the patterns of data to be
>>>> processed by the Gobblin.
>>>> May be we can have a document explaining various data patterns, how to
>>>> partition them and use in the Gobblin.
>>>>
>>>> Thanks,
>>>> Vicky
>>>>
>>>>
>>>>
>>>>
>>>
>>
>

",http://mail-archives.apache.org/mod_mbox/incubator-gobblin-user/201709.mbox/%3cCAPaCpY9enNtiG5T8bnBv_5rmZ+HP4gnyGkWWYaa6Ct_gJm-h1g@mail.gmail.com%3e,Vicky Kak <vicky....@gmail.com>,0,1
35,36,"S4 clusters are currently statically partitioned.

We have been looking at how to enable some elasticity by providing means to (efficiently)
repartition the cluster while it's running. 

This is the objective of S4-110 (https://issues.apache.org/jira/browse/S4-110) where we leverage
Apache Helix for that purpose. It's not that simple though and we don't have a working implementation
yet. Any help would be welcome!

Regards,

Matthieu



On Jun 9, 2013, at 08:32 , baojian Zhou wrote:

> when we start a newCluster, we must define the number of Tasks, and i notice that in
TaskSetup.class:
> 
> public void setup(String cluster, int tasks, int initialPort) {
> 		try {
> 			zkclient.createPersistent(""/s4/streams"", true);
> 		} catch (ZkException ignored) {
> 			// ignore if exists
> 		}
> 
> 		zkclient.createPersistent(""/s4/clusters/"" + cluster + ""/tasks"", true);
> 		zkclient.createPersistent(""/s4/clusters/"" + cluster + ""/process"", true);
> 		zkclient.createPersistent(""/s4/clusters/"" + cluster + ""/app"", true);
> 		for (int i = 0; i < tasks; i++) {
> 			String taskId = ""Task-"" + i;
> 			ZNRecord record = new ZNRecord(taskId);
> 			record.putSimpleField(""taskId"", taskId);
> 			record.putSimpleField(""port"", String.valueOf(initialPort + i));
> 			record.putSimpleField(""partition"", String.valueOf(i));
> 			record.putSimpleField(""cluster"", cluster);
> 			zkclient.createPersistent(""/s4/clusters/"" + cluster + ""/tasks/""
> 					+ taskId, record);
> 		}
> 	}
> 
> 
> so my question is:
> if i want to change the nbTasks dynamically and then add the corresponding S4Nodes ,
do i just need to write some codes in the  TaskSetUp class？ or i have to change some other
codes


",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201306.mbox/%3c7E8710F0-A47A-4917-B5C3-9FD9C5E88F68@apache.org%3e,Matthieu Morel <mmo...@apache.org>,0,0
185,186,"
    [ https://issues.apache.org/jira/browse/IOTA-31?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15545028#comment-15545028
] 

Kwang-in (Dennis) JUNG commented on IOTA-31:
--------------------------------------------

[~dalir] Thanks for comment. Hope to see soon.

> Prepare for OSX
> ---------------
>
>                 Key: IOTA-31
>                 URL: https://issues.apache.org/jira/browse/IOTA-31
>             Project: Iota
>          Issue Type: Bug
>            Reporter: Kwang-in (Dennis) JUNG
>            Priority: Minor
>
> Hello~
> Is there some kind of dev guide document? I tried on build script 'iota_standalone_builder.sh'
but it failed. It seems this file is for Linux. Is there some way to run on OSX? Because as
I remember I saw that standalone is also supported on Mac.
> Thanks.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",http://mail-archives.apache.org/mod_mbox/iota-dev/201610.mbox/<JIRA.13009036.1475306732000.729814.1475577561540@Atlassian.JIRA>,"""Kwang-in (Dennis) JUNG (JIRA)"" <j...@apache.org>",0,0
148,149,"I think you just need to fill in the report like one of the examples, and
post it to that wiki.

On Fri, Apr 29, 2016 at 1:36 PM, Siddharth Anand <
siddharthanand@yahoo.com.invalid> wrote:

> Jacob,What's the process?
> -s
>
>     On Friday, April 29, 2016 1:26 PM, Jakob Homan <jghoman@gmail.com>
> wrote:
>
>
>  Here are the examples from last month:
> https://wiki.apache.org/incubator/April2016
>
> Here's where to edit it for May: https://wiki.apache.org/incubator/May2016
>
> Don't feel pressure to fill in a lot; the project is just starting,
> which is a fine response.
>
> -jg
>
>
> On 29 April 2016 at 13:23, Siddharth Anand
> <siddharthanand@yahoo.com.invalid> wrote:
> > I can if someone wants to send me pointers.
> > -s
> >
> >    On Friday, April 29, 2016 1:18 PM, Chris Riccomini <
> criccomini@apache.org> wrote:
> >
> >
> >  So, who wants to write the board report? :)
> >
> >
> >
>
>
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201604.mbox/<CABYbY7cw6ROF5nUS5NgA-b8C7aNGKDTPx68bnsBNk5ONviS5JQ@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
227,228,"I am trying set up OpenWhisk using
https://github.com/apache/incubator-openwhisk-deploy-kube. I ran into a
problem with the invoker. It is stuck in a crash loop with this message:

[ERROR] [#sid_101] [ContainerUtils] stdout:  stderr: /usr/bin/docker: Error
response from daemon: client is newer than server (client API version:
1.24, server API version: 1.22). See '/usr/bin/docker run --help'.
[marker:invoker_docker.run_error:29093:13]

The docker API version on my machine is 1.22. Is there a way I can change
the version of the docker client API that the invoker uses? I have been
looking in the gradle scripts but have not found anything yet.

Thanks

John

",http://mail-archives.apache.org/mod_mbox/openwhisk-dev/201708.mbox/<CAH-hyAfJ3rKWF7fqm_UJDC8mi6b4AWV0kh+k8cenkH8CkGYZ+g@mail.gmail.com>,John Sanda <jsa...@redhat.com>,0,0
103,104,"I think `dag.parent_dag.dag_id` should work, either in a template of
through the context in a PythonOperator.

Max

On Wed, Feb 1, 2017 at 4:23 PM, Daniel Huang <dxhuang@gmail.com> wrote:

> Hey everyone,
> I currently have a parent DAG that generates a filename based on the
> current timestamp. That same filename needs to be used down the line in
> multiple operators, including some subdag operators. So I was hoping to use
> XComs to pass this filename along. It works fine for operators within the
> parent DAG, but not for any operators in the sub DAGs because the dag_id
> arg on xcom_pull() defaults to self.dag_id. I'm pulling this XCom value
> from templates, so I don't want to have to hardcode a parent's dag id in it
> because it should work for different parent dags. The sub dag template
> would still have to make assumptions about what is available in xcom under
> a task id, but I think that's alright?
>
> So my question is if there is a way to programmatically specify the parent
> dag id in an xcom pull from within a template or if there's a better way of
> doing what I'm trying to achieve. Note, I also have sub dags within sub
> dags that require the same filename.
>
> If there is no way to this already, I did come across
> https://issues.apache.org/jira/browse/AIRFLOW-54 and wondered if we need
> something similar for the dag_id arg, like ti.xcom_pull(dag_id='@parent',
> task_ids='foo').
>
> Thanks,
> Daniel
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CAHEEp7WTyoJqvatZoC24-mFKG7KuRjigsWxwu1Z+V9pYva0-Rg@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
272,273,"Agreed, was just going to ask the same.

On Wed, Jun 7, 2017 at 12:34 AM, Bolke de Bruin <bdbruin@gmail.com> wrote:

> Hi Max,
>
> Are you picking this up? I see some open PRs from you that are not too
> active. It would be nice to have a release in 3-4 weeks that also targets
> full compatibility with Apache so we can graduate to top level. Besides
> summer break is getting close and after the summer 1.9.0 is scheduled.
>
> Cheers
> Bolke
>
> > On 18 May 2017, at 20:54, Bolke de Bruin <bdbruin@gmail.com> wrote:
> >
> > https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow <
> https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow>
> >
> > (See higher up in the thread)
> >
> > Please make sure to address some of the outstanding Apache issues (see
> also quote below):
> >
> > 1. Your name is still mentioned somewhere as author. A patch wasn’t
> cherry picked earlier for this
> > 2. Copyrights 2016-2017 Apache, before Airbnb / you
> > 3. License file formatting
> >
> > Otherwise it won’t pass the IPMC.
> >
> >
> > Quote from the IPMC:
> > ====
> >
> > +1, however there's a few issues with the LICENSE file:
> >
> > - Would be good to list out the locations of each file (or path to a
> group
> > of files) (some have this, and others do not so its hard to follow)
> > - There's errant /* .. */ around each license declaration, which should
> be
> > removed.
> > - Missing license bodies for FooTable v2, jQuery Clock Plugin,
> >
> > Likewise, your NOTICE has copyright 2011-2017, however Airflow hasn't
> been
> > incubating that long.  If you like, you can give origination notices to
> the
> > original creators here to specify the original copyright dates.
> >
> > I would challenge the podling to see if there's a way to simplify their
> > LICENSE by instead using npm or some other javascript packaging tool to
> > build a distribution, rather than shipping the dependencies in the source
> > release, makes it much easier to use.
> >
> > As the podling matures, would be good to see information about the author
> > switch from an individual to a community (in setup.cfg, its already in
> > setup.py so may have been a miss)
> >
> > It would be great to see a binary distribution in the next vote to see
> how
> > that may work, its not clear how to build it from this.  Likewise, don't
> > hesitate to clean up your old release artifacts, I downloaded the wrong
> > artifact at first.
> >
> > ====
> >
> > Bolke.
> >
> >
> >
> >> On 18 May 2017, at 20:49, Maxime Beauchemin <maximebeauchemin@gmail.com
> <mailto:maximebeauchemin@gmail.com>> wrote:
> >>
> >> Chris & Bolke, do you have a TODO list / wiki detailing the step-by-step
> >> process?
> >>
> >> Max
> >>
> >> On Thu, May 18, 2017 at 11:46 AM, Maxime Beauchemin <
> >> maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com>> wrote:
> >>
> >>> @Andrewm, we can only assume that the author of each commit in master
> on
> >>> top of 1.8.1 wants their commits into 1.8.2.
> >>>
> >>> -------------------------
> >>>
> >>> Ok cool, I'll take this on then, and I'm asking Arthur to see if he
> wants
> >>> to help / oversee the process.
> >>>
> >>> I'm planning to make 1.8.2 essentially same as 1.8.1 plus the set of
> >>> ""cherries"" that we use at Airbnb in production and every bugfix / minor
> >>> feature that looks benign to us. Given that, we're committing to try
> out RC
> >>> along with everyone else.
> >>>
> >>> What cadence are we aiming at? What should be the target date for the
> RC?
> >>>
> >>> Max
> >>>
> >>> On Thu, May 18, 2017 at 11:29 AM, Bolke de Bruin <bdbruin@gmail.com
> <mailto:bdbruin@gmail.com>>
> >>> wrote:
> >>>
> >>>> Hi Max,
> >>>>
> >>>> Sounds reasonable. For the Release Manager it is really mostly a
> >>>> management job. Chasing, prioritising etc. While it is nice to have
a
> rm
> >>>> also being able to run the RCs themselves I don’t think it is an
> absolute
> >>>> requirement. Especially, as I think we should trust the community to
> test
> >>>> and then vote.
> >>>>
> >>>> As mentioned the 1.8.X release series should focus on bug fixes,
> >>>> performance issue and minor feature updates (UI fixes, fixes to some
> >>>> hooks/operators). 1.9.X is for the larger changes. So indeed please
> keep
> >>>> 1.8.2 simple!
> >>>>
> >>>> Fully understand that business priorities can take precedence. I (and
> I
> >>>> guess Chris as well) were just hoping that also some of the other
> >>>> committers would chime in.
> >>>>
> >>>> Cheers
> >>>> Bolke
> >>>>
> >>>>
> >>>>> On 18 May 2017, at 20:18, Maxime Beauchemin <
> maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com>>
> >>>> wrote:
> >>>>>
> >>>>> Hey,
> >>>>>
> >>>>> Sorry about the delay answering, I wanted to sync up with the Airflow
> >>>> team
> >>>>> here at Airbnb before I replied here.
> >>>>>
> >>>>> Quick note to say that the folks at Airbnb are putting a plan
> together
> >>>> as
> >>>>> to how we can move towards smooth releases with higher confidence
in
> the
> >>>>> future. That plan involves improving the build/test process as well
> as
> >>>> our
> >>>>> staging infrastructure, possibly enabling progressive rollouts
> >>>> internally.
> >>>>>
> >>>>> For context, the team that works on Airflow at Airbnb is ""Data
> Platform""
> >>>>> and is also on the hook for big chunks of non-Airflow-related
> >>>>> infrastructure work that hit us recently and accounts for more than
> the
> >>>>> team's bandwidth at this time. Given that, the team doesn't want
to
> >>>> commit
> >>>>> the time/risk to deploy RCs in production in the short term. Clearly
> >>>>> Airflow is still a priority for the team, but on the short term
we
> have
> >>>>> critical things prioritized above that.
> >>>>>
> >>>>> Part of the solution is for us to hire more engineers, and one of
the
> >>>> open
> >>>>> seats is a dedicated role on Airflow tackling things from feature
> >>>> building
> >>>>> to release management. Hopefully we can widen our bandwidth shortly.
> >>>>>
> >>>>> In the meantime, I can commit the time to handle a release, but
this
> >>>>> release won't hit production at Airbnb for a little while, which
> makes
> >>>> me
> >>>>> wonder whether it's worth committing the time. Maybe there's a
> >>>>> Fedora/RHEL-type scenario here (using a cutting-edge community
> edition
> >>>> to
> >>>>> stabilize LTS releases), but we know it's not ideal for Airbnb and
> for
> >>>> the
> >>>>> community. The end goal is clearly to have steady, high-confidence,
> >>>> mostly
> >>>>> automated, regular releases and it feels like time is best spent
> >>>> working in
> >>>>> that direction.
> >>>>>
> >>>>> Another option is to make [upcoming] 1.8.2 very simple, as 1.8.1
+
> the
> >>>> few
> >>>>> cherries we run in production already at Airbnb, holding the 50+
> extra
> >>>>> commits in master for 1.8.3. This is marginally useful but helps
> getting
> >>>>> the release mechanics oiled up.
> >>>>>
> >>>>> I'm trying to be as transparent as I can here, and open to discuss
> the
> >>>>> different ways we can move forward.
> >>>>>
> >>>>> Max
> >>>>>
> >>>>> On Sun, May 14, 2017 at 4:44 AM, Bolke de Bruin <bdbruin@gmail.com
> <mailto:bdbruin@gmail.com>>
> >>>> wrote:
> >>>>>
> >>>>>> Hi Folks,
> >>>>>>
> >>>>>> With 1.8.1 we have very much improved the reliability airflow,
> which is
> >>>>>> great as many new features entered 1.8.0 and the gap from 1.7.1
was
> >>>> huge.
> >>>>>> What is also great is that we are slowly but surely increasing
the
> test
> >>>>>> coverage which mitigates some of the risk of regressions going
> >>>> forward. As
> >>>>>> you know the 1.8.X releases will continue to focus on improved
> >>>> reliability,
> >>>>>> performance improvements and minor feature updates. The 1.9.X
> release
> >>>>>> cycle, which should start around September, will allow for larger
> >>>> feature
> >>>>>> updates.
> >>>>>>
> >>>>>> I expect 1.8.2 not to have too many PRs, so it will be a relatively
> >>>> simple
> >>>>>> release process:
> >>>>>>
> >>>>>> 1. Apply bug fixes
> >>>>>> 2. Add performance fixes
> >>>>>> 3. Fix some outstanding Apache requirements (Author, Licensing
etc)
> >>>>>>
> >>>>>> The process of creating a distribution has been detailed by
Chris
> here:
> >>>>>> https://cwiki.apache.org/confluence/display/AIRFLOW/
> Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/
> Releasing+Airflow>
> >>>> <
> >>>>>> https://cwiki.apache.org/confluence/display/AIRFLOW/
> Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/
> Releasing+Airflow>>
> >>>>>>
> >>>>>> Now we just need a volunteer (preferably from the committers)
to be
> the
> >>>>>> Release Manager for 1.8.2 :-).
> >>>>>>
> >>>>>> Who is willing to take this on and make history?
> >>>>>>
> >>>>>> Regards,
> >>>>>> Bolke
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>
> >>>>
> >>>
> >
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201706.mbox/<CABYbY7cTJRACLG61Ex8uvmkruT8bxUzzK9WfVGA0ks7UJbVi2w@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
238,239,"Ashok,

I added your GC to startup and removed System.gc();
The 1,000 loop test still crashed.

One thing I did do was switch to using a FileInputStream which seemed to 
help a bit (file handles actually close), but eventually still crashed 
anyway.

  FileInputStream fis = new FileInputStream(""/home2/templates/"" + 
templateName);
 textDoc = (TextDocument)TextDocument.loadDocument(fis);
 fis.close();



Kevin



From:
Ashok Hariharan <ashok@parliaments.info>
To:
odf-users@incubator.apache.org
Date:
11/15/2011 02:11 PM
Subject:
Re: TextDocument.loadDocument() locking up in an x86_Linux_64 environment.



On Tue, Nov 15, 2011 at 10:10 PM, Kevin Skelton <Kevin_Skelton@mgic.com> 
wrote:
> The program is loading an .odt doc from the linux file system in order 
to
> match/merge it with a data file.
> I am not writing or saving the document back to the file system, I am
> sending the newly created doc over the wire.
> This is a single threaded application.  (MQSeries triggered).
> No exceptions at all.  I have the load in a try/catch loop but never see
> an exception.
>

Try removing the System.gc()

and starting the test app with -XX:-UseParallelGC

do you get the same problem ?



",http://mail-archives.apache.org/mod_mbox/incubator-odf-users/201111.mbox/%3cOF0E163AC8.4A74479A-ON86257949.0070D22A-86257949.007795A0@mgic.com%3e,Kevin Skelton <Kevin_Skel...@mgic.com>,0,0
15,15,"See <https://builds.apache.org/job/provisionr-master/171/>  ------------------------------------------ [...truncated 3976 lines...] [INFO]  [INFO] --- apache-rat-plugin:0.9:check (default) @ provisionr-assembly-tests --- [INFO] 51 implicit excludes (use -debug for more details). [INFO] Exclude: **/*.md [INFO] Exclude: NOTICE [INFO] Exclude: .git/** [INFO] Exclude: .repository/** [INFO] Exclude: .gitignore [INFO] Exclude: .idea/** [INFO] Exclude: **/*.iml [INFO] Exclude: **/*.project [INFO] Exclude: **/*.classpath [INFO] Exclude: **/*.prefs [INFO] Exclude: **/id_rsa_test [INFO] Exclude: **/id_rsa_test.pub [INFO] Exclude: **/*.bpmn20.xml [INFO] Exclude: **/*.activiti [INFO] Exclude: **/*.csv [INFO] Exclude: **/target/** [INFO] 3 resources included (use -debug for more details) [INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 3 licence. [INFO]                                                                          [INFO] ------------------------------------------------------------------------ [INFO] Building Apache Provisionr :: Aggregator 0.5.0-incubating-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO]  [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ provisionr-aggregator --- [INFO]  [INFO] --- maven-remote-resources-plugin:1.4:process (default) @ provisionr-aggregator --- [INFO]  [INFO] --- maven-site-plugin:3.2:attach-descriptor (attach-descriptor) @ provisionr-aggregator --- [WARNING] Failed to getClass for org.apache.maven.plugin.source.SourceJarMojo [INFO]  [INFO] --- maven-source-plugin:2.2.1:jar (default) @ provisionr-aggregator --- [INFO]  [INFO] --- apache-rat-plugin:0.9:check (default) @ provisionr-aggregator --- [INFO] 70 implicit excludes (use -debug for more details). [INFO] Exclude: **/*.md [INFO] Exclude: NOTICE [INFO] Exclude: .git/** [INFO] Exclude: .repository/** [INFO] Exclude: .gitignore [INFO] Exclude: .idea/** [INFO] Exclude: **/*.iml [INFO] Exclude: **/*.project [INFO] Exclude: **/*.classpath [INFO] Exclude: **/*.prefs [INFO] Exclude: **/id_rsa_test [INFO] Exclude: **/id_rsa_test.pub [INFO] Exclude: **/*.bpmn20.xml [INFO] Exclude: **/*.activiti [INFO] Exclude: **/*.csv [INFO] Exclude: **/target/** [INFO] 7 resources included (use -debug for more details) [INFO] Rat check: Summary of files. Unapproved: 0 unknown: 0 generated: 0 approved: 5 licence. [INFO]  [INFO] --- maven-install-plugin:2.3.1:install (default-install) @ provisionr-aggregator --- [INFO] Installing <https://builds.apache.org/job/provisionr-master/ws/pom.xml> to /home/jenkins/jenkins-slave/maven-repositories/0/org/apache/provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/provisionr-aggregator-0.5.0-incubating-SNAPSHOT.pom [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO]  [INFO] Apache Provisionr :: Parent ....................... SUCCESS [9.370s] [INFO] Apache Provisionr :: API .......................... SUCCESS [20.712s] [INFO] Apache Provisionr :: Activiti Database ............ SUCCESS [9.125s] [INFO] Apache Provisionr :: Test Support ................. SUCCESS [23.072s] [INFO] Apache Provisionr :: Core ......................... SUCCESS [15.311s] [INFO] Apache Provisionr :: Examples ..................... SUCCESS [11.394s] [INFO] Apache Provisionr :: Web Console & REST API ....... SUCCESS [12.023s] [INFO] Apache Provisionr :: Amazon ....................... SUCCESS [13.278s] [INFO] Apache Provisionr :: CloudStack ................... SUCCESS [23.845s] [INFO] Apache Provisionr :: Rundeck ...................... SUCCESS [15.165s] [INFO] Apache Provisionr :: Activiti Karaf :: Commands ... SUCCESS [22.943s] [INFO] Apache Provisionr :: Activiti Karaf :: Explorer ... SUCCESS [21.540s] [INFO] Apache Provisionr :: Karaf Commands ............... SUCCESS [14.126s] [INFO] Apache Provisionr :: Karaf Branding ............... SUCCESS [7.379s] [INFO] Apache Provisionr :: Karaf Features ............... SUCCESS [13.470s] [INFO] Apache Provisionr :: Amazon Integration Tests ..... SUCCESS [21.330s] [INFO] Apache Provisionr :: CloudStack Integration Tests . SUCCESS [10.464s] [INFO] Apache Provisionr :: Custom Karaf Distribution .... SUCCESS [29.619s] [INFO] Apache Provisionr :: Custom Karaf Distribution Tests  SUCCESS [56.973s] [INFO] Apache Provisionr :: Aggregator ................... SUCCESS [5.189s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 9:30.749s [INFO] Finished at: Fri Nov 08 03:28:03 UTC 2013 [INFO] Final Memory: 55M/294M [INFO] ------------------------------------------------------------------------ Waiting for Jenkins to finish collecting data [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/pom.xml> to org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/target/provisionr-test-support-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/target/provisionr-test-support-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/test-support/target/features.xml> to org.apache.provisionr/provisionr-test-support/0.5.0-incubating-SNAPSHOT/provisionr-test-support-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/explorer/pom.xml> to org.apache.provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/explorer/target/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.war> to org.apache.provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT.war [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/explorer/target/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/activiti-karaf-web-explorer/0.5.0-incubating-SNAPSHOT/activiti-karaf-web-explorer-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/pom.xml> to org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/classes/features.xml> to org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack/target/classes/org.apache.provisionr.cloudstack.cfg> to org.apache.provisionr/provisionr-cloudstack/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/pom.xml> to org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/target/provisionr-features-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/target/provisionr-features-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/features/target/features.xml> to org.apache.provisionr/provisionr-features/0.5.0-incubating-SNAPSHOT/provisionr-features-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack-tests/pom.xml> to org.apache.provisionr/provisionr-cloudstack-tests/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack-tests/target/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-cloudstack-tests/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/cloudstack-tests/target/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-cloudstack-tests/0.5.0-incubating-SNAPSHOT/provisionr-cloudstack-tests-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/pom.xml> to org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/provisionr-console-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/provisionr-console-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/classes/features.xml> to org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/console/target/classes/org.apache.provisionr.console.cfg> to org.apache.provisionr/provisionr-console/0.5.0-incubating-SNAPSHOT/provisionr-console-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/branding/pom.xml> to org.apache.provisionr/provisionr-branding/0.5.0-incubating-SNAPSHOT/provisionr-branding-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/branding/target/provisionr-branding-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-branding/0.5.0-incubating-SNAPSHOT/provisionr-branding-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/branding/target/provisionr-branding-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-branding/0.5.0-incubating-SNAPSHOT/provisionr-branding-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/pom.xml> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/provisionr-examples-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/provisionr-examples-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/features.xml> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/org/apache/provisionr/examples/templates/cdh3.xml> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-cdh3.template [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/org/apache/provisionr/examples/templates/cdh4.xml> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-cdh4.template [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/examples/target/classes/org/apache/provisionr/examples/templates/jenkins.xml> to org.apache.provisionr/provisionr-examples/0.5.0-incubating-SNAPSHOT/provisionr-examples-0.5.0-incubating-SNAPSHOT-jenkins.template [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon-tests/pom.xml> to org.apache.provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon-tests/target/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon-tests/target/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-amazon-tests/0.5.0-incubating-SNAPSHOT/provisionr-amazon-tests-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/pom.xml> to org.apache.provisionr/provisionr-assembly-tests/0.5.0-incubating-SNAPSHOT/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/target/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-assembly-tests/0.5.0-incubating-SNAPSHOT/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly-tests/target/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-assembly-tests/0.5.0-incubating-SNAPSHOT/provisionr-assembly-tests-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/pom.xml> to org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/provisionr-core-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/provisionr-core-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/classes/features.xml> to org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-ma
ter/ws/core/target/classes/org.apache.provisionr.cfg> to org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/core/target/classes/org.apache.felix.fileinstall-templates.cfg> to org.apache.provisionr/provisionr-core/0.5.0-incubating-SNAPSHOT/provisionr-core-0.5.0-incubating-SNAPSHOT-fileinstall.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/pom.xml> to org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/provisionr-amazon-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/provisionr-amazon-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/classes/features.xml> to org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/providers/amazon/target/classes/org.apache.provisionr.amazon.cfg> to org.apache.provisionr/provisionr-amazon/0.5.0-incubating-SNAPSHOT/provisionr-amazon-0.5.0-incubating-SNAPSHOT-defaults.cfg [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/pom.xml> to org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/target/provisionr-rundeck-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/target/provisionr-rundeck-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/integration/rundeck/target/classes/features.xml> to org.apache.provisionr/provisionr-rundeck/0.5.0-incubating-SNAPSHOT/provisionr-rundeck-0.5.0-incubating-SNAPSHOT-features.xml [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/pom.xml> to org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/target/provisionr-assembly-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/target/provisionr-assembly-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/assembly/target/provisionr-0.5.0-incubating-SNAPSHOT.tar.gz> to org.apache.provisionr/provisionr-assembly/0.5.0-incubating-SNAPSHOT/provisionr-assembly-0.5.0-incubating-SNAPSHOT.tar.gz [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/pom.xml> to org.apache.provisionr/provisionr-aggregator/0.5.0-incubating-SNAPSHOT/provisionr-aggregator-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/parent/pom.xml> to org.apache.provisionr/provisionr-parent/0.5.0-incubating-SNAPSHOT/provisionr-parent-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/database/pom.xml> to org.apache.provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/database/target/activiti-database-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/database/target/activiti-database-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/api/pom.xml> to org.apache.provisionr/provisionr-api/0.5.0-incubating-SNAPSHOT/provisionr-api-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/api/target/provisionr-api-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-api/0.5.0-incubating-SNAPSHOT/provisionr-api-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/api/target/provisionr-api-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-api/0.5.0-incubating-SNAPSHOT/provisionr-api-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/commands/pom.xml> to org.apache.provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/commands/target/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/activiti/commands/target/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-SNAPSHOT-sources.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/commands/pom.xml> to org.apache.provisionr/provisionr-commands/0.5.0-incubating-SNAPSHOT/provisionr-commands-0.5.0-incubating-SNAPSHOT.pom [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/commands/target/provisionr-commands-0.5.0-incubating-SNAPSHOT.jar> to org.apache.provisionr/provisionr-commands/0.5.0-incubating-SNAPSHOT/provisionr-commands-0.5.0-incubating-SNAPSHOT.jar [JENKINS] Archiving <https://builds.apache.org/job/provisionr-master/ws/karaf/commands/target/provisionr-commands-0.5.0-incubating-SNAPSHOT-sources.jar> to org.apache.provisionr/provisionr-commands/0.5.0-incubating-SNAPSHOT/provisionr-commands-0.5.0-incubating-SNAPSHOT-sources.jar channel stopped Archiving artifacts Maven RedeployPublisher use remote ubuntu3 maven settings from : /home/jenkins/.m2/settings.xml [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact jenkins-5819745017457693397activiti-database-0.5.0-incubating-SNAPSHOT.jar Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 0.0 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20131108.032921-110.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20131108.032921-110.jar (539 KB at 2977.0 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20131108.032921-110.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20131108.032921-110.pom (3 KB at 32.5 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml (309 B at 0.2 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 9.7 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/maven-metadata.xml (309 B at 2.8 KB/sec) Deploying the main artifact jenkins-4349088143287386076activiti-database-0.5.0-incubating-SNAPSHOT-sources.jar Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20131108.032921-110-sources.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/activiti-database-0.5.0-incubating-20131108.032921-110-sources.jar (6 KB at 0.3 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-database/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 18.1 KB/sec) [INFO] Deployment in https://repository.apache.org/content/repositories/snapshots (id=apache.snapshots.https,uniqueVersion=true) Deploying the main artifact jenkins-1740025011370866412activiti-karaf-commands-0.5.0-incubating-SNAPSHOT.jar Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml (2 KB at 0.0 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20131108.033021-110.jar Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20131108.033021-110.jar (48 KB at 685.6 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20131108.033021-110.pom Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/activiti-karaf-commands-0.5.0-incubating-20131108.033021-110.pom (7 KB at 91.9 KB/sec) Downloading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml Downloaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml (315 B at 0.1 KB/sec) Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/0.5.0-incubating-SNAPSHOT/maven-metadata.xml Uploading: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml Uploaded: https://repository.apache.org/content/repositories/snapshots/org/apache/provisionr/activiti-karaf-commands/maven-metadata.xml (315 B at 2.3 KB/sec) ERROR: Failed to deploy metadata: Could not transfer metadata org.apache.provisionr:activiti-karaf-commands:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond org.apache.maven.artifact.deployer.ArtifactDeploymentException: Failed to deploy metadata: Could not transfer metadata org.apache.provisionr:activiti-karaf-commands:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond 	at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:143) 	at hudson.maven.reporters.MavenArtifactRecord.deploy(MavenArtifactRecord.java:191) 	at hudson.maven.RedeployPublisher.perform(RedeployPublisher.java:176) 	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:781) 	at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:753) 	at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.post2(MavenModuleSetBuild.java:1020) 	at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:706) 	at hudson.model.Run.execute(Run.java:1690) 	at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:509) 	at hudson.model.ResourceController.execute(ResourceController.java:88) 	at hudson.model.Executor.run(Executor.java:230) Caused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy metadata: Could not transfer metadata org.apache.provisionr:activiti-karaf-commands:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond 	at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:372) 	at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:269) 	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy(DefaultRepositorySystem.java:413) 	at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:139) 	... 11 more Caused by: org.eclipse.aether.transfer.MetadataTransferException: Could not transfer metadata org.apache.provisionr:activiti-karaf-commands:0.5.0-incubating-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): The target server failed to respond 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$5.wrap(WagonRepositoryConnector.java:995) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$5.wrap(WagonRepositoryConnector.java:983) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$PutTask.run(WagonRepositoryConnector.java:895) 	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector.put(WagonRepositoryConnector.java:530) 	at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:366) 	... 14 more Caused by: org.apache.maven.wagon.TransferFailedException: The target server failed to respond 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:580) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:524) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:505) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:485) 	at org.ec
ipse.aether.connector.wagon.WagonRepositoryConnector$PutTask.run(WagonRepositoryConnector.java:871) 	... 16 more Caused by: org.apache.http.NoHttpResponseException: The target server failed to respond 	at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:95) 	at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:62) 	at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:254) 	at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:289) 	at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:252) 	at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:191) 	at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:300) 	at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:127) 	at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:715) 	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:520) 	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906) 	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:746) 	at org.apache.maven.wagon.shared.http4.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:574) 	... 20 more [INFO] Deployment failed after 1 min 51 sec Build step 'Deploy artifacts to Maven repository' changed build result to FAILURE ",https://mail-archives.apache.org/mod_mbox/provisionr-dev/201311.mbox/raw/%3C1799734173.49.1383881445338.JavaMail.hudson%40aegis%3E,Apache Jenkins Server  <jenk...@builds.apache.org>,0,0
280,281,"Hi Dmitry,

Please provide more information, such as logs and the DAG definition itself. This is very
little to go on unfortunately.

Bolke

> On 3 May 2017, at 10:22, Dmitry Smirnov <dmi.smirnov07@gmail.com> wrote:
> 
> Hi everyone,
> 
> I'm using Airflow version 1.8.0, just upgraded from 1.7.1.3. The issue that
> I'm going to describe started already in 1.7.1.3, I upgraded hoping it
> might help resolve it.
> 
> I have several DAGs for which the *last* task is not moving from queued to
> running.
> These DAGs used to run fine some time ago, but then we had issues with
> rabbitmq cluster we use, and after resetting it up, the problem emerged.
> I'm pretty sure the queue is working fine, since all the tasks except the
> very last one are queued automatically and run fine.
> For the sake of testing, I added a copy of the last task to the DAG, and
> interestingly, the task that used to be the last and did not run, now
> started to run normally, but the new last task is stuck.
> I checked logs at the DEBUG level and I could see that scheduler queues the
> tasks, but those tasks don't show up in the Celery/Flower dashboard in the
> corresponding queue.
> When I run the task that is stuck from the webserver interface, they show
> up in the queue in Flower dashboard and run successfully.
> So, overall, it seems that the issue is present with the scheduler but not
> with webserver, and that this issue is only related to the very last task
> in the DAG.
> I'm really stuck now, I would welcome any suggestions / ideas on what can
> be done.
> 
> Thank you in advance!
> BR, Dima
> 
> -- 
> 
> Dmitry Smirnov (MSc.)
> Data Engineer @ Yousician
> mobile: +358 50 3015072


",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<8EAD686C-54A2-4370-AC46-D955EBC2F352@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,0
82,83,"Hi,
I already used ""hot deployment"" of CSS files with eclipse / JBoss (JBoss uses Tomcat as web
server, but don't know if there are differences when it comes to hot deployment)
The only think I needed to do is to disable the browser-cache completely (possible on latest
opera versions or on firefox with WebDeveloper plugin [1])

Greetings
Markus Döring

[1] http://chrispederick.com/work/webdeveloper/


> -----Ursprüngliche Nachricht-----
> Von: Felix Gonschorek [mailto:felix@gg-media.biz]
> Gesendet: Montag, 19. März 2007 13:44
> An: adffaces-user@incubator.apache.org
> Betreff: Skinning / Detecting Style-Sheet-Changes
> 
> Hello all,
> 
> i am trying to implement my own style for trinidad components. but for
> every change i make to the css, i have to stop tomcat, clear the cache
> folder and then start tomcat again. if i try to delete the cached css,
> the file is locked and can't be deleted. Changes to the css are not
> recognized immediately. i have played around with some web.xml
> config-settings, but i haven't had success yet.
> this way it takes a lot of time to create a whole new style, is'nt there
> a possibility to see immediately results after changes to the css an a
> page reload?
> 
> does sombody have a hint for me? i would greatly appreciate some help.
> 
> thanks in advance
> felix gonschorek

",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200703.mbox/%3c8F9BA684796C2848A3DD79037C5F75BF19DA85@MOES0037.tcc.local%3e,Döring Markus <Markus.Doer...@tcc-products.de>,0,0
50,51,"Update with changes from https://github.com/jclouds/jclouds-cli/pull/14, depending on how the
discussion about those goes..?

---
Reply to this email directly or view it on GitHub:
https://github.com/jclouds/jclouds-cli/pull/12#issuecomment-21766903

",http://mail-archives.apache.org/mod_mbox/jclouds-dev/201307.mbox/<jclouds/jclouds-cli/pull/12/c21766903@github.com>,Andrew Phillips <notificati...@github.com>,0,0
79,80,"Simple workaround: downgrade to beta 1 first. 

Sent from my iPhone

> On 26 Jan 2017, at 07:34, Bolke de Bruin <bdbruin@gmail.com> wrote:
> 
> Mmm that is due to the reverting of one changes to the db. Need to look into that how
to fix it. 
> 
> Sent from my iPhone
> 
>> On 26 Jan 2017, at 00:51, Alex Van Boxel <alex@vanboxel.be> wrote:
>> 
>> I do seem to have a problem upgrading the MySQL database with the last
>> commit:
>> 
>> 
>> 2017-01-25T23:41:55.662572654Z
>> /usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/messaging.py:69:
>> UserWarning: Revision 1a5a9e6bf2b5 referenced from 1a5a9e6bf2b5 ->
>> 127d2bf2dfa7 (head), Add dag_id/state index on dag_run table is not present
>> 2017-01-25T23:41:55.662613670Z   warnings.warn(msg)
>> 2017-01-25T23:41:55.664560884Z Traceback (most recent call last):
>> 2017-01-25T23:41:55.664582238Z   File ""/usr/local/bin/airflow"", line 4, in
>> <module>
>> 2017-01-25T23:41:55.664758457Z
>> __import__('pkg_resources').run_script('airflow==1.8.0b1+apache.incubating',
>> 'airflow')
>> 2017-01-25T23:41:55.664776553Z   File
>> ""/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line
>> 739, in run_script
>> 2017-01-25T23:41:55.664937644Z
>> self.require(requires)[0].run_script(script_name, ns)
>> 2017-01-25T23:41:55.664952001Z   File
>> ""/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line
>> 1494, in run_script
>> 2017-01-25T23:41:55.665072840Z     exec(code, namespace, namespace)
>> 2017-01-25T23:41:55.665086884Z   File
>> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/EGG-INFO/scripts/airflow"",
>> line 28, in <module>
>> 2017-01-25T23:41:55.665211755Z     args.func(args)
>> 2017-01-25T23:41:55.665225157Z   File
>> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/bin/cli.py"",
>> line 931, in upgradedb
>> 2017-01-25T23:41:55.665326691Z     db_utils.upgradedb()
>> 2017-01-25T23:41:55.665346979Z   File
>> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/utils/db.py"",
>> line 292, in upgradedb
>> 2017-01-25T23:41:55.665449297Z     command.upgrade(config, 'heads')
>> 2017-01-25T23:41:55.665462333Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/command.py"",
>> line 174, in upgrade
>> 2017-01-25T23:41:55.665521466Z     script.run_env()
>> 2017-01-25T23:41:55.665527232Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/base.py"",
>> line 416, in run_env2017-01-25T23:41:55.665638879Z
>> util.load_python_file(self.dir, 'env.py')
>> 2017-01-25T23:41:55.665656437Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/pyfiles.py"",
>> line 93, in load_python_file
>> 2017-01-25T23:41:55.665682760Z     module = load_module_py(module_id, path)
>> 2017-01-25T23:41:55.665699257Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/compat.py"",
>> line 79, in load_module_py
>> 2017-01-25T23:41:55.665725036Z     mod = imp.load_source(module_id, path,
>> fp)
>> 2017-01-25T23:41:55.665751433Z   File
>> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/migrations/env.py"",
>> line 88, in <module>
>> 2017-01-25T23:41:55.665778219Z     run_migrations_online()
>> 2017-01-25T23:41:55.665787820Z   File
>> ""/usr/local/lib/python2.7/site-packages/airflow-1.8.0b1+apache.incubating-py2.7.egg/airflow/migrations/env.py"",
>> line 83, in run_migrations_online
>> 2017-01-25T23:41:55.665811353Z     context.run_migrations()
>> 2017-01-25T23:41:55.665818933Z   File ""<string>"", line 8, in run_migrations
>> 2017-01-25T23:41:55.666707547Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/runtime/environment.py"",
>> line 797, in run_migrations
>> 2017-01-25T23:41:55.666839945Z     self.get_context().run_migrations(**kw)
>> 2017-01-25T23:41:55.666859336Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/runtime/migration.py"",
>> line 305, in run_migrations
>> 2017-01-25T23:41:55.666970791Z     for step in self._migrations_fn(heads,
>> self):
>> 2017-01-25T23:41:55.666992265Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/command.py"",
>> line 163, in upgrade
>> 2017-01-25T23:41:55.667021804Z     return script._upgrade_revs(revision,
>> rev)
>> 2017-01-25T23:41:55.667032496Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/base.py"",
>> line 329, in _upgrade_revs
>> 2017-01-25T23:41:55.667144295Z     revs = list(revs)
>> 2017-01-25T23:41:55.667157407Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
>> line 641, in _iterate_revisions
>> 2017-01-25T23:41:55.667302027Z     requested_lowers =
>> self.get_revisions(lower)
>> 2017-01-25T23:41:55.667315913Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
>> line 298, in get_revisions
>> 2017-01-25T23:41:55.667390590Z     return sum([self.get_revisions(id_elem)
>> for id_elem in id_], ())
>> 2017-01-25T23:41:55.667411867Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
>> line 300, in get_revisions
>> 2017-01-25T23:41:55.667419226Z     resolved_id, branch_label =
>> self._resolve_revision_number(id_)
>> 2017-01-25T23:41:55.667434110Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
>> line 433, in _resolve_revision_number
>> 2017-01-25T23:41:55.667576273Z     self._revision_map
>> 2017-01-25T23:41:55.667590641Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/util/langhelpers.py"",
>> line 240, in __get__
>> 2017-01-25T23:41:55.667648561Z     obj.__dict__[self.__name__] = result =
>> self.fget(obj)
>> 2017-01-25T23:41:55.667658598Z   File
>> ""/usr/local/lib/python2.7/site-packages/alembic-0.8.9-py2.7.egg/alembic/script/revision.py"",
>> line 151, in _revision_map
>> 2017-01-25T23:41:55.667680311Z     down_revision = map_[downrev]
>> 2017-01-25T23:41:55.667746364Z KeyError: '1a5a9e6bf2b5'
>> 
>> 
>> 
>> 
>> 
>> 
>>> On Wed, Jan 25, 2017 at 11:15 PM Bolke de Bruin <bdbruin@gmail.com> wrote:
>>> 
>>> Hi All,
>>> 
>>> I have made the THIRD beta of Airflow 1.8.0 available at:
>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/ <
>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/> , public keys
>>> are available at
>>> https://dist.apache.org/repos/dist/release/incubator/airflow/ <
>>> https://dist.apache.org/repos/dist/release/incubator/airflow/> . It is
>>> tagged with a local version “apache.incubating” so it allows upgrading from
>>> earlier releases. This beta is available for testing in a more production
>>> like setting (acceptance environment?).
>>> 
>>> I would like to encourage everyone  to try it out, to report back any
>>> issues so we get to a rock solid release of 1.8.0. When reporting issues a
>>> test case or even a fix is highly appreciated.
>>> 
>>> Issues cleared:
>>> 
>>> * Manual trigger not working
>>> * Performance issues with MySQL
>>> * Postgres auto-commit support left over to the driver
>>> * Poison pill taken while task has exited
>>> * Keep cgroups optional
>>> * Funcsigs pinned to 1.0.0
>>> 
>>> Issue(s) remaining (blocker for RC):
>>> * Cgroups not py3 compatible
>>> 
>>> If all goes well we should have a Release Candidate on Feb 2. Thanks for
>>> reporting issues and keep on testing please :). Moving towards RC I tend to
>>> like small bug fixes only. When we mark RC (do we need to vote on this?)
>>> the procedure becomes even more strict. Please remember that the FINAL
>>> release is dependent on a vote on the IPMC mailinglist.
>>> 
>>> Cheers
>>> Bolke
>> 
>> -- 
>> _/
>> _/ Alex Van Boxel

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<BA5E77FF-5232-4058-9208-4269121435B3@gmail.com>,Bolke de Bruin <bdbr...@gmail.com>,0,1
237,238,"Hi David,

> But I want to use supervisor. And below is the terrible error I get

We've been using supervisord successfully to start and manage Airflow
processes for a while now. Below is our /etc/supervisord.conf file.

The only quirk with our setup is that we use Python 3.5 to run Airflow, but
Supervisor is only officially supported on Python 2 so we use pyenv to
install 2.7.11 just to run Supervisor. We modify /usr/local/bin/supervisord
and supervisorctl files to start with:

#!/root/.pyenv/versions/2.7.11/bin/python


Hope this helps,
-Joe


; Configuration for Airflow webserver and scheduler in Supervisor

[program:webserver]
command=/usr/local/bin/airflow webserver
stopsignal=QUIT
stopasgroup=true
user=root
stdout_logfile=/var/log/airflow/webserver-stdout.log
stderr_logfile=/var/log/airflow/webserver-stderr.log
environment=HOME=""/root"",AIRFLOW_HOME=""/usr/local/airflow"",TMPDIR=""/tmp/airflow""
pidfile = /usr/local/airflow/airflow-webserver.pid

[program:scheduler]
command=/usr/local/bin/airflow scheduler -n 5
stopsignal=QUIT
stopasgroup=true
killasgroup=true
user=root
stdout_logfile=/var/log/airflow/scheduler-stdout.log
stderr_logfile=/var/log/airflow/scheduler-stderr.log
environment=HOME=""/root"",AIRFLOW_HOME=""/usr/local/airflow"",TMPDIR=""/tmp/airflow""
autorestart=true

[program:worker]
command=/usr/local/bin/airflow worker
stopsignal=QUIT
stopasgroup=true
killasgroup=true
user=root
stdout_logfile=/var/log/airflow/worker-stdout.log
stderr_logfile=/var/log/airflow/worker-stderr.log
environment=HOME=""/root"",AIRFLOW_HOME=""/usr/local/airflow"",TMPDIR=""/tmp/airflow""
autorestart=true

[program:flower]
command=/usr/local/bin/celery flower --broker=redis://localhost:6379/0
--basic_auth=airflow:%(ENV_FLOWER_PASSWORD)s %(ENV_FLOWER_CERTFILE_OPTION)s
%(ENV_FLOWER_KEYFILE_OPTION)s
stopsignal=QUIT
stopasgroup=true
killasgroup=true
user=root
stdout_logfile=/var/log/airflow/flower-stdout.log
stderr_logfile=/var/log/airflow/flower-stderr.log
environment=HOME=""/root"",AIRFLOW_HOME=""/usr/local/airflow"",TMPDIR=""/tmp/airflow""
autorestart=true

[supervisord]
logfile = /tmp/supervisord.log
logfile_maxbytes = 50MB
logfile_backups=10
loglevel = info
pidfile = /tmp/supervisord.pid

[unix_http_server]
file=/tmp/supervisor.sock

[supervisorctl]
serverurl = unix:///tmp/supervisor.sock
prompt = airflow

[rpcinterface:supervisor]
supervisor.rpcinterface_factory =
supervisor.rpcinterface:make_main_rpcinterface

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<CAK+2U_2BqDEfyvf2xa=RaGuDA6Fusmqx+HyyHX0DxE9ti=K5Xw@mail.gmail.com>,Joe Schmid <jsch...@symphonyrm.com>,0,0
273,274,"Thanks Andrew and Max, I explored the docs and various code snippets shared
by other developers. I have managed to use a custom hook in my dag.

Regarding contributing back to the project, I would love to, I have
explored the basic part of Adobe Analytics API as of now. I would add more
functionality and probably have a PR in near future.

Next step is to complete my custom hook for boto3 (for aws).

For other folks, so here are the steps that I have followed:
1. Created a new folder within the DAGs folder. I am not sure if I could
have put the hook in the plugins folder, will explore this option at a
later stage.
2. Put my python file containing the custom hook code in the new folder
3. Put my dag file in the Dag's folder.
4. Restarted the webserver (although I am not sure if this step played a
role or not). I did this step as I had a new python library which I had
imported.
5. Started worker using command: airflow worker
6. Started scheduler using command: airflow scheduler

End result, I could see my DAG on the UI and I could see the data been
fetched from the API.

Thanks & Regards,
Vikas


On Fri, Sep 23, 2016 at 2:30 PM, Maxime Beauchemin <
maximebeauchemin@gmail.com> wrote:

> Any reason why you want to package it as a plugin?
>
> It's pretty easy to just have your custom hooks and operators live
> alongside your pipelines, maybe in a `common` folder. In your pipeline file
> you just import what you need, relative to your file, from the same repo.
> Just don't forget the `__init__.py` files that make a folder a package in
> python so that the import statements can work.
>
> If you want to contribute it back to the project, send a PR that add it to
> the `contrib/hooks` folder.
>
> Max
>
> On Fri, Sep 23, 2016 at 8:24 AM, Andrew Phillips <aphillips@qrmedia.com>
> wrote:
>
>> Are the above steps correct or am I missing something?
>>>
>>
>> Is something not working as expected, e.g. are you unable to *use* the
>> plugin from your DAGs? If so, could you provide a few more details on the
>> error?
>>
>> Regards
>>
>> ap
>>
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201609.mbox/<CABNWVv-YdgJ_ygTDKL8X86Gdv=A-fTLdpfYYc44Ve4iBFpHEHA@mail.gmail.com>,Vikas Malhotra <vikas.malhotr...@gmail.com>,0,1
86,87,"Hi All,

I have a hcatalog table ""partitioned by (d string)"".

I have couple of days worth of data and when i run ""show partitions"" it
provides the correct daa.

d=20111215
d=20111216
d=20111217
d=20111218
d=20111219
d=20111220
d=20111221
d=20111222
d=20111223
d=20111224
d=20111225
d=20120415

However, when I run PIG with ""filter a by d == '20120415'"", it ends up
scanning all data.

Is this a known bug/enhancement in HCatalog?. Ideally, shouldn't it scan
only the d=20120415 directory?

Any pointers would be of great help.


-- 
~Rajesh.B

",http://mail-archives.apache.org/mod_mbox/incubator-hcatalog-user/201204.mbox/%3cCAJqL3E+fx3yRFj=J01P3cGogCVt8=dsmits-pFU2nWhKvUg1Yg@mail.gmail.com%3e,Rajesh Balamohan <rajesh.balamo...@gmail.com>,0,0
188,189,"Hi All,


We are running master in our environment and have noticed something new (that wasn't present
in release 1.7.1.3).


I have the following DAG, which I ran a backfill on:

#                                                     ------ BuildTask6 ------ BuildTask7
------ BuildTask8 ------ BuildTask9
#                                                    /                                   
 /
#   InitBuildTask ------ BuildTask1 ------ BuildTask2 ------ BuildTask3                  
/
#                \                                                     \                 /
#                 ------ BuildTask4 ------------------------------------------ BuildTask5
#


When the backfill begins, the outputs gives warnings for all tasks that have not yet had their
dependencies met (full output attached) i.e. all tasks except the one task that has no dependencies.
This continues until the backfill has completed. Is this expected behaviour?


Cheers,
Luke Maycock
OLIVER WYMAN
luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com>
www.oliverwyman.com<http://www.oliverwyman.com/>


________________________________
From: Chris Riccomini <criccomini@apache.org>
Sent: 29 September 2016 18:14:02
To: dev@airflow.incubator.apache.org
Subject: Re: Airflow Releases

Hey Luke,

> Is there anything we can do to help get the next release in place?

One thing that would definitely help is running master somewhere in
your environment, and reporting any issues that see. Over the next few
weeks, AirBNB and a few other folks will be doing the same in an
effort to harden the 1.8 release.

Cheers,
Chris

On Thu, Sep 29, 2016 at 3:08 AM, Maycock, Luke
<luke.maycock@affiliate.oliverwyman.com> wrote:
> Airflow Developers,
>
>
> We were looking at writing a workflow framework in Python when we found Airflow. We have
carried out some proof of concept work for using Airflow and wish to continue using it as
it comes with lots of great features out-of-the-box.
>
>
> We have created our own fork here:
>
> https://github.com/owlabs/incubator-airflow
>
>
> So far, the only thing we have committed back to the main repository is the following
fix to the mssql_hook:
>
> https://github.com/apache/incubator-airflow/pull/1626
>
>
> Among other types of tasks, we wish to be able to run mssql tasks using Airflow. In order
to do so, the above and below fixes are required:
>
> https://github.com/apache/incubator-airflow/pull/1458<https://github.com/apache/incubator-airflow/pull/1458/commits/e7e655fde3c29742149d047028cbb21aecba86ed>
>
>
> We have created a Chef cookbook for configuring VMs with Airflow and its prerequisites.
As part of this cookbook, we are installing the latest release of Airflow. However, it appears
that the latest release does not have the aforementioned fixes.
>
> Do you know when the next release of Airflow is expected? Is there anything we can do
to help get the next release in place?
>
>
> Luke Maycock
> OLIVER WYMAN
> luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com>
> www.oliverwyman.com<http://www.oliverwyman.com/>
>
>
> ________________________________
> This e-mail and any attachments may be confidential or legally privileged. If you received
this message in error or are not the intended recipient, you should destroy the e-mail message
and any attachments or copies, and you are prohibited from retaining, distributing, disclosing
or using any information contained herein. Please inform us of the erroneous delivery by return
e-mail. Thank you for your cooperation.

________________________________
This e-mail and any attachments may be confidential or legally privileged. If you received
this message in error or are not the intended recipient, you should destroy the e-mail message
and any attachments or copies, and you are prohibited from retaining, distributing, disclosing
or using any information contained herein. Please inform us of the erroneous delivery by return
e-mail. Thank you for your cooperation.
",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<DM2PR07MB156367B9581C2B29F2CE1BA3C9C20@DM2PR07MB1563.namprd07.prod.outlook.com>,"""Maycock, Luke"" <luke.mayc...@affiliate.oliverwyman.com>",0,0
215,216,"Interesting -- I also run on Kubernetes with a git-sync sidecar, but the
containers wait for the synced repo to apprar before starting since it
contains some dependencies -- I assume that's why I didn't experience the
same issue.

On Sun, Feb 12, 2017 at 6:29 AM Bolke de Bruin <bdbruin@gmail.com> wrote:

> Although the race condition doesn't explain why “num_runs = None” resolved
> the issue for you earlier, but it does give a clue now: the PR that
> introduced “num_runs = -1” was there to be able to work with empty dag
> dirs, maybe it wasn’t fully covered yet.
>
> Bolke
>
> > On 12 Feb 2017, at 12:26, Bolke de Bruin <bdbruin@gmail.com> wrote:
> >
> > Ok great! Thanks! That sounds like a race condition: module not
> available yet at time of reading. I would expect that it resolves itself
> after a while.
> >
> > After talking to some people at the Warsaw BigData conf I have some
> ideas around syncing dags, Spoiler: no dependency on git.
> >
> > - Bolke
> >
> >> On 12 Feb 2017, at 11:17, Alex Van Boxel <alex@vanboxel.be> wrote:
> >>
> >> Running ok, in staging... @bolke I'm running patch-less. I've switched
> my
> >> Kubernetes from:
> >>
> >> - each container (webserver/scheduler/worker) had a git-sync'er (getting
> >> the dags from git)
> >>> this meant that the scheduler had 0 dags at startup, and should have
> >> picked them up later
> >>
> >> to
> >>
> >> - single NFS share that shares airflow_home over each container
> >>> the git sync'er is now a seperate container running before the other
> >> containers
> >>
> >> This resolved my mystery DAG crashes.
> >>
> >> I'll be updating production to a patchless RC3 today, you get my vote
> after
> >> that.
> >>
> >>
> >>
> >>
> >> On Sun, Feb 12, 2017 at 4:59 AM Boris Tyukin <boris@boristyukin.com>
> wrote:
> >>
> >>> awesome! thanks Jeremiah
> >>>
> >>> On Sat, Feb 11, 2017 at 12:53 PM, Jeremiah Lowin <jlowin@apache.org>
> >>> wrote:
> >>>
> >>>> Boris, I submitted a PR to address your second point --
> >>>> https://github.com/apache/incubator-airflow/pull/2068. Thanks!
> >>>>
> >>>> On Sat, Feb 11, 2017 at 10:42 AM Boris Tyukin <boris@boristyukin.com>
> >>>> wrote:
> >>>>
> >>>>> I am running LocalExecutor and not doing crazy things but use DAG
> >>>>> generation heavily - everything runs fine as before. As I mentioned
> in
> >>>>> other threads only had a few issues:
> >>>>>
> >>>>> 1) had to upgrade MySQL which was a PAIN. Cloudera CDH is running
old
> >>>>> version of MySQL which was compatible with 1.7.1 but not compatible
> now
> >>>>> with 1.8 because of fractional seconds support PR.
> >>>>>
> >>>>> 2) when you install airflow, there are two new example DAGs
> >>>>> (last_task_only) which are going back very far in the past and
> >>> scheduled
> >>>> to
> >>>>> run every hour - a bunch of dags triggered on the first start of
> >>>> scheduler
> >>>>> and hosed my CPU
> >>>>>
> >>>>> Everything else was fine and I LOVE lots of small UI changes, which
> >>>> reduced
> >>>>> a lot my use of cli.
> >>>>>
> >>>>> Thanks again for the amazing work and an awesome project!
> >>>>>
> >>>>>
> >>>>> On Sat, Feb 11, 2017 at 9:17 AM, Jeremiah Lowin <jlowin@apache.org>
> >>>> wrote:
> >>>>>
> >>>>>> I was able to deploy successfully. +1 (binding)
> >>>>>>
> >>>>>> On Fri, Feb 10, 2017 at 7:37 PM Maxime Beauchemin <
> >>>>>> maximebeauchemin@gmail.com> wrote:
> >>>>>>
> >>>>>>> +1 (binding)
> >>>>>>>
> >>>>>>> On Fri, Feb 10, 2017 at 3:44 PM, Arthur Wiedmer <
> >>>>>> arthur.wiedmer@gmail.com>
> >>>>>>> wrote:
> >>>>>>>
> >>>>>>>> +1 (binding)
> >>>>>>>>
> >>>>>>>> On Feb 10, 2017 3:13 PM, ""Dan Davydov"" <dan.davydov@airbnb.com.
> >>>>>> invalid>
> >>>>>>>> wrote:
> >>>>>>>>
> >>>>>>>>> Our staging looks good, all the DAGs there pass.
> >>>>>>>>> +1 (binding)
> >>>>>>>>>
> >>>>>>>>> On Fri, Feb 10, 2017 at 10:21 AM, Chris Riccomini
<
> >>>>>>> criccomini@apache.org
> >>>>>>>>>
> >>>>>>>>> wrote:
> >>>>>>>>>
> >>>>>>>>>> Running in all environments. Will vote after
the weekend to
> >>>> make
> >>>>>> sure
> >>>>>>>>>> things are working properly, but so far so good.
> >>>>>>>>>>
> >>>>>>>>>> On Fri, Feb 10, 2017 at 6:05 AM, Bolke de Bruin
<
> >>>>> bdbruin@gmail.com
> >>>>>>>
> >>>>>>>>> wrote:
> >>>>>>>>>>
> >>>>>>>>>>> Dear All,
> >>>>>>>>>>>
> >>>>>>>>>>> Let’s try again!
> >>>>>>>>>>>
> >>>>>>>>>>> I have made the THIRD RELEASE CANDIDATE
of Airflow 1.8.0
> >>>>>> available
> >>>>>>>> at:
> >>>>>>>>>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/
> >>> <
> >>>>>>>>>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/>
> >>> ,
> >>>>>>> public
> >>>>>>>>> keys
> >>>>>>>>>>> are available at https://dist.apache.org/repos/
> >>>>>>>> dist/release/incubator/
> >>>>>>>>>>> airflow/ <
> >>>>> https://dist.apache.org/repos/dist/release/incubator/
> >>>>>>>>> airflow/>
> >>>>>>>>>>> . It is tagged with a local version “apache.incubating”
so
> >>> it
> >>>>>>> allows
> >>>>>>>>>>> upgrading from earlier releases.
> >>>>>>>>>>>
> >>>>>>>>>>> Two issues have been fixed since release
candidate 2:
> >>>>>>>>>>>
> >>>>>>>>>>> * trigger_dag could create dags with fractional
seconds,
> >>> not
> >>>>>>>> supported
> >>>>>>>>> by
> >>>>>>>>>>> logging and UI at the moment
> >>>>>>>>>>> * local api client trigger_dag had hardcoded
execution of
> >>>> None
> >>>>>>>>>>>
> >>>>>>>>>>> Known issue:
> >>>>>>>>>>> * Airflow on kubernetes and num_runs -1
(default) can
> >>> expose
> >>>>>> import
> >>>>>>>>>> issues.
> >>>>>>>>>>>
> >>>>>>>>>>> I have extensively discussed this with Alex
(reporter) and
> >>> we
> >>>>>>>> consider
> >>>>>>>>>>> this a known issue with a workaround available
as we are
> >>>> unable
> >>>>>> to
> >>>>>>>>>>> replicate this in a different environment.
UPDATING.md has
> >>>> been
> >>>>>>>> updated
> >>>>>>>>>>> with the work around.
> >>>>>>>>>>>
> >>>>>>>>>>> As these issues are confined to a very specific
area and
> >>> full
> >>>>>> unit
> >>>>>>>>> tests
> >>>>>>>>>>> were added I would also like to raise a
VOTE for releasing
> >>>>> 1.8.0
> >>>>>>>> based
> >>>>>>>>> on
> >>>>>>>>>>> release candidate 3, i.e. just renaming
release candidate 3
> >>>> to
> >>>>>>> 1.8.0
> >>>>>>>>>>> release.
> >>>>>>>>>>>
> >>>>>>>>>>> Please respond to this email by:
> >>>>>>>>>>>
> >>>>>>>>>>> +1,0,-1 with *binding* if you are a PMC
member or
> >>>> *non-binding*
> >>>>>> if
> >>>>>>>> you
> >>>>>>>>>> are
> >>>>>>>>>>> not.
> >>>>>>>>>>>
> >>>>>>>>>>> Thanks!
> >>>>>>>>>>> Bolke
> >>>>>>>>>>>
> >>>>>>>>>>> My VOTE: +1 (binding)
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>
> >> --
> >> _/
> >> _/ Alex Van Boxel
> >
>
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CADsgxrG5vNcxde5muT9+itw9V_iVaanirpaEyCZ8D0g+YLhQ6w@mail.gmail.com>,Jeremiah Lowin <jlo...@apache.org>,0,1
166,167,"I guess instance-wise l2 normalization is mandatory for FFM.
https://github.com/guestwalk/libffm/blob/master/ffm.cpp#L688
https://github.com/CNevd/libffm-ftrl/blob/4247440cc190346daa0b675135e0542e4933cb0f/ffm.cpp#L310

Makoto

2017-10-18 21:27 GMT+09:00 Makoto Yui <myui@apache.org>:
> At the first update, loss is large but average loss for each update is
> very small using your test.
> https://github.com/apache/incubator-hivemall/blob/master/core/src/test/java/hivemall/fm/FieldAwareFactorizationMachineUDTFTest.java#L85
>
> It might better to implement instance-wise l2 normalization to reduce
> initial losses.
>
> Further investigation is required but I need to focus on the first
> Apache release for this month.
>
> GA of FFM will be v0.5.1 release scheduled on Dec.
>
> Makoto
>
> 2017-10-18 1:36 GMT+09:00 Makoto Yui <myui@apache.org>:
>> Thanks. I'll test FFM with it tomorrow.
>>
>> Makoto
>>
>> 2017-10-18 1:19 GMT+09:00 Shadi Mari <shadimari@gmail.com>:
>>> Attached us a sample of 500 examples from my training set represented as
>>> vector of features.
>>>
>>> Regards,
>>>
>>>
>>> On Tue, Oct 17, 2017 at 7:08 PM, Makoto Yui <myui@apache.org> wrote:
>>>>
>>>> I need to reproduce your test.
>>>>
>>>> Could you give me the sample (100~500 examples are enough) of your
>>>> training input in gzipped tsv/csv?
>>>>
>>>> FFM input format is <field>:<index>:<value>.
>>>>
>>>> Thanks,
>>>> Makoto
>>>>
>>>> 2017-10-18 0:59 GMT+09:00 Shadi Mari <shadimari@gmail.com>:
>>>> > Makoto,
>>>> >
>>>> > I am using the default hyper-parameters in addition to the following
>>>> > settings:
>>>> >
>>>> > feature_hashing: 20
>>>> > classification is enabled
>>>> > Iterations = 10
>>>> > K = 2, another test using K = 4
>>>> > Opt: FTRL (default)
>>>> >
>>>> > I tried setting the initial learning to 0.2 and optimizer to AdaGrad
>>>> > with no
>>>> > significant changes on the empirical loss.
>>>> >
>>>> > Thanks
>>>> >
>>>> >
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > On Tue, Oct 17, 2017 at 6:51 PM, Makoto Yui <myui@apache.org>
wrote:
>>>> >>
>>>> >> The empirical loss (cumulative logloss) is too large.
>>>> >>
>>>> >> The simple test in FieldAwareFactorizationMachineUDTFTest shows
that
>>>> >> empirical loss is decreasing properly but it seems optimization
is not
>>>> >> working correctly in your case.
>>>> >>
>>>> >> Could you show me the training hyperparameters?
>>>> >>
>>>> >> Makoto
>>>> >>
>>>> >> 2017-10-17 19:01 GMT+09:00 Shadi Mari <shadimari@gmail.com>:
>>>> >> > Hello,
>>>> >> >
>>>> >> > I am trying to understand the results produced by FFM on each
>>>> >> > iteration
>>>> >> > during the training of Criteo 2014 dataset.
>>>> >> >
>>>> >> > Basically, I have 10 mappers running concurrently (each has
~4.5M
>>>> >> > records),
>>>> >> > and follows is an output by one of the mappers:
>>>> >> >
>>>> >> > -----------------------------
>>>> >> >
>>>> >> > fm.FactorizationMachineUDTF|: Wrote 4479491 records to a temporary
>>>> >> > file
>>>> >> > for
>>>> >> > iterative training: hivemall_fm392724107368114556.sgmt (2.02
GiB)
>>>> >> > Iteration #2 [curLosses=1.5967339372694769E10,
>>>> >> > prevLosses=4.182558816480771E10, changeRate=0.6182399322209704,
>>>> >> > #trainingExamples=4479491]
>>>> >> >
>>>> >> > -----------------------------
>>>> >> >
>>>> >> > Looking at the source code, FFM implementation uses LogLess
>>>> >> > performance
>>>> >> > metric when classification is specified, however the curLossess
>>>> >> > counter
>>>> >> > is
>>>> >> > very high 1.5967339372694769E10
>>>> >> >
>>>> >> >
>>>> >> > What does this mean?
>>>> >> >
>>>> >> > Regards
>>>> >> >
>>>> >> >
>>>> >>
>>>> >>
>>>> >>
>>>> >> --
>>>> >> Makoto YUI <myui AT apache.org>
>>>> >> Research Engineer, Treasure Data, Inc.
>>>> >> http://myui.github.io/
>>>> >
>>>> >
>>>>
>>>>
>>>>
>>>> --
>>>> Makoto YUI <myui AT apache.org>
>>>> Research Engineer, Treasure Data, Inc.
>>>> http://myui.github.io/
>>>
>>>
>>
>>
>>
>> --
>> Makoto YUI <myui AT apache.org>
>> Research Engineer, Treasure Data, Inc.
>> http://myui.github.io/
>
>
>
> --
> Makoto YUI <myui AT apache.org>
> Research Engineer, Treasure Data, Inc.
> http://myui.github.io/



-- 
Makoto YUI <myui AT apache.org>
Research Engineer, Treasure Data, Inc.
http://myui.github.io/

",http://mail-archives.apache.org/mod_mbox/incubator-hivemall-user/201710.mbox/%3cCAGJoAUkhnVv2UBR6oHc38mSrPjd_hHHzrNwVYtPHSsthxN8dPw@mail.gmail.com%3e,Makoto Yui <m...@apache.org>,0,0
112,113,"After two RC version, a new candidate for the ODF Toolkit 0.5-incubating
release is available  at:


http://people.apache.org/~devinhan/odftoolkit-release/odftoolkit-0.5-incubating-rc3/

The release candidate is a zip archive of the sources in:


https://svn.apache.org/repos/asf/incubator/odf/tags/0.5-incubating-rc3/

The SHA1 checksum of the archive is
a6eab56b2926c1e94f2670288dc0edcac08ce80d.

Please vote on releasing this package as Apache ODF Toolkit
0.5-incubating-rc3.

The vote is open for the next 72 hours and passes if a majority of at least
three +1 ODF Toolkit PMC votes are cast.

   [ ] +1 Release this package as Apache ODF Toolkit 0.5-incubating-rc3
   [ ] -1 Do not release this package because...

-- 
-Devin

",http://mail-archives.apache.org/mod_mbox/incubator-odf-users/201112.mbox/%3cCAFJd6yRXT-Ty=MX1MjA2B0d=ZRUwOg1xtoY-7k1=Ep2LUhexOw@mail.gmail.com%3e,Devin Han <devin...@apache.org>,0,1
60,61,"Github user minahlee commented on the pull request:

    https://github.com/apache/incubator-zeppelin/pull/526#issuecomment-163819332
  
    LGTM


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

",http://mail-archives.apache.org/mod_mbox/zeppelin-dev/201512.mbox/<20151211024429.6DF10DFBDE@git1-us-west.apache.org>,minahlee <...@git.apache.org>,0,0
277,278,"Hi Kim,

Thanks. That worked.

-Harold

--- On Thu, 10/22/09, Kim LiChong <Kim.Lichong@Sun.COM> wrote:

> From: Kim LiChong <Kim.Lichong@Sun.COM>
> Subject: Re: cannot Find Symbol: com.sun.faban.driver.transport.hc3.ApacheHC3Transport
> To: olio-user@incubator.apache.org
> Date: Thursday, October 22, 2009, 1:10 AM
> Hi Harold,
> 
> Recent checkins for the workload driver for Java now depend
> on a newer 
> version of Faban which is now available.
> Can you please get Faban version 1.0RC2 from the faban
> website 
> <http://faban.sunsource.net/index.html#Downloads> and
> use this as your 
> FABAN_HOME when compiling?
> 
> thanks,
> 
> Kim
> > Hi All,
> >
> > When I try to compile the workload driver of Olio, I'm
> getting the following errors.
> >
> > /mount/olio/workload/java/trunk# ant deploy.jar
> >
> > Buildfile: build.xml
> >
> > init:
> >
> > compile:
> >   [javac] Compiling 25 source files to
> /mount/olio/workload/java/trunk/build/classes
> >   [javac]
> /mount/olio/workload/java/trunk/src/org/apache/olio/workload/driver/UIDriver.java:541:
> cannot find symbol
> >   [javac] symbol  : method
> readURL(java.lang.String,java.util.ArrayList<org.apache.commons.httpclient.methods.multipart.Part>)
> >
> >   [javac] location: class
> com.sun.faban.driver.transport.hc3.ApacheHC3Transport
> >   [javac]       
>      ((ApacheHC3Transport)
> http).readURL(fileUploadEventURL, params);
> >
> > Am I missing a library/JAR file?
> >
> > -Harold
> >
> >
> >       
> >   
> 
> 


      

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200910.mbox/%3c731509.41356.qm@web51011.mail.re2.yahoo.com%3e,Harold Lim <rold...@yahoo.com>,0,0
101,102,"sorry for hearing that. is there any workaround method for this?

On Thu, Feb 7, 2013 at 11:11 AM, Aaron McCurry <amccurry@gmail.com> wrote:
> Unfortunately yes the minimum for all the features to work is 0.20.2 (with
> appends).  I have never tried to run it on anything less than 0.20.2.
>  Sorry.
>
> Aaron
>
>
> On Wed, Feb 6, 2013 at 9:38 PM, Li Li <fancyerii@gmail.com> wrote:
>
>> I am using hadoop 0.18. does blur need higher version hadoop?
>> I can't upgrade it. it's out of my control.
>>
>> On Thu, Feb 7, 2013 at 10:14 AM, Aaron McCurry <amccurry@gmail.com> wrote:
>> > See comments below.
>> >
>> >
>> > On Tue, Feb 5, 2013 at 10:53 PM, Li Li <fancyerii@gmail.com> wrote:
>> >
>> >> I followed all the steps of http://wiki.apache.org/blur/QuickStart
>> >> the only difference is the machine I use don't have git installed. so
>> >> I use another machine to clone by:
>> >>
>> >> git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git
>> >> git checkout 0.2-dev
>> >>
>> >> then I copy it to my build machine
>> >> all is fine before start-up
>> >> there are still errors in log
>> >> RROR 20130206_11:47:00:000_CST [main]
>> >> concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread
>> >> [Thread[main,5,main]]
>> >> java.lang.RuntimeException: Safemode data missing
>> >> [/blur/clusters/default/safemode]
>> >>         at
>> >>
>> org.apache.blur.manager.indexserver.DistributedIndexServer.waitInSafeModeIfNeeded(DistributedIndexServer.java:177)
>> >>         at
>> >>
>> org.apache.blur.manager.indexserver.DistributedIndexServer.init(DistributedIndexServer.java:135)
>> >>         at
>> >>
>> org.apache.blur.thrift.ThriftBlurServer.createServer(ThriftBlurServer.java:187)
>> >>         at
>> >> org.apache.blur.thrift.ThriftBlurServer.main(ThriftBlurServer.java:88)
>> >>
>> >
>> > This could be a bug with an empty ZooKeeper (meaning maybe blur isn't
>> > setting things up correctly).  I'm going to retest that blur sets up
>> > ZooKeeper correctly for safe mode and report back.
>> >
>> >
>> >>
>> >> when I run the client
>> >> $./apache-blur-0.2.0-SNAPSHOT/bin/blur shell localhost:40020
>> >> java.lang.NoSuchMethodError:
>> >> org.apache.thrift.meta_data.FieldValueMetaData.<init>(BZ)V
>> >>         at
>> >> org.apache.blur.thrift.generated.QueryArgs.<clinit>(QueryArgs.java:255)
>> >>         at java.lang.Class.forName0(Native Method)
>> >>         at java.lang.Class.forName(Class.java:169)
>> >>         at $Proxy0.<clinit>(Unknown Source)
>> >>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> >> Method)
>> >>         at
>> >>
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
>> >>         at
>> >>
>> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
>> >>         at
>> java.lang.reflect.Constructor.newInstance(Constructor.java:513)
>> >>         at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)
>> >>         at
>> org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)
>> >>         at
>> org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)
>> >>         at org.apache.blur.shell.Main.main(Main.java:171)
>> >> Exception in thread ""main"" java.lang.NoSuchMethodError:
>> >> org.apache.thrift.meta_data.FieldValueMetaData.<init>(BZ)V
>> >>         at
>> >> org.apache.blur.thrift.generated.QueryArgs.<clinit>(QueryArgs.java:255)
>> >>         at java.lang.Class.forName0(Native Method)
>> >>         at java.lang.Class.forName(Class.java:169)
>> >>         at $Proxy0.<clinit>(Unknown Source)
>> >>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> >> Method)
>> >>         at
>> >>
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
>> >>         at
>> >>
>> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
>> >>         at
>> java.lang.reflect.Constructor.newInstance(Constructor.java:513)
>> >>         at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)
>> >>         at
>> org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)
>> >>         at
>> org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)
>> >>         at org.apache.blur.shell.Main.main(Main.java:171)
>> >>
>> >
>> > This is a very strange error, it's almost like the wrong thrift library
>> is
>> > in the classpath somewhere.  What hadoop version are you using?  Could
>> > there be another version of thrift pulled in from another project
>> somehow?
>> >  Maybe hbase?  I know that the latest version of hbase is still using
>> 0.8.0
>> > of thrift.
>> >
>> >
>> >>
>> >> On Tue, Feb 5, 2013 at 11:53 PM, Aaron McCurry <amccurry@gmail.com>
>> wrote:
>> >> > This is an interesting error, because it's looks like there is a
>> mixture
>> >> of
>> >> > old code the trunk (0.1.x) with an error because of Lucene 4.0.  Try
>> >> using
>> >> > and building with the 0.2-dev, should become the trunk soon.  Follow
>> this
>> >> > guide and let me know what issues you find.  Thanks!
>> >> >
>> >> > http://wiki.apache.org/blur/QuickStart
>> >> >
>> >> > Aaron
>> >> >
>> >> >
>> >> > On Tue, Feb 5, 2013 at 4:41 AM, Li Li <fancyerii@gmail.com> wrote:
>> >> >
>> >> >> hi
>> >> >>     I follow the instructions of the README.md. when I
>> >> >> ./bin/start-all.sh, there are errors in log files
>> >> >>     1. blur-shard-server ****-0.log
>> >> >>     ERROR 20130205_17:34:57:057_CST [main]
>> >> >> concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread
>> >> >> [Thread[main,5,main]]
>> >> >> java.lang.NoSuchFieldError: NOT_ANALYZED_NO_NORMS
>> >> >>         at
>> >> >> org.apache.blur.utils.BlurConstants.<clinit>(BlurConstants.java:105)
>> >> >>         at
>> >> >>
>> >>
>> org.apache.blur.thrift.ThriftBlurShardServer.createServer(ThriftBlurShardServer.java:187)
>> >> >>         at
>> >> >>
>> >>
>> org.apache.blur.thrift.ThriftBlurShardServer.main(ThriftBlurShardServer.java:92)
>> >> >>     2. logs/blur-controller-server**-0.log
>> >> >>     ERROR 20130205_17:35:00:000_CST [main]
>> >> >> concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread
>> >> >> [Thread[main,5,main]]
>> >> >> java.lang.NoSuchMethodError:
>> >> >>
>> >> >>
>> >>
>> org.apache.thrift.transport.TNonblockingServerSocket.<init>(Ljava/net/InetSocketAddress;)V
>> >> >>         at
>> >> org.apache.blur.thrift.ThriftServer.start(ThriftServer.java:80)
>> >> >>         at
>> >> >>
>> >>
>> org.apache.blur.thrift.ThriftBlurControllerServer.main(ThriftBlurControllerServer.java:72)
>> >> >>
>> >>
>>

",http://mail-archives.apache.org/mod_mbox/incubator-blur-user/201302.mbox/%3cCAFAd71VCaOefPufEvEAwd=47w9DGZ7rRd4jTuMRzt0VeeLRgQg@mail.gmail.com%3e,Li Li <fancye...@gmail.com>,0,1
177,178,"> We at Blue Apron would be very interested.

Same here.

ap

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201612.mbox/<c4e3a1e072ce4f3c9965c722f8571640@qrmedia.com>,Andrew Phillips <andr...@apache.org>,0,1
156,157,"Hi Gunnar,

No, the tests simulates a version of fey. It starts its own actor system.

The fey-test-actor.jar is included in fey-core. It is only a performer test used to make sure
fey handles its performers in the right way.

For now I would proceed removing the tests from assembly so you can generate the executable
and proceed with the steps.

I will have to do some investigation and see if I figure out why the tests breaks for you.

Regards

Barbara Gomes
+1 (650) 713-6092

> On Feb 19, 2017, at 11:41 AM, Gunnar Tapper <tapper.gunnar@gmail.com> wrote:
> 
> Hi Barbara,
> 
> The same five tests fail with this change.
> 
> Doesn't the iota-fey-core.jar file have to be created before running tests?
> 
> [gunnar@localhost incubator-iota]$ find fey-core/target/scala-2.11 | grep jar
> fey-core/target/scala-2.11/test-classes/fey-test-actor.jar
> 
> Thanks,
> 
> Gunnar
> 
> 
>> On Sun, Feb 19, 2017 at 11:21 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com>
wrote:
>> Hi Gunnar,
>> 
>> I have noticed that when assembly runs the test, in some cases, some of the tests
fails.
>> 
>> I will need sometime to investigate why it does not work 100% when assembly calls
it.
>> 
>> For now, you can run the tests separately:
>> 
>> >> sbt
>> >> projetc fey-core
>> >> test
>> 
>> and for now add the following line to the fey project in  project/Build.scala
>> 
>> test in assembly := {}
>> 
>> it should look like:
>> <Screen Shot 2017-02-19 at 10.18.47 AM.png>
>> 
>> Please, let me know if that works for you
>> 
>> Regards
>> 
>>> On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com>
wrote:
>>> HI Gunnar, 
>>> 
>>> Thanks for the info.
>>> 
>>> I will try to reproduce the issue on CentOS.
>>> 
>>> If you re-run the tests, do you still get the SAME failed tests or it fails but
in different tests?
>>> 
>>> Regards
>>> 
>>>> On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com>
wrote:
>>>> Hi Barbara:
>>>> 
>>>> I'm on an x86 box with:
>>>> 
>>>> [gunnar@localhost ~]$ lsb_release -a
>>>> LSB Version:    :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch
>>>> Distributor ID: CentOS
>>>> Description:    CentOS release 6.8 (Final)
>>>> Release:        6.8
>>>> Codename:       Final
>>>> 
>>>> It may be too weak for what's needed here. I can spin up a AWS micro instance
once I know what OS to install.
>>>> 
>>>> Thanks,
>>>> 
>>>> Gunnar
>>>> 
>>>>> On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com>
wrote:
>>>>> Hi Gunnar,
>>>>> 
>>>>> Which Operational System are you using?
>>>>> sbt picks the order of the tests execution and it might be the problem
on your machine.
>>>>> 
>>>>> As you can see in the picture bellow, I pulled the repo from github and
run the tests and all of them succeeded
>>>>> 
>>>>> 
>>>>> 
>>>>> I'm using Mac OS
>>>>> 
>>>>> About the ERROR messages during the tests, those are fey logs caused
intentionally by the tests.
>>>>> 
>>>>> Also, try to run the tests separated:
>>>>> 
>>>>> >> sbt
>>>>> >> project fey-core
>>>>> >> test
>>>>> 
>>>>> Regards,
>>>>> 
>>>>>> On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com>
wrote:
>>>>>> Thanks for pointing this out will take a look.
>>>>>> -Tony
>>>>>> 
>>>>>> 
>>>>>>> On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com>
wrote:
>>>>>>> 
>>>>>>> Hi,
>>>>>>> 
>>>>>>> I verified the required versions and then I ran assembly of the
fey-core project. 5 failures. 
>>>>>>> 
>>>>>>> [info] - should result in creating a global Performer child actor
with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/GLOBAL-TEST'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFORMER-SCHEDULER'
>>>>>>> [info] - should result in one global actor created for orchestration
>>>>>>> [info] - should result in right number of running actors
>>>>>>> [info] Stopping performer inside ensemble
>>>>>>> [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8]
[akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer PERFORMER-SCHEDULER
>>>>>>> org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER
>>>>>>>         at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(Ensemble.scala:60)
>>>>>>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>>>>>>         at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29)
>>>>>>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>>>>>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)
>>>>>>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>>>>>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>>>>>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>>>>>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
>>>>>>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>>>>>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>>>>> 
>>>>>>> [info] - should Send stop message to monitor
>>>>>>> [info] Stopping ensemble
>>>>>>> [info] - should Send stop message to monitor
>>>>>>> [info] - should result in no orchestration running
>>>>>>> [info] - should not affect global performer
>>>>>>> [info] Stopping global performer
>>>>>>> [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7]
[akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer GLOBAL-TEST
>>>>>>> org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer
GLOBAL-TEST
>>>>>>>         at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)
>>>>>>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>>>>>>         at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)
>>>>>>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>>>>>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)
>>>>>>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>>>>>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>>>>>>         at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestActorRef.scala:60)
>>>>>>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>>>>>>         at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:250)
>>>>>>>         at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:283)
>>>>>>>         at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:191)
>>>>>>>         at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch.scala:147)
>>>>>>>         at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)
>>>>>>>         at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:402)
>>>>>>>         at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:213)
>>>>>>>         at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
>>>>>>>         at akka.actor.ActorCell.terminate(ActorCell.scala:374)
>>>>>>>         at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)
>>>>>>>         at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)
>>>>>>>         at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282)
>>>>>>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)
>>>>>>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>>>>>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>>>>> 
>>>>>>> [info] - should result in restart the orchestration
>>>>>>> [info] - should all previous actors restarted
>>>>>>> [info] Stopping orchestration
>>>>>>> [info] - should result in empty global
>>>>>>> [info] EnsembleSpec:
>>>>>>> [info] Creating a simple Ensemble MY-ENSEMBLE-0005
>>>>>>> [info] - should result in creation of Ensemble actor 'akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'
>>>>>>> [info] - should result in sending START to monitor actor
>>>>>>> [info] - should result in creation of Performer 'TEST-0004'
>>>>>>> [info] - should result in Empty state variable Ensemble.connectors
>>>>>>> [info] - should result in one entry added to state variable Ensemble.performer
>>>>>>> [info] - should result in one right entry to state variable Ensemble.performers_metadata
>>>>>>> [info] - should result in two paths added to IdentifyFeyActors.actorsPath
>>>>>>> [info] Sending Ensemble.STOP_PERFORMERS to Ensemble
>>>>>>> [info] - should result in Terminate message of actor 'TEST-0004'
and throw RestartEnsemble Exception
>>>>>>> [info] - should result in Performer 'TEST-0004' restarted
>>>>>>> [info] - should result in two paths added to IdentifyFeyActors.actorsPath
>>>>>>> [info] Sending PoisonPill to Ensemble
>>>>>>> [info] - should result in termination of actor 'MY-ENSEMBLE-0005'
>>>>>>> [info] - should result in sending TERMINATE to monitor actor
>>>>>>> [info] - should result in termination of ensemble and performer
>>>>>>> [info] - should result in empty IdentifyFeyActors.actorsPath
>>>>>>> [info] creating more detailed Ensemble
>>>>>>> [info] - should result in creation of Ensemble actor
>>>>>>> [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'
>>>>>>> [info] - should result in creation of Performer 'PERFORMER-PARAMS'
>>>>>>> [info] - should create connection PERFORMER-SCHEDULER -> PERFORMER-PARAMS
>>>>>>> [info] - should create 'PERFORMER-SCHEDULER' with schedule time
equal to 200ms
>>>>>>> [info] - should create 'PERFORMER-SCHEDULER' with connection
to 'PERFORMER-PARAMS'
>>>>>>> [info] - should create 'PERFORMER-PARAMS' with no connections
>>>>>>> [info] - should create 'PERFORMER-PARAMS' with specified params
>>>>>>> [info] 'PERFORMER-SCHEDULER'
>>>>>>> [info] - should produce 5 messages in 1 seconds
>>>>>>> [info] - should produce 10 messages in 2 seconds
>>>>>>> [info] 'PERFORMER-PARAMS'
>>>>>>> [info] - should process 5 messages in 1 seconds
>>>>>>> [info] - should produce 10 messages in 2 seconds
>>>>>>> [info] Stopping any Performer that belongs to the Ensemble
>>>>>>> [info] - should force restart of entire Ensemble
>>>>>>> [info] - should result in sending STOP - RESTART to monitor actor
>>>>>>> [info] - should keep ensemble actorRef when restarted
>>>>>>> [info] - should stop and start the performer with a new reference
>>>>>>> [info] Restarting an Ensemble
>>>>>>> [info] - should Consuming left messages on Process
>>>>>>> [info] - should Cleanup TestProbs
>>>>>>> [info] Redefining TestProbe for performers
>>>>>>> [info] - should start receiving messages
>>>>>>> [info] Sending PoisonPill to detailed Ensemble
>>>>>>> [info] - should result in termination of Ensemble
>>>>>>> [info] - should result in empty IdentifyFeyActors.actorsPath
>>>>>>> [info] creating Ensemble with Backoff performer
>>>>>>> [info] - should result in creation of Ensemble actor
>>>>>>> [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'
>>>>>>> [info] - should result in creation of Performer 'PERFORMER-PARAMS'
>>>>>>> [info] - should create 'PERFORMER-PARAMS' with backoff time equal
to 1 second
>>>>>>> [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal
to true
>>>>>>> [info] Performer with backoff enabled
>>>>>>> [info] - should not process messages during the backoff period
>>>>>>> [info] Performer with autoScale
>>>>>>> [info] - should result in router and routees created
>>>>>>> [info] IdentifyFeyActorsSpec:
>>>>>>> [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors
>>>>>>> [info] - should result in one path added to IdentifyFeyActors.actorsPath
>>>>>>> [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'
>>>>>>> [info] Creating a new actor in the system and sending IdentifyFeyActors.IDENTIFY_TREE
to IdentifyFeyActors
>>>>>>> [info] - should result in two paths added to IdentifyFeyActors.actorsPath
>>>>>>> [info] - should result in matching paths
>>>>>>> [info] Stopping previous added actor and sending IdentifyFeyActors.IDENTIFY_TREE
to IdentifyFeyActors
>>>>>>> [info] - should result in going back to have just one path added
to IdentifyFeyActors.actorsPath
>>>>>>> [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'
>>>>>>> [info] WatchServiceReceiverSpec:
>>>>>>> [info] Creating WatchServiceReceiver
>>>>>>> [info] - should process initial files in the JSON repository
>>>>>>> [info] Start a Thread with WatchServiceReceiver
>>>>>>> [info] - should Start Thread
>>>>>>> [info] Start watching directory
>>>>>>> [info] - should Starting receiving CREATED event
>>>>>>> [info] - should Starting receiving UPDATE event
>>>>>>> [info] processJson
>>>>>>> [info] - should log to warn level when json has invalid schema
>>>>>>> [info] interrupt watchservice
>>>>>>> [info] - should interrupt thread
>>>>>>> [info] FeyCoreSpec:
>>>>>>> [info] Creating FeyCore
>>>>>>> [info] - should result in creating a child actor with the name
'FEY_IDENTIFIER'
>>>>>>> [info] - should result in sending START message to Monitor actor
>>>>>>> [info] Sending FeyCore.START to FeyCore
>>>>>>> [info] - should result in creating a child actor with the name
'JSON_RECEIVER'
>>>>>>> [info] - should result in starting FeyWatchService Thread
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command
to FeyCore
>>>>>>> [info] - should result in creating an Orchestration child actor
with the name 'TEST-ACTOR'
>>>>>>> [info] - should result in creating an Ensemble child actor with
the name 'TEST-ACTOR/MY-ENSEMBLE-0001'
>>>>>>> [info] - should result in creating an Ensemble child actor with
the name 'TEST-ACTOR/MY-ENSEMBLE-0002'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'
>>>>>>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
with key 'TEST-ACTOR'
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command
to FeyCore
>>>>>>> [info] - should result in creating a new Performer child actor
with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command
and DELETE ensemble to FeyCore
>>>>>>> [info] - should result in termination of Ensemble with the name
'TEST-ACTOR/MY-ENSEMBLE-0001'
>>>>>>> [info] - should result in termination of Performer with the name
'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'
>>>>>>> [info] - should result in termination of Performer with the name
'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command
and same Timestamp to FeyCore
>>>>>>> [info] - should result in logging a 'not recreated' message at
Warn
>>>>>>> [info] Sending FeyCore.JSON_TREE to FeyCore
>>>>>>> [info] - should result in logging a 6 path messages at Info
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command
to FeyCore
>>>>>>> [info] - should result in termination of Orchestration with the
name 'TEST-ACTOR'
>>>>>>> [info] - should result in sending TERMINATE message to Monitor
actor
>>>>>>> [info] - should result in termination of Ensemble with the name
'TEST-ACTOR/MY-ENSEMBLE-0002'
>>>>>>> [info] - should result in termination of Performer with the name
'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'
>>>>>>> [info] - should result in removing key 'TEST-ACTOR' at FEY_CACHE.activeOrchestrations
>>>>>>> [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore
>>>>>>> [info] - should result in termination of 'TEST-ORCH-2' *** FAILED
***
>>>>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2""
-> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) had size
2 instead of expected size 1 (FeyCoreSpec.scala:144)
>>>>>>> [info] - should result in sending Terminate message to Monitor
actor *** FAILED ***
>>>>>>> [info]   java.lang.AssertionError: assertion failed: timeout
(1 second) during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TERMINATE
>>>>>>> [info]   at scala.Predef$.assert(Predef.scala:170)
>>>>>>> [info]   at akka.testkit.TestKitBase$class.expectMsgClass_internal(TestKit.scala:435)
>>>>>>> [info]   at akka.testkit.TestKitBase$class.expectMsgClass(TestKit.scala:431)
>>>>>>> [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)
>>>>>>> [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)
>>>>>>> [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)
>>>>>>> [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>>>>>>> [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>>>>>>> [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
>>>>>>> [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
>>>>>>> [info]   ...
>>>>>>> [info] - should result in empty FEY_CACHE.activeOrchestrations
*** FAILED ***
>>>>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2""
-> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) was not
empty (FeyCoreSpec.scala:153)
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command
to FeyCore of a GenericReceiverActor
>>>>>>> [info] - should result in creating an Orchestration child actor
with the name 'RECEIVER_ORCHESTRATION'
>>>>>>> [info] - should result in creating an Ensemble child actor with
the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'
>>>>>>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
with key 'RECEIVER_ORCHESTRATION'
>>>>>>> [info] Sending PROCESS message to the Receiver Performer
>>>>>>> [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore
>>>>>>> [info] - should result in creating an Orchestration child actor
with the name 'RECEIVED-BY-ACTOR-RECEIVER'
>>>>>>> [info] - should result in creating an Ensemble child actor with
the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'
>>>>>>> [info] - should result in creating an Ensemble child actor with
the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'
>>>>>>> [info] - should result in one new entry to FEY_CACHE.activeOrchestrations
with key 'RECEIVED-BY-ACTOR-RECEIVER' *** FAILED ***
>>>>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""RECEIVED-BY-ACTOR-RECEIVER""
-> (213263914979,Actor[akka://FEY-MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]),
""TEST-ORCH-2"" -> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]),
""RECEIVER_ORCHESTRATION"" -> (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299]))
had size 4 instead of expected size 2 (FeyCoreSpec.scala:200)
>>>>>>> [info] Sending PROCESS message to the Receiver Performer with
command DELETE
>>>>>>> [info] - should STOP running orchestration
>>>>>>> [info] - should result in one entry in FEY_CACHE.activeOrchestrations
*** FAILED ***
>>>>>>> [info]   Map(""TEST_ORCHESTRATION_FOR_UTILS"" -> (,null), ""TEST-ORCH-2""
-> (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), ""RECEIVER_ORCHESTRATION""
-> (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299]))
had size 3 instead of expected size 1 (FeyCoreSpec.scala:213)
>>>>>>> [info] Sending PROCESS message to Receiver with checkpoint enabled
>>>>>>> [info] - should Save received JSON to checkpoint dir
>>>>>>> [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND
GLOBAL performer command to FeyCore
>>>>>>> [info] - should result in creating an Orchestration child actor
with the name 'GLOBAL-PERFORMER'
>>>>>>> [info] - should result in creating an Ensemble child actor with
the name 'GLOBAL-PERFORMER/ENS-GLOBAL'
>>>>>>> [info] - should result in creating a global Performer child actor
with the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'
>>>>>>> [info] - should result in creating a Performer child actor with
the name 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'
>>>>>>> [info] - should result in new entry to FEY_CACHE.activeOrchestrations
with key 'GLOBAL-PERFORMER'
>>>>>>> [info] - should result in one global actor created for orchestration
>>>>>>> [info] - should result in globa metadata add to table
>>>>>>> [info] - should result in right running actors
>>>>>>> [info] Stopping Global actor
>>>>>>> [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3]
[akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer GLOBAL-TEST
>>>>>>> org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer
GLOBAL-TEST
>>>>>>>         at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)
>>>>>>>         at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
>>>>>>>         at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)
>>>>>>>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
>>>>>>>         at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)
>>>>>>>         at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)
>>>>>>>         at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)
>>>>>>>         at akka.actor.ActorCell.invoke(ActorCell.scala:494)
>>>>>>>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
>>>>>>>         at akka.dispatch.Mailbox.run(Mailbox.scala:224)
>>>>>>>         at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>>>>>>         at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>>>>>> 
>>>>>>> [info] - should result in sending logging error
>>>>>>> [info] - should result in orchestration restarted
>>>>>>> [info] - should all previous actors restarted
>>>>>>> [info] Stopping orchestration with global performer
>>>>>>> [info] - should result in sending TERMINATE message to Monitor
actor
>>>>>>> [info] - should result in no global actors for orchestration
>>>>>>> [info] Stopping FeyCore
>>>>>>> [info] - should result in sending STOP message to Monitor actor
>>>>>>> [info] BaseAkkaSpec:
>>>>>>> [info] JsonReceiverSpec:
>>>>>>> [info] Executing validJson in JsonReceiver
>>>>>>> [info] - should return false when json schema is not right
>>>>>>> [info] - should log message to Error
>>>>>>> [info] - should return true when Json schema is valid
>>>>>>> [info] Executing checkForLocation in JsonReceiver
>>>>>>> [info] - should log message at Debug level
>>>>>>> [info] - should download jar dynamically from URL
>>>>>>> [info] Start a Thread with the JSON receiver
>>>>>>> [info] - should Start Thread
>>>>>>> [info] - should execute execute() method inside run
>>>>>>> [info] Interrupting the receiver Thread
>>>>>>> [info] - should Throw Interrupted exception
>>>>>>> [info] - should execute exceptionOnRun method
>>>>>>> [info] FeyGenericActorSpec:
>>>>>>> [info] Creating a GenericActor with Schedule time defined
>>>>>>> [info] - should result in scheduler started
>>>>>>> [info] - should result in onStart method called
>>>>>>> [info] - should result in START message sent to Monitor
>>>>>>> [info] - should result in one active actor
>>>>>>> [info] Backoff of GenericActor
>>>>>>> [info] - should be zero until the first PROCESS message
>>>>>>> [info] - should change when first PROCESS message was received
>>>>>>> [info] Sending PROCESS message to GenericActor
>>>>>>> [info] - should call processMessage method
>>>>>>> [info] customReceive method
>>>>>>> [info] - should process any non treated message
>>>>>>> [info] Sending PROCESS message to GenericActor
>>>>>>> [info] - should be discarded when backoff is enabled
>>>>>>> [info] - should be processed when backoff has finished
>>>>>>> [info] Calling startBackoff
>>>>>>> [info] - should set endBackoff with time now
>>>>>>> [info] Calling propagateMessage
>>>>>>> [info] - should send message to connectTo actors
>>>>>>> [info] Scheduler component
>>>>>>> [info] - should call execute() method
>>>>>>> [info] Sending EXCEPTION(IllegalArgumentException) message to
GenericActor
>>>>>>> [info] - should Throw IllegalArgumentException
>>>>>>> [info] - should Result in restart of the actor with sequence
of Monitoring: STOP -> RESTART -> START
>>>>>>> [info] - should call onStart method
>>>>>>> [info] - should call onRestart method
>>>>>>> [info] - should restart scheduler
>>>>>>> [info] Sending STOP to GenericActor
>>>>>>> [info] - should terminate GenericActor
>>>>>>> [info] - should call onStop method
>>>>>>> [info] - should cancel scheduler
>>>>>>> [info] - should send STOP - TERMINATE message to Monitor
>>>>>>> [info] - should result in no active actors
>>>>>>> [info] Creating GenericActor with schedule anc backoff equal
to zero
>>>>>>> [info] - should not start a scheduler
>>>>>>> [info] - should result in one active actor
>>>>>>> [info] - should result in no discarded PROCESS messages
>>>>>>> [info] FeyGenericActorReceiverSpec:
>>>>>>> [info] Creating a GenericActor with Schedule time defined
>>>>>>> [info] - should result in scheduler started
>>>>>>> [info] - should result in onStart method called
>>>>>>> [info] - should result in START message sent to Monitor
>>>>>>> [info] - should result in one active actor
>>>>>>> [info] - should result in normal functioning of GenericActor
>>>>>>> [info] Sending PROCESS message to GenericReceiver
>>>>>>> [info] - should log message to Warn saying that the JSON could
not be forwarded to FeyCore when JSON is invalid
>>>>>>> [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON
to be processed has a valid schema
>>>>>>> [info] - should Download jar from location and send ORCHESTRATION_RECEIVED
to FeyCore when JSON has a location defined
>>>>>>> [info] Scheduler component
>>>>>>> [info] - should call execute() method
>>>>>>> [info] Sending EXCEPTION(IllegalArgumentException) message to
GenericActor
>>>>>>> [info] - should Throw IllegalArgumentException
>>>>>>> [info] - should Result in restart of the actor with sequence
of Monitoring: STOP -> RESTART -> START
>>>>>>> [info] - should call onStart method
>>>>>>> [info] - should call onRestart method
>>>>>>> [info] - should restart scheduler
>>>>>>> [info] Sending STOP to GenericActor
>>>>>>> [info] - should terminate GenericActor
>>>>>>> [info] - should call onStop method
>>>>>>> [info] - should cancel scheduler
>>>>>>> [info] - should send STOP - TERMINATE message to Monitor
>>>>>>> [info] - should result in no active actors
>>>>>>> 
>>>>>>> CLeaning up[info] Run completed in 44 seconds, 724 milliseconds.
>>>>>>> [info] Total number of tests run: 243
>>>>>>> [info] Suites: completed 12, aborted 0
>>>>>>> [info] Tests: succeeded 238, failed 5, canceled 0, ignored 0,
pending 0
>>>>>>> [info] *** 5 TESTS FAILED ***
>>>>>>> [error] Failed tests:
>>>>>>> [error]         org.apache.iota.fey.FeyCoreSpec
>>>>>>> [error] (fey-core/test:test) sbt.TestsFailedException: Tests
unsuccessful
>>>>>>> [error] Total time: 46 s, completed Feb 19, 2017 12:36:25 AM
>>>>>>> 
>>>>>>> 
>>>>>>> -- 
>>>>>>> Thanks,
>>>>>>> 
>>>>>>> Gunnar
>>>>>>> If you think you can you can, if you think you can't you're right.
>>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> -- 
>>>>> Barbara Gomes
>>>>> Computer Engineer
>>>>> San Jose, CA
>>>> 
>>>> 
>>>> 
>>>> -- 
>>>> Thanks,
>>>> 
>>>> Gunnar
>>>> If you think you can you can, if you think you can't you're right.
>>> 
>>> 
>>> 
>>> -- 
>>> Barbara Gomes
>>> Computer Engineer
>>> San Jose, CA
>> 
>> 
>> 
>> -- 
>> Barbara Gomes
>> Computer Engineer
>> San Jose, CA
> 
> 
> 
> -- 
> Thanks,
> 
> Gunnar
> If you think you can you can, if you think you can't you're right.

",http://mail-archives.apache.org/mod_mbox/iota-dev/201702.mbox/<EF928CF0-21E1-40E5-8A3A-40ECE228A7A5@gmail.com>,Barbara <barbaramaltago...@gmail.com>,0,0
31,32,"Github user mmiklavc commented on the issue:

    https://github.com/apache/incubator-metron/pull/507
  
    Merge with master again


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

",http://mail-archives.apache.org/mod_mbox/metron-dev/201704.mbox/<20170425160629.9B187DF999@git1-us-west.apache.org>,mmiklavc <...@git.apache.org>,0,0
116,117,"Hi,

I would like to create a few users that only can see their respective DAGs
and have no global access like user administration.
I have enabled password authentication and created the initial user as
instructed here:

https://airflow.incubator.apache.org/security.html?highlight=multi%20tenancy#web-authentication

My airflow.cfg has the following settings:

# Set to true to turn on authentication:

# http://pythonhosted.org/airflow/security.html#web-authentication

authenticate = True

auth_backend = airflow.contrib.auth.backends.password_auth

# Filter the list of dags by owner name (requires authentication to be
enabled)

filter_by_owner = True

The (admin?) user that I created will after login see all DAGs (even though
filtering was enabled) and is able to create other users. However, while
creating new users through the UI, there seems to be no way to specify a
password?

Thanks,

Thomas

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201703.mbox/<CA+5xAo3T951aJnCXVMbt5DnjjqczWrNK4UfGOObLTSebF0PA3A@mail.gmail.com>,Thomas Weise <...@apache.org>,1,0
232,233,"Hey Rob,

That would be great! Could you please send me a title/abstract similar
to the others listed on the meetup?

Cheers,
Chris

On Thu, Oct 20, 2016 at 6:24 PM, Rob Froetscher
<rfroetscher@lumoslabs.com> wrote:
> Hi Chris,
>
> If you guys think it would be helpful/interesting to the meetup audience,
> my colleague Tim and I would be interested in talking about how we use
> Airflow to coordinate our Spark jobs on EMR.
>
> I wrote the EMR tooling
> <https://github.com/apache/incubator-airflow/pull/1630> (hook, operators,
> sensors) for Airflow so hopefully this could be a good video documentation
> of how to use it. Tim wrote many of our Spark jobs and has more context on
> how/why we use Airflow in this way for it.
>
> Let me know if you think this would be good. Happy to put together a short
> abstract. Thanks,
>
> Rob
>
> On Thu, Oct 20, 2016 at 8:43 AM, Chris Riccomini <criccomini@apache.org>
> wrote:
>
>> Hey all,
>>
>> We have one more slot to fill for the next meetup at WePay on November 16:
>>
>>   http://www.meetup.com/Bay-Area-Apache-Airflow-Incubating-Meetup/events/
>> 234778571/
>>
>> The slot is for 15m + 5m Q&A, so it's not too much of a commitment.
>> Would you like to speak? If so, let me know!
>>
>> Cheers,
>> Chris
>>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<CABYbY7dZ32_yYabs0i7MrdBzNkfNoePce4k7TtAUZ64saZX8Zg@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
184,185,"
    [ https://issues.apache.org/jira/browse/JSPWIKI-390?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12635715#action_12635715
] 

Harry Metske commented on JSPWIKI-390:
--------------------------------------

This is a DUP of JSPWIKI-185 and should have been fixed in 2.7.0.svn-42.

> NullPointerException during start of JSPWiki.war
> ------------------------------------------------
>
>                 Key: JSPWIKI-390
>                 URL: https://issues.apache.org/jira/browse/JSPWIKI-390
>             Project: JSPWiki
>          Issue Type: Bug
>          Components: Core & storage
>         Environment: Linux (Red Hat 4 as well as Fedora 8, 32bit), Tomcat 6.0.18, Java
JRE 1.6.0_u7
>            Reporter: Ingolf Knopf
>            Priority: Minor
>
> trying to start JSPWiki in Tomcat I get the following error. JSPWiki don't start.
> INFO   | jvm 1    | 2008/09/30 09:54:57 | SCHWERWIEGEND: Context [/JSPWiki] startup failed
due to previous errors
> INFO   | jvm 1    | 2008/09/30 09:54:57 | Background thread error: (stack trace follows)
> INFO   | jvm 1    | 2008/09/30 09:54:57 | java.lang.NullPointerException
> INFO   | jvm 1    | 2008/09/30 09:54:57 |       at com.ecyrd.jspwiki.util.WatchDog.enterState(WatchDog.java:231)
> INFO   | jvm 1    | 2008/09/30 09:54:57 |       at com.ecyrd.jspwiki.util.WatchDog.enterState(WatchDog.java:209)
> INFO   | jvm 1    | 2008/09/30 09:54:57 |       at com.ecyrd.jspwiki.search.LuceneSearchProvider$LuceneUpdater.startupTask(LuceneSearchProvider.java:703)
> INFO   | jvm 1    | 2008/09/30 09:54:57 |       at com.ecyrd.jspwiki.util.WikiBackgroundThread.run(WikiBackgroundThread.java:127)
> INFO   | jvm 1    | 2008/09/30 09:54:57 | Exception in thread ""JSPWiki Lucene Indexer""
com.ecyrd.jspwiki.InternalWikiException
> INFO   | jvm 1    | 2008/09/30 09:54:57 |       at com.ecyrd.jspwiki.util.WikiBackgroundThread.run(WikiBackgroundThread.java:171)
> INFO   | jvm 1    | 2008/09/30 09:54:58 | Background thread error: (stack trace follows)
> INFO   | jvm 1    | 2008/09/30 09:54:58 | java.lang.NullPointerException
> INFO   | jvm 1    | 2008/09/30 09:54:58 |       at com.ecyrd.jspwiki.util.WatchDog.enterState(WatchDog.java:231)
> INFO   | jvm 1    | 2008/09/30 09:54:58 |       at com.ecyrd.jspwiki.search.LuceneSearchProvider$LuceneUpdater.backgroundTask(LuceneSearchProvider.java:711)
> INFO   | jvm 1    | 2008/09/30 09:54:58 |       at com.ecyrd.jspwiki.util.WikiBackgroundThread.run(WikiBackgroundThread.java:135)
> INFO   | jvm 1    | 2008/09/30 09:54:58 | Exception in thread ""JSPWiki Lucene Indexer""
com.ecyrd.jspwiki.InternalWikiException
> INFO   | jvm 1    | 2008/09/30 09:54:58 |       at com.ecyrd.jspwiki.util.WikiBackgroundThread.run(WikiBackgroundThread.java:171)

-- 
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.


",http://mail-archives.apache.org/mod_mbox/jspwiki-dev/200809.mbox/<1642821810.1222771964571.JavaMail.jira@brutus>,"""Harry Metske (JIRA)"" <j...@apache.org>",0,0
91,92,"Sure sounds like bugs. Can you send the whole schema please?
________________________________
From: Costello, Roger L. <costello@mitre.org>
Sent: Monday, April 13, 2020 11:11 AM
To: users@daffodil.apache.org <users@daffodil.apache.org>
Subject: Two bugs in Daffodil?

Hi Folks,

I have a binary data format. One field is a 1-byte name representing a person's age. I have
it defined like so:

<xs:element name=""age"" type=""xs:nonNegativeInteger""
    dfdl:representation=""binary""
    dfdl:lengthKind=""explicit""
    dfdl:lengthUnits=""bytes""
    dfdl:length=""1""
    dfdl:alignment=""1""
    dfdl:alignmentUnits=""bytes""
    dfdl:byteOrder=""littleEndian""
    dfdl:binaryNumberRep=""binary""
/>

Daffodil gives this error message:

[error] Schema Definition Error: Property encoding is not defined.

Huh?

Why do I have to specify a (character) encoding on a binary nonNegativeInteger field?

Next, I thought, ""Okay, that doesn't make sense, but let me put dfdl:encoding=""ASCII"" on the
element declaration."" I did so, and yet I got the same error message!

Curiously, when I put the encoding in dfdl:format then the error message went away.

Conclusion: Daffodil has a bug. Daffodil sometimes does not recognize the encoding property
on an element declaration. Do you agree?

There is the exact same problem with initiator. I put dfdl:initiator="""" on the element declaration
and Daffodil says there is no initiator. When I put it on dfdl:format, Daffodil doesn't give
an error. I believe this is aother bug. Do you agree?

/Roger






",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/202004.mbox/%3cMN2PR15MB3615707D378A0237347845B2D3DD0@MN2PR15MB3615.namprd15.prod.outlook.com%3e,"""Beckerle, Mike"" <mbecke...@tresys.com>",0,0
222,223,"On Tuesday, October 21, 2019 Mike Beckerle wrote:


  *   ... an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving
the ""data archaeology"" problem.

I read the article. It provides a nice example of trying to figure out the structure of a
file that was poorly documented.

Assertion: that example has nothing to do with DFDL. DFDL says nothing about how to figure
out the structure of files. What DFDL does do is it tells you, once you have figured out the
structure, then here's a way to describe the structure.

Do you agree with my assertion?

/Roger



From: Beckerle, Mike <mbeckerle@tresys.com>
Sent: Tuesday, October 22, 2019 7:44 PM
To: users@daffodil.apache.org
Subject: [EXT] Re: Why use DFDL? Why parse and unparse data formats?

Roger,

I think you may have been blessed not have run into the need to deal with describing data
formats as much as is commonplace.  Dealing with them is day-to-day work for many engineers.

This is a link to an article I wrote a long time ago (at least 7 years ago) about DFDL, and
solving the ""data archaeology"" problem. It gives an example of an absolutely mundane day-to-day
data problem that was typical of what I and many many other people were constantly solving
for customers. It was some COBOL-ish data records that the customer couldn't figure out how
to export from one system to get into some new system. They didn't have good configuration
management, so the exact software that created the data wasn't clear. Absolutely typical stuff.

https://cboblog.typepad.com/cboblog/2008/07/dfdl-data-forma.html

I hope that helps.

...mikeb




________________________________
From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org>>
Sent: Tuesday, October 22, 2019 1:03 PM
To: users@daffodil.apache.org<mailto:users@daffodil.apache.org> <users@daffodil.apache.org<mailto:users@daffodil.apache.org>>
Subject: RE: Why use DFDL? Why parse and unparse data formats?


Hi Folks,



I want to elaborate on my question, as I feel it is a very important question.



Does your average software engineer need to parse data formats?



I can't think of any time, when implementing some software application, that I have needed
to write a parser for a data format. The parsing is already done under the hood by some library.
For example, when I write code in XSLT I don't ever have to parse the input because there
is a parser under the hood that does the parsing for me.



I can understand why cybersecurity people want to parse data formats - they need to inspect
the parsed data for malicious stuff. But as Mike noted, that's a tiny niche.



Is DFDL relevant only to that tiny niche community? Hopefully it is relevant to other communities.
What communities? Who needs to parse? Who needs DFDL?



/Roger





From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org>>
Sent: Monday, October 21, 2019 5:36 PM
To: users@daffodil.apache.org<mailto:users@daffodil.apache.org>
Subject: Re: Why use DFDL? Why parse and unparse data formats?



Hi Mike,



Thank you for your great feedback.



I think that I have everything (or almost everything) you said already in my tutorial.



I am trying to address a different concern: is parsing and unparsing applicable to the everyday
software engineer? Where is parsing and unparsing needed? Why do I (the everyday software
engineer) need parsing and unparsing, i.e., why do I need DFDL? As you point out, the cybercommunity
needs to do parsing and unparsing, but the cybercommunity is a very small niche. Are there
other niches that need to do parsing and unparsing? Ideally most or all software engineers
need to do parsing and unparsing; then DFDL would/should be in high demand. Again, I think
the answer to these questions is supremely important. If I can't tell people why DFDL is important
to them, well, then it's gonna be tough getting them interested in learning DFDL.



/Roger



From: Beckerle, Mike <mbeckerle@tresys.com<mailto:mbeckerle@tresys.com>>
Sent: Monday, October 21, 2019 3:11 PM
To: users@daffodil.apache.org<mailto:users@daffodil.apache.org>
Subject: [EXT] Re: Why use DFDL? Why parse and unparse data formats?



The first 3 points are about the ""Cybersecurity Use Case"" specifically. This is one small
slice of computerdom. Important one, but a niche area that is beside the point for most people
who want to get on with using the data for some actual end purpose.



Your last sentence on the slide wants to end with ""data"" not ""tool"".



Your phrase ""Why parse and unparse data formats?"" gives me some concern. I mean do you have
a choice?

So I'm assuming you have to use the data, so the issue is why use DFDL vs. other ways of using
the data:



The reasons to use DFDL are:

  *   it is an emerging open standard. In the long run standards give users power over vendors,
reduce costs, increase skills leverage, etc.
  *   it is comprehensive - can handle everything from military messaging formats to COBOL,
binary and text and mixtures thereof.  There are a few things it cannot describe as yet (TIFF
for example), but it will evolve to cover those as well.
  *   it has superior unparsing capability to any existing data format description system
- this is one area where the DFDL standard advances the state-of-the-art.
  *   there are multiple implementations including open-source and commercial.

Some additional related points:

  *   why use DFDL to parse/unparse  to/from an alternative standard textual form such as
JSON or XML ?

     *   Note that DFDL doesn't per-se require this. It is one common way to use the Daffodil
implementation
     *   Note that not all DFDL implementations even support this.  E.g., the ESA's DFDL4Space
tool doesn't convert to/from JSON or XML.  IBM DFDL can be used to convert data to XML, but
when used in the most common ways, it takes data directly to/from the native internal data
format used by that particular data-handling product/system. No intermediate step of XML/JSON
is used.

So I'd say the above point is really about Daffodil, not DFDL generally.  The above is about
skills leverage and tools leverage. JSON support is built into all javascript based platforms
such as web browsers, NODE.js, etc.  XML has standard tools available from vendors. DFDL is
another standard that adds capability to people with JSON/XML skills and/or tools already.
Use of a textual format as an intermediate form has significant QA benefits for most systems.

  *   Why learn and use the DFDL standard vs. some other approach?

     *   Such as any of the hundreds of data description tools/systems in the marketplace
- one of which commonly comes with any given enterprise software package.

        *   Note that most of these tools are quite declarative, so the ""be declarative"" argument
is orthogonal to the ""why DFDL"" argument.
        *   Note that many of these tools have much better user interfaces than Daffodil (today).
In the short run this may be a good reason not to use DFDL.

           *    In the long run we expect the power of open-source and standards to address
this.

     *   Such as just writing software code to handle/parse the data. The problem with this
is it is typically procedural, not declarative.

        *   DFDL is also not turing complete. It is far easier to show correctness of a schema
than a program.



Point (a) above is just the standards vs. non-standards argument.  At this point, I am digressing
into lots of points made in this slideshare deck in slides 3, 4, 5:

https://www.slideshare.net/mbeckerle/tresys-dfdl-data-format-description-language-daffodil-open-source-public-overview-100432615



Another point is about the benefits and power of standards. DFDL is simply better than existing
ad-hoc data format description languages in that it is far more comprehensive than most commercial
and other open-source systems, and it is an emerging open standard, with multiple implementations
with a good deal of demonstrated interoperability: https://cwiki.apache.org/confluence/display/DAFFODIL/Daffodil+Compatibility+with+IBM+DFDL



DFDL is still quite new, and I would expect some users to choose other things until Daffodil
gets out of Apache Incubator status, and the DFDL standard is fully ratified by Open Grid
Forum, and is proposed as a standard by a larger/more-recognized body.  The fact that IBM
has DFDL in multiple products now is a strong statement of support going forward.















________________________________

From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org>>
Sent: Sunday, October 20, 2019 8:08 AM
To: users@daffodil.apache.org<mailto:users@daffodil.apache.org> <users@daffodil.apache.org<mailto:users@daffodil.apache.org>>
Subject: Why use DFDL? Why parse and unparse data formats?



Hi Folks,



It occurred to me that in my tutorial I have explained what DFDL is and how to use DFDL, but
I never explained why DFDL should be used. Below is a slide that takes a stab at why. I am
sure there are other reasons. Would you provide other reasons, please? I think answering why
is a crucial thing. I consider this slide to be very important. /Roger



[cid:image001.png@01D58B3B.21404890]

",http://mail-archives.apache.org/mod_mbox/incubator-daffodil-users/201910.mbox/%3cSN6PR09MB3135FACFE99B81BD53870587C8650@SN6PR09MB3135.namprd09.prod.outlook.com%3e,"""Costello, Roger L."" <coste...@mitre.org>",0,0
220,221,"
Thanks lot Adam! Also for the halign in tr:panelButtonBar. I owe you a beer for that. :)

While the tr:panelButtonBar's halign is doing great, I got a problem with the tr:table's actions
facet: Since the navigation bar is only displayed if there are more then ""rows"" entries to
display, my action-facet contents disappear if there are to few rows in the table. I would
generally prefer to have the navigaton bar always displayed when the rows attribute is >
0, independent of the current count of rows to display. Navigation controls could be disabled
if the number of rows in the model is less then the ""rows"" attribute, but IMHO they should
be displayed to have the page optically more ""stable"" for the user.

Regards,
Thorsten

-----Ursprüngliche Nachricht-----
Von: Adam Winer [mailto:awiner@gmail.com] 
Gesendet: Donnerstag, 12. Oktober 2006 19:30
An: adffaces-user@incubator.apache.org
Betreff: Re: Re: Re: decorateCollection component

It just did. :)  SVN revision 463333.

Hrm, it should be at http://incubator.apache.org/adffaces/,
but that site is out-of-date, *and* the Javadoc doesn't
work...  I'll ask Matthias what's up with that.

-- Adam


On 10/12/06, Brian Smith <unobriani@gmail.com> wrote:
> Cool thanks Adam, is the new ""actions"" facet going to make it into the Trunk
> build?  Also on a side note, do you know where a javadoc for trinidad can be
> found (if there is one)?
> Thanks,
> -Brian
>
> On 10/12/06, Adam Winer <awiner@gmail.com> wrote:
> >
> > I'm coding this up now as an ""actions"" facet on the table. We had this
> > back in ADF Faces, then got rid of it - I forget why - but it's
> > definitely useful here.  It'll be in there shortly.
> >
> > -- Adam
> >
> >
> > On 10/12/06, Brian Smith <unobriani@gmail.com> wrote:
> > > Thanks Adam, I am working with a build from the trunk, (built
> > yesterday).
> > > Do you know when/if this component is going to make it in the
> > build?  Rough
> > > guess?  Is there an alternative for now?  I guess I could just use the
> > table
> > > header facet but it is not quite what I am wanting.  I am placing a
> > > selectOneChoice box on the table to select the number of rows visble.  I
> > > would like it inline with the range navigation stuff.
> > >
> > > Thanks,
> > > -Brian
> > >
> > > On 10/12/06, Adam Winer <awiner@gmail.com> wrote:
> > > >
> > > > Brian,
> > > >
> > > > No, you're not missing anything.  It's missing, and needs to be
> > > > added;  it looks like it just didn't make it in the open-source drop.
> > > >
> > > > -- Adam
> > > >
> > > >
> > > > On 10/11/06, Brian Smith <unobriani@gmail.com> wrote:
> > > > > I am trying to convert some adf faces pages over to trinidad.  I
was
> > > > using
> > > > > the ""actions"" facet of the af:table.  In the ""API Changes from
> > Oracle's
> > > > ADF
> > > > > Faces"" doc here
> > http://incubator.apache.org/adffaces/api-changes.html
> > > >   It
> > > > > says to use the ""toolbar"" facet of the decorateCollection
> > component.  I
> > > > > can't seem to find this component is the latest build.  Has it been
> > > > renamed
> > > > > or am I missing some other way to do what I am wanting?
> > > > > Thanks,
> > > > > Brian
> > > > >
> > > > >
> > > >
> > >
> > >
> >
>
>



",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200610.mbox/%3cAE05CA843975CA47BF5D9D631E5B76DE1C330B@dc01.euregio-dc.de%3e,"Günther, Thorsten <Thorsten.Guent...@medocino.de>",0,0
80,81,"This has just been checked into the repository. Check it out. The setup 
instructions are in docs/java_setup.html.

Shanti

Harold Lim wrote:
> Hi All,
>
> When will the Java Implementation of Olio will be released?
>
> Also, will there be possibly implementations that use HBase in the future? 
>
> I am currently studying the viability of using a completely distributed file systems
(HBase + HDFS) for Web 2.0 applications.
>
>
>
> Thanks,
> Harold
>
>
>       
>   

",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200905.mbox/%3c4A09D7CC.7060907@sun.com%3e,Shanti Subramanyam <Shanti.Subraman...@Sun.COM>,0,0
285,286,"I think this sounds like a great idea, though having not had to set up
connections in a while it's a little hard for me to picture (though I do
remember the pain of figuring out the very first Google Cloud connectors).
Could provide a little example?

For example, would I just define a connection as a string, like
""mongo://user@password/h.o.s.t:port/database"" and trust a plugin to
register as a ""mongohandler"" and turn that URI into a connection? To Alex's
point, perhaps more complex needs could be handled as query parameters
(today they are just handwritten JSON in ""extras"", so that's not much of a
change): ""gcp://project?keyfile=path/to/keyfile"" Just shooting out some
ideas...

Thanks!

On Mon, Jan 9, 2017 at 3:44 PM George Leslie-Waksman
<george@cloverhealth.com.invalid> wrote:

Could registering new types be handled through the plugin infrastructure?

On Mon, Jan 9, 2017 at 5:14 AM Alex Van Boxel <alex@vanboxel.be> wrote:

> I was actually going to propose something different with entry-points, but
> your requirement beat me to it (but that's ok :-). Actually I think with
> this mechanism people would be able to extend Airflow connection mechanism
> (and later other stuff) by doing *pip install airflow-sexy-new-connection*
> (for example).
>
> On Mon, Jan 9, 2017 at 1:39 PM Gael Magnan <gaelmagnan@gmail.com> wrote:
>
> > Thank you for the read, I'm gonna look at it, it's probably gonna be
> better
> > that what I have.
> >
> > Point taken about the URI, I'll see if i can find something generic
> enough
> > to handle all those cases.
> >
> > Le lun. 9 janv. 2017 à 13:36, Alex Van Boxel <alex@vanboxel.be> a écrit
> :
> >
> > > Thanks a lot, yes it clarifies a lot and I do agree you really need to
> > hack
> > > inside Airflow to add a Connection type. While you're working at this
> > could
> > > you have a look at the standard python *entry-point mechanism* for
> > > registering Connection types/components.
> > >
> > > A good read on this:
> > >
> > >
> >
>
http://docs.pylonsproject.org/projects/pylons-webframework/en/latest/advanced_pylons/entry_points_and_plugins.html
> > >
> > > My first though would be that just by adding an entry to the factory
> > method
> > > would be enough to register your Connection + ConnectionType and UI.
> > >
> > > Also note that not everything works with a URI. The Google Cloud
> > Connection
> > > doesn't have one, it uses a secret key file stored on disk, so don't
> > force
> > > every connection type to work with URI's.
> > >
> > >
> > >
> > > On Mon, Jan 9, 2017 at 1:15 PM Gael Magnan <gaelmagnan@gmail.com>
> wrote:
> > >
> > > > Yes sure,
> > > >
> > > > The question was the following:
> > > > ""I was looking at the code of the connections, and I realized you
> can't
> > > > easily add a connection type without modifying the airflow code
> > source. I
> > > > wanted to create a mongodb connection type, but I think the best
> > approche
> > > > would be to refactor connections first. Thoughts anyone?""
> > > >
> > > > The answer of Bolke de Bruin was: ""making it more generic would be
> > > > appreciated""
> > > >
> > > > So basically the way the code is set up actually every types of
> > > connection
> > > > existing is defined though a list in the Connection class. It
> > implements
> > > > exactly the same code for parsing uri to get connections info and
> > doesn't
> > > > allow for a simple way to get back the uri from the connection
infos.
> > > >
> > > > I need to add a mongodb connection and a way to get it back as a
uri,
> > so
> > > i
> > > > could use an other type of connection and play around with that or
> > juste
> > > > add one more hard coded connection type, but I though this might be
> > > > something that comes back regularly and having a simple way to plug
> in
> > > new
> > > > types of connection would make it easier for anyone to contribute a
> new
> > > > connection type.
> > > >
> > > > Hope this clarifies my proposal.
> > > >
> > > > Le lun. 9 janv. 2017 à 12:46, Alex Van Boxel <alex@vanboxel.be>
a
> > écrit
> > > :
> > > >
> > > > > Hey Gael,
> > > > >
> > > > > could you please recap the question here and provide some context.
> > Not
> > > > > everyone on the mailinglist is actively following Gitter,
including
> > me.
> > > > > With some context it would be easier to give feedback. Thanks.
> > > > >
> > > > > On Mon, Jan 9, 2017 at 11:15 AM Gael Magnan <gaelmagnan@gmail.com>
> > > > wrote:
> > > > >
> > > > > > Hi,
> > > > > >
> > > > > > following my question on gitter the other day and the response
> from
> > > > Bolke
> > > > > > de Bruin, I've started working on refactoring the connections
in
> > > > airflow.
> > > > > >
> > > > > > Before submitting a PR I wanted to share my proposal with you
and
> > get
> > > > > > feedbacks.
> > > > > >
> > > > > > The idea is quite simple, I've divided the Connection class
in
> two,
> > > > > > Connection and ConnectionType, connection has the same interface
> it
> > > had
> > > > > > before plus a few methods, but the class keeps a reference to
a
> > > > > dictionary
> > > > > > of registered ConnectionType. It delegates the work of parsing
> from
> > > > URI,
> > > > > > formatting to URI (added) and getting the hook to the
> > ConnectionType
> > > > > > associated with the conn_type.
> > > > > >
> > > > > > I've thought of two ways of registering new ConnectionTypes,
the
> > > first
> > > > is
> > > > > > making the BaseConnectionType use a metaclass that registered
any
> > new
> > > > > > ConnectionType with Connection when the class is declared, it
> would
> > > > > require
> > > > > > the less work to extend the connection module, as just importing
> > the
> > > > file
> > > > > > with the connection would do the trick.
> > > > > > The second one is juste to have a function/classmethod that
you
> > call
> > > > > > manually to register your connection. It would be simpler to
> > > understand
> > > > > but
> > > > > > requires more work every time you create a new ConnectionType.
> > > > > >
> > > > > > Hope this proposal is clear enough, and I'm waiting for feebacks
> > and
> > > > > > possible improvements.
> > > > > >
> > > > > > Regards
> > > > > > Gael Magnan de Bornier
> > > > > >
> > > > > --
> > > > >   _/
> > > > > _/ Alex Van Boxel
> > > > >
> > > >
> > > --
> > >   _/
> > > _/ Alex Van Boxel
> > >
> >
> --
>   _/
> _/ Alex Van Boxel
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201701.mbox/<CADsgxrG28Qaf6G=Rn71p5WgSu+N7QVYMcN6oK0OfG6HVosMpUg@mail.gmail.com>,Jeremiah Lowin <jlo...@apache.org>,0,1
183,184,"Mentors,
Question. Why is that we needed to move to JIRA, yet INFRA has allowed for
the creation (but not closing) of Github issues in Github? There is no
workflow or way for us to close accidently created GitHub issues.

-s


---------- Forwarded message ----------
From: Siddharth Anand (JIRA) <jira@apache.org>
Date: Fri, May 20, 2016 at 3:24 AM
Subject: [jira] [Updated] (AIRFLOW-151) trigger_rule='one_success' not
allowing tasks downstream of a BranchPythonOperator to be executed
To: commits@airflow.incubator.apache.org



     [
https://issues.apache.org/jira/browse/AIRFLOW-151?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
]

Siddharth Anand updated AIRFLOW-151:
------------------------------------
    External issue URL:
https://github.com/apache/incubator-airflow/issues/1521

> trigger_rule='one_success' not allowing tasks downstream of a
BranchPythonOperator to be executed
>
-------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-151
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-151
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Siddharth Anand
>            Assignee: Siddharth Anand
>
> https://github.com/apache/incubator-airflow/issues/1521



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201605.mbox/<CANLtMidEdKKwMHO3NTgNTko21RcgwM2BkeCHF36hd+qJq_spWA@mail.gmail.com>,siddharth anand <r39...@gmail.com>,0,1
152,153,"-1.

Not sure if these have been called out earlier.

For all the bundled files with different licenses (MIT, BSD, etc), the full
texts of these licenses should be in the source tarball preferably at the
end of the LICENSE file.
webgl-2d needs to be called out as MIT license.
Version in pkg-info has an rc0 notation. It should just be
1.8.1-incubating.
A bunch of files under apache_airflow.egg-info/ and scripts/systemd/ need a
license header
Likewise for airflow/www/templates/airflow/variables/README.md

Nice to have:
Fix the top-level dir in the tarball to be
""apache-airflow-1.8.1-incubating"" instead of
""apache-airflow-1.8.1rc0+apache.incubating""

For all the other binary files (images, gifs), is there source provenance
for all of them and that all of them are covered by the licenses in the
LICENSE file?

Last point - are all the entries in the NOTICE file required or do they
just need to be in the LICENSE file? Any additions to the NOTICE have
downstream repercussions as they need to be propagated down by any other
project using airflow.

thanks
-- Hitesh



On Mon, Apr 17, 2017 at 11:24 AM, Chris Riccomini <criccomini@apache.org>
wrote:

> Dear All,
>
> I have been able to make the Airflow 1.8.1 RC0 available at:
> https://dist.apache.org/repos/dist/dev/incubator/airflow, public keys are
> available at https://dist.apache.org/repos/dist/release/incubator/airflow.
>
> Issues fixed:
>
> [AIRFLOW-1062] DagRun#find returns wrong result if external_trigg
> [AIRFLOW-1054] Fix broken import on test_dag
> [AIRFLOW-1050] Retries ignored - regression
> [AIRFLOW-1033] TypeError: can't compare datetime.datetime to None
> [AIRFLOW-1030] HttpHook error when creating HttpSensor
> [AIRFLOW-1017] get_task_instance should return None instead of th
> [AIRFLOW-1011] Fix bug in BackfillJob._execute() for SubDAGs
> [AIRFLOW-1001] Landing Time shows ""unsupported operand type(s) fo
> [AIRFLOW-1000] Rebrand to Apache Airflow instead of Airflow
> [AIRFLOW-989] Clear Task Regression
> [AIRFLOW-974] airflow.util.file mkdir has a race condition
> [AIRFLOW-906] Update Code icon from lightning bolt to file
> [AIRFLOW-858] Configurable database name for DB operators
> [AIRFLOW-853] ssh_execute_operator.py stdout decode default to A
> [AIRFLOW-832] Fix debug server
> [AIRFLOW-817] Trigger dag fails when using CLI + API
> [AIRFLOW-816] Make sure to pull nvd3 from local resources
> [AIRFLOW-815] Add previous/next execution dates to available def
> [AIRFLOW-813] Fix unterminated unit tests in tests.job (tests/jo
> [AIRFLOW-812] Scheduler job terminates when there is no dag file
> [AIRFLOW-806] UI should properly ignore DAG doc when it is None
> [AIRFLOW-794] Consistent access to DAGS_FOLDER and SQL_ALCHEMY_C
> [AIRFLOW-785] ImportError if cgroupspy is not installed
> [AIRFLOW-784] Cannot install with funcsigs > 1.0.0
> [AIRFLOW-780] The UI no longer shows broken DAGs
> [AIRFLOW-777] dag_is_running is initlialized to True instead of
> [AIRFLOW-719] Skipped operations make DAG finish prematurely
> [AIRFLOW-694] Empty env vars do not overwrite non-empty config v
> [AIRFLOW-139] Executing VACUUM with PostgresOperator
> [AIRFLOW-111] DAG concurrency is not honored
> [AIRFLOW-88] Improve clarity Travis CI reports
>
> I would like to raise a VOTE for releasing 1.8.1 based on release candidate
> 0, i.e. just renaming release candidate 0 to 1.8.1 release.
>
> Please respond to this email by:
>
> +1,0,-1 with *binding* if you are a PMC member or *non-binding* if you are
> not.
>
> Vote will run for 72 hours (ends this Thursday).
>
> Thanks!
> Chris
>
> My VOTE: +1 (binding)
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201704.mbox/<CAOgkC2MxwNBk0DCA1qxGbW0ZMfp6k5ojV6OusFeZ53xV6yM68A@mail.gmail.com>,Hitesh Shah <hit...@apache.org>,1,1
209,210,"Hi,
I searched in the documentation for a way to limit a specific task
concurrency to 1,
but didn't find a way.
I thought that 'depends_on_past' should achieve this goal, but I want the
task to run even if the previous task failed - just to be sure the they
don't run in parallel.

The task doesn't have a downstream task, so I can't use
'wait_for_downstream'.

Am I Missing something?

Thanks,
Hila

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201608.mbox/<CACgKEnjRUA+bwd_t+Kq-9vbdQJUvWKRcCLh+0hgBfti24ByqPQ@mail.gmail.com>,הילה ויזן <hila...@gmail.com>,0,0
305,306,"In Airflows main window, click at the DAG and you see the graph view. Clicking on one of the
tasks should open a window with the possibility to “Run” that task. When you use the first
task, then the whole DAG will be executed (given no errors occur).

I hope that’s what you’re looking for 😊

Cheers, 
 --alex

--
B: mapredit.blogspot.com

From: twinkle sachdeva
Sent: Friday, October 21, 2016 12:09 PM
To: wget.null@gmail.com
Cc: dev@airflow.incubator.apache.org
Subject: Re: Regarding force running the whole graph from UI

Hi Alex,

We are using Celery Executor.
I am not able to identify how can i do it from UI. Is it possible?

Command line is awesome.

Regards,
Twinkle


On Fri, Oct 21, 2016 at 3:36 PM, <wget.null@gmail.com> wrote:
Hey Twinkle,
 
Triggering DAG’s per UI works only by using the CeleryExecutor in airflow.cfg, probably
with the mesos one, too. Both are execute tasks remotely. 
 
P.S: Personally I favorite LocalExecutor and trigger DAG’s per CLI. 
 
Cheers,
--alex
 
--
B: mapredit.blogspot.com
 
From: twinkle sachdeva
Sent: Friday, October 21, 2016 11:58 AM
To: wget.null@gmail.com
Cc: dev@airflow.incubator.apache.org
Subject: Re: Regarding force running the whole graph from UI
 
Hi Alex, 
 
I mean the complete DAG.
In technical terms, being able to run 'trigger_dag' command from the UI.
 
Regards,
Twinkle
 
 
On Fri, Oct 21, 2016 at 2:47 PM, <wget.null@gmail.com> wrote:
Hey,
 
Something like this:
https://pythonhosted.org/airflow/cli.html ?
 
What do you mean with the whole graph? The complete DAG, or a task from a specific DAG? 
 
--alex
 
--
B: https://mapredit.blogspot.com
 
From: twinkle sachdeva
Sent: Friday, October 21, 2016 8:30 AM
To: dev@airflow.incubator.apache.org
Subject: Regarding force running the whole graph from UI
 
Hi,
 
Is there a way by which we can force run the whole graph from Airflow UI?
 
Also, is there any documentation available regarding all the options which
are there in the pop-up dialog for running the graph?
 
Thanks & Regards,
Twinkle
 
 
 



",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<5809eaa8.4a4cc20a.9f52e.a114@mx.google.com>,<wget.n...@gmail.com>,0,0
230,231,"Hi Matthieu, can you share the test report for 200,000 events/s

e.g.

Server: 2? Mem: 4G?
Node: 4


Running cost:
Cpu ?%, mem ?%

I will refer to this  and compare it, thanks first.
From: Sky Zhao [mailto:sky.zhao@ericsson.com]
Sent: Wednesday, June 26, 2013 3:47 PM
To: 's4-user@incubator.apache.org'
Subject: RE: About 200,000 events/s

Also I noticed the cpu is very high almost 100%, but mem<10%, whether it still says too
many IO operation or other causes?


/Sky

From: Sky Zhao [mailto:sky.zhao@ericsson.com]
Sent: Wednesday, June 26, 2013 10:09 AM
To: 's4-user@incubator.apache.org'
Subject: RE: About 200,000 events/s

Thanks Matthieu very careful suggestions, your direction is right.

The main reason is serialization/deserlization problems,

1)      I changed DataEvent (which extends Event and javaBean) into Event, using default Event
to send s4 event in adapter

2)      In s4 app,  create new dataEvent in memory and put stream into next PE,

Then performance improve a lot, up to 200,000 events/20s maxium, think there still has space
to improve, I keep checking, thanks Matthieu.



From: Matthieu Morel [mailto:mmorel@apache.org]
Sent: Tuesday, June 25, 2013 11:44 PM
To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org>
Subject: Re: About 200,000 events/s

>From what I see in your code, the problem might be in the definition of keys. In your
GenKeyFinder, you use the event timestamp in the key, and therefore you might be creating
a new PE instance for every single event! (unless the timestamp you set is somehow repeated,
which sounds peculiar).

I would suggest to modify the keyfinder in a more suitable way. Probably by removing the timestamp
from the key. In the twitter example for instance, the key is the topic of the tweet, and
we aggregate counts by topic.

Hope this helps,

Matthieu

On Jun 25, 2013, at 14:39 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com>>
wrote:

So I only guess,
The serialization/deserlization costs much time, and occupy some limited memory,
Once the serialization/deserlization upp bound is max, it will occupy much memory, the events
starts to be blocked, so more memory in JVM and less (de)serlization, the performance could
be more events for general.


From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com>]
Sent: Tuesday, June 25, 2013 8:15 PM
To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org>'
Subject: RE: About 200,000 events/s


I use 4g memory to handle the events, so I feel s4 eat more memory and cpus and server numbers.
How many servers and cpu and memory to handle 200,000 events?


/Sky


From: Sky Zhao [mailto:sky.zhao@ericsson.com]
Sent: Tuesday, June 25, 2013 7:08 PM
To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org>'
Subject: RE: About 200,000 events/s

I list my code here, in my app, I created 3 PEs(the logic is very simple, just emit stream)


               @Override
               protected void onInit() {

                              CsvReporter.enable(new File(mpath), 20, TimeUnit.SECONDS);

                              // create a prototype
        EntryPE entryPE = createPE(EntryPE.class);


        createInputStream(""seaRawStream"", new KeyFinder<Event>() {

            @Override
            public List<String> get(Event event) {
               return Arrays.asList(new String[] { event.get(""seadata"") });
            }
        }, entryPE);


        ResultPE resultPE = createPE(ResultPE.class);
               Stream<DataEvent> processStream = createStream(""Process Stream"", new
GenKeyFinder(), resultPE);
               processStream.setParallelism(Integer.parseInt(thread));


        ProcessPE processPE = createPE(ProcessPE.class);
        processPE.setDataStream(processStream);



        Stream<DataEvent> entryStream = createStream(""Entry Stream"", new GenKeyFinder(),
processPE);
        entryStream.setParallelism(Integer.parseInt(thread));

               entryPE.setStreams(entryStream);
               }



The event data is
String kpi_name;
               String mo_name;
               double kpi_value;
               long timestamp;

               public DataEvent() {

               }

               public DataEvent(String kpi_name, String mo_name, double kpi_value,
                                             long timestamp) {
                              this.kpi_name = kpi_name;
                              this.mo_name = mo_name;
                              this.kpi_value = kpi_value;
                              this.timestamp = timestamp;

               }

....
Get/set methods


Keyfinder:

public class GenKeyFinder implements KeyFinder<DataEvent> {


    public List<String> get(DataEvent event) {

        List<String> results = new ArrayList<String>();

        /* Retrieve the kpi_name,mo_name and add them to the list. */
        results.add(event.getKpi_name()+"":""+event.getMo_name()+"":""+String.valueOf(event.getTimestamp()));

        return results;
    }
}



=====adapter part code


                                                                           // ////////sending
to s4
                                                                           DataEvent event
= new DataEvent(kpi_name, mo_name,
                                                                                         
               kpi_value, timestamp);
                                                                           event.put(""seadata"",
String.class, kpi_name+"":""+""mo_name""+"":""+timestamp);
                                                                           rstream.put(event);


whether the keyfinder or event key impact the event sending/receiving?
From: Matthieu Morel [mailto:mmorel@apache.org]
Sent: Tuesday, June 25, 2013 5:03 PM
To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org>
Subject: Re: About 200,000 events/s

Not sure what is the issue in your setting. Performance issues in stream processing can be
related to I/O or GC. But the app design can have a dramatic impact as well. Have a look at
your CPU usage as well.

You might want to profile the app and adapter to identify the culprit in your application.

Another path to explore is related to the content of events. Serialization/deserialization
may be costly for complex objects. What kind of data structure are you keeping in the events?


On Jun 25, 2013, at 09:56 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com>>
wrote:

Also, can you teach me how to find or trace the block place, I am still confused why and where
is block?

What are you referring to?


How many nodes in twitter example for up to 200,000 events/s?

You'll never get to that number with that application, since the twitter sprinkler feed is
~ 1% of the total feed, and the reported peak rate of the total feed is a few tens of thousands
of tweets / s

But if you were to read from a dump, I'd say a few nodes for the adapter and a few nodes for
the app.


Matthieu



From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com/>]
Sent: Tuesday, June 25, 2013 10:25 AM
To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org>'
Subject: RE: About 200,000 events/s

Hi Matthieu, I tried to test again after modifying some configuration codes, see below, no
any PE logic, just send events(only spend 38s for adapter sending events) don't know where
is blocked?



From: Matthieu Morel [mailto:mmorel@apache.org]
Sent: Tuesday, June 25, 2013 12:25 AM
To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org>
Subject: Re: About 200,000 events/s

Hi,

I would suggest to:

1/ check how much you can generate when creating events read from the file - without event
sending to a remote stream. This gives you the upper bound for a single adapter (producer)

It costs 38s for only file-read from adapters


2/ check how much you can consume in the app cluster. By default the remote senders are blocking,
i.e. the adapter won't inject more than what the app cluster can consume. This gives you an
upper bound for the consumer.
I removed all PE logic, just emit functions, very strange, it still cost 600s, seems somewhere
blocking


3/ use more adapter processes. In the benchmarks subprojects, one can configure the number
of injection processes, and you might need more than one
I tried, seems improve a bit, but not obivouse, 2500 events/s maximum.

4/ make sure the tuning parameters you are setting are appropriate. For instance, I am not
sure using 100 threads for serializing events is a good setting (see my notes about context
switching in a previous mail).
Already changed to 10 threads

Also note that 200k msg/s/stream/node corresponds to the average rate in one minute _once
the cluster has reached stable mode_. Indeed JVMs typically perform better after a while,
due to various kinds of dynamic optimizations. Do make sure you experiments are long enough.

Here is metrics report, already run 620s(NO pe logic,)
List event-emitted@seacluster1@partition-0.csv<mailto:event-emitted@seacluster1@partition-0.csv>
file

# time,count,1 min rate,mean rate,5 min rate,15 min rate
20,30482,728.4264567247864,1532.1851387286329,577.7027401449616,550.7086872323928
40,68256,1003.4693068790475,1710.158498360041,651.1838653093574,576.4097533046603
60,126665,1526.423951717675,2114.1420782852447,792.5372602881871,626.2019978354255
80,159222,1631.4770294401224,1992.409242530607,865.8704071266087,654.9705320161032
100,206876,1821.551542703833,2070.5009200329328,958.5809893144597,691.1999843184435
120,245115,1852.9758861695257,2044.0217493195803,1021.5173199955651,718.5310141264627
140,286057,1892.8858688327914,2044.444972093294,1084.2498192357743,746.5688788373484
160,324662,1952.4301928970413,2030.146074235012,1151.037578221836,776.8090098793148
180,371829,1959.4350067303067,2066.6134765239876,1202.907690109737,802.6284106918696
200,433557,2283.734039729985,2168.043356631868,1325.4143582100112,853.1681426416412
220,464815,2165.047817425384,2113.0019773033414,1362.4434184467839,876.2949286924105
240,504620,2065.9291371459913,2102.751240490619,1390.829435765405,896.605433400642
260,558158,2192.29985605397,2146.9020962821564,1462.6363950595012,931.9095985316526
280,595910,2199.1021924478428,2128.3677435991726,1513.937239488627,961.2098181907241
300,627181,1994.0377201896988,2090.694355349902,1511.0411290249551,972.3477079289204
320,664342,1959.8933962067783,2076.1406486970463,1534.4815810411815,992.1780521538831
340,698717,1912.6664972668054,2055.1073335596802,1551.413320562825,1009.8797759279687
360,738468,1940.3861933767728,2051.3430577192726,1579.897070517331,1031.4220907479778
380,769673,1808.904535049888,2025.4290025114635,1577.8601886321867,1045.7495364549434
400,807406,1834.34177336605,2018.480153382513,1597.9097843354657,1064.241627735365
420,851009,1918.7250831019285,2026.1694030316874,1634.835168023956,1088.691609058512
440,884427,1854.4469403637247,2010.0049048826818,1637.4121396194184,1101.510206100955
460,927835,1951.4786440268247,2016.9731382075943,1672.0801089577474,1125.0229080682857
480,975961,2076.6338294064494,2033.1853468961958,1719.2462212252974,1153.158261927912
500,1018777,2094.823218886582,2037.4852753404898,1746.4288709310954,1174.8624939692247
520,1063733,2137.998709779158,2045.5028969721004,1778.8271760046641,1198.4702839108422
540,1105407,2121.91619308999,2046.9043296729049,1798.2960826750962,1217.8576817035882
560,1148338,2126.4127152635606,2050.4509561002797,1820.616007231527,1238.2444234074262
580,1189999,2113.61602051879,2051.570290715584,1837.4446097375073,1256.7786337375683
600,1219584,1937.6445710562814,2031.6753069957329,1814.671731059335,1261.7477397513687
620,1237632,1632.4506998235333,1995.2499874637524,1755.4487350018405,1253.8445924435982



Seems very strange value for my example, far from 200,000 events/s/node/stream


Regards,

Matthieu


On Jun 24, 2013, at 11:19 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com>>
wrote:

I try to use Adapter to send s4 events. With metrics report,
20,10462,88.63259092217602,539.6449108859357,18.577650313690874,6.241814566462701
40,36006,417.83633322358764,914.1057643161282,97.55624823196746,33.40088245418529
60,63859,674.1012974987167,1075.2326549158463,176.33878995148274,61.646803531230724
80,97835,953.6282787690939,1232.2934375999696,271.48890371088254,96.56144395108957
100,131535,1162.2060916405578,1323.3704459079934,363.98505627735324,131.98430793014757
120,165282,1327.52314133145,1384.2675551261093,453.5195236495672,167.61679021575551
140,190776,1305.7285112621298,1368.4361242524062,504.7782182758366,191.36049732440895

20,000 events per 20s  => 1000 EVENTS/s

Very slow, I modify the S4_HOME/subprojects/s4-comm/bin/default.s4.comm.properties

s4.comm.emitter.class=org.apache.s4.comm.tcp.TCPEmitter
s4.comm.emitter.remote.class=org.apache.s4.comm.tcp.TCPRemoteEmitter
s4.comm.listener.class=org.apache.s4.comm.tcp.TCPListener

# I/O channel connection timeout, when applicable (e.g. used by netty)
s4.comm.timeout=1000

# NOTE: the following numbers should be tuned according to the application, use case, and
infrastructure

# how many threads to use for the sender stage (i.e. serialization)
#s4.sender.parallelism=1
s4.sender.parallelism=100
# maximum number of events in the buffer of the sender stage
#s4.sender.workQueueSize=10000
s4.sender.workQueueSize=100000
# maximum sending rate from a given node, in events / s (used with throttling sender executors)
s4.sender.maxRate=200000

# how many threads to use for the *remote* sender stage (i.e. serialization)
#s4.remoteSender.parallelism=1
s4.remoteSender.parallelism=100
# maximum number of events in the buffer of the *remote* sender stage
#s4.remoteSender.workQueueSize=10000
s4.remoteSender.workQueueSize=100000
# maximum *remote* sending rate from a given node, in events / s (used with throttling *remote*
sender executors)
s4.remoteSender.maxRate=200000

# maximum number of pending writes to a given comm channel
#s4.emitter.maxPendingWrites=1000
s4.emitter.maxPendingWrites=10000

# maximum number of events in the buffer of the processing stage
#s4.stream.workQueueSize=10000
s4.stream.workQueueSize=100000

only improve from 500 events 1000 events,

I read file 88m only need 8s, but send events cost 620s now for 1,237,632 events, why slow,
s4 can trigger 200,000 events/s, how I can do up to this values, pls give me detail instructions.





",http://mail-archives.apache.org/mod_mbox/incubator-s4-user/201306.mbox/%3c387873ABC9A55249A6FD608D4DFCEA3516707864@ESGSCMB101.ericsson.se%3e,Sky Zhao <sky.z...@ericsson.com>,0,0
274,275,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/594#issuecomment-35004540
  
    /cc @tdas


",http://mail-archives.apache.org/mod_mbox/spark-dev/201402.mbox/<20140213174138.481698A9142@tyr.zones.apache.org>,pwendell <...@git.apache.org>,0,0
18,19,"     [ https://issues.apache.org/jira/browse/PROVISIONR-31?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Andrei Savu updated PROVISIONR-31: ----------------------------------      Attachment: PROVISIONR-31.patch  I've spent 10 minutes on this fixing the violations. I'm attaching an updated version of the patch. Wdyt?                   > Add Apache Rat plugin for license auditing > ------------------------------------------ > >                 Key: PROVISIONR-31 >                 URL: https://issues.apache.org/jira/browse/PROVISIONR-31 >             Project: Provisionr >          Issue Type: Bug >          Components: Core >    Affects Versions: 0.4.0-incubating >            Reporter: Amandeep Khurana >            Assignee: Amandeep Khurana >             Fix For: 0.4.0-incubating > >         Attachments: PROVISIONR-31.patch, PROVISIONR-31.patch > > > Need to add rat plugin (http://creadur.apache.org/rat/)  -- This message is automatically generated by JIRA. If you think it was sent incorrectly, please contact your JIRA administrators For more information on JIRA, see: http://www.atlassian.com/software/jira",https://mail-archives.apache.org/mod_mbox/provisionr-dev/201306.mbox/raw/%3CJIRA.12646758.1368079656761.89329.1370710940458%40arcas%3E,"""Andrei Savu (JIRA)"" <j...@apache.org>",1,0
299,300,"Hi folks,
I have a dag that does propagate failures correctly in sequential executor:

https://www.dropbox.com/s/zh0quoj99e44qxh/Screenshot%202017-05-06%2012.54.10.png?dl=0

but does not propagate failures when using celery executor:

https://www.dropbox.com/s/mfxqhawwf0760gm/Screenshot%202017-05-06%2019.14.06.png?dl=0
Below is sample dag which I used to recreate the problem. I force the
failure in the dataops_weekly_update_reviews task by using a non-existent
keyword argument.

```
import airflow
import datetime
from airflow.operators.python_operator import PythonOperator
from airflow.models import DAG

args = {
    'owner': 'airflow',
    'start_date': datetime.datetime(2017, 5, 5),
    'queue': 'development'
}

dag = DAG(
    dag_id='example_dataops_weekly_reviews', default_args=args,
    schedule_interval=None)


def instantiate_emr_cluster(*args, **kwargs):
    return ""instantiating emr cluster""

task_instantiate_emr_cluster = PythonOperator(
    task_id=""instantiate_emr_cluster"",
    python_callable=instantiate_emr_cluster,
    provide_context=True,
    dag=dag)


def initialize_tables(*args, **kwargs):
    return ""initializing tables {}"".format(kwargs[""ds""])


task_initialize_tables = PythonOperator(
    task_id=""initialize_tables"",
    python_callable=initialize_tables,
    provide_context=True,
    dag=dag)


def dataops_weekly_update_reviews(*args, **kwargs):
    return ""UPDATING weekly reviews {}"".format(kwargs[""dsasdfdsfa""])


task_dataops_weekly_update_reviews = PythonOperator(
    task_id=""dataops_weekly_update_reviews"",
    python_callable=dataops_weekly_update_reviews,
    provide_context=True,
    dag=dag)


def load_dataops_reviews(*args, **kwargs):
    return ""loading dataops reviews""


task_load_dataops_reviews = PythonOperator(
    task_id=""load_dataops_reviews"",
    python_callable=load_dataops_reviews,
    provide_context=True,
    dag=dag)


def load_dataops_surveys(**kwargs):
    return ""Print out the running EMR cluster""


task_load_dataops_surveys = PythonOperator(
    task_id=""load_dataops_surveys"",
    provide_context=True,
    python_callable=load_dataops_surveys,
    dag=dag)


def load_cs_survey_answers(**kwargs):
    return ""load cs survey answers""


task_load_cs_survey_answers = PythonOperator(
    task_id=""load_cs_survey_answers"",
    provide_context=True,
    python_callable=load_cs_survey_answers,
    dag=dag)


def terminate_emr_cluster(*args, **kwargs):
    return ""terminate emr cluster""


task_terminate_emr_cluster = PythonOperator(
    task_id=""terminate_emr_cluster"",
    python_callable=terminate_emr_cluster,
    provide_context=True,
    trigger_rule=""all_done"",
    dag=dag)


task_initialize_tables.set_upstream(task_instantiate_emr_cluster)
task_dataops_weekly_update_reviews.set_upstream(task_initialize_tables)
task_load_dataops_reviews.set_upstream(task_dataops_weekly_update_reviews)
task_terminate_emr_cluster.set_upstream(task_load_dataops_reviews)
task_load_dataops_surveys.set_upstream(task_dataops_weekly_update_reviews)
task_terminate_emr_cluster.set_upstream(task_load_dataops_surveys)
task_load_cs_survey_answers.set_upstream(task_dataops_weekly_update_reviews)
task_terminate_emr_cluster.set_upstream(task_load_cs_survey_answers)

```

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201705.mbox/<CANUeqc1f+1FeDkj12mLFuTa_OKG0ruaFGOHXz0C5WvbCKf41-Q@mail.gmail.com>,Ali Naqvi <ali.na...@conornash.com>,0,0
75,76,">From memory, I think this is related to having the wrong authentication
method.
https://github.com/apache/incubator-airflow/blob/master/airflow/hooks/hive_hooks.py#L578

You may want to try NOSASL. To do that i think you have to put something
like `{ ""authMechanism"": ""NOSASL"" }` in your Connection's extra params.

On Wed, Oct 26, 2016 at 1:50 AM, twinkle sachdeva <
twinkle.sachdeva@gmail.com> wrote:

> Hi,
>
> I am trying to use HiveToMySqlTransfer operator, but I am not able to read
> any data with the following configuration:
>
> TSocket.py"", line 120, in read
>
>     message='TSocket read 0 bytes')
>
> thrift.transport.TTransport.TTransportException: TSocket read 0 bytes
>
> It seems to happen due to some mismatch in thrift protocol etc
> specification.
>
> Please help me on what can be done.
>
>
> Regards,
>
> Twinkle
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201610.mbox/<CAHEEp7UhcRDWS8uzwoTRZzhE-VdzuVcDsL9dzrAym-v18jdrdA@mail.gmail.com>,Maxime Beauchemin <maximebeauche...@gmail.com>,0,0
175,176,"Hi Mari,

Do you have a specific example that does class weighting?

libffm does not have such feature.
https://github.com/ycjuan/libffm

Sckit SGD [1] adjust y as follows:
""n_samples / (n_classes * np.bincount(y))""
[1] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html

I think this can easily be achieved using SQL.

Thanks,
Makoto

2019年10月31日(木) 19:38 Shadi Mari <shadimari@gmail.com>:
>
> Hello
> I am having extremely imbalanced dataset and trying to find support for class weights
in hivemall ffm classifier in specific, however couldnt find any mention in the docs.
>
> Is this feature supported, otherwise i had to go with negative downsampling.
>
> Please advise
>
> Thank you

",http://mail-archives.apache.org/mod_mbox/incubator-hivemall-user/201910.mbox/%3cCAGJoAU=b542u304nNF+1Zj-5tC4VywjwV_ybTT2PxALpdDGGBg@mail.gmail.com%3e,Makoto Yui <yuin...@gmail.com>,0,0
193,194,"May as well.  Technically headers have to be added to anything where
human creativity was involved (ie, not in generated source code), so
one could argue they're not necessary in __init__.pys.  A lot of
projects will use tools to verify that headers are present in source
files, it'll probably be easier to add the header to the empty files
than to configure the tool to ignore them.

-jg


On 15 June 2016 at 10:37, Ajay Yadav <ajaynsit@gmail.com> wrote:
> I can take a stab at writing a script and sending a pull request for #2.
> Should we add license headers to empty __init__.py as well? Shall I create
> a JIRA for it(I couldn't find an already existing one)?
>
> On Wed, Jun 15, 2016 at 1:17 AM, Jeremiah Lowin <jlowin@apache.org> wrote:
>
>> Related to #2, I am dusting off AIRFLOW-31 (
>> https://github.com/apache/incubator-airflow/pull/1272), which also
>> includes
>> about 50 license headers. I rebased it today and am waiting for travis to
>> finish...
>>
>>
>> On Tue, Jun 14, 2016 at 2:15 PM Bolke de Bruin <bdbruin@gmail.com> wrote:
>>
>> > Nice! Why don’t we also merge 1 into master then and fix issues from
>> > there? It will get wider exposure (We run master in our dev environments
>> > for example), it won’t put everything into the lab or Airbnb. Master
>> hardly
>> > should be considered production so imho it is allowed some time to settle
>> > before it is stable again.
>> >
>> > Does someone want to pick up 2 say by the end of week or next week to fix
>> > all headers?
>> >
>> > - Bolke
>> >
>> >
>> > > Op 14 jun. 2016, om 20:01 heeft Maxime Beauchemin <
>> > maximebeauchemin@gmail.com> het volgende geschreven:
>> > >
>> > > I think (1) is good to go. I can cherry pick it into our production to
>> > make
>> > > sure it's ready for release.
>> > >
>> > > (2) should be fine, git should be able to merge easily, otherwise it's
>> > > super easy to resolve any conflicts
>> > >
>> > > Max
>> > >
>> > > On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <
>> criccomini@apache.org>
>> > > wrote:
>> > >
>> > >> Hey Bolke,
>> > >>
>> > >> I think your list is good. (1) is what I'm most concerned about, as
it
>> > >> requires actually touching the code, and is blocking on graduation.
I
>> > >> *think* Max had a partial PR on that, but don't know the current
>> state.
>> > >>
>> > >> Re: (2), agree. Should just do a bulk PR for it.
>> > >>
>> > >> Cheers,
>> > >> Chris
>> > >>
>> > >> On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com>
>> > wrote:
>> > >>
>> > >>> Hi,
>> > >>>
>> > >>> I am wondering what needs to be done to get to an Apache release?
I
>> > think
>> > >>> now 1.7.1.3 is out we should be focused on getting one out as we
are
>> > kind
>> > >>> of half way the incubation process. What comes to my mind is:
>> > >>>
>> > >>> 1. Replace highcharts by D3 (WIP:
>> > >>> https://github.com/apache/incubator-airflow/pull/1469)
>> > >>> 2. Add license headers everywhere (TM) (Sucks, as it will break
many
>> > PRs
>> > >> -
>> > >>> but lets do it quickly)
>> > >>> 3. Have a review by Apache
>> > >>>
>> > >>> Anything I am missing?
>> > >>>
>> > >>> - Bolke
>> > >>
>> >
>> >
>>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201606.mbox/<CADiKvVv0OS8fVEoKTdD2k60s-7=UKUnnhDVbHE9g8GF6P2ZeyA@mail.gmail.com>,Jakob Homan <jgho...@gmail.com>,1,1
10,10,Github user michaelarusso commented on the issue:      https://github.com/apache/usergrid/pull/597        LGTM,http://mail-archives.apache.org/mod_mbox/usergrid-dev/201810.mbox/raw/%3C20181023031658.EBB6FDFC30%40git1-us-west.apache.org%3E,michaelarusso <...@git.apache.org>,0,0
226,227,"https://cwiki.apache.org/confluence/display/AIRFLOW/Contributor+Guide

Please add to it.. also share thoughts on what is unclear or missing.

-s

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201605.mbox/<CANLtMicCv6GQkNiHqQeJyz4ys4NUoYrEyV-NFBUt40O9V0ouOA@mail.gmail.com>,siddharth anand <san...@apache.org>,0,1
265,266,"Hi *

I'm using olio (PHP) with NetBeans 6.5.

Setting apache's DocumentRoot to olio/public_html works fine. However 
given a problem I have configuring the NetBeans project, I need to set 
the DocumentRoot one step above (at olio level and not public_html) but 
doing so the web app doesn't work anymore

/Looking in the error logs here is what I get:/

/PHP Fatal error: Class 'RequestUrl' not found in 
/var/apache2/2.2/htdocs/olio/public_html/index.php on line 38
/

Is it possible to chage the DocumentRoot, and if so is there soem 
tweaking to do in the PHP code to comply to the nrw DocumentRoot ?

Thanks for your help

Amir



",http://mail-archives.apache.org/mod_mbox/incubator-olio-user/200905.mbox/%3c4A0C07A4.2080606@sun.com%3e,Amir Javanshir <Amir.Javans...@Sun.COM>,0,0
195,196,"I wrote a document called 'Using XAP TreeView"" describing the TreeView
component, how to use it and a bit about XAP functionaility in general.
If there is interest I can post it as word or pdf.

 

Ben

 

Regards

 

 

Ben Bloch

Director, Community Development

Nexaweb Technologies

1 Van de Graaff, Burlington MA 01803

o 781.345.5449, c 617.834.1769

 


",http://mail-archives.apache.org/mod_mbox/incubator-xap-user/200706.mbox/%3cDC7475DE75130445B186DBCB617B4A7B0172C576@dhost002-43.dex002.intermedia.net%3e,"""Ben Bloch"" <bbl...@nexaweb.com>",0,1
252,253,"Hi,

I want to include custom filtering logic into process_file when loading
DAGs from dag_folder.

For that I need the file path and the DAG object that contains owner and
name. Based on these 3 pieces of information, I want to decide whether to
accept the DAG or not.

It is possible to modify airflow/models.py to add this custom logic, but
I'm looking for a way to accomplish that via configuration vs. code change.
Are there any suggestions how to address this (maybe by adding a plug-in or
a configurable expression)?

Thanks,
Thomas

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201704.mbox/<CA+5xAo0HO5pS_=_uMRv+YrD3W4naXTyauec8Znkd7APAfPxnew@mail.gmail.com>,Thomas Weise <...@apache.org>,1,0
54,55,">From: Adrian Gonzalez <adr_gonzalez@yahoo.fr> 
>
> Thank you very much everyone for your help on this topic. 
> 
> I'll have a look at Facelets (and perhaps Clay, but latter since it's less 
> used). 
> 
> I really have the impression that everyone (or quite everyone) using JSF is 
> using Facelets, so.... 
> 
> Just another question (last, I swear it !) : 
> Do you, JSF app developpers (and not component developers), use an IDE with drag 
> and drop capabilities (RAD kind of development) for facelets or no ? Which one ? 
> I'm just using IBM RAD 6, and it's really difficult (ibm proprietary components, 
> JSF 1.0, JSP...) 
>


You might also ask this one on the shale users list.  Ryan Wynn works with Websphere (IBM
Consulting) hangs out there [1].  Although, a Shale Clay enthusiast - something like 20 porlets
using Shale Clay.


[1] https://issues.apache.org/struts/browse/SHALE-402
 

Gary

> ----- Message d'origine ---- 
> De : Gary VanMatre 
> � : adffaces-user@incubator.apache.org 
> Envoy� le : Jeudi, 1 F�vrier 2007, 16h49mn 50s 
> Objet : Re: RE : AW: templating mecanism ? 
> 
> >From: Adrian Gonzalez 
> > 
> > Thank you very much 
> > But I really need to use JSP for rendering (not 
> > facelets) - my IDE doesn't support JSF design with 
> > facelets syntax. 
> > 
> 
> Another less popular alternative would be Shale Clay [1]. 
> It can be used with JSP and has a number of options. 
> 
> 
> 
> [1] http://shale.apache.org/shale-clay/index.html 
> 
> 
> > Do you know a templating solution for JSP engine ? 
> > 
> > --- D�ring Markus a 
> > �crit : 
> 
> 
> Gary 
> 
> > 
> > > Hello, 
> > > Trinidad comes with a buildin Facelets library and 
> > > Facelets has a very powerfull templating mechanism. 
> > > Just have a look at it and test if it's what you 
> > > need. 
> > > 
> > > Greetings 
> > > Markus 
> > > 
> > > 
> > > > -----Urspr�ngliche Nachricht----- 
> > > > Von: Adrian Gonzalez 
> > > [mailto:adr_gonzalez@yahoo.fr] 
> > > > Gesendet: Donnerstag, 1. Februar 2007 14:32 
> > > > An: adffaces-user@incubator.apache.org 
> > > > Betreff: templating mecanism ? 
> > > > 
> > > > Hello, 
> > > > 
> > > > I would like to know if there's a templating 
> > > mecanism 
> > > > provided by trinidad (or usable with trinidad) and 
> > > > usable for JSP rendering ? 
> > > > 
> > > > I've tried Shale Tiles extension with ADF Faces, 
> > > and 
> > > > it doesn't work, so I would like to know if 
> > > there's a 
> > > > substitute. 
> > > > 
> > > > Moreover I don't want to use sitemesh engine for 
> > > > composition (cause of the parsing performance 
> > > impact). 
> > > > 
> > > > The region tags enable composition but not 
> > > templating 
> > > > I think. 
> > > > 
> > > > Thanks 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > 
> > __________________________________________________________________________ 
> > > > _ 
> > > > D�couvrez une nouvelle fa�on d'obtenir des 
> > > r�ponses � toutes vos questions 
> > > > ! 
> > > > Profitez des connaissances, des opinions et des 
> > > exp�riences des 
> > > > internautes sur Yahoo! Questions/R�ponses 
> > > > http://fr.answers.yahoo.com 
> > > 
> > 
> > 
> > 
> > 
> > 
> > 
> > 
> > ___________________________________________________________________________ 
> > D�couvrez une nouvelle fa�on d'obtenir des r�ponses � toutes vos questions ! 
> > Profitez des connaissances, des opinions et des exp�riences des internautes 
> sur 
> > Yahoo! Questions/R�ponses 
> > http://fr.answers.yahoo.com 
> 
> 
> 
> 
> 
> 
> 
> 
> ___________________________________________________________________________ 
> D�couvrez une nouvelle fa�on d'obtenir des r�ponses � toutes vos questions ! 
> Profitez des connaissances, des opinions et des exp�riences des internautes sur 
> Yahoo! Questions/R�ponses 
> http://fr.answers.yahoo.com 
",http://mail-archives.apache.org/mod_mbox/incubator-adffaces-user/200702.mbox/%3c020220070355.26905.45C2B6490001B6A10000691922007507840A9D9B0E03020E9009@comcast.net%3e,gvanma...@comcast.net (Gary VanMatre),0,0
284,285,"Upgraded to RC1 in all environments this morning. So far so good.

On Fri, Feb 3, 2017 at 6:04 PM, Jeremiah Lowin <jlowin@apache.org> wrote:

> For what it's worth -- everything running smoothly after 24+ hours in a
> production(ish) environment.
>
> On Thu, Feb 2, 2017 at 11:25 PM Jayesh Senjaliya <jhsonline@gmail.com>
> wrote:
>
> > Thank You Bolke for all the efforts you are putting in !!
> >
> > I have deployed this RC now.
> >
> > On Thu, Feb 2, 2017 at 3:02 PM, Jeremiah Lowin <jlowin@apache.org>
> wrote:
> >
> > > Fantastic work on this Bolke, thank you!
> > >
> > > We've deployed the RC and will report if there are any issues...
> > >
> > > On Thu, Feb 2, 2017 at 4:32 PM Bolke de Bruin <bdbruin@gmail.com>
> wrote:
> > >
> > > > Now I am blushing :-)
> > > >
> > > > Sent from my iPhone
> > > >
> > > > > On 2 Feb 2017, at 22:05, Boris Tyukin <boris@boristyukin.com>
> wrote:
> > > > >
> > > > > LOL awesome!
> > > > >
> > > > > On Thu, Feb 2, 2017 at 4:00 PM, Maxime Beauchemin <
> > > > > maximebeauchemin@gmail.com> wrote:
> > > > >
> > > > >> The Apache mailing doesn't support images so here's a link:
> > > > >>
> > > > >> http://i.imgur.com/DUkpjZu.png
> > > > >> ​
> > > > >>
> > > > >> On Thu, Feb 2, 2017 at 12:52 PM, Boris Tyukin <
> > boris@boristyukin.com>
> > > > >> wrote:
> > > > >>
> > > > >>> Bolke, you are our hero! I am sure you put a lot of your
time to
> > make
> > > > it
> > > > >>> happen
> > > > >>>
> > > > >>> On Thu, Feb 2, 2017 at 2:50 PM, Bolke de Bruin <
> bdbruin@gmail.com>
> > > > >> wrote:
> > > > >>>
> > > > >>>> Hi All,
> > > > >>>>
> > > > >>>> I have made the (first) RELEASE CANDIDATE of Airflow
1.8.0
> > available
> > > > >> at:
> > > > >>>> https://dist.apache.org/repos/dist/dev/incubator/airflow/
,
> > public
> > > > >> keys
> > > > >>>> are available at
> > > > https://dist.apache.org/repos/dist/release/incubator/
> > > > >>>> airflow/ . It is tagged with a local version “apache.incubating”
> > so
> > > it
> > > > >>>> allows upgrading from earlier releases. This should be
> considered
> > of
> > > > >>>> release quality, but not yet officially vetted as a release
yet.
> > > > >>>>
> > > > >>>> Issues fixed:
> > > > >>>> * Use static nvd3 and d3
> > > > >>>> * Python 3 incompatibilities
> > > > >>>> * CLI API trigger dag issue
> > > > >>>>
> > > > >>>> As the difference between beta 5 and the release candidate
is
> > > > >> relatively
> > > > >>>> small I hope to start the VOTE for releasing 1.8.0 quite
soon (2
> > > > >> days?),
> > > > >>> if
> > > > >>>> the vote passes also a vote needs to happen at the IPMC
> > mailinglist.
> > > > As
> > > > >>>> this is our first Apache release I expect some comments
and
> > required
> > > > >>>> changes and probably a RC 2.
> > > > >>>>
> > > > >>>> Furthermore, we now have a “v1-8-stable” branch.
This has
> version
> > > > >>>> “1.8.0rc1” and will graduate to “1.8.0” when
we release. The
> > > > >> “v1-8-test”
> > > > >>>> branch now has version “1.8.1alpha0” as version and
“master” has
> > > > >> version
> > > > >>>> “1.9.0dev0”. Note that “v1-8-stable” is now closed.
This means
> > that,
> > > > >> per
> > > > >>>> release guidelines, patches accompanied with an ASSIGNED
Jira
> and
> > a
> > > > >>>> sign-off from a committer. Only then the release manager
applies
> > the
> > > > >>> patch
> > > > >>>> to stable (In this case that would be me). The release
manager
> > then
> > > > >>> closes
> > > > >>>> the bug when the patches have landed in the appropriate
> branches.
> > > For
> > > > >>> more
> > > > >>>> information please see: https://cwiki.apache.org/
> > > > >>>> confluence/display/AIRFLOW/Airflow+Release+Planning+and+
> > > > >>>> Supported+Release+Lifetime <https://cwiki.apache.org/
> > > > >>>> confluence/display/AIRFLOW/Airflow+Release+Planning+and+
> > > > >>>> Supported+Release+Lifetime> .
> > > > >>>>
> > > > >>>> Any questions or suggestions don’t hesitate to ask!
> > > > >>>>
> > > > >>>> Cheers
> > > > >>>> Bolke
> > > > >>>
> > > > >>
> > > >
> > >
> >
>

",http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CABYbY7fMpHZqoesPR7wAH0w9viU9jbfkt1BrMWeeE07yZe7kuQ@mail.gmail.com>,Chris Riccomini <criccom...@apache.org>,0,1
19,20,"     [ https://issues.apache.org/jira/browse/PROVISIONR-24?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Tom White resolved PROVISIONR-24. ---------------------------------      Resolution: Fixed  I updated the status page.                  > Update project incubation status file > ------------------------------------- > >                 Key: PROVISIONR-24 >                 URL: https://issues.apache.org/jira/browse/PROVISIONR-24 >             Project: Provisionr >          Issue Type: Sub-task >            Reporter: Andrei Savu >             Fix For: 0.4.0-incubating > > > Many tasks are now completed:  > http://incubator.apache.org/projects/provisionr.html  -- This message is automatically generated by JIRA. If you think it was sent incorrectly, please contact your JIRA administrators For more information on JIRA, see: http://www.atlassian.com/software/jira",https://mail-archives.apache.org/mod_mbox/provisionr-dev/201306.mbox/raw/%3CJIRA.12639824.1364568069898.114537.1371203120026%40arcas%3E,"""Tom White (JIRA)"" <j...@apache.org>",1,0
