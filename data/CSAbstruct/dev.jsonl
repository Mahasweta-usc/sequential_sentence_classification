{"abstract_id": 0, "sentences": ["  On 06/12/10 21:15, Matthew Daniel wrote:  Hello,   Congratulations on your upcoming move to Apache.", ":-)   I have an example source, very similar to the ExTDB[1-3].java files  which I feel is closer to a \"hello, world\" than the three examples  that ship with TDB currently.", "I think this is relevant, especially in  light of questions such as these:  http://tech.groups.yahoo.com/group/jena-dev/message/46166   My example creates a new Dataset, grabs a named (but empty, of course)  Model, opens the Graph for it, inserts a Statement (showing usage of  the IRIFactory) and then persists everything.", "If it is run again, it  then uses the ARQ API (QueryFactory et al) to show the contents of  that Model.", "Simple, but I feel that having this kind of example will  lower the cost of entry into TDB (and possibly Jena, seeing as it  touches on so many of the parts)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":-)   I have an example source, very similar to the ExTDB[1-3].java files  which I feel is closer to a \"hello, world\" than the three examples  that ship with TDB currently.", "I think this is relevant, especially in  light of questions such as these:  http://tech.groups.yahoo.com/group/jena-dev/message/46166   My example creates a new Dataset, grabs a named (but empty, of course)  Model, opens the Graph for it, inserts a Statement (showing usage of  the IRIFactory) and then persists everything.", "If it is run again, it  then uses the ARQ API (QueryFactory et al) to show the contents of  that Model.", "Simple, but I feel that having this kind of example will  lower the cost of entry into TDB (and possibly Jena, seeing as it  touches on so many of the parts).", "I saw the message about the Apache contributor agreement  (http://sourceforge.net/mailarchive/forum.php?thread_name=4CEFC4A7.8000104%40epimorphics.com&forum_name=jena-devel)  but I didn't want to jump through those hoops until I learn whether my  idea would be welcome."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think this is relevant, especially in  light of questions such as these:  http://tech.groups.yahoo.com/group/jena-dev/message/46166   My example creates a new Dataset, grabs a named (but empty, of course)  Model, opens the Graph for it, inserts a Statement (showing usage of  the IRIFactory) and then persists everything.", "If it is run again, it  then uses the ARQ API (QueryFactory et al) to show the contents of  that Model.", "Simple, but I feel that having this kind of example will  lower the cost of entry into TDB (and possibly Jena, seeing as it  touches on so many of the parts).", "I saw the message about the Apache contributor agreement  (http://sourceforge.net/mailarchive/forum.php?thread_name=4CEFC4A7.8000104%40epimorphics.com&forum_name=jena-devel)  but I didn't want to jump through those hoops until I learn whether my  idea would be welcome.", "It's welcome."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If it is run again, it  then uses the ARQ API (QueryFactory et al) to show the contents of  that Model.", "Simple, but I feel that having this kind of example will  lower the cost of entry into TDB (and possibly Jena, seeing as it  touches on so many of the parts).", "I saw the message about the Apache contributor agreement  (http://sourceforge.net/mailarchive/forum.php?thread_name=4CEFC4A7.8000104%40epimorphics.com&forum_name=jena-devel)  but I didn't want to jump through those hoops until I learn whether my  idea would be welcome.", "It's welcome.", "Send it to jena-dev@groups.yahoo.com and see what the  reaction is."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Simple, but I feel that having this kind of example will  lower the cost of entry into TDB (and possibly Jena, seeing as it  touches on so many of the parts).", "I saw the message about the Apache contributor agreement  (http://sourceforge.net/mailarchive/forum.php?thread_name=4CEFC4A7.8000104%40epimorphics.com&forum_name=jena-devel)  but I didn't want to jump through those hoops until I learn whether my  idea would be welcome.", "It's welcome.", "Send it to jena-dev@groups.yahoo.com and see what the  reaction is.", "For adding something like this, there is a lighter weight way."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["I saw the message about the Apache contributor agreement  (http://sourceforge.net/mailarchive/forum.php?thread_name=4CEFC4A7.8000104%40epimorphics.com&forum_name=jena-devel)  but I didn't want to jump through those hoops until I learn whether my  idea would be welcome.", "It's welcome.", "Send it to jena-dev@groups.yahoo.com and see what the  reaction is.", "For adding something like this, there is a lighter weight way.", "Only  committers need to sign an ICLA, and software grants are only needed  separately for large blobs of pre-existing code."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["It's welcome.", "Send it to jena-dev@groups.yahoo.com and see what the  reaction is.", "For adding something like this, there is a lighter weight way.", "Only  committers need to sign an ICLA, and software grants are only needed  separately for large blobs of pre-existing code.", "When we're in Apache, you can contribute by putting it on the Jena JIRA  as a patch file."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Send it to jena-dev@groups.yahoo.com and see what the  reaction is.", "For adding something like this, there is a lighter weight way.", "Only  committers need to sign an ICLA, and software grants are only needed  separately for large blobs of pre-existing code.", "When we're in Apache, you can contribute by putting it on the Jena JIRA  as a patch file.", "This implies the necessary legal permission for ASF to use the patch  just by the act of doing it (the Jira installation has a note about this  when you create the entry, I think - we're all a bit new to Apache  processes)."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["When we're in Apache, you can contribute by putting it on the Jena JIRA  as a patch file.", "This implies the necessary legal permission for ASF to use the patch  just by the act of doing it (the Jira installation has a note about this  when you create the entry, I think - we're all a bit new to Apache  processes).", "Some committer then handles it - I think (we new to this ...) by making  sure the project group is OK with it and then applying the patch.", "Or  the other way round.", "The point is an email goes to the public  development mailing list for everyone, inc committers, to see."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["This implies the necessary legal permission for ASF to use the patch  just by the act of doing it (the Jira installation has a note about this  when you create the entry, I think - we're all a bit new to Apache  processes).", "Some committer then handles it - I think (we new to this ...) by making  sure the project group is OK with it and then applying the patch.", "Or  the other way round.", "The point is an email goes to the public  development mailing list for everyone, inc committers, to see.", "For  added examples, it's a bit of a no-op, but it means it's \"the project\"  dciding, not one person in isolation."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Some committer then handles it - I think (we new to this ...) by making  sure the project group is OK with it and then applying the patch.", "Or  the other way round.", "The point is an email goes to the public  development mailing list for everyone, inc committers, to see.", "For  added examples, it's a bit of a no-op, but it means it's \"the project\"  dciding, not one person in isolation.", "It's no work for the contributor  - they just upload patch to JIRA."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["The point is an email goes to the public  development mailing list for everyone, inc committers, to see.", "For  added examples, it's a bit of a no-op, but it means it's \"the project\"  dciding, not one person in isolation.", "It's no work for the contributor  - they just upload patch to JIRA.", "We'd be swamped by (electronic)  paperwork otherwise.", "The full details are: http://www.apache.org/foundation/getinvolved.html   Thank you so much for all the hard work on Jena, and I hope I can  improve it (even if just a little)."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["It's no work for the contributor  - they just upload patch to JIRA.", "We'd be swamped by (electronic)  paperwork otherwise.", "The full details are: http://www.apache.org/foundation/getinvolved.html   Thank you so much for all the hard work on Jena, and I hope I can  improve it (even if just a little).", "The more, the merrier.", "-- /v\\atthew  Thins may be a bit slow as we get sorted out but JIRA never forgets."], "labels": ["1", "1", "1", "0", "1"]}
{"abstract_id": 0, "sentences": ["We'd be swamped by (electronic)  paperwork otherwise.", "The full details are: http://www.apache.org/foundation/getinvolved.html   Thank you so much for all the hard work on Jena, and I hope I can  improve it (even if just a little).", "The more, the merrier.", "-- /v\\atthew  Thins may be a bit slow as we get sorted out but JIRA never forgets.", "If  you could send it for now to jena-dev@groups.yahoo.com then submit it  when we are up and running properly at Apache."], "labels": ["1", "1", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["The full details are: http://www.apache.org/foundation/getinvolved.html   Thank you so much for all the hard work on Jena, and I hope I can  improve it (even if just a little).", "The more, the merrier.", "-- /v\\atthew  Thins may be a bit slow as we get sorted out but JIRA never forgets.", "If  you could send it for now to jena-dev@groups.yahoo.com then submit it  when we are up and running properly at Apache.", "Andy  "], "labels": ["1", "0", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["deft-dev@incubator.apache.org -- awf-dev@incubator.apache.org deft-users@incubator.apache.org -- awf-users@incubator.apache.org deft-commits@incubator.apache.org -- awf-commits@incubator.apache.org  Thanks, Matt  "], "labels": ["1"]}
{"abstract_id": 0, "sentences": [" SO, to return to the most important question of the moment:  I'm gonna assume that all of the \"non-controversial\" name changes  have general approval, so that one of the committers can go ahead  and make the change...  To review, that list is now:   decorateCollection  panelCollection  navigationLevel     navigationPane  objectIcon          icon  objectImage         image  objectMedia         media  objectSeparator     separator  objectSpacer        spacer  panelBorder         panelBorderLayout  panelForm           panelFormLayout  panelGroup          panelGroupLayout  panelHorizontal     panelHorizontalLayout  selectInputColor    inputColor  selectInputDate     inputDate  showOneAccordion    panelAccordion  showManyAccordion   panelAccordion (merge with showOneAccordion)  showOneChoice       panelChoice  showOneRadio        panelRadio  showOneTabs         panelTabbed   +1    -- Adam    On 7/13/06, Benj Fayle <bfayle@maketechnologies.com wrote:   Agreed which is why abbreviations such as inputLOV should be avoided   inputListOfValues is much more self-explanatory.", "-----Original Message-----   From: Adam Winer [mailto:awiner@gmail.com]   Sent: Wednesday, July 12, 2006 10:13 AM   To: adffaces-user@incubator.apache.org   Subject: Re: Re: Re: Tag renaming proposal     On 7/12/06, Mike Kienenberger <mkienenb@gmail.com wrote:       On 7/12/06, Matthias Wessendorf <matzew@apache.org wrote:      I've gotten a decent bit of feedback in the past that people      just couldn't find selectInputDate or selectInputColor, but      if they'd been called inputDate or inputColor, they would have      been found.", "yes, that is true for me, to be honest .", "On the other hand, if someone explains up-front in the component table    of contents that \"input\" meant unassisted entry and \"select\" meant    assisted entry, and \"inputSelect\" means both, I could have figured    this out.", "... but as \"selectInputDate\", anyone looking alphabetically is just   totally   lost."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-----Original Message-----   From: Adam Winer [mailto:awiner@gmail.com]   Sent: Wednesday, July 12, 2006 10:13 AM   To: adffaces-user@incubator.apache.org   Subject: Re: Re: Re: Tag renaming proposal     On 7/12/06, Mike Kienenberger <mkienenb@gmail.com wrote:       On 7/12/06, Matthias Wessendorf <matzew@apache.org wrote:      I've gotten a decent bit of feedback in the past that people      just couldn't find selectInputDate or selectInputColor, but      if they'd been called inputDate or inputColor, they would have      been found.", "yes, that is true for me, to be honest .", "On the other hand, if someone explains up-front in the component table    of contents that \"input\" meant unassisted entry and \"select\" meant    assisted entry, and \"inputSelect\" means both, I could have figured    this out.", "... but as \"selectInputDate\", anyone looking alphabetically is just   totally   lost.", "One of the goals of the renaming is to minimize any need for up-front   explanation;  the best name is one that requires no documentation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["yes, that is true for me, to be honest .", "On the other hand, if someone explains up-front in the component table    of contents that \"input\" meant unassisted entry and \"select\" meant    assisted entry, and \"inputSelect\" means both, I could have figured    this out.", "... but as \"selectInputDate\", anyone looking alphabetically is just   totally   lost.", "One of the goals of the renaming is to minimize any need for up-front   explanation;  the best name is one that requires no documentation.", "-- Adam      --  Matthias Wessendorf  further stuff: blog: http://jroller.com/page/mwessendorf mail: mwessendorf-at-gmail-dot-com  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am not that familiar with Xen.", "Maybe Michael S can answer from here.", "Where do you see hostid-none error?", "I you could just copy and paste your messages to the screen maybe I can help.", "It has been awhile since I have encountered these error so I might not remember exactly what to do."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Maybe Michael S can answer from here.", "Where do you see hostid-none error?", "I you could just copy and paste your messages to the screen maybe I can help.", "It has been awhile since I have encountered these error so I might not remember exactly what to do.", "However, if you show me exactly what is printed to the screen, it may job my memory."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Where do you see hostid-none error?", "I you could just copy and paste your messages to the screen maybe I can help.", "It has been awhile since I have encountered these error so I might not remember exactly what to do.", "However, if you show me exactly what is printed to the screen, it may job my memory.", "Richard   On Fri, Apr 29, 2011 at 2:06 AM, hari narayanan <hari.zlatan@gmail.com wrote:  Sorry for the trouble ... We created the Vm based on the hostname found in  /etc/hosts."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I you could just copy and paste your messages to the screen maybe I can help.", "It has been awhile since I have encountered these error so I might not remember exactly what to do.", "However, if you show me exactly what is printed to the screen, it may job my memory.", "Richard   On Fri, Apr 29, 2011 at 2:06 AM, hari narayanan <hari.zlatan@gmail.com wrote:  Sorry for the trouble ... We created the Vm based on the hostname found in  /etc/hosts.", "After creating the vm, we still get hostid-none error...  gethosts at the CM lists our laptop, but primitive.py says \"Failed to  schedule or activate vm\" ... vncviewer works for domU images in xen... so,  if we add the host (laptop) , then how can we vncview into it considering  its dom0?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It has been awhile since I have encountered these error so I might not remember exactly what to do.", "However, if you show me exactly what is printed to the screen, it may job my memory.", "Richard   On Fri, Apr 29, 2011 at 2:06 AM, hari narayanan <hari.zlatan@gmail.com wrote:  Sorry for the trouble ... We created the Vm based on the hostname found in  /etc/hosts.", "After creating the vm, we still get hostid-none error...  gethosts at the CM lists our laptop, but primitive.py says \"Failed to  schedule or activate vm\" ... vncviewer works for domU images in xen... so,  if we add the host (laptop) , then how can we vncview into it considering  its dom0?", "On Thu, Apr 28, 2011 at 5:14 PM, Richard Gass <richardgass@gmail.com wrote:   Tashi doesn't use thrift anymore."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, if you show me exactly what is printed to the screen, it may job my memory.", "Richard   On Fri, Apr 29, 2011 at 2:06 AM, hari narayanan <hari.zlatan@gmail.com wrote:  Sorry for the trouble ... We created the Vm based on the hostname found in  /etc/hosts.", "After creating the vm, we still get hostid-none error...  gethosts at the CM lists our laptop, but primitive.py says \"Failed to  schedule or activate vm\" ... vncviewer works for domU images in xen... so,  if we add the host (laptop) , then how can we vncview into it considering  its dom0?", "On Thu, Apr 28, 2011 at 5:14 PM, Richard Gass <richardgass@gmail.com wrote:   Tashi doesn't use thrift anymore.", "We switched to rpyc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Richard   On Fri, Apr 29, 2011 at 2:06 AM, hari narayanan <hari.zlatan@gmail.com wrote:  Sorry for the trouble ... We created the Vm based on the hostname found in  /etc/hosts.", "After creating the vm, we still get hostid-none error...  gethosts at the CM lists our laptop, but primitive.py says \"Failed to  schedule or activate vm\" ... vncviewer works for domU images in xen... so,  if we add the host (laptop) , then how can we vncview into it considering  its dom0?", "On Thu, Apr 28, 2011 at 5:14 PM, Richard Gass <richardgass@gmail.com wrote:   Tashi doesn't use thrift anymore.", "We switched to rpyc.", "Install that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["After creating the vm, we still get hostid-none error...  gethosts at the CM lists our laptop, but primitive.py says \"Failed to  schedule or activate vm\" ... vncviewer works for domU images in xen... so,  if we add the host (laptop) , then how can we vncview into it considering  its dom0?", "On Thu, Apr 28, 2011 at 5:14 PM, Richard Gass <richardgass@gmail.com wrote:   Tashi doesn't use thrift anymore.", "We switched to rpyc.", "Install that.", "If you didn't have ipython installed, when you start the CM or NM, you  would not have an active prompt, just log messages."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Apr 28, 2011 at 5:14 PM, Richard Gass <richardgass@gmail.com wrote:   Tashi doesn't use thrift anymore.", "We switched to rpyc.", "Install that.", "If you didn't have ipython installed, when you start the CM or NM, you  would not have an active prompt, just log messages.", "If you restart  now, you should get an ipython prompt."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We switched to rpyc.", "Install that.", "If you didn't have ipython installed, when you start the CM or NM, you  would not have an active prompt, just log messages.", "If you restart  now, you should get an ipython prompt.", "Can you show the output of  data.baseDataObject.hosts now?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you didn't have ipython installed, when you start the CM or NM, you  would not have an active prompt, just log messages.", "If you restart  now, you should get an ipython prompt.", "Can you show the output of  data.baseDataObject.hosts now?", "Richard    On Fri, Apr 29, 2011 at 12:10 AM, hari narayanan <hari.zlatan@gmail.com  wrote:   No, but we installed it just now ... we dont have thrift installed ....   should we get that also ??", "And we always start CM as u said ....         On Thu, Apr 28, 2011 at 4:49 PM, Richard Gass <richardgass@gmail.com   wrote:     Do you have ipython installed?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you restart  now, you should get an ipython prompt.", "Can you show the output of  data.baseDataObject.hosts now?", "Richard    On Fri, Apr 29, 2011 at 12:10 AM, hari narayanan <hari.zlatan@gmail.com  wrote:   No, but we installed it just now ... we dont have thrift installed ....   should we get that also ??", "And we always start CM as u said ....         On Thu, Apr 28, 2011 at 4:49 PM, Richard Gass <richardgass@gmail.com   wrote:     Do you have ipython installed?", "On Thu, Apr 28, 2011 at 11:38 PM, hari narayanan   <hari.zlatan@gmail.com   wrote:    We dont get output for data.baseDataObject.hosts in CM .... but, we    get    the    laptop name in tashiclient."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Richard    On Fri, Apr 29, 2011 at 12:10 AM, hari narayanan <hari.zlatan@gmail.com  wrote:   No, but we installed it just now ... we dont have thrift installed ....   should we get that also ??", "And we always start CM as u said ....         On Thu, Apr 28, 2011 at 4:49 PM, Richard Gass <richardgass@gmail.com   wrote:     Do you have ipython installed?", "On Thu, Apr 28, 2011 at 11:38 PM, hari narayanan   <hari.zlatan@gmail.com   wrote:    We dont get output for data.baseDataObject.hosts in CM .... but, we    get    the    laptop name in tashiclient.", "getHosts with state Normal...       On Thu, Apr 28, 2011 at 4:32 PM, Richard Gass <richardgass@gmail.com    wrote:       Send me the following information.", "on the CM    \"data.baseDataObject.hosts\" or from the tashi client \"tashi    gethosts\"          On Thu, Apr 28, 2011 at 11:22 PM, hari narayanan    <hari.zlatan@gmail.com    wrote:     Hi,         we are able to start the vm image using xen command... but, it     doesnt     work     when we try with tashi ..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And we always start CM as u said ....         On Thu, Apr 28, 2011 at 4:49 PM, Richard Gass <richardgass@gmail.com   wrote:     Do you have ipython installed?", "On Thu, Apr 28, 2011 at 11:38 PM, hari narayanan   <hari.zlatan@gmail.com   wrote:    We dont get output for data.baseDataObject.hosts in CM .... but, we    get    the    laptop name in tashiclient.", "getHosts with state Normal...       On Thu, Apr 28, 2011 at 4:32 PM, Richard Gass <richardgass@gmail.com    wrote:       Send me the following information.", "on the CM    \"data.baseDataObject.hosts\" or from the tashi client \"tashi    gethosts\"          On Thu, Apr 28, 2011 at 11:22 PM, hari narayanan    <hari.zlatan@gmail.com    wrote:     Hi,         we are able to start the vm image using xen command... but, it     doesnt     work     when we try with tashi ...", "Still get the same error \"no     hostid-none\"     ...     We     tried to change the CM and NM config files to make sure that     hostid     value is     equal to our actual laptop's hostname \"akshay\" ... and also ,we     are     able     to     ping the address and hostname of the laptop ... what could be the     prob?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Apr 28, 2011 at 11:38 PM, hari narayanan   <hari.zlatan@gmail.com   wrote:    We dont get output for data.baseDataObject.hosts in CM .... but, we    get    the    laptop name in tashiclient.", "getHosts with state Normal...       On Thu, Apr 28, 2011 at 4:32 PM, Richard Gass <richardgass@gmail.com    wrote:       Send me the following information.", "on the CM    \"data.baseDataObject.hosts\" or from the tashi client \"tashi    gethosts\"          On Thu, Apr 28, 2011 at 11:22 PM, hari narayanan    <hari.zlatan@gmail.com    wrote:     Hi,         we are able to start the vm image using xen command... but, it     doesnt     work     when we try with tashi ...", "Still get the same error \"no     hostid-none\"     ...     We     tried to change the CM and NM config files to make sure that     hostid     value is     equal to our actual laptop's hostname \"akshay\" ... and also ,we     are     able     to     ping the address and hostname of the laptop ... what could be the     prob?", "On Tue, Apr 26, 2011 at 10:16 PM, Akshay Sheth     <aks.sheth88@gmail.com     wrote:         Hey Micheal,         I was planning to use Sqlite3 but when I write the insert queries     that     the     tables hosts and networks dont exist."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["getHosts with state Normal...       On Thu, Apr 28, 2011 at 4:32 PM, Richard Gass <richardgass@gmail.com    wrote:       Send me the following information.", "on the CM    \"data.baseDataObject.hosts\" or from the tashi client \"tashi    gethosts\"          On Thu, Apr 28, 2011 at 11:22 PM, hari narayanan    <hari.zlatan@gmail.com    wrote:     Hi,         we are able to start the vm image using xen command... but, it     doesnt     work     when we try with tashi ...", "Still get the same error \"no     hostid-none\"     ...     We     tried to change the CM and NM config files to make sure that     hostid     value is     equal to our actual laptop's hostname \"akshay\" ... and also ,we     are     able     to     ping the address and hostname of the laptop ... what could be the     prob?", "On Tue, Apr 26, 2011 at 10:16 PM, Akshay Sheth     <aks.sheth88@gmail.com     wrote:         Hey Micheal,         I was planning to use Sqlite3 but when I write the insert queries     that     the     tables hosts and networks dont exist.", "How do I fix this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Apr 26, 2011 at 10:16 PM, Akshay Sheth     <aks.sheth88@gmail.com     wrote:         Hey Micheal,         I was planning to use Sqlite3 but when I write the insert queries     that     the     tables hosts and networks dont exist.", "How do I fix this?", "Also     when I     put     data.baseDataObject.getHosts on CM I dont get any output.", "Also     eventually I     get no hostId for the VM.", "What could be wrong?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How do I fix this?", "Also     when I     put     data.baseDataObject.getHosts on CM I dont get any output.", "Also     eventually I     get no hostId for the VM.", "What could be wrong?", "Thanks     Akshay         On Tue, Apr 26, 2011 at 9:55 PM, hari narayanan     <hari.zlatan@gmail.com     wrote:         Hi,         we have managed to setup internet in xen and downloaded tashi     also     ...     now, we get a different error\u00a0 in node manager... failed to load     vminfo     /var/tmp/nm.data ... and no vm information found in     /var/tmp/vmcontrolQemu     ..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also     when I     put     data.baseDataObject.getHosts on CM I dont get any output.", "Also     eventually I     get no hostId for the VM.", "What could be wrong?", "Thanks     Akshay         On Tue, Apr 26, 2011 at 9:55 PM, hari narayanan     <hari.zlatan@gmail.com     wrote:         Hi,         we have managed to setup internet in xen and downloaded tashi     also     ...     now, we get a different error\u00a0 in node manager... failed to load     vminfo     /var/tmp/nm.data ... and no vm information found in     /var/tmp/vmcontrolQemu     ...", "When we used vmcspecificcall, we get the same no hostd none     error         Thanks,     Hari         On Tue, Apr 26, 2011 at 6:50 PM, Michael Stroucken <mxs@cmu.edu     wrote:         hari narayanan wrote:         Sorry ... We couldnt get it work ."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also     eventually I     get no hostId for the VM.", "What could be wrong?", "Thanks     Akshay         On Tue, Apr 26, 2011 at 9:55 PM, hari narayanan     <hari.zlatan@gmail.com     wrote:         Hi,         we have managed to setup internet in xen and downloaded tashi     also     ...     now, we get a different error\u00a0 in node manager... failed to load     vminfo     /var/tmp/nm.data ... and no vm information found in     /var/tmp/vmcontrolQemu     ...", "When we used vmcspecificcall, we get the same no hostd none     error         Thanks,     Hari         On Tue, Apr 26, 2011 at 6:50 PM, Michael Stroucken <mxs@cmu.edu     wrote:         hari narayanan wrote:         Sorry ... We couldnt get it work .", "so, we moved to Xen ....         Hi Hari,         I used to use Xen until I had too many problems with booting     kernels     inside the VM image."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What could be wrong?", "Thanks     Akshay         On Tue, Apr 26, 2011 at 9:55 PM, hari narayanan     <hari.zlatan@gmail.com     wrote:         Hi,         we have managed to setup internet in xen and downloaded tashi     also     ...     now, we get a different error\u00a0 in node manager... failed to load     vminfo     /var/tmp/nm.data ... and no vm information found in     /var/tmp/vmcontrolQemu     ...", "When we used vmcspecificcall, we get the same no hostd none     error         Thanks,     Hari         On Tue, Apr 26, 2011 at 6:50 PM, Michael Stroucken <mxs@cmu.edu     wrote:         hari narayanan wrote:         Sorry ... We couldnt get it work .", "so, we moved to Xen ....         Hi Hari,         I used to use Xen until I had too many problems with booting     kernels     inside the VM image.", "But with kvm I'm using the same network     setup     as     I did     with Xen, except for having to add the qemu-ifup files that I     mentioned     before."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks     Akshay         On Tue, Apr 26, 2011 at 9:55 PM, hari narayanan     <hari.zlatan@gmail.com     wrote:         Hi,         we have managed to setup internet in xen and downloaded tashi     also     ...     now, we get a different error\u00a0 in node manager... failed to load     vminfo     /var/tmp/nm.data ... and no vm information found in     /var/tmp/vmcontrolQemu     ...", "When we used vmcspecificcall, we get the same no hostd none     error         Thanks,     Hari         On Tue, Apr 26, 2011 at 6:50 PM, Michael Stroucken <mxs@cmu.edu     wrote:         hari narayanan wrote:         Sorry ... We couldnt get it work .", "so, we moved to Xen ....         Hi Hari,         I used to use Xen until I had too many problems with booting     kernels     inside the VM image.", "But with kvm I'm using the same network     setup     as     I did     with Xen, except for having to add the qemu-ifup files that I     mentioned     before.", "Greetings,     Michael."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When we used vmcspecificcall, we get the same no hostd none     error         Thanks,     Hari         On Tue, Apr 26, 2011 at 6:50 PM, Michael Stroucken <mxs@cmu.edu     wrote:         hari narayanan wrote:         Sorry ... We couldnt get it work .", "so, we moved to Xen ....         Hi Hari,         I used to use Xen until I had too many problems with booting     kernels     inside the VM image.", "But with kvm I'm using the same network     setup     as     I did     with Xen, except for having to add the qemu-ifup files that I     mentioned     before.", "Greetings,     Michael.", "--    Richard Gass               --   Richard Gass         --  Richard Gass      --  Richard Gass  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["+1  I checked the following:  [ X ] Build and Unit Tests Pass [ X ] Integration Tests Pass [ X ] \"Incubating\" in References to Project and Distribution File Names [ X ] Signatures and Hashes Match Keys [ X ] DISCLAIMER, LICENSE, and NOTICE Files in Source and Binary Release Packages [ X ] DISCLAIMER, LICENSE, and NOTICE are consistent with ASF and Incubator Policy [ X ] CHANGELOG included with release distribution [ X ] All Source Files Have Correct ASF Headers [ X ] No Binary Files in Source Release Packages   On Feb 7, 2020, at 4:12 PM, Joshua Poore <poorejc@apache.org wrote:    Hi Folks,     Please VOTE on the Apache Flagon 2.1.0 Release Candidate 01     About Flagon: http://flagon.incubator.apache.org/ <http://flagon.incubator.apache.org/     This {Major/Minor/Patch} release includes :     \t\u2022 Module package-bundler deployment support (include UserALE.js via 'import' & 'require')  \t\u2022 Updated existing example page to include a range of HTML elements  \t\u2022 New example page for including UserALE.js as a module (Webpack example)  \t\u2022 Added support for logging from HTML Forms  \t\u2022 Added SessionId persistence via SessionStorage  \t\u2022 Exposes a wide range of functions to support custom logging with UserALE.js  \t\u2022 Added support for passing auth-headers via log pipeline to back-end  \t\u2022 Added additional log fields: browser type and version, inner width/height (for heatmaps)     We solved 31 issues: https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12345442&styleName=Text&projectId=12320621&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED_8301b4e9c1c91354ea85ab02c89ec979db077d9a_lin <https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12345442&styleName=Text&projectId=12320621&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED_8301b4e9c1c91354ea85ab02c89ec979db077d9a_lin     Git source tag (7746500): https://github.com/apache/incubator-flagon-useralejs/releases/tag/2.1.0-RC-01 <https://github.com/apache/incubator-flagon-useralejs/releases/tag/2.1.0-RC-01     Staging repo: https://dist.apache.org/repos/dist/dev/incubator/flagon/ <https://dist.apache.org/repos/dist/dev/incubator/flagon/     Source Release Artifacts: https://dist.apache.org/repos/dist/dev/incubator/flagon/apache-flagon-useralejs-incubating-2.1.0-RC-01/ <https://dist.apache.org/repos/dist/dev/incubator/flagon/apache-flagon-useralejs-incubating-2.1.0-RC-01/     PGP release keys (signed using F9374FAE3FCADF6E): https://dist.apache.org/repos/dist/dev/incubator/flagon/KEYS <https://dist.apache.org/repos/dist/dev/incubator/flagon/KEYS     Link to Successful Jenkins Build: https://builds.apache.org/job/useralejs-ci/101/ <https://builds.apache.org/job/useralejs-ci/101/     Reference to UserALE.js testing framework to assist in verifying this release: https://cwiki.apache.org/confluence/display/FLAGON/UserALE.js+Testing+Framework <https://cwiki.apache.org/confluence/display/FLAGON/UserALE.js+Testing+Framework     Vote will be open for 72 hours.", "Please VOTE as follows:     [ ] +1, let's get it released!!!", "[ ] +/-0, fine, but consider to fix few issues before...  [ ] -1, nope, because... (and please explain why)     Along with your VOTE, please indicate testing and checks you've made against build artifacts, src, and documentation:     [ ] Build and Unit Tests Pass  [ ] Integration Tests Pass  [ ] \"Incubating\" in References to Project and Distribution File Names  [ ] Signatures and Hashes Match Keys  [ ] DISCLAIMER, LICENSE, and NOTICE Files in Source and Binary Release Packages  [ ] DISCLAIMER, LICENSE, and NOTICE are consistent with ASF and Incubator Policy  [ ] CHANGELOG included with release distribution  [ ] All Source Files Have Correct ASF Headers  [ ] No Binary Files in Source Release Packages     Thank you to everyone that is able to VOTE as well as everyone that contributed to Apache Flagon 2.1.0   "], "labels": ["1", "1", "1"]}
{"abstract_id": 0, "sentences": ["I'll have a look at Facelets (and perhaps Clay, but latter since it's less   used).", "I really have the impression that everyone (or quite everyone) using JSF is   using Facelets, so....     Just another question (last, I swear it !)", ":   Do you, JSF app developpers (and not component developers), use an IDE with drag   and drop capabilities (RAD kind of development) for facelets or no ?", "Which one ?", "I'm just using IBM RAD 6, and it's really difficult (ibm proprietary components,   JSF 1.0, JSP...)     You might also ask this one on the shale users list."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":   Do you, JSF app developpers (and not component developers), use an IDE with drag   and drop capabilities (RAD kind of development) for facelets or no ?", "Which one ?", "I'm just using IBM RAD 6, and it's really difficult (ibm proprietary components,   JSF 1.0, JSP...)     You might also ask this one on the shale users list.", "Ryan Wynn works with Websphere (IBM Consulting) hangs out there [1].", "Although, a Shale Clay enthusiast - something like 20 porlets using Shale Clay."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Which one ?", "I'm just using IBM RAD 6, and it's really difficult (ibm proprietary components,   JSF 1.0, JSP...)     You might also ask this one on the shale users list.", "Ryan Wynn works with Websphere (IBM Consulting) hangs out there [1].", "Although, a Shale Clay enthusiast - something like 20 porlets using Shale Clay.", "[1] https://issues.apache.org/struts/browse/SHALE-402    Gary   ----- Message d'origine ----   De : Gary VanMatre   \ufffd : adffaces-user@incubator.apache.org   Envoy\ufffd le : Jeudi, 1 F\ufffdvrier 2007, 16h49mn 50s   Objet : Re: RE : AW: templating mecanism ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Ryan Wynn works with Websphere (IBM Consulting) hangs out there [1].", "Although, a Shale Clay enthusiast - something like 20 porlets using Shale Clay.", "[1] https://issues.apache.org/struts/browse/SHALE-402    Gary   ----- Message d'origine ----   De : Gary VanMatre   \ufffd : adffaces-user@incubator.apache.org   Envoy\ufffd le : Jeudi, 1 F\ufffdvrier 2007, 16h49mn 50s   Objet : Re: RE : AW: templating mecanism ?", "From: Adrian Gonzalez       Thank you very much    But I really need to use JSP for rendering (not    facelets) - my IDE doesn't support JSF design with    facelets syntax.", "Another less popular alternative would be Shale Clay [1]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[1] https://issues.apache.org/struts/browse/SHALE-402    Gary   ----- Message d'origine ----   De : Gary VanMatre   \ufffd : adffaces-user@incubator.apache.org   Envoy\ufffd le : Jeudi, 1 F\ufffdvrier 2007, 16h49mn 50s   Objet : Re: RE : AW: templating mecanism ?", "From: Adrian Gonzalez       Thank you very much    But I really need to use JSP for rendering (not    facelets) - my IDE doesn't support JSF design with    facelets syntax.", "Another less popular alternative would be Shale Clay [1].", "It can be used with JSP and has a number of options.", "[1] http://shale.apache.org/shale-clay/index.html        Do you know a templating solution for JSP engine ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Adrian Gonzalez       Thank you very much    But I really need to use JSP for rendering (not    facelets) - my IDE doesn't support JSF design with    facelets syntax.", "Another less popular alternative would be Shale Clay [1].", "It can be used with JSP and has a number of options.", "[1] http://shale.apache.org/shale-clay/index.html        Do you know a templating solution for JSP engine ?", "--- D\ufffdring Markus a    \ufffdcrit :       Gary          Hello,     Trinidad comes with a buildin Facelets library and     Facelets has a very powerfull templating mechanism."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Another less popular alternative would be Shale Clay [1].", "It can be used with JSP and has a number of options.", "[1] http://shale.apache.org/shale-clay/index.html        Do you know a templating solution for JSP engine ?", "--- D\ufffdring Markus a    \ufffdcrit :       Gary          Hello,     Trinidad comes with a buildin Facelets library and     Facelets has a very powerfull templating mechanism.", "Just have a look at it and test if it's what you     need."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It can be used with JSP and has a number of options.", "[1] http://shale.apache.org/shale-clay/index.html        Do you know a templating solution for JSP engine ?", "--- D\ufffdring Markus a    \ufffdcrit :       Gary          Hello,     Trinidad comes with a buildin Facelets library and     Facelets has a very powerfull templating mechanism.", "Just have a look at it and test if it's what you     need.", "Greetings     Markus              -----Urspr\ufffdngliche Nachricht-----      Von: Adrian Gonzalez     [mailto:adr_gonzalez@yahoo.fr]      Gesendet: Donnerstag, 1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["--- D\ufffdring Markus a    \ufffdcrit :       Gary          Hello,     Trinidad comes with a buildin Facelets library and     Facelets has a very powerfull templating mechanism.", "Just have a look at it and test if it's what you     need.", "Greetings     Markus              -----Urspr\ufffdngliche Nachricht-----      Von: Adrian Gonzalez     [mailto:adr_gonzalez@yahoo.fr]      Gesendet: Donnerstag, 1.", "Februar 2007 14:32      An: adffaces-user@incubator.apache.org      Betreff: templating mecanism ?", "Hello,           I would like to know if there's a templating     mecanism      provided by trinidad (or usable with trinidad) and      usable for JSP rendering ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Just have a look at it and test if it's what you     need.", "Greetings     Markus              -----Urspr\ufffdngliche Nachricht-----      Von: Adrian Gonzalez     [mailto:adr_gonzalez@yahoo.fr]      Gesendet: Donnerstag, 1.", "Februar 2007 14:32      An: adffaces-user@incubator.apache.org      Betreff: templating mecanism ?", "Hello,           I would like to know if there's a templating     mecanism      provided by trinidad (or usable with trinidad) and      usable for JSP rendering ?", "I've tried Shale Tiles extension with ADF Faces,     and      it doesn't work, so I would like to know if     there's a      substitute."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Greetings     Markus              -----Urspr\ufffdngliche Nachricht-----      Von: Adrian Gonzalez     [mailto:adr_gonzalez@yahoo.fr]      Gesendet: Donnerstag, 1.", "Februar 2007 14:32      An: adffaces-user@incubator.apache.org      Betreff: templating mecanism ?", "Hello,           I would like to know if there's a templating     mecanism      provided by trinidad (or usable with trinidad) and      usable for JSP rendering ?", "I've tried Shale Tiles extension with ADF Faces,     and      it doesn't work, so I would like to know if     there's a      substitute.", "Moreover I don't want to use sitemesh engine for      composition (cause of the parsing performance     impact)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Februar 2007 14:32      An: adffaces-user@incubator.apache.org      Betreff: templating mecanism ?", "Hello,           I would like to know if there's a templating     mecanism      provided by trinidad (or usable with trinidad) and      usable for JSP rendering ?", "I've tried Shale Tiles extension with ADF Faces,     and      it doesn't work, so I would like to know if     there's a      substitute.", "Moreover I don't want to use sitemesh engine for      composition (cause of the parsing performance     impact).", "The region tags enable composition but not     templating      I think."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello,           I would like to know if there's a templating     mecanism      provided by trinidad (or usable with trinidad) and      usable for JSP rendering ?", "I've tried Shale Tiles extension with ADF Faces,     and      it doesn't work, so I would like to know if     there's a      substitute.", "Moreover I don't want to use sitemesh engine for      composition (cause of the parsing performance     impact).", "The region tags enable composition but not     templating      I think.", "Thanks                                           __________________________________________________________________________      _      D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des     r\ufffdponses \ufffd toutes vos questions      !"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I've tried Shale Tiles extension with ADF Faces,     and      it doesn't work, so I would like to know if     there's a      substitute.", "Moreover I don't want to use sitemesh engine for      composition (cause of the parsing performance     impact).", "The region tags enable composition but not     templating      I think.", "Thanks                                           __________________________________________________________________________      _      D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des     r\ufffdponses \ufffd toutes vos questions      !", "Profitez des connaissances, des opinions et des     exp\ufffdriences des      internautes sur Yahoo!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The region tags enable composition but not     templating      I think.", "Thanks                                           __________________________________________________________________________      _      D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des     r\ufffdponses \ufffd toutes vos questions      !", "Profitez des connaissances, des opinions et des     exp\ufffdriences des      internautes sur Yahoo!", "Questions/R\ufffdponses      http://fr.answers.yahoo.com                             ___________________________________________________________________________    D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !", "Profitez des connaissances, des opinions et des exp\ufffdriences des internautes   sur    Yahoo!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks                                           __________________________________________________________________________      _      D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des     r\ufffdponses \ufffd toutes vos questions      !", "Profitez des connaissances, des opinions et des     exp\ufffdriences des      internautes sur Yahoo!", "Questions/R\ufffdponses      http://fr.answers.yahoo.com                             ___________________________________________________________________________    D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !", "Profitez des connaissances, des opinions et des exp\ufffdriences des internautes   sur    Yahoo!", "Questions/R\ufffdponses    http://fr.answers.yahoo.com                   ___________________________________________________________________________   D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Profitez des connaissances, des opinions et des     exp\ufffdriences des      internautes sur Yahoo!", "Questions/R\ufffdponses      http://fr.answers.yahoo.com                             ___________________________________________________________________________    D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !", "Profitez des connaissances, des opinions et des exp\ufffdriences des internautes   sur    Yahoo!", "Questions/R\ufffdponses    http://fr.answers.yahoo.com                   ___________________________________________________________________________   D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !", "Profitez des connaissances, des opinions et des exp\ufffdriences des internautes sur   Yahoo!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Questions/R\ufffdponses      http://fr.answers.yahoo.com                             ___________________________________________________________________________    D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !", "Profitez des connaissances, des opinions et des exp\ufffdriences des internautes   sur    Yahoo!", "Questions/R\ufffdponses    http://fr.answers.yahoo.com                   ___________________________________________________________________________   D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !", "Profitez des connaissances, des opinions et des exp\ufffdriences des internautes sur   Yahoo!", "Questions/R\ufffdponses   http://fr.answers.yahoo.com  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm a little confused about what is meant as a job here, after all this discussion...  For \"interactive sessions\", stopping a session means stopping the SparkContext.", "So the final state of any running jobs in that session should be the same as if you stopped the SparkContext without explicitly stopping the jobs in a normal, non-Livy application.", "For batches, stopping a batch means killing the Spark application, so all bets are off as to what happens there.", "On Wed, Jan 24, 2018 at 1:08 PM, Alex Bozarth <ajbozart@us.ibm.com wrote:   You are correct that you are using the term Job incorrectly (at least  according to how Spark/Livy uses it).", "Each spark-submit is a a single Spark  Application and can include many jobs (which are broken down themselves  into stages and tasks)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So the final state of any running jobs in that session should be the same as if you stopped the SparkContext without explicitly stopping the jobs in a normal, non-Livy application.", "For batches, stopping a batch means killing the Spark application, so all bets are off as to what happens there.", "On Wed, Jan 24, 2018 at 1:08 PM, Alex Bozarth <ajbozart@us.ibm.com wrote:   You are correct that you are using the term Job incorrectly (at least  according to how Spark/Livy uses it).", "Each spark-submit is a a single Spark  Application and can include many jobs (which are broken down themselves  into stages and tasks).", "In Livy using sessions would be like using  spark-shell rather than spark-submit, you probably want to use batches  instead (which utilize spark-submit), then you would use that delete  command as mentioned earlier."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Jan 24, 2018 at 1:08 PM, Alex Bozarth <ajbozart@us.ibm.com wrote:   You are correct that you are using the term Job incorrectly (at least  according to how Spark/Livy uses it).", "Each spark-submit is a a single Spark  Application and can include many jobs (which are broken down themselves  into stages and tasks).", "In Livy using sessions would be like using  spark-shell rather than spark-submit, you probably want to use batches  instead (which utilize spark-submit), then you would use that delete  command as mentioned earlier.", "As for the result being listed as FAILED and  not CANCELLED, that is as intended.", "When a Livy Session is stopped  (deleted) is sends a command to all the running jobs (in your case each of  you apps only have one \"Job\") to set as failed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Each spark-submit is a a single Spark  Application and can include many jobs (which are broken down themselves  into stages and tasks).", "In Livy using sessions would be like using  spark-shell rather than spark-submit, you probably want to use batches  instead (which utilize spark-submit), then you would use that delete  command as mentioned earlier.", "As for the result being listed as FAILED and  not CANCELLED, that is as intended.", "When a Livy Session is stopped  (deleted) is sends a command to all the running jobs (in your case each of  you apps only have one \"Job\") to set as failed.", "@Marcelo you wrote the code that does this, do you remember why you had  Jobs killed instead of cancelled when a Livy session is stopped?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As for the result being listed as FAILED and  not CANCELLED, that is as intended.", "When a Livy Session is stopped  (deleted) is sends a command to all the running jobs (in your case each of  you apps only have one \"Job\") to set as failed.", "@Marcelo you wrote the code that does this, do you remember why you had  Jobs killed instead of cancelled when a Livy session is stopped?", "Otherwise  we may be able to open a JIRA and change this, but I am unsure of any  potential consequences.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth* <https://github.com/ajbozarth    505 Howard Street  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  San Francisco, CA 94105  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  United States  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g     [image: Inactive hide details for kant kodali ---01/23/2018 11:44:26  PM---I tried POST to sessions/{session id}/jobs/{job id}/cancel a]kant  kodali ---01/23/2018 11:44:26 PM---I tried POST to sessions/{session  id}/jobs/{job id}/cancel and that doesn't seem to cancel either."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["When a Livy Session is stopped  (deleted) is sends a command to all the running jobs (in your case each of  you apps only have one \"Job\") to set as failed.", "@Marcelo you wrote the code that does this, do you remember why you had  Jobs killed instead of cancelled when a Livy session is stopped?", "Otherwise  we may be able to open a JIRA and change this, but I am unsure of any  potential consequences.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth* <https://github.com/ajbozarth    505 Howard Street  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  San Francisco, CA 94105  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  United States  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g     [image: Inactive hide details for kant kodali ---01/23/2018 11:44:26  PM---I tried POST to sessions/{session id}/jobs/{job id}/cancel a]kant  kodali ---01/23/2018 11:44:26 PM---I tried POST to sessions/{session  id}/jobs/{job id}/cancel and that doesn't seem to cancel either.", "From: kant kodali <kanth909@gmail.com  To: user@livy.incubator.apache.org  Date: 01/23/2018 11:44 PM   Subject: Re: How to cancel the running streaming job using livy?"], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth* <https://github.com/ajbozarth    505 Howard Street  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  San Francisco, CA 94105  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  United States  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g     [image: Inactive hide details for kant kodali ---01/23/2018 11:44:26  PM---I tried POST to sessions/{session id}/jobs/{job id}/cancel a]kant  kodali ---01/23/2018 11:44:26 PM---I tried POST to sessions/{session  id}/jobs/{job id}/cancel and that doesn't seem to cancel either.", "From: kant kodali <kanth909@gmail.com  To: user@livy.incubator.apache.org  Date: 01/23/2018 11:44 PM   Subject: Re: How to cancel the running streaming job using livy?", "------------------------------     I tried  POST to sessions/{session id}/jobs/{job id}/cancel and that  doesn't seem to cancel either.", "I think first of all the word \"job\" is used  in so many context that it might be misleading.", "Imagine for a second I don't have livy and I just use spark-submit command  line to spawn ."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: kant kodali <kanth909@gmail.com  To: user@livy.incubator.apache.org  Date: 01/23/2018 11:44 PM   Subject: Re: How to cancel the running streaming job using livy?", "------------------------------     I tried  POST to sessions/{session id}/jobs/{job id}/cancel and that  doesn't seem to cancel either.", "I think first of all the word \"job\" is used  in so many context that it might be misleading.", "Imagine for a second I don't have livy and I just use spark-submit command  line to spawn .", "say I do that following   spark-submit hello1.jar // streaming job1 (runs forever)  spark-submit hello2.jar //streaming job2 (runs forever)   The number of jobs I spawned is two and now I want to be able to cancel  one of them..These jobs reads data from kafka and will be split into stages  and task now sometimes these tasks are also called jobs according to SPARK  UI for some reason."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["------------------------------     I tried  POST to sessions/{session id}/jobs/{job id}/cancel and that  doesn't seem to cancel either.", "I think first of all the word \"job\" is used  in so many context that it might be misleading.", "Imagine for a second I don't have livy and I just use spark-submit command  line to spawn .", "say I do that following   spark-submit hello1.jar // streaming job1 (runs forever)  spark-submit hello2.jar //streaming job2 (runs forever)   The number of jobs I spawned is two and now I want to be able to cancel  one of them..These jobs reads data from kafka and will be split into stages  and task now sometimes these tasks are also called jobs according to SPARK  UI for some reason.", "And looks like live may be is cancelling those with the  above end point."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think first of all the word \"job\" is used  in so many context that it might be misleading.", "Imagine for a second I don't have livy and I just use spark-submit command  line to spawn .", "say I do that following   spark-submit hello1.jar // streaming job1 (runs forever)  spark-submit hello2.jar //streaming job2 (runs forever)   The number of jobs I spawned is two and now I want to be able to cancel  one of them..These jobs reads data from kafka and will be split into stages  and task now sometimes these tasks are also called jobs according to SPARK  UI for some reason.", "And looks like live may be is cancelling those with the  above end point.", "It would be great help if someone could try from their end and see if they  are able to cancel the jobs?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Imagine for a second I don't have livy and I just use spark-submit command  line to spawn .", "say I do that following   spark-submit hello1.jar // streaming job1 (runs forever)  spark-submit hello2.jar //streaming job2 (runs forever)   The number of jobs I spawned is two and now I want to be able to cancel  one of them..These jobs reads data from kafka and will be split into stages  and task now sometimes these tasks are also called jobs according to SPARK  UI for some reason.", "And looks like live may be is cancelling those with the  above end point.", "It would be great help if someone could try from their end and see if they  are able to cancel the jobs?", "Thanks!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And looks like live may be is cancelling those with the  above end point.", "It would be great help if someone could try from their end and see if they  are able to cancel the jobs?", "Thanks!", "On Fri, Jan 19, 2018 at 4:03 PM, Alex Bozarth <*ajbozart@us.ibm.com*  <ajbozart@us.ibm.com wrote:      Ah, that's why I couldn't find cancel in JobHandle, but it was     implemented in all it's implementations, which all implement it as would be     expected.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=fDK7aF_qwcx3-sCSfUCbzeju-yaB8rqcutS_AuW_BRs&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=         [image: Inactive hide details for Marcelo Vanzin ---01/19/2018     03:55:43 PM---A JobHandle (which you get by submitting a Job) is a Futur]Marcelo     Vanzin ---01/19/2018 03:55:43 PM---A JobHandle (which you get by submitting     a Job) is a Future, and Futures have a \"cancel()\" method."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It would be great help if someone could try from their end and see if they  are able to cancel the jobs?", "Thanks!", "On Fri, Jan 19, 2018 at 4:03 PM, Alex Bozarth <*ajbozart@us.ibm.com*  <ajbozart@us.ibm.com wrote:      Ah, that's why I couldn't find cancel in JobHandle, but it was     implemented in all it's implementations, which all implement it as would be     expected.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=fDK7aF_qwcx3-sCSfUCbzeju-yaB8rqcutS_AuW_BRs&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=         [image: Inactive hide details for Marcelo Vanzin ---01/19/2018     03:55:43 PM---A JobHandle (which you get by submitting a Job) is a Futur]Marcelo     Vanzin ---01/19/2018 03:55:43 PM---A JobHandle (which you get by submitting     a Job) is a Future, and Futures have a \"cancel()\" method.", "From: Marcelo Vanzin <*vanzin@cloudera.com* <vanzin@cloudera.com     To: *user@livy.incubator.apache.org* <user@livy.incubator.apache.org     Date: 01/19/2018 03:55 PM      Subject: Re: How to cancel the running streaming job using livy?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks!", "On Fri, Jan 19, 2018 at 4:03 PM, Alex Bozarth <*ajbozart@us.ibm.com*  <ajbozart@us.ibm.com wrote:      Ah, that's why I couldn't find cancel in JobHandle, but it was     implemented in all it's implementations, which all implement it as would be     expected.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=fDK7aF_qwcx3-sCSfUCbzeju-yaB8rqcutS_AuW_BRs&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=         [image: Inactive hide details for Marcelo Vanzin ---01/19/2018     03:55:43 PM---A JobHandle (which you get by submitting a Job) is a Futur]Marcelo     Vanzin ---01/19/2018 03:55:43 PM---A JobHandle (which you get by submitting     a Job) is a Future, and Futures have a \"cancel()\" method.", "From: Marcelo Vanzin <*vanzin@cloudera.com* <vanzin@cloudera.com     To: *user@livy.incubator.apache.org* <user@livy.incubator.apache.org     Date: 01/19/2018 03:55 PM      Subject: Re: How to cancel the running streaming job using livy?", "------------------------------        A JobHandle (which you get by submitting a Job) is a Future, and     Futures have a \"cancel()\" method."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Marcelo Vanzin <*vanzin@cloudera.com* <vanzin@cloudera.com     To: *user@livy.incubator.apache.org* <user@livy.incubator.apache.org     Date: 01/19/2018 03:55 PM      Subject: Re: How to cancel the running streaming job using livy?", "------------------------------        A JobHandle (which you get by submitting a Job) is a Future, and     Futures have a \"cancel()\" method.", "I don't remember the details about how \"cancel()\" is implemented in     Livy, though.", "On Fri, Jan 19, 2018 at 3:52 PM, Alex Bozarth <*ajbozart@us.ibm.com*     <ajbozart@us.ibm.com wrote:        Ok so I looked into this a bit more.", "I misunderstood you a bit           before, the delete call is for ending livy sessions using the rest API, not           jobs and not via the Java API."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["------------------------------        A JobHandle (which you get by submitting a Job) is a Future, and     Futures have a \"cancel()\" method.", "I don't remember the details about how \"cancel()\" is implemented in     Livy, though.", "On Fri, Jan 19, 2018 at 3:52 PM, Alex Bozarth <*ajbozart@us.ibm.com*     <ajbozart@us.ibm.com wrote:        Ok so I looked into this a bit more.", "I misunderstood you a bit           before, the delete call is for ending livy sessions using the rest API, not           jobs and not via the Java API.", "As for the Job state that makes sense, if           you end the session the session kills all currently running jobs."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Jan 19, 2018 at 3:52 PM, Alex Bozarth <*ajbozart@us.ibm.com*     <ajbozart@us.ibm.com wrote:        Ok so I looked into this a bit more.", "I misunderstood you a bit           before, the delete call is for ending livy sessions using the rest API, not           jobs and not via the Java API.", "As for the Job state that makes sense, if           you end the session the session kills all currently running jobs.", "What you           want to to send cancel requests to the jobs the session is running.", "From my           research I found that there is a way to do this via the REST API, but it           isn't documented for some reason."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I misunderstood you a bit           before, the delete call is for ending livy sessions using the rest API, not           jobs and not via the Java API.", "As for the Job state that makes sense, if           you end the session the session kills all currently running jobs.", "What you           want to to send cancel requests to the jobs the session is running.", "From my           research I found that there is a way to do this via the REST API, but it           isn't documented for some reason.", "Doing a POST to /{session id}/jobs/{job           id}/cancel will cancel a job."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As for the Job state that makes sense, if           you end the session the session kills all currently running jobs.", "What you           want to to send cancel requests to the jobs the session is running.", "From my           research I found that there is a way to do this via the REST API, but it           isn't documented for some reason.", "Doing a POST to /{session id}/jobs/{job           id}/cancel will cancel a job.", "As for the Java API, the feature isn't part           of the Java interface, but most implementations of it add it, such as the           Scala API which ScalaJobHandle class on sumbit which has a cancel function."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What you           want to to send cancel requests to the jobs the session is running.", "From my           research I found that there is a way to do this via the REST API, but it           isn't documented for some reason.", "Doing a POST to /{session id}/jobs/{job           id}/cancel will cancel a job.", "As for the Java API, the feature isn't part           of the Java interface, but most implementations of it add it, such as the           Scala API which ScalaJobHandle class on sumbit which has a cancel function.", "I'm not sure how you're submitting you jobs, but there should be a cancel           function available to you somewhere depending on the client you're using."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Doing a POST to /{session id}/jobs/{job           id}/cancel will cancel a job.", "As for the Java API, the feature isn't part           of the Java interface, but most implementations of it add it, such as the           Scala API which ScalaJobHandle class on sumbit which has a cancel function.", "I'm not sure how you're submitting you jobs, but there should be a cancel           function available to you somewhere depending on the client you're using.", "From this discussion I've realized our current documentation is even more           lacking that I had thought.", "*Alex Bozarth*     Software Engineer     Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=gMcUXOnL9YD3_CIOpwNX4jqFVWhx0l6DAsJYTKN9HVU&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=               [image: Inactive hide details for kant kodali ---01/18/2018           06:09:59 PM---Also just tried the below and got the state."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As for the Java API, the feature isn't part           of the Java interface, but most implementations of it add it, such as the           Scala API which ScalaJobHandle class on sumbit which has a cancel function.", "I'm not sure how you're submitting you jobs, but there should be a cancel           function available to you somewhere depending on the client you're using.", "From this discussion I've realized our current documentation is even more           lacking that I had thought.", "*Alex Bozarth*     Software Engineer     Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=gMcUXOnL9YD3_CIOpwNX4jqFVWhx0l6DAsJYTKN9HVU&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=               [image: Inactive hide details for kant kodali ---01/18/2018           06:09:59 PM---Also just tried the below and got the state.", "It ended up in \"]kant           kodali ---01/18/2018 06:09:59 PM---Also just tried the below and got the           state."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm not sure how you're submitting you jobs, but there should be a cancel           function available to you somewhere depending on the client you're using.", "From this discussion I've realized our current documentation is even more           lacking that I had thought.", "*Alex Bozarth*     Software Engineer     Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=gMcUXOnL9YD3_CIOpwNX4jqFVWhx0l6DAsJYTKN9HVU&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=               [image: Inactive hide details for kant kodali ---01/18/2018           06:09:59 PM---Also just tried the below and got the state.", "It ended up in \"]kant           kodali ---01/18/2018 06:09:59 PM---Also just tried the below and got the           state.", "It ended up in \"FAILED\" stated when I expected it to be            From: kant kodali <*kanth909@gmail.com* <kanth909@gmail.com           To: *user@livy.incubator.apache.org*           <user@livy.incubator.apache.org           Date: 01/18/2018 06:09 PM           Subject: Re: How to cancel the running streaming job using livy?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From this discussion I've realized our current documentation is even more           lacking that I had thought.", "*Alex Bozarth*     Software Engineer     Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=gMcUXOnL9YD3_CIOpwNX4jqFVWhx0l6DAsJYTKN9HVU&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=               [image: Inactive hide details for kant kodali ---01/18/2018           06:09:59 PM---Also just tried the below and got the state.", "It ended up in \"]kant           kodali ---01/18/2018 06:09:59 PM---Also just tried the below and got the           state.", "It ended up in \"FAILED\" stated when I expected it to be            From: kant kodali <*kanth909@gmail.com* <kanth909@gmail.com           To: *user@livy.incubator.apache.org*           <user@livy.incubator.apache.org           Date: 01/18/2018 06:09 PM           Subject: Re: How to cancel the running streaming job using livy?", "------------------------------              Also just tried the below and got the state."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*Alex Bozarth*     Software Engineer     Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=gMcUXOnL9YD3_CIOpwNX4jqFVWhx0l6DAsJYTKN9HVU&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=dcr5yrMAHblD8Ur9vfpBsXcOzNGHtaEF9jk5yMBv4Kk&s=Iu4BJQb_gsqB3B1AXW2WTuFJsI-peBqIQyczkuK3MMU&e=               [image: Inactive hide details for kant kodali ---01/18/2018           06:09:59 PM---Also just tried the below and got the state.", "It ended up in \"]kant           kodali ---01/18/2018 06:09:59 PM---Also just tried the below and got the           state.", "It ended up in \"FAILED\" stated when I expected it to be            From: kant kodali <*kanth909@gmail.com* <kanth909@gmail.com           To: *user@livy.incubator.apache.org*           <user@livy.incubator.apache.org           Date: 01/18/2018 06:09 PM           Subject: Re: How to cancel the running streaming job using livy?", "------------------------------              Also just tried the below and got the state.", "It ended up in           \"FAILED\" stated when I expected it to be in \"CANCELLED\" state."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It ended up in \"]kant           kodali ---01/18/2018 06:09:59 PM---Also just tried the below and got the           state.", "It ended up in \"FAILED\" stated when I expected it to be            From: kant kodali <*kanth909@gmail.com* <kanth909@gmail.com           To: *user@livy.incubator.apache.org*           <user@livy.incubator.apache.org           Date: 01/18/2018 06:09 PM           Subject: Re: How to cancel the running streaming job using livy?", "------------------------------              Also just tried the below and got the state.", "It ended up in           \"FAILED\" stated when I expected it to be in \"CANCELLED\" state.", "Also from           the docs it is not clear if it kills the session or the job?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It ended up in \"FAILED\" stated when I expected it to be            From: kant kodali <*kanth909@gmail.com* <kanth909@gmail.com           To: *user@livy.incubator.apache.org*           <user@livy.incubator.apache.org           Date: 01/18/2018 06:09 PM           Subject: Re: How to cancel the running streaming job using livy?", "------------------------------              Also just tried the below and got the state.", "It ended up in           \"FAILED\" stated when I expected it to be in \"CANCELLED\" state.", "Also from           the docs it is not clear if it kills the session or the job?", "if it kills           the session I can't spawn any other Job."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["if it kills           the session I can't spawn any other Job.", "Sorry cancelling job had been a           bit confusing for me.", "DELETE /sessions/0              On Thu, Jan 18, 2018 at 5:55 PM, kant kodali <           *kanth909@gmail.com* <kanth909@gmail.com wrote:              oh this raises couple questions.", "1) Is there a programmatic way to cancel a job?", "2) is  there any programmatic way to get session id?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sorry cancelling job had been a           bit confusing for me.", "DELETE /sessions/0              On Thu, Jan 18, 2018 at 5:55 PM, kant kodali <           *kanth909@gmail.com* <kanth909@gmail.com wrote:              oh this raises couple questions.", "1) Is there a programmatic way to cancel a job?", "2) is  there any programmatic way to get session id?", "If not, how do I get a sessionId when I spawn multiple jobs or multiple                       sessions?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["DELETE /sessions/0              On Thu, Jan 18, 2018 at 5:55 PM, kant kodali <           *kanth909@gmail.com* <kanth909@gmail.com wrote:              oh this raises couple questions.", "1) Is there a programmatic way to cancel a job?", "2) is  there any programmatic way to get session id?", "If not, how do I get a sessionId when I spawn multiple jobs or multiple                       sessions?", "On Thu, Jan 18, 2018 at 5:39 PM, Alex Bozarth <                       *ajbozart@us.ibm.com* <ajbozart@us.ibm.com wrote:                       You make a DELETE call as detailed here:                       *http://livy.apache.org/docs/latest/rest-api.html#response*                       <https://urldefense.proofpoint.com/v2/url?u=http-3A__livy.apache.org_docs_latest_rest-2Dapi.html-23response&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=eAcZY6sAN_mkDv5Ves9UtZaotVvvUc3BBdkCEV_CqVg&e=                       *Alex Bozarth*                       Software Engineer                       Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=EV7HPze6ToE8xgFtDOw9zE2b3sGYWSW1rB-7ZhiJRok&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=                           [image: Inactive hide details for kant kodali                       ---01/18/2018 05:34:07 PM---Hi All, I was able to submit a streaming job to                       livy however]kant kodali ---01/18/2018 05:34:07                       PM---Hi All, I was able to submit a streaming job to livy however I wasn't                       able to find                        From: kant kodali <*kanth909@gmail.com*                       <kanth909@gmail.com                       To: *user@livy.incubator.apache.org*                       <user@livy.incubator.apache.org                       Date: 01/18/2018 05:34 PM                       Subject: How to cancel the running streaming job                       using livy?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2) is  there any programmatic way to get session id?", "If not, how do I get a sessionId when I spawn multiple jobs or multiple                       sessions?", "On Thu, Jan 18, 2018 at 5:39 PM, Alex Bozarth <                       *ajbozart@us.ibm.com* <ajbozart@us.ibm.com wrote:                       You make a DELETE call as detailed here:                       *http://livy.apache.org/docs/latest/rest-api.html#response*                       <https://urldefense.proofpoint.com/v2/url?u=http-3A__livy.apache.org_docs_latest_rest-2Dapi.html-23response&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=eAcZY6sAN_mkDv5Ves9UtZaotVvvUc3BBdkCEV_CqVg&e=                       *Alex Bozarth*                       Software Engineer                       Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=EV7HPze6ToE8xgFtDOw9zE2b3sGYWSW1rB-7ZhiJRok&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=                           [image: Inactive hide details for kant kodali                       ---01/18/2018 05:34:07 PM---Hi All, I was able to submit a streaming job to                       livy however]kant kodali ---01/18/2018 05:34:07                       PM---Hi All, I was able to submit a streaming job to livy however I wasn't                       able to find                        From: kant kodali <*kanth909@gmail.com*                       <kanth909@gmail.com                       To: *user@livy.incubator.apache.org*                       <user@livy.incubator.apache.org                       Date: 01/18/2018 05:34 PM                       Subject: How to cancel the running streaming job                       using livy?", "------------------------------                          Hi All,                        I was able to submit a streaming job to livy however                       I wasn't able to find any way to cancel the running the job?", "Please let me                       know."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Jan 18, 2018 at 5:39 PM, Alex Bozarth <                       *ajbozart@us.ibm.com* <ajbozart@us.ibm.com wrote:                       You make a DELETE call as detailed here:                       *http://livy.apache.org/docs/latest/rest-api.html#response*                       <https://urldefense.proofpoint.com/v2/url?u=http-3A__livy.apache.org_docs_latest_rest-2Dapi.html-23response&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=eAcZY6sAN_mkDv5Ves9UtZaotVvvUc3BBdkCEV_CqVg&e=                       *Alex Bozarth*                       Software Engineer                       Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=EV7HPze6ToE8xgFtDOw9zE2b3sGYWSW1rB-7ZhiJRok&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=                           [image: Inactive hide details for kant kodali                       ---01/18/2018 05:34:07 PM---Hi All, I was able to submit a streaming job to                       livy however]kant kodali ---01/18/2018 05:34:07                       PM---Hi All, I was able to submit a streaming job to livy however I wasn't                       able to find                        From: kant kodali <*kanth909@gmail.com*                       <kanth909@gmail.com                       To: *user@livy.incubator.apache.org*                       <user@livy.incubator.apache.org                       Date: 01/18/2018 05:34 PM                       Subject: How to cancel the running streaming job                       using livy?", "------------------------------                          Hi All,                        I was able to submit a streaming job to livy however                       I wasn't able to find any way to cancel the running the job?", "Please let me                       know.", "Thanks!", "--     Marcelo        --  Marcelo  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello everbody!", "I'd like use a basic template feature with trinidad and like to inform me about the actual situation of this issue.", "I've found a statement of Matthias Wessendorf, he states that one should use facelets.", "However, facelets isn't a standard.", "Furthermore, I couldn't find any examples with trinidad and my migrated JSF examples didn't work (\"illegal component hierarchy detected, expected UIXCommand but found another type of component instead.\")."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'd like use a basic template feature with trinidad and like to inform me about the actual situation of this issue.", "I've found a statement of Matthias Wessendorf, he states that one should use facelets.", "However, facelets isn't a standard.", "Furthermore, I couldn't find any examples with trinidad and my migrated JSF examples didn't work (\"illegal component hierarchy detected, expected UIXCommand but found another type of component instead.\").", "I reckon, it might be a problem with nested forms, however, this would be another entry..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I've found a statement of Matthias Wessendorf, he states that one should use facelets.", "However, facelets isn't a standard.", "Furthermore, I couldn't find any examples with trinidad and my migrated JSF examples didn't work (\"illegal component hierarchy detected, expected UIXCommand but found another type of component instead.\").", "I reckon, it might be a problem with nested forms, however, this would be another entry...", "In my opinion, this is more than one reason do not move to facelets."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, facelets isn't a standard.", "Furthermore, I couldn't find any examples with trinidad and my migrated JSF examples didn't work (\"illegal component hierarchy detected, expected UIXCommand but found another type of component instead.\").", "I reckon, it might be a problem with nested forms, however, this would be another entry...", "In my opinion, this is more than one reason do not move to facelets.", "Are there any other options?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I reckon, it might be a problem with nested forms, however, this would be another entry...", "In my opinion, this is more than one reason do not move to facelets.", "Are there any other options?", "Did somebody get Tiles working with trinidad?", "Is there any roadmap for this issue?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In my opinion, this is more than one reason do not move to facelets.", "Are there any other options?", "Did somebody get Tiles working with trinidad?", "Is there any roadmap for this issue?", "Many thanks  Kuno   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Forwarding to a couple other related lists for feedback.", "Responses back to ooo-dev@incubator.apache.org, please.", "-Rob   ---------- Forwarded message ---------- From: Noah Tilton <noahktilton@gmail.com Date: Tue, May 8, 2012 at 1:15 AM Subject: ODF Command Line Tools -- Request for community feedback To: odf-dev@incubator.apache.org   Hello,  I was selected as a gsoc2012 participant by the Apache Software Foundation (ASF) for the ODF Command Line Tools (OCLT) project.", "At Rob Weir's suggestion, I have proposed an initial feedback period (during Community Bonding) to gauge the interest and leanings of the community regarding the OCLT tools.", "My proposal document is less a roadmap and more a hodgepodge of different ideas to solving the problem of the OCLS:  https://docs.google.com/open?id=0B8g_FgudO4EdbWk0T01Zbmo4RFU  Please feel free to discuss pros/cons of the different approaches I have proposed, or to propose new ideas."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Responses back to ooo-dev@incubator.apache.org, please.", "-Rob   ---------- Forwarded message ---------- From: Noah Tilton <noahktilton@gmail.com Date: Tue, May 8, 2012 at 1:15 AM Subject: ODF Command Line Tools -- Request for community feedback To: odf-dev@incubator.apache.org   Hello,  I was selected as a gsoc2012 participant by the Apache Software Foundation (ASF) for the ODF Command Line Tools (OCLT) project.", "At Rob Weir's suggestion, I have proposed an initial feedback period (during Community Bonding) to gauge the interest and leanings of the community regarding the OCLT tools.", "My proposal document is less a roadmap and more a hodgepodge of different ideas to solving the problem of the OCLS:  https://docs.google.com/open?id=0B8g_FgudO4EdbWk0T01Zbmo4RFU  Please feel free to discuss pros/cons of the different approaches I have proposed, or to propose new ideas.", "Cheers,  -- Noah  "], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["Check it out.", "The setup  instructions are in docs/java_setup.html.", "Shanti  Harold Lim wrote:  Hi All,   When will the Java Implementation of Olio will be released?", "Also, will there be possibly implementations that use HBase in the future?", "I am currently studying the viability of using a completely distributed file systems (HBase + HDFS) for Web 2.0 applications."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The setup  instructions are in docs/java_setup.html.", "Shanti  Harold Lim wrote:  Hi All,   When will the Java Implementation of Olio will be released?", "Also, will there be possibly implementations that use HBase in the future?", "I am currently studying the viability of using a completely distributed file systems (HBase + HDFS) for Web 2.0 applications.", "Thanks,  Harold                "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Interesting question.", "So if you have legacy/pre-existing data formats, then the use case for DFDL is clear.", "So I think of your question as this really: What are use cases for DFDL for \"new\" applications?", "I think new applications that are inventing file formats may end up using DFDL if the application authors are too lazy to use say, XML as the file format.", "If they just do whatever is easiest to write-out from their favorite programming language, then they're going to get an ad-hoc file format, and in the future if some *other* software wants to read that file, then DFDL is a tool of choice."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So if you have legacy/pre-existing data formats, then the use case for DFDL is clear.", "So I think of your question as this really: What are use cases for DFDL for \"new\" applications?", "I think new applications that are inventing file formats may end up using DFDL if the application authors are too lazy to use say, XML as the file format.", "If they just do whatever is easiest to write-out from their favorite programming language, then they're going to get an ad-hoc file format, and in the future if some *other* software wants to read that file, then DFDL is a tool of choice.", "But it is preferable if new applications that invent file formats do so purposefully and use a standard text-oriented representation like XML."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So I think of your question as this really: What are use cases for DFDL for \"new\" applications?", "I think new applications that are inventing file formats may end up using DFDL if the application authors are too lazy to use say, XML as the file format.", "If they just do whatever is easiest to write-out from their favorite programming language, then they're going to get an ad-hoc file format, and in the future if some *other* software wants to read that file, then DFDL is a tool of choice.", "But it is preferable if new applications that invent file formats do so purposefully and use a standard text-oriented representation like XML.", "(Could be JSON too, but lack of a schema language for JSON makes it far less desirable IMHO.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think new applications that are inventing file formats may end up using DFDL if the application authors are too lazy to use say, XML as the file format.", "If they just do whatever is easiest to write-out from their favorite programming language, then they're going to get an ad-hoc file format, and in the future if some *other* software wants to read that file, then DFDL is a tool of choice.", "But it is preferable if new applications that invent file formats do so purposefully and use a standard text-oriented representation like XML.", "(Could be JSON too, but lack of a schema language for JSON makes it far less desirable IMHO.)", "The exceptions here are if speed/space concerns make the overhead of XML too high."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If they just do whatever is easiest to write-out from their favorite programming language, then they're going to get an ad-hoc file format, and in the future if some *other* software wants to read that file, then DFDL is a tool of choice.", "But it is preferable if new applications that invent file formats do so purposefully and use a standard text-oriented representation like XML.", "(Could be JSON too, but lack of a schema language for JSON makes it far less desirable IMHO.)", "The exceptions here are if speed/space concerns make the overhead of XML too high.", "There is an environmental argument against using XML."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But it is preferable if new applications that invent file formats do so purposefully and use a standard text-oriented representation like XML.", "(Could be JSON too, but lack of a schema language for JSON makes it far less desirable IMHO.)", "The exceptions here are if speed/space concerns make the overhead of XML too high.", "There is an environmental argument against using XML.", "Consider all the wasted CPU cycles in the world dealing with XML's verbose and redundant structure."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The exceptions here are if speed/space concerns make the overhead of XML too high.", "There is an environmental argument against using XML.", "Consider all the wasted CPU cycles in the world dealing with XML's verbose and redundant structure.", "Given that computers use lots of energy, the \"Carbon Footprint\" of XML on global scale is something to think about.", "Makes me wish EXI would catch on more."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is an environmental argument against using XML.", "Consider all the wasted CPU cycles in the world dealing with XML's verbose and redundant structure.", "Given that computers use lots of energy, the \"Carbon Footprint\" of XML on global scale is something to think about.", "Makes me wish EXI would catch on more.", "I also wish XML would just allow a non-verbose close tag like <foovalue</ where the end tag doesn't have to repeat the open tag."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Consider all the wasted CPU cycles in the world dealing with XML's verbose and redundant structure.", "Given that computers use lots of energy, the \"Carbon Footprint\" of XML on global scale is something to think about.", "Makes me wish EXI would catch on more.", "I also wish XML would just allow a non-verbose close tag like <foovalue</ where the end tag doesn't have to repeat the open tag.", "This would reduce XML's overhead to much closer to JSON or Lisp S-expressions again."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Given that computers use lots of energy, the \"Carbon Footprint\" of XML on global scale is something to think about.", "Makes me wish EXI would catch on more.", "I also wish XML would just allow a non-verbose close tag like <foovalue</ where the end tag doesn't have to repeat the open tag.", "This would reduce XML's overhead to much closer to JSON or Lisp S-expressions again.", "But I digress."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Makes me wish EXI would catch on more.", "I also wish XML would just allow a non-verbose close tag like <foovalue</ where the end tag doesn't have to repeat the open tag.", "This would reduce XML's overhead to much closer to JSON or Lisp S-expressions again.", "But I digress.", "But ignoring all that, there are cases where use of an expensive data format like XML just won't allow you to achieve the goal of your software."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I also wish XML would just allow a non-verbose close tag like <foovalue</ where the end tag doesn't have to repeat the open tag.", "This would reduce XML's overhead to much closer to JSON or Lisp S-expressions again.", "But I digress.", "But ignoring all that, there are cases where use of an expensive data format like XML just won't allow you to achieve the goal of your software.", "The two cases I know of where something like XML is unacceptable and one might prefer a dense binary data format are cutting-edge supercomputing applications - where every bit counts in space/speed if the application is going to work at all, and also ultra-low-power computing, where every bit counts, because the cost of just data compress/decompress consumes too much battery power."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This would reduce XML's overhead to much closer to JSON or Lisp S-expressions again.", "But I digress.", "But ignoring all that, there are cases where use of an expensive data format like XML just won't allow you to achieve the goal of your software.", "The two cases I know of where something like XML is unacceptable and one might prefer a dense binary data format are cutting-edge supercomputing applications - where every bit counts in space/speed if the application is going to work at all, and also ultra-low-power computing, where every bit counts, because the cost of just data compress/decompress consumes too much battery power.", "But even then, a standard binary format like EXI (binary XML - same infoset as XML, just denser binary representation) may be preferable to an ad-hoc file format with DFDL schema."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But I digress.", "But ignoring all that, there are cases where use of an expensive data format like XML just won't allow you to achieve the goal of your software.", "The two cases I know of where something like XML is unacceptable and one might prefer a dense binary data format are cutting-edge supercomputing applications - where every bit counts in space/speed if the application is going to work at all, and also ultra-low-power computing, where every bit counts, because the cost of just data compress/decompress consumes too much battery power.", "But even then, a standard binary format like EXI (binary XML - same infoset as XML, just denser binary representation) may be preferable to an ad-hoc file format with DFDL schema.", "Lastly another use case I've found for DFDL is what I call \"CSV-like\" data files."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The two cases I know of where something like XML is unacceptable and one might prefer a dense binary data format are cutting-edge supercomputing applications - where every bit counts in space/speed if the application is going to work at all, and also ultra-low-power computing, where every bit counts, because the cost of just data compress/decompress consumes too much battery power.", "But even then, a standard binary format like EXI (binary XML - same infoset as XML, just denser binary representation) may be preferable to an ad-hoc file format with DFDL schema.", "Lastly another use case I've found for DFDL is what I call \"CSV-like\" data files.", "These arise when human beings will be editing data files by hand.", "I have a lot of experience of \"CSV\" data files that aren't at all well behaved as true CSV data files are supposed to be."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But even then, a standard binary format like EXI (binary XML - same infoset as XML, just denser binary representation) may be preferable to an ad-hoc file format with DFDL schema.", "Lastly another use case I've found for DFDL is what I call \"CSV-like\" data files.", "These arise when human beings will be editing data files by hand.", "I have a lot of experience of \"CSV\" data files that aren't at all well behaved as true CSV data files are supposed to be.", "Given a spreadsheet program like MS-Excel, people will create a spreadsheet document with all sorts of headers and sections on a sheet."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Lastly another use case I've found for DFDL is what I call \"CSV-like\" data files.", "These arise when human beings will be editing data files by hand.", "I have a lot of experience of \"CSV\" data files that aren't at all well behaved as true CSV data files are supposed to be.", "Given a spreadsheet program like MS-Excel, people will create a spreadsheet document with all sorts of headers and sections on a sheet.", "Then they'll export that sheet as \"CSV\" and claim the file is CSV data."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Given a spreadsheet program like MS-Excel, people will create a spreadsheet document with all sorts of headers and sections on a sheet.", "Then they'll export that sheet as \"CSV\" and claim the file is CSV data.", "These sorts of \"CSV-like\" files are often full of inconsistencies.", "Empty cells are sometimes empty string, sometimes all-whitespace strings, sometimes  various markers like \"--\" or \"N/A\" or \"none\"  A DFDL schema can be written which handles all these human inconsistency factors, skipping section headers, standardizing \"--\", \"N/A\", etc.", "The result is well-behaved XML data set from an inconsistent human-edited CSV-like data file."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Then they'll export that sheet as \"CSV\" and claim the file is CSV data.", "These sorts of \"CSV-like\" files are often full of inconsistencies.", "Empty cells are sometimes empty string, sometimes all-whitespace strings, sometimes  various markers like \"--\" or \"N/A\" or \"none\"  A DFDL schema can be written which handles all these human inconsistency factors, skipping section headers, standardizing \"--\", \"N/A\", etc.", "The result is well-behaved XML data set from an inconsistent human-edited CSV-like data file.", "-mike beckerle Tresys Technology    ________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Tuesday, February 19, 2019 12:16:09 PM To: users@daffodil.apache.org Subject: With the tremendous agility that DFDL provides, what is the role of XML?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These sorts of \"CSV-like\" files are often full of inconsistencies.", "Empty cells are sometimes empty string, sometimes all-whitespace strings, sometimes  various markers like \"--\" or \"N/A\" or \"none\"  A DFDL schema can be written which handles all these human inconsistency factors, skipping section headers, standardizing \"--\", \"N/A\", etc.", "The result is well-behaved XML data set from an inconsistent human-edited CSV-like data file.", "-mike beckerle Tresys Technology    ________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Tuesday, February 19, 2019 12:16:09 PM To: users@daffodil.apache.org Subject: With the tremendous agility that DFDL provides, what is the role of XML?", "What is the role of binary?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Empty cells are sometimes empty string, sometimes all-whitespace strings, sometimes  various markers like \"--\" or \"N/A\" or \"none\"  A DFDL schema can be written which handles all these human inconsistency factors, skipping section headers, standardizing \"--\", \"N/A\", etc.", "The result is well-behaved XML data set from an inconsistent human-edited CSV-like data file.", "-mike beckerle Tresys Technology    ________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Tuesday, February 19, 2019 12:16:09 PM To: users@daffodil.apache.org Subject: With the tremendous agility that DFDL provides, what is the role of XML?", "What is the role of binary?", "Hello DFDL community,  DFDL gives us tremendous agility - we can quickly and easily transform binary to XML and XML to binary."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The result is well-behaved XML data set from an inconsistent human-edited CSV-like data file.", "-mike beckerle Tresys Technology    ________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Tuesday, February 19, 2019 12:16:09 PM To: users@daffodil.apache.org Subject: With the tremendous agility that DFDL provides, what is the role of XML?", "What is the role of binary?", "Hello DFDL community,  DFDL gives us tremendous agility - we can quickly and easily transform binary to XML and XML to binary.", "Binary, with its conciseness, is beautiful for moving data."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-mike beckerle Tresys Technology    ________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Tuesday, February 19, 2019 12:16:09 PM To: users@daffodil.apache.org Subject: With the tremendous agility that DFDL provides, what is the role of XML?", "What is the role of binary?", "Hello DFDL community,  DFDL gives us tremendous agility - we can quickly and easily transform binary to XML and XML to binary.", "Binary, with its conciseness, is beautiful for moving data.", "XML, with its vast tool suite, is beautiful for processing data."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Binary, with its conciseness, is beautiful for moving data.", "XML, with its vast tool suite, is beautiful for processing data.", "What do you see as the role of XML?", "The role of binary?", "Use binary when moving data, use XML when processing data?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["XML, with its vast tool suite, is beautiful for processing data.", "What do you see as the role of XML?", "The role of binary?", "Use binary when moving data, use XML when processing data?", "Most images (JPEG, GIF, PNG, etc.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What do you see as the role of XML?", "The role of binary?", "Use binary when moving data, use XML when processing data?", "Most images (JPEG, GIF, PNG, etc.)", "are binary and are processed in their binary form."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The role of binary?", "Use binary when moving data, use XML when processing data?", "Most images (JPEG, GIF, PNG, etc.)", "are binary and are processed in their binary form.", "So XML isn't necessarily the ideal form for processing data."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Use binary when moving data, use XML when processing data?", "Most images (JPEG, GIF, PNG, etc.)", "are binary and are processed in their binary form.", "So XML isn't necessarily the ideal form for processing data.", "I am eager to hear your thoughts/opinions/comments on this subject."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Most images (JPEG, GIF, PNG, etc.)", "are binary and are processed in their binary form.", "So XML isn't necessarily the ideal form for processing data.", "I am eager to hear your thoughts/opinions/comments on this subject.", "/Roger  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Mukul   On 30/11/19 4:13 pm, tison wrote:  Hello here again,   I'm not sure how \"Ozone uses the shaded ByteString version\".", "Could you   please show me the way for doing so?", "Best,  tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com   \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db \u4e0a\u534811:20\u5199\u9053\uff1a       Thanks for your information Mukul!", "I have one more question."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could you   please show me the way for doing so?", "Best,  tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com   \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db \u4e0a\u534811:20\u5199\u9053\uff1a       Thanks for your information Mukul!", "I have one more question.", "Even if I want to use the shaded      ByteString version when      compile with protoc it requires protobuf-java deps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Best,  tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com   \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db \u4e0a\u534811:20\u5199\u9053\uff1a       Thanks for your information Mukul!", "I have one more question.", "Even if I want to use the shaded      ByteString version when      compile with protoc it requires protobuf-java deps.", "How can I      configure protoc use      the shaded version to generate codes?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["tison <wander4096@gmail.com <mailto:wander4096@gmail.com   \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db \u4e0a\u534811:20\u5199\u9053\uff1a       Thanks for your information Mukul!", "I have one more question.", "Even if I want to use the shaded      ByteString version when      compile with protoc it requires protobuf-java deps.", "How can I      configure protoc use      the shaded version to generate codes?", "Best,      tison."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Even if I want to use the shaded      ByteString version when      compile with protoc it requires protobuf-java deps.", "How can I      configure protoc use      the shaded version to generate codes?", "Best,      tison.", "Mukul Kumar Singh <mksingh.apache@gmail.com      <mailto:mksingh.apache@gmail.com \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db      \u4e0a\u534811:15\u5199\u9053\uff1a           Hi Tison,           Thanks for the interest in Ratis.", "There are 2 options as you          have already noticed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Best,      tison.", "Mukul Kumar Singh <mksingh.apache@gmail.com      <mailto:mksingh.apache@gmail.com \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db      \u4e0a\u534811:15\u5199\u9053\uff1a           Hi Tison,           Thanks for the interest in Ratis.", "There are 2 options as you          have already noticed.", "a) Ozone which is a consumer of Ratis, we have used the shaded          ByteString version in the Ozone codebase.", "b) We can have a utility for conversion as you have already          pointed out."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Mukul Kumar Singh <mksingh.apache@gmail.com      <mailto:mksingh.apache@gmail.com \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db      \u4e0a\u534811:15\u5199\u9053\uff1a           Hi Tison,           Thanks for the interest in Ratis.", "There are 2 options as you          have already noticed.", "a) Ozone which is a consumer of Ratis, we have used the shaded          ByteString version in the Ozone codebase.", "b) We can have a utility for conversion as you have already          pointed out.", "Thanks,          Mukul            On 13/11/19 8:28 pm, tison wrote:          Well I find a way to write a utility for convertion."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There are 2 options as you          have already noticed.", "a) Ozone which is a consumer of Ratis, we have used the shaded          ByteString version in the Ozone codebase.", "b) We can have a utility for conversion as you have already          pointed out.", "Thanks,          Mukul            On 13/11/19 8:28 pm, tison wrote:          Well I find a way to write a utility for convertion.", "Best,          tison."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["a) Ozone which is a consumer of Ratis, we have used the shaded          ByteString version in the Ozone codebase.", "b) We can have a utility for conversion as you have already          pointed out.", "Thanks,          Mukul            On 13/11/19 8:28 pm, tison wrote:          Well I find a way to write a utility for convertion.", "Best,          tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com          \u4e8e2019\u5e7411\u670813\u65e5\u5468\u4e09 \u4e0b\u534810:46\u5199\u9053\uff1a               Hi devs,               I am trying to develop a filesystem-view storage on              ratis, and here is the problem I meet:               When I trying to reply within `StateMachine#query` while              generating Message, compiler fails on              ByteString is not compatible with ByteString."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["b) We can have a utility for conversion as you have already          pointed out.", "Thanks,          Mukul            On 13/11/19 8:28 pm, tison wrote:          Well I find a way to write a utility for convertion.", "Best,          tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com          \u4e8e2019\u5e7411\u670813\u65e5\u5468\u4e09 \u4e0b\u534810:46\u5199\u9053\uff1a               Hi devs,               I am trying to develop a filesystem-view storage on              ratis, and here is the problem I meet:               When I trying to reply within `StateMachine#query` while              generating Message, compiler fails on              ByteString is not compatible with ByteString.", "I think it              is because ratis use shaded protobuf deps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,          Mukul            On 13/11/19 8:28 pm, tison wrote:          Well I find a way to write a utility for convertion.", "Best,          tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com          \u4e8e2019\u5e7411\u670813\u65e5\u5468\u4e09 \u4e0b\u534810:46\u5199\u9053\uff1a               Hi devs,               I am trying to develop a filesystem-view storage on              ratis, and here is the problem I meet:               When I trying to reply within `StateMachine#query` while              generating Message, compiler fails on              ByteString is not compatible with ByteString.", "I think it              is because ratis use shaded protobuf deps.", "However, how can I instance\u00a0Message outside ratis              project?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Best,          tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com          \u4e8e2019\u5e7411\u670813\u65e5\u5468\u4e09 \u4e0b\u534810:46\u5199\u9053\uff1a               Hi devs,               I am trying to develop a filesystem-view storage on              ratis, and here is the problem I meet:               When I trying to reply within `StateMachine#query` while              generating Message, compiler fails on              ByteString is not compatible with ByteString.", "I think it              is because ratis use shaded protobuf deps.", "However, how can I instance\u00a0Message outside ratis              project?", "Shall I also depends on              ratis-thirdparty-misc?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["tison <wander4096@gmail.com <mailto:wander4096@gmail.com          \u4e8e2019\u5e7411\u670813\u65e5\u5468\u4e09 \u4e0b\u534810:46\u5199\u9053\uff1a               Hi devs,               I am trying to develop a filesystem-view storage on              ratis, and here is the problem I meet:               When I trying to reply within `StateMachine#query` while              generating Message, compiler fails on              ByteString is not compatible with ByteString.", "I think it              is because ratis use shaded protobuf deps.", "However, how can I instance\u00a0Message outside ratis              project?", "Shall I also depends on              ratis-thirdparty-misc?", "Is there a workaround?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think it              is because ratis use shaded protobuf deps.", "However, how can I instance\u00a0Message outside ratis              project?", "Shall I also depends on              ratis-thirdparty-misc?", "Is there a workaround?", "Or will              ratis provides its own abstraction for              resolving dep issues?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, how can I instance\u00a0Message outside ratis              project?", "Shall I also depends on              ratis-thirdparty-misc?", "Is there a workaround?", "Or will              ratis provides its own abstraction for              resolving dep issues?", "Best,              tison.   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The specification says this about ES:  Used in whitespace separated lists when empty string is one of the values.", "Recall that in XML an attribute's value may be delimited by either double or single quotes.", "So, are these two whitespace-separated lists equivalent?", "\"A B %ES; C D\"                 'A B \"\" C D'  /Roger  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Folks,  If we want to describe a text data format, then we specify that the data has a text representation, i.e.,  representation=\"text\"  To describe a binary data format we do this:  representation=\"binary\"  The \"representation\" terminology seems reasonable to me.", "Next, we want to describe the fundamental (atomic) units of the representation.", "For the text representation the units are characters.", "For the binary representation the units may be either bytes or bits.", "It seems to me, that \"units\" is the appropriate term for this, i.e.,  units=\"characters\" units=\"bytes\" units=\"bits\"  Sadly, that is not the term that DFDL uses."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Next, we want to describe the fundamental (atomic) units of the representation.", "For the text representation the units are characters.", "For the binary representation the units may be either bytes or bits.", "It seems to me, that \"units\" is the appropriate term for this, i.e.,  units=\"characters\" units=\"bytes\" units=\"bits\"  Sadly, that is not the term that DFDL uses.", "Instead, it uses \"lengthUnits\" as the term, i.e.,  lengthUnits=\"characters\" lengthUnits=\"bytes\" lengthUnits=\"bits\"  Why?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For the text representation the units are characters.", "For the binary representation the units may be either bytes or bits.", "It seems to me, that \"units\" is the appropriate term for this, i.e.,  units=\"characters\" units=\"bytes\" units=\"bits\"  Sadly, that is not the term that DFDL uses.", "Instead, it uses \"lengthUnits\" as the term, i.e.,  lengthUnits=\"characters\" lengthUnits=\"bytes\" lengthUnits=\"bits\"  Why?", "That terminology makes no sense to me."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For the binary representation the units may be either bytes or bits.", "It seems to me, that \"units\" is the appropriate term for this, i.e.,  units=\"characters\" units=\"bytes\" units=\"bits\"  Sadly, that is not the term that DFDL uses.", "Instead, it uses \"lengthUnits\" as the term, i.e.,  lengthUnits=\"characters\" lengthUnits=\"bytes\" lengthUnits=\"bits\"  Why?", "That terminology makes no sense to me.", "Can you give a rationale for why the term is \"lengthUnits\" and not \"units\"?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It seems to me, that \"units\" is the appropriate term for this, i.e.,  units=\"characters\" units=\"bytes\" units=\"bits\"  Sadly, that is not the term that DFDL uses.", "Instead, it uses \"lengthUnits\" as the term, i.e.,  lengthUnits=\"characters\" lengthUnits=\"bytes\" lengthUnits=\"bits\"  Why?", "That terminology makes no sense to me.", "Can you give a rationale for why the term is \"lengthUnits\" and not \"units\"?", "/Roger  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello,  If you want a skinnable icon where tr:icon name=\"arrow\", for example,  then you would create this definition in your skin file:  .AFArrowIcon:alias {   content:url(/skins/purple/images/next.png); width:11px; height: 15px;  }  - Jeanne  Meyer, Stefan wrote:  I want ro display skin specific images.", "Can I add new icons and display them with tr:icon?", "How?        "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Sergio,  dotnet-executable-config is not really a packaging, but rather only a  dependency type.", "This means, there is no lifecycle bound by default:  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  You can use the build-helper-maven-plugin with the goal attach-artifact,  to package a config together with its executable... http://mojo.codehaus.org/build-helper-maven-plugin/usage.html Section  \"Attach additional artifacts to your project\".", "I think npanday will resolve the config together with the exe.", "If it  doesn't, you add an extra dependency with the same group, name and  version plus <typedotnet-executable-config</type  The Exe-Pom could look like this...  hope that helps, -Lars  <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://maven.apache.org/POM/4.0.0\"     <modelVersion4.0.0</modelVersion     <groupIdTestGroup</groupId     <artifactIdTestArtifactName</artifactId     <packagingdotnet-executable</packaging     <nameExecutable-pom</name     <version1.0-SNAPSHOT</version     <build      <plugins        <plugin          <groupIdorg.codehaus.mojo</groupId          <artifactIdbuild-helper-maven-plugin</artifactId          <version1.5</version          <executions            <execution              <idattach-artifacts</id              <phasepackage</phase              <goals                <goalattach-artifact</goal              </goals              <configuration                <artifacts                  <artifact                    <filepathtobin/bin-name.exe.config</file                    <typedotnet-executable-config</type                  </artifact                </artifacts              </configuration            </execution          </executions        </plugin      </plugins    </build  </project      Am 12.10.10 01:12, schrieb Sergio Rupena:    I am trying to create a pom which allows me to bundle my 'app.config'  file together with my application."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This means, there is no lifecycle bound by default:  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  You can use the build-helper-maven-plugin with the goal attach-artifact,  to package a config together with its executable... http://mojo.codehaus.org/build-helper-maven-plugin/usage.html Section  \"Attach additional artifacts to your project\".", "I think npanday will resolve the config together with the exe.", "If it  doesn't, you add an extra dependency with the same group, name and  version plus <typedotnet-executable-config</type  The Exe-Pom could look like this...  hope that helps, -Lars  <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://maven.apache.org/POM/4.0.0\"     <modelVersion4.0.0</modelVersion     <groupIdTestGroup</groupId     <artifactIdTestArtifactName</artifactId     <packagingdotnet-executable</packaging     <nameExecutable-pom</name     <version1.0-SNAPSHOT</version     <build      <plugins        <plugin          <groupIdorg.codehaus.mojo</groupId          <artifactIdbuild-helper-maven-plugin</artifactId          <version1.5</version          <executions            <execution              <idattach-artifacts</id              <phasepackage</phase              <goals                <goalattach-artifact</goal              </goals              <configuration                <artifacts                  <artifact                    <filepathtobin/bin-name.exe.config</file                    <typedotnet-executable-config</type                  </artifact                </artifacts              </configuration            </execution          </executions        </plugin      </plugins    </build  </project      Am 12.10.10 01:12, schrieb Sergio Rupena:    I am trying to create a pom which allows me to bundle my 'app.config'  file together with my application.", "The documentation (see  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  <http://www.npanday.org/docs/1.2/guide/maven/project-types.html  )  suggests that this should be possible using the dotnet-executable-config  packaging type."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think npanday will resolve the config together with the exe.", "If it  doesn't, you add an extra dependency with the same group, name and  version plus <typedotnet-executable-config</type  The Exe-Pom could look like this...  hope that helps, -Lars  <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://maven.apache.org/POM/4.0.0\"     <modelVersion4.0.0</modelVersion     <groupIdTestGroup</groupId     <artifactIdTestArtifactName</artifactId     <packagingdotnet-executable</packaging     <nameExecutable-pom</name     <version1.0-SNAPSHOT</version     <build      <plugins        <plugin          <groupIdorg.codehaus.mojo</groupId          <artifactIdbuild-helper-maven-plugin</artifactId          <version1.5</version          <executions            <execution              <idattach-artifacts</id              <phasepackage</phase              <goals                <goalattach-artifact</goal              </goals              <configuration                <artifacts                  <artifact                    <filepathtobin/bin-name.exe.config</file                    <typedotnet-executable-config</type                  </artifact                </artifacts              </configuration            </execution          </executions        </plugin      </plugins    </build  </project      Am 12.10.10 01:12, schrieb Sergio Rupena:    I am trying to create a pom which allows me to bundle my 'app.config'  file together with my application.", "The documentation (see  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  <http://www.npanday.org/docs/1.2/guide/maven/project-types.html  )  suggests that this should be possible using the dotnet-executable-config  packaging type.", "Using the normal maven-compile plugin this should be doable using the  following pom:     <?xml version=\"1.0\" encoding=\"utf-8\"?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If it  doesn't, you add an extra dependency with the same group, name and  version plus <typedotnet-executable-config</type  The Exe-Pom could look like this...  hope that helps, -Lars  <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://maven.apache.org/POM/4.0.0\"     <modelVersion4.0.0</modelVersion     <groupIdTestGroup</groupId     <artifactIdTestArtifactName</artifactId     <packagingdotnet-executable</packaging     <nameExecutable-pom</name     <version1.0-SNAPSHOT</version     <build      <plugins        <plugin          <groupIdorg.codehaus.mojo</groupId          <artifactIdbuild-helper-maven-plugin</artifactId          <version1.5</version          <executions            <execution              <idattach-artifacts</id              <phasepackage</phase              <goals                <goalattach-artifact</goal              </goals              <configuration                <artifacts                  <artifact                    <filepathtobin/bin-name.exe.config</file                    <typedotnet-executable-config</type                  </artifact                </artifacts              </configuration            </execution          </executions        </plugin      </plugins    </build  </project      Am 12.10.10 01:12, schrieb Sergio Rupena:    I am trying to create a pom which allows me to bundle my 'app.config'  file together with my application.", "The documentation (see  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  <http://www.npanday.org/docs/1.2/guide/maven/project-types.html  )  suggests that this should be possible using the dotnet-executable-config  packaging type.", "Using the normal maven-compile plugin this should be doable using the  following pom:     <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"  xmlns=\"http://maven.apache.org/POM/4.0.0\"      <modelVersion4.0.0</modelVersion      <groupIdTestGroup</groupId      <artifactIdTestArtifactName.Config</artifactId      <packagingdotnet-executable-config</packaging      <nameconfiguration file pom</name      <version1.0-SNAPSHOT</version   </project     But this results in an error:     [INFO] Scanning for projects...   [INFO]  ------------------------------------------------------------------------   [INFO] Building configuration file pom   [INFO]    task-segment: [install]   [INFO]  ------------------------------------------------------------------------   [INFO]  ------------------------------------------------------------------------   [ERROR] BUILD ERROR   [INFO]  ------------------------------------------------------------------------   [INFO] Cannot find lifecycle mapping for packaging:  'dotnet-executable-config'."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The documentation (see  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  <http://www.npanday.org/docs/1.2/guide/maven/project-types.html  )  suggests that this should be possible using the dotnet-executable-config  packaging type.", "Using the normal maven-compile plugin this should be doable using the  following pom:     <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"  xmlns=\"http://maven.apache.org/POM/4.0.0\"      <modelVersion4.0.0</modelVersion      <groupIdTestGroup</groupId      <artifactIdTestArtifactName.Config</artifactId      <packagingdotnet-executable-config</packaging      <nameconfiguration file pom</name      <version1.0-SNAPSHOT</version   </project     But this results in an error:     [INFO] Scanning for projects...   [INFO]  ------------------------------------------------------------------------   [INFO] Building configuration file pom   [INFO]    task-segment: [install]   [INFO]  ------------------------------------------------------------------------   [INFO]  ------------------------------------------------------------------------   [ERROR] BUILD ERROR   [INFO]  ------------------------------------------------------------------------   [INFO] Cannot find lifecycle mapping for packaging:  'dotnet-executable-config'.", "Component descriptor cannot be found in the component repository:  org.apache.maven.lifecycle.mapping.LifecycleMappingdotnet-executable-con  fig.", "[INFO]  ------------------------------------------------------------------------   [INFO] For more information, run Maven with the -e switch   [INFO]  ------------------------------------------------------------------------   [INFO] Total time:<  1 second   [INFO] Finished at: Tue Oct 12 01:10:08 CEST 2010   [INFO] Final Memory: 1M/15M     I am using npanday 1.2.1     Any help would be appreciated,     /joe          "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Steve,   In the case of decimal numbers, Daffodil creates  an infoset output with the minimum number of  digits necessary to display it with the same precision.", "So 5000.00 will be output as 5000  If I want to retain the digits to the right of the decimal point, then I should declare the price element with the type xs:string, is that correct?", "/Roger   -----Original Message----- From: Steve Lawrence <slawrence@apache.org  Sent: Monday, December 3, 2018 9:36 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: Re: How to retain the digits to the right of the decimal point?", "The pattern defines the format of the data.", "It does not define the format of the infoset."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger   -----Original Message----- From: Steve Lawrence <slawrence@apache.org  Sent: Monday, December 3, 2018 9:36 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: Re: How to retain the digits to the right of the decimal point?", "The pattern defines the format of the data.", "It does not define the format of the infoset.", "I believe the spec is ambiguous or silent about how various data fields should be output to the infoset.", "I know we've had this issue with date/time fields recently."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The pattern defines the format of the data.", "It does not define the format of the infoset.", "I believe the spec is ambiguous or silent about how various data fields should be output to the infoset.", "I know we've had this issue with date/time fields recently.", "In the case of decimal numbers, Daffodil creates an infoset output with the minimum number of digits necessary to display it with the same precision."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It does not define the format of the infoset.", "I believe the spec is ambiguous or silent about how various data fields should be output to the infoset.", "I know we've had this issue with date/time fields recently.", "In the case of decimal numbers, Daffodil creates an infoset output with the minimum number of digits necessary to display it with the same precision.", "So 5000.00 will be output as 5000, but 5000.99 will be output with the extra decimal precision."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I believe the spec is ambiguous or silent about how various data fields should be output to the infoset.", "I know we've had this issue with date/time fields recently.", "In the case of decimal numbers, Daffodil creates an infoset output with the minimum number of digits necessary to display it with the same precision.", "So 5000.00 will be output as 5000, but 5000.99 will be output with the extra decimal precision.", "- Steve  On 12/3/18 9:30 AM, Costello, Roger L. wrote:  Hi Mike,      * Use 0 instead of # for the rightmost two."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the case of decimal numbers, Daffodil creates an infoset output with the minimum number of digits necessary to display it with the same precision.", "So 5000.00 will be output as 5000, but 5000.99 will be output with the extra decimal precision.", "- Steve  On 12/3/18 9:30 AM, Costello, Roger L. wrote:  Hi Mike,      * Use 0 instead of # for the rightmost two.", "I tried that:    <xs:elementname=\"price\"type=\"xs:decimal\"       dfdl:textStandardDecimalSeparator=\".\"", "dfdl:textNumberPattern=\"####.00\"/    It gave the same result (the .00 is removed):    5000.00 -- parse -- 5000    Thoughts?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Steve  On 12/3/18 9:30 AM, Costello, Roger L. wrote:  Hi Mike,      * Use 0 instead of # for the rightmost two.", "I tried that:    <xs:elementname=\"price\"type=\"xs:decimal\"       dfdl:textStandardDecimalSeparator=\".\"", "dfdl:textNumberPattern=\"####.00\"/    It gave the same result (the .00 is removed):    5000.00 -- parse -- 5000    Thoughts?", "/Roger    *From:* Mike Beckerle <mbeckerle@tresys.com  *Sent:* Monday, December 3, 2018 9:21 AM  *To:* users@daffodil.apache.org  *Subject:* Re: How to retain the digits to the right of the decimal point?", "Use 0 instead of # for the rightmost two."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I tried that:    <xs:elementname=\"price\"type=\"xs:decimal\"       dfdl:textStandardDecimalSeparator=\".\"", "dfdl:textNumberPattern=\"####.00\"/    It gave the same result (the .00 is removed):    5000.00 -- parse -- 5000    Thoughts?", "/Roger    *From:* Mike Beckerle <mbeckerle@tresys.com  *Sent:* Monday, December 3, 2018 9:21 AM  *To:* users@daffodil.apache.org  *Subject:* Re: How to retain the digits to the right of the decimal point?", "Use 0 instead of # for the rightmost two.", "In a pattern, a zero denotes   any digit."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["dfdl:textNumberPattern=\"####.00\"/    It gave the same result (the .00 is removed):    5000.00 -- parse -- 5000    Thoughts?", "/Roger    *From:* Mike Beckerle <mbeckerle@tresys.com  *Sent:* Monday, December 3, 2018 9:21 AM  *To:* users@daffodil.apache.org  *Subject:* Re: How to retain the digits to the right of the decimal point?", "Use 0 instead of # for the rightmost two.", "In a pattern, a zero denotes   any digit.", "A # denotes an optional digit."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger    *From:* Mike Beckerle <mbeckerle@tresys.com  *Sent:* Monday, December 3, 2018 9:21 AM  *To:* users@daffodil.apache.org  *Subject:* Re: How to retain the digits to the right of the decimal point?", "Use 0 instead of # for the rightmost two.", "In a pattern, a zero denotes   any digit.", "A # denotes an optional digit.", "-------- Original message --------    From: \"Costello, Roger L.\" <costello@mitre.org   <mailto:costello@mitre.org    Date: 12/3/18 8:46 AM (GMT-05:00)    To: users@daffodil.apache.org <mailto:users@daffodil.apache.org    Subject: How to retain the digits to the right of the decimal point?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Use 0 instead of # for the rightmost two.", "In a pattern, a zero denotes   any digit.", "A # denotes an optional digit.", "-------- Original message --------    From: \"Costello, Roger L.\" <costello@mitre.org   <mailto:costello@mitre.org    Date: 12/3/18 8:46 AM (GMT-05:00)    To: users@daffodil.apache.org <mailto:users@daffodil.apache.org    Subject: How to retain the digits to the right of the decimal point?", "Hello DFDL community,    My input contains decimal values such as: 2999.99 and 5000.00    When I parse my input, the .00 gets removed, e.g.,    5000.00 -- parse -- 5000    But the .99 is not removed, e.g.,    2999.99 -- parse -- 2999.99    I want to retain the two digits to the right of the decimal point,   even if they are 00    How to retain the digits?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hmmm, the apache mailing list won't let my current email post a reply.", "Trying again.", "-Akara  -------- Original Message -------- Subject: \tRe: Parsing Olio runtimeStats Date: \tWed, 05 May 2010 14:17:35 -0700 From: \tAkara Sucharitakul <akara.sucharitakul@oracle.com Reply-To: \takara.sucharitakul@oracle.com Organization: \tOracle To: \tolio-user@incubator.apache.org CC: \takara.sucharitakul <Akara.Sucharitakul@sun.com, Shanti Subramanyam  <shanti.subramanyam@gmail.com References:  <s2s89c38a6f1005051212z7cb84f71o589fa41c7ca8704d@mail.gmail.com    Let me try to address this below:  On 05/05/10 12:12, Vasileios Kontorinis wrote:  Akara and Shanti hi,      I am parsing the runtimeStats from the driver.log file and I run   into this interesting issue.", "In the output for _steady state_ I get \"-\" for the response time when   there are no successful operations since the last time runtimeStats   were printed.", "This can happen for two reasons:  1) The interval for printing the runtimeStats is small  (in my case   5secs) and some operations, especially the ones that take long and   have small frequency in matrix (add event, add user) just never happened."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Trying again.", "-Akara  -------- Original Message -------- Subject: \tRe: Parsing Olio runtimeStats Date: \tWed, 05 May 2010 14:17:35 -0700 From: \tAkara Sucharitakul <akara.sucharitakul@oracle.com Reply-To: \takara.sucharitakul@oracle.com Organization: \tOracle To: \tolio-user@incubator.apache.org CC: \takara.sucharitakul <Akara.Sucharitakul@sun.com, Shanti Subramanyam  <shanti.subramanyam@gmail.com References:  <s2s89c38a6f1005051212z7cb84f71o589fa41c7ca8704d@mail.gmail.com    Let me try to address this below:  On 05/05/10 12:12, Vasileios Kontorinis wrote:  Akara and Shanti hi,      I am parsing the runtimeStats from the driver.log file and I run   into this interesting issue.", "In the output for _steady state_ I get \"-\" for the response time when   there are no successful operations since the last time runtimeStats   were printed.", "This can happen for two reasons:  1) The interval for printing the runtimeStats is small  (in my case   5secs) and some operations, especially the ones that take long and   have small frequency in matrix (add event, add user) just never happened.", "Yes, if the number of successful operations in that period is 0, most  calculations will be a divide by 0."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That's why it is printing a '-'.", "2) There are a bunch of requests going on and none of them is   successful.", "This can happen when there is no sufficient memory in the   machine.", "In that case there is lots of swapping, the cpu goes to 100%   utilization and all the operations time-out.", "Is there any small change I can do to distinguish between the two   cases?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2) There are a bunch of requests going on and none of them is   successful.", "This can happen when there is no sufficient memory in the   machine.", "In that case there is lots of swapping, the cpu goes to 100%   utilization and all the operations time-out.", "Is there any small change I can do to distinguish between the two   cases?", "Maybe printing a small \"t\" when there are many requests timing   out, so that I can distinguish between the two cases?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This can happen when there is no sufficient memory in the   machine.", "In that case there is lots of swapping, the cpu goes to 100%   utilization and all the operations time-out.", "Is there any small change I can do to distinguish between the two   cases?", "Maybe printing a small \"t\" when there are many requests timing   out, so that I can distinguish between the two cases?", "Any ideas are   welcome."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there any small change I can do to distinguish between the two   cases?", "Maybe printing a small \"t\" when there are many requests timing   out, so that I can distinguish between the two cases?", "Any ideas are   welcome.", "There is the error count in the runtime stats that tell you about error  cases.", "This would also include timeouts."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Maybe printing a small \"t\" when there are many requests timing   out, so that I can distinguish between the two cases?", "Any ideas are   welcome.", "There is the error count in the runtime stats that tell you about error  cases.", "This would also include timeouts.", "But the problem is not that  simple."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is the error count in the runtime stats that tell you about error  cases.", "This would also include timeouts.", "But the problem is not that  simple.", "The real problem is not the operations timing out but rather the  operations waiting that would time out.", "And  we cannot distinguish  between these waiting operations and other operations waiting just with  slow response time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This would also include timeouts.", "But the problem is not that  simple.", "The real problem is not the operations timing out but rather the  operations waiting that would time out.", "And  we cannot distinguish  between these waiting operations and other operations waiting just with  slow response time.", "One more thing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But the problem is not that  simple.", "The real problem is not the operations timing out but rather the  operations waiting that would time out.", "And  we cannot distinguish  between these waiting operations and other operations waiting just with  slow response time.", "One more thing.", "I use the variableLoad setting and in order  to know   how many users are simulated while the benchmark run, I  parse log.xml   looking for \"Active threads: \"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The real problem is not the operations timing out but rather the  operations waiting that would time out.", "And  we cannot distinguish  between these waiting operations and other operations waiting just with  slow response time.", "One more thing.", "I use the variableLoad setting and in order  to know   how many users are simulated while the benchmark run, I  parse log.xml   looking for \"Active threads: \".", "The problem is that log.xml is big   especially when many requests timeout."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And  we cannot distinguish  between these waiting operations and other operations waiting just with  slow response time.", "One more thing.", "I use the variableLoad setting and in order  to know   how many users are simulated while the benchmark run, I  parse log.xml   looking for \"Active threads: \".", "The problem is that log.xml is big   especially when many requests timeout.", "Is it easy to also log the   changes of active threads in the drive.log file ??"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One more thing.", "I use the variableLoad setting and in order  to know   how many users are simulated while the benchmark run, I  parse log.xml   looking for \"Active threads: \".", "The problem is that log.xml is big   especially when many requests timeout.", "Is it easy to also log the   changes of active threads in the drive.log file ??", "Please look into log configuration in $FABAN/config/logging.properties."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I use the variableLoad setting and in order  to know   how many users are simulated while the benchmark run, I  parse log.xml   looking for \"Active threads: \".", "The problem is that log.xml is big   especially when many requests timeout.", "Is it easy to also log the   changes of active threads in the drive.log file ??", "Please look into log configuration in $FABAN/config/logging.properties.", "You should be able to make certain loggers log to a particular file."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The problem is that log.xml is big   especially when many requests timeout.", "Is it easy to also log the   changes of active threads in the drive.log file ??", "Please look into log configuration in $FABAN/config/logging.properties.", "You should be able to make certain loggers log to a particular file.", "I  don't have the detail off  the  top of my head."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Matthieu, can you share the test report for 200,000 events/s  e.g.", "Server: 2?", "Mem: 4G?", "Node: 4   Running cost: Cpu ?%, mem ?%  I will refer to this  and compare it, thanks first.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 3:47 PM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Also I noticed the cpu is very high almost 100%, but mem<10%, whether it still says too many IO operation or other causes?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Mem: 4G?", "Node: 4   Running cost: Cpu ?%, mem ?%  I will refer to this  and compare it, thanks first.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 3:47 PM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Also I noticed the cpu is very high almost 100%, but mem<10%, whether it still says too many IO operation or other causes?", "/Sky  From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 10:09 AM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Thanks Matthieu very careful suggestions, your direction is right.", "The main reason is serialization/deserlization problems,  1)      I changed DataEvent (which extends Event and javaBean) into Event, using default Event to send s4 event in adapter  2)      In s4 app,  create new dataEvent in memory and put stream into next PE,  Then performance improve a lot, up to 200,000 events/20s maxium, think there still has space to improve, I keep checking, thanks Matthieu."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Node: 4   Running cost: Cpu ?%, mem ?%  I will refer to this  and compare it, thanks first.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 3:47 PM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Also I noticed the cpu is very high almost 100%, but mem<10%, whether it still says too many IO operation or other causes?", "/Sky  From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 10:09 AM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Thanks Matthieu very careful suggestions, your direction is right.", "The main reason is serialization/deserlization problems,  1)      I changed DataEvent (which extends Event and javaBean) into Event, using default Event to send s4 event in adapter  2)      In s4 app,  create new dataEvent in memory and put stream into next PE,  Then performance improve a lot, up to 200,000 events/20s maxium, think there still has space to improve, I keep checking, thanks Matthieu.", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 11:44 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  From what I see in your code, the problem might be in the definition of keys."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 3:47 PM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Also I noticed the cpu is very high almost 100%, but mem<10%, whether it still says too many IO operation or other causes?", "/Sky  From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 10:09 AM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Thanks Matthieu very careful suggestions, your direction is right.", "The main reason is serialization/deserlization problems,  1)      I changed DataEvent (which extends Event and javaBean) into Event, using default Event to send s4 event in adapter  2)      In s4 app,  create new dataEvent in memory and put stream into next PE,  Then performance improve a lot, up to 200,000 events/20s maxium, think there still has space to improve, I keep checking, thanks Matthieu.", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 11:44 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  From what I see in your code, the problem might be in the definition of keys.", "In your GenKeyFinder, you use the event timestamp in the key, and therefore you might be creating a new PE instance for every single event!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Sky  From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 10:09 AM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Thanks Matthieu very careful suggestions, your direction is right.", "The main reason is serialization/deserlization problems,  1)      I changed DataEvent (which extends Event and javaBean) into Event, using default Event to send s4 event in adapter  2)      In s4 app,  create new dataEvent in memory and put stream into next PE,  Then performance improve a lot, up to 200,000 events/20s maxium, think there still has space to improve, I keep checking, thanks Matthieu.", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 11:44 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  From what I see in your code, the problem might be in the definition of keys.", "In your GenKeyFinder, you use the event timestamp in the key, and therefore you might be creating a new PE instance for every single event!", "(unless the timestamp you set is somehow repeated, which sounds peculiar)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The main reason is serialization/deserlization problems,  1)      I changed DataEvent (which extends Event and javaBean) into Event, using default Event to send s4 event in adapter  2)      In s4 app,  create new dataEvent in memory and put stream into next PE,  Then performance improve a lot, up to 200,000 events/20s maxium, think there still has space to improve, I keep checking, thanks Matthieu.", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 11:44 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  From what I see in your code, the problem might be in the definition of keys.", "In your GenKeyFinder, you use the event timestamp in the key, and therefore you might be creating a new PE instance for every single event!", "(unless the timestamp you set is somehow repeated, which sounds peculiar).", "I would suggest to modify the keyfinder in a more suitable way."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 11:44 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  From what I see in your code, the problem might be in the definition of keys.", "In your GenKeyFinder, you use the event timestamp in the key, and therefore you might be creating a new PE instance for every single event!", "(unless the timestamp you set is somehow repeated, which sounds peculiar).", "I would suggest to modify the keyfinder in a more suitable way.", "Probably by removing the timestamp from the key."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In your GenKeyFinder, you use the event timestamp in the key, and therefore you might be creating a new PE instance for every single event!", "(unless the timestamp you set is somehow repeated, which sounds peculiar).", "I would suggest to modify the keyfinder in a more suitable way.", "Probably by removing the timestamp from the key.", "In the twitter example for instance, the key is the topic of the tweet, and we aggregate counts by topic."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would suggest to modify the keyfinder in a more suitable way.", "Probably by removing the timestamp from the key.", "In the twitter example for instance, the key is the topic of the tweet, and we aggregate counts by topic.", "Hope this helps,  Matthieu  On Jun 25, 2013, at 14:39 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  So I only guess, The serialization/deserlization costs much time, and occupy some limited memory, Once the serialization/deserlization upp bound is max, it will occupy much memory, the events starts to be blocked, so more memory in JVM and less (de)serlization, the performance could be more events for general.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com] Sent: Tuesday, June 25, 2013 8:15 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s   I use 4g memory to handle the events, so I feel s4 eat more memory and cpus and server numbers."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Probably by removing the timestamp from the key.", "In the twitter example for instance, the key is the topic of the tweet, and we aggregate counts by topic.", "Hope this helps,  Matthieu  On Jun 25, 2013, at 14:39 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  So I only guess, The serialization/deserlization costs much time, and occupy some limited memory, Once the serialization/deserlization upp bound is max, it will occupy much memory, the events starts to be blocked, so more memory in JVM and less (de)serlization, the performance could be more events for general.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com] Sent: Tuesday, June 25, 2013 8:15 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s   I use 4g memory to handle the events, so I feel s4 eat more memory and cpus and server numbers.", "How many servers and cpu and memory to handle 200,000 events?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the twitter example for instance, the key is the topic of the tweet, and we aggregate counts by topic.", "Hope this helps,  Matthieu  On Jun 25, 2013, at 14:39 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  So I only guess, The serialization/deserlization costs much time, and occupy some limited memory, Once the serialization/deserlization upp bound is max, it will occupy much memory, the events starts to be blocked, so more memory in JVM and less (de)serlization, the performance could be more events for general.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com] Sent: Tuesday, June 25, 2013 8:15 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s   I use 4g memory to handle the events, so I feel s4 eat more memory and cpus and server numbers.", "How many servers and cpu and memory to handle 200,000 events?", "/Sky   From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Tuesday, June 25, 2013 7:08 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  I list my code here, in my app, I created 3 PEs(the logic is very simple, just emit stream)                  @Override                protected void onInit() {                                CsvReporter.enable(new File(mpath), 20, TimeUnit.SECONDS);                                // create a prototype         EntryPE entryPE = createPE(EntryPE.class);           createInputStream(\"seaRawStream\", new KeyFinder<Event() {              @Override             public List<String get(Event event) {                return Arrays.asList(new String[] { event.get(\"seadata\") });             }         }, entryPE);           ResultPE resultPE = createPE(ResultPE.class);                Stream<DataEvent processStream = createStream(\"Process Stream\", new GenKeyFinder(), resultPE);                processStream.setParallelism(Integer.parseInt(thread));           ProcessPE processPE = createPE(ProcessPE.class);         processPE.setDataStream(processStream);            Stream<DataEvent entryStream = createStream(\"Entry Stream\", new GenKeyFinder(), processPE);         entryStream.setParallelism(Integer.parseInt(thread));                 entryPE.setStreams(entryStream);                }    The event data is String kpi_name;                String mo_name;                double kpi_value;                long timestamp;                 public DataEvent() {                 }                 public DataEvent(String kpi_name, String mo_name, double kpi_value,                                              long timestamp) {                               this.kpi_name = kpi_name;                               this.mo_name = mo_name;                               this.kpi_value = kpi_value;                               this.timestamp = timestamp;                 }  .... Get/set methods   Keyfinder:  public class GenKeyFinder implements KeyFinder<DataEvent {       public List<String get(DataEvent event) {          List<String results = new ArrayList<String();          /* Retrieve the kpi_name,mo_name and add them to the list."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hope this helps,  Matthieu  On Jun 25, 2013, at 14:39 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  So I only guess, The serialization/deserlization costs much time, and occupy some limited memory, Once the serialization/deserlization upp bound is max, it will occupy much memory, the events starts to be blocked, so more memory in JVM and less (de)serlization, the performance could be more events for general.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com] Sent: Tuesday, June 25, 2013 8:15 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s   I use 4g memory to handle the events, so I feel s4 eat more memory and cpus and server numbers.", "How many servers and cpu and memory to handle 200,000 events?", "/Sky   From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Tuesday, June 25, 2013 7:08 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  I list my code here, in my app, I created 3 PEs(the logic is very simple, just emit stream)                  @Override                protected void onInit() {                                CsvReporter.enable(new File(mpath), 20, TimeUnit.SECONDS);                                // create a prototype         EntryPE entryPE = createPE(EntryPE.class);           createInputStream(\"seaRawStream\", new KeyFinder<Event() {              @Override             public List<String get(Event event) {                return Arrays.asList(new String[] { event.get(\"seadata\") });             }         }, entryPE);           ResultPE resultPE = createPE(ResultPE.class);                Stream<DataEvent processStream = createStream(\"Process Stream\", new GenKeyFinder(), resultPE);                processStream.setParallelism(Integer.parseInt(thread));           ProcessPE processPE = createPE(ProcessPE.class);         processPE.setDataStream(processStream);            Stream<DataEvent entryStream = createStream(\"Entry Stream\", new GenKeyFinder(), processPE);         entryStream.setParallelism(Integer.parseInt(thread));                 entryPE.setStreams(entryStream);                }    The event data is String kpi_name;                String mo_name;                double kpi_value;                long timestamp;                 public DataEvent() {                 }                 public DataEvent(String kpi_name, String mo_name, double kpi_value,                                              long timestamp) {                               this.kpi_name = kpi_name;                               this.mo_name = mo_name;                               this.kpi_value = kpi_value;                               this.timestamp = timestamp;                 }  .... Get/set methods   Keyfinder:  public class GenKeyFinder implements KeyFinder<DataEvent {       public List<String get(DataEvent event) {          List<String results = new ArrayList<String();          /* Retrieve the kpi_name,mo_name and add them to the list.", "*/         results.add(event.getKpi_name()+\":\"+event.getMo_name()+\":\"+String.valueOf(event.getTimestamp()));          return results;     } }    =====adapter part code                                                                              // ////////sending to s4                                                                            DataEvent event = new DataEvent(kpi_name, mo_name,                                                                                                          kpi_value, timestamp);                                                                            event.put(\"seadata\", String.class, kpi_name+\":\"+\"mo_name\"+\":\"+timestamp);                                                                            rstream.put(event);   whether the keyfinder or event key impact the event sending/receiving?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com] Sent: Tuesday, June 25, 2013 8:15 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s   I use 4g memory to handle the events, so I feel s4 eat more memory and cpus and server numbers.", "How many servers and cpu and memory to handle 200,000 events?", "/Sky   From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Tuesday, June 25, 2013 7:08 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  I list my code here, in my app, I created 3 PEs(the logic is very simple, just emit stream)                  @Override                protected void onInit() {                                CsvReporter.enable(new File(mpath), 20, TimeUnit.SECONDS);                                // create a prototype         EntryPE entryPE = createPE(EntryPE.class);           createInputStream(\"seaRawStream\", new KeyFinder<Event() {              @Override             public List<String get(Event event) {                return Arrays.asList(new String[] { event.get(\"seadata\") });             }         }, entryPE);           ResultPE resultPE = createPE(ResultPE.class);                Stream<DataEvent processStream = createStream(\"Process Stream\", new GenKeyFinder(), resultPE);                processStream.setParallelism(Integer.parseInt(thread));           ProcessPE processPE = createPE(ProcessPE.class);         processPE.setDataStream(processStream);            Stream<DataEvent entryStream = createStream(\"Entry Stream\", new GenKeyFinder(), processPE);         entryStream.setParallelism(Integer.parseInt(thread));                 entryPE.setStreams(entryStream);                }    The event data is String kpi_name;                String mo_name;                double kpi_value;                long timestamp;                 public DataEvent() {                 }                 public DataEvent(String kpi_name, String mo_name, double kpi_value,                                              long timestamp) {                               this.kpi_name = kpi_name;                               this.mo_name = mo_name;                               this.kpi_value = kpi_value;                               this.timestamp = timestamp;                 }  .... Get/set methods   Keyfinder:  public class GenKeyFinder implements KeyFinder<DataEvent {       public List<String get(DataEvent event) {          List<String results = new ArrayList<String();          /* Retrieve the kpi_name,mo_name and add them to the list.", "*/         results.add(event.getKpi_name()+\":\"+event.getMo_name()+\":\"+String.valueOf(event.getTimestamp()));          return results;     } }    =====adapter part code                                                                              // ////////sending to s4                                                                            DataEvent event = new DataEvent(kpi_name, mo_name,                                                                                                          kpi_value, timestamp);                                                                            event.put(\"seadata\", String.class, kpi_name+\":\"+\"mo_name\"+\":\"+timestamp);                                                                            rstream.put(event);   whether the keyfinder or event key impact the event sending/receiving?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 5:03 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Not sure what is the issue in your setting."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How many servers and cpu and memory to handle 200,000 events?", "/Sky   From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Tuesday, June 25, 2013 7:08 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  I list my code here, in my app, I created 3 PEs(the logic is very simple, just emit stream)                  @Override                protected void onInit() {                                CsvReporter.enable(new File(mpath), 20, TimeUnit.SECONDS);                                // create a prototype         EntryPE entryPE = createPE(EntryPE.class);           createInputStream(\"seaRawStream\", new KeyFinder<Event() {              @Override             public List<String get(Event event) {                return Arrays.asList(new String[] { event.get(\"seadata\") });             }         }, entryPE);           ResultPE resultPE = createPE(ResultPE.class);                Stream<DataEvent processStream = createStream(\"Process Stream\", new GenKeyFinder(), resultPE);                processStream.setParallelism(Integer.parseInt(thread));           ProcessPE processPE = createPE(ProcessPE.class);         processPE.setDataStream(processStream);            Stream<DataEvent entryStream = createStream(\"Entry Stream\", new GenKeyFinder(), processPE);         entryStream.setParallelism(Integer.parseInt(thread));                 entryPE.setStreams(entryStream);                }    The event data is String kpi_name;                String mo_name;                double kpi_value;                long timestamp;                 public DataEvent() {                 }                 public DataEvent(String kpi_name, String mo_name, double kpi_value,                                              long timestamp) {                               this.kpi_name = kpi_name;                               this.mo_name = mo_name;                               this.kpi_value = kpi_value;                               this.timestamp = timestamp;                 }  .... Get/set methods   Keyfinder:  public class GenKeyFinder implements KeyFinder<DataEvent {       public List<String get(DataEvent event) {          List<String results = new ArrayList<String();          /* Retrieve the kpi_name,mo_name and add them to the list.", "*/         results.add(event.getKpi_name()+\":\"+event.getMo_name()+\":\"+String.valueOf(event.getTimestamp()));          return results;     } }    =====adapter part code                                                                              // ////////sending to s4                                                                            DataEvent event = new DataEvent(kpi_name, mo_name,                                                                                                          kpi_value, timestamp);                                                                            event.put(\"seadata\", String.class, kpi_name+\":\"+\"mo_name\"+\":\"+timestamp);                                                                            rstream.put(event);   whether the keyfinder or event key impact the event sending/receiving?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 5:03 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Not sure what is the issue in your setting.", "Performance issues in stream processing can be related to I/O or GC."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Sky   From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Tuesday, June 25, 2013 7:08 PM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  I list my code here, in my app, I created 3 PEs(the logic is very simple, just emit stream)                  @Override                protected void onInit() {                                CsvReporter.enable(new File(mpath), 20, TimeUnit.SECONDS);                                // create a prototype         EntryPE entryPE = createPE(EntryPE.class);           createInputStream(\"seaRawStream\", new KeyFinder<Event() {              @Override             public List<String get(Event event) {                return Arrays.asList(new String[] { event.get(\"seadata\") });             }         }, entryPE);           ResultPE resultPE = createPE(ResultPE.class);                Stream<DataEvent processStream = createStream(\"Process Stream\", new GenKeyFinder(), resultPE);                processStream.setParallelism(Integer.parseInt(thread));           ProcessPE processPE = createPE(ProcessPE.class);         processPE.setDataStream(processStream);            Stream<DataEvent entryStream = createStream(\"Entry Stream\", new GenKeyFinder(), processPE);         entryStream.setParallelism(Integer.parseInt(thread));                 entryPE.setStreams(entryStream);                }    The event data is String kpi_name;                String mo_name;                double kpi_value;                long timestamp;                 public DataEvent() {                 }                 public DataEvent(String kpi_name, String mo_name, double kpi_value,                                              long timestamp) {                               this.kpi_name = kpi_name;                               this.mo_name = mo_name;                               this.kpi_value = kpi_value;                               this.timestamp = timestamp;                 }  .... Get/set methods   Keyfinder:  public class GenKeyFinder implements KeyFinder<DataEvent {       public List<String get(DataEvent event) {          List<String results = new ArrayList<String();          /* Retrieve the kpi_name,mo_name and add them to the list.", "*/         results.add(event.getKpi_name()+\":\"+event.getMo_name()+\":\"+String.valueOf(event.getTimestamp()));          return results;     } }    =====adapter part code                                                                              // ////////sending to s4                                                                            DataEvent event = new DataEvent(kpi_name, mo_name,                                                                                                          kpi_value, timestamp);                                                                            event.put(\"seadata\", String.class, kpi_name+\":\"+\"mo_name\"+\":\"+timestamp);                                                                            rstream.put(event);   whether the keyfinder or event key impact the event sending/receiving?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 5:03 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Not sure what is the issue in your setting.", "Performance issues in stream processing can be related to I/O or GC.", "But the app design can have a dramatic impact as well."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*/         results.add(event.getKpi_name()+\":\"+event.getMo_name()+\":\"+String.valueOf(event.getTimestamp()));          return results;     } }    =====adapter part code                                                                              // ////////sending to s4                                                                            DataEvent event = new DataEvent(kpi_name, mo_name,                                                                                                          kpi_value, timestamp);                                                                            event.put(\"seadata\", String.class, kpi_name+\":\"+\"mo_name\"+\":\"+timestamp);                                                                            rstream.put(event);   whether the keyfinder or event key impact the event sending/receiving?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 5:03 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Not sure what is the issue in your setting.", "Performance issues in stream processing can be related to I/O or GC.", "But the app design can have a dramatic impact as well.", "Have a look at your CPU usage as well."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 5:03 PM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Not sure what is the issue in your setting.", "Performance issues in stream processing can be related to I/O or GC.", "But the app design can have a dramatic impact as well.", "Have a look at your CPU usage as well.", "You might want to profile the app and adapter to identify the culprit in your application."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But the app design can have a dramatic impact as well.", "Have a look at your CPU usage as well.", "You might want to profile the app and adapter to identify the culprit in your application.", "Another path to explore is related to the content of events.", "Serialization/deserialization may be costly for complex objects."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Have a look at your CPU usage as well.", "You might want to profile the app and adapter to identify the culprit in your application.", "Another path to explore is related to the content of events.", "Serialization/deserialization may be costly for complex objects.", "What kind of data structure are you keeping in the events?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You might want to profile the app and adapter to identify the culprit in your application.", "Another path to explore is related to the content of events.", "Serialization/deserialization may be costly for complex objects.", "What kind of data structure are you keeping in the events?", "On Jun 25, 2013, at 09:56 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  Also, can you teach me how to find or trace the block place, I am still confused why and where is block?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What kind of data structure are you keeping in the events?", "On Jun 25, 2013, at 09:56 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  Also, can you teach me how to find or trace the block place, I am still confused why and where is block?", "What are you referring to?", "How many nodes in twitter example for up to 200,000 events/s?", "You'll never get to that number with that application, since the twitter sprinkler feed is ~ 1% of the total feed, and the reported peak rate of the total feed is a few tens of thousands of tweets / s  But if you were to read from a dump, I'd say a few nodes for the adapter and a few nodes for the app."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Jun 25, 2013, at 09:56 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  Also, can you teach me how to find or trace the block place, I am still confused why and where is block?", "What are you referring to?", "How many nodes in twitter example for up to 200,000 events/s?", "You'll never get to that number with that application, since the twitter sprinkler feed is ~ 1% of the total feed, and the reported peak rate of the total feed is a few tens of thousands of tweets / s  But if you were to read from a dump, I'd say a few nodes for the adapter and a few nodes for the app.", "Matthieu    From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com/] Sent: Tuesday, June 25, 2013 10:25 AM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Hi Matthieu, I tried to test again after modifying some configuration codes, see below, no any PE logic, just send events(only spend 38s for adapter sending events) don't know where is blocked?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What are you referring to?", "How many nodes in twitter example for up to 200,000 events/s?", "You'll never get to that number with that application, since the twitter sprinkler feed is ~ 1% of the total feed, and the reported peak rate of the total feed is a few tens of thousands of tweets / s  But if you were to read from a dump, I'd say a few nodes for the adapter and a few nodes for the app.", "Matthieu    From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com/] Sent: Tuesday, June 25, 2013 10:25 AM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Hi Matthieu, I tried to test again after modifying some configuration codes, see below, no any PE logic, just send events(only spend 38s for adapter sending events) don't know where is blocked?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 12:25 AM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Hi,  I would suggest to:  1/ check how much you can generate when creating events read from the file - without event sending to a remote stream."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Matthieu    From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com/] Sent: Tuesday, June 25, 2013 10:25 AM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Hi Matthieu, I tried to test again after modifying some configuration codes, see below, no any PE logic, just send events(only spend 38s for adapter sending events) don't know where is blocked?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 12:25 AM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Hi,  I would suggest to:  1/ check how much you can generate when creating events read from the file - without event sending to a remote stream.", "This gives you the upper bound for a single adapter (producer)  It costs 38s for only file-read from adapters   2/ check how much you can consume in the app cluster.", "By default the remote senders are blocking, i.e.", "the adapter won't inject more than what the app cluster can consume."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 12:25 AM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Hi,  I would suggest to:  1/ check how much you can generate when creating events read from the file - without event sending to a remote stream.", "This gives you the upper bound for a single adapter (producer)  It costs 38s for only file-read from adapters   2/ check how much you can consume in the app cluster.", "By default the remote senders are blocking, i.e.", "the adapter won't inject more than what the app cluster can consume.", "This gives you an upper bound for the consumer."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This gives you the upper bound for a single adapter (producer)  It costs 38s for only file-read from adapters   2/ check how much you can consume in the app cluster.", "By default the remote senders are blocking, i.e.", "the adapter won't inject more than what the app cluster can consume.", "This gives you an upper bound for the consumer.", "I removed all PE logic, just emit functions, very strange, it still cost 600s, seems somewhere blocking   3/ use more adapter processes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["By default the remote senders are blocking, i.e.", "the adapter won't inject more than what the app cluster can consume.", "This gives you an upper bound for the consumer.", "I removed all PE logic, just emit functions, very strange, it still cost 600s, seems somewhere blocking   3/ use more adapter processes.", "In the benchmarks subprojects, one can configure the number of injection processes, and you might need more than one I tried, seems improve a bit, but not obivouse, 2500 events/s maximum."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["the adapter won't inject more than what the app cluster can consume.", "This gives you an upper bound for the consumer.", "I removed all PE logic, just emit functions, very strange, it still cost 600s, seems somewhere blocking   3/ use more adapter processes.", "In the benchmarks subprojects, one can configure the number of injection processes, and you might need more than one I tried, seems improve a bit, but not obivouse, 2500 events/s maximum.", "4/ make sure the tuning parameters you are setting are appropriate."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This gives you an upper bound for the consumer.", "I removed all PE logic, just emit functions, very strange, it still cost 600s, seems somewhere blocking   3/ use more adapter processes.", "In the benchmarks subprojects, one can configure the number of injection processes, and you might need more than one I tried, seems improve a bit, but not obivouse, 2500 events/s maximum.", "4/ make sure the tuning parameters you are setting are appropriate.", "For instance, I am not sure using 100 threads for serializing events is a good setting (see my notes about context switching in a previous mail)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the benchmarks subprojects, one can configure the number of injection processes, and you might need more than one I tried, seems improve a bit, but not obivouse, 2500 events/s maximum.", "4/ make sure the tuning parameters you are setting are appropriate.", "For instance, I am not sure using 100 threads for serializing events is a good setting (see my notes about context switching in a previous mail).", "Already changed to 10 threads  Also note that 200k msg/s/stream/node corresponds to the average rate in one minute _once the cluster has reached stable mode_.", "Indeed JVMs typically perform better after a while, due to various kinds of dynamic optimizations."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["4/ make sure the tuning parameters you are setting are appropriate.", "For instance, I am not sure using 100 threads for serializing events is a good setting (see my notes about context switching in a previous mail).", "Already changed to 10 threads  Also note that 200k msg/s/stream/node corresponds to the average rate in one minute _once the cluster has reached stable mode_.", "Indeed JVMs typically perform better after a while, due to various kinds of dynamic optimizations.", "Do make sure you experiments are long enough."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Already changed to 10 threads  Also note that 200k msg/s/stream/node corresponds to the average rate in one minute _once the cluster has reached stable mode_.", "Indeed JVMs typically perform better after a while, due to various kinds of dynamic optimizations.", "Do make sure you experiments are long enough.", "Here is metrics report, already run 620s(NO pe logic,) List event-emitted@seacluster1@partition-0.csv<mailto:event-emitted@seacluster1@partition-0.csv file  # time,count,1 min rate,mean rate,5 min rate,15 min rate 20,30482,728.4264567247864,1532.1851387286329,577.7027401449616,550.7086872323928 40,68256,1003.4693068790475,1710.158498360041,651.1838653093574,576.4097533046603 60,126665,1526.423951717675,2114.1420782852447,792.5372602881871,626.2019978354255 80,159222,1631.4770294401224,1992.409242530607,865.8704071266087,654.9705320161032 100,206876,1821.551542703833,2070.5009200329328,958.5809893144597,691.1999843184435 120,245115,1852.9758861695257,2044.0217493195803,1021.5173199955651,718.5310141264627 140,286057,1892.8858688327914,2044.444972093294,1084.2498192357743,746.5688788373484 160,324662,1952.4301928970413,2030.146074235012,1151.037578221836,776.8090098793148 180,371829,1959.4350067303067,2066.6134765239876,1202.907690109737,802.6284106918696 200,433557,2283.734039729985,2168.043356631868,1325.4143582100112,853.1681426416412 220,464815,2165.047817425384,2113.0019773033414,1362.4434184467839,876.2949286924105 240,504620,2065.9291371459913,2102.751240490619,1390.829435765405,896.605433400642 260,558158,2192.29985605397,2146.9020962821564,1462.6363950595012,931.9095985316526 280,595910,2199.1021924478428,2128.3677435991726,1513.937239488627,961.2098181907241 300,627181,1994.0377201896988,2090.694355349902,1511.0411290249551,972.3477079289204 320,664342,1959.8933962067783,2076.1406486970463,1534.4815810411815,992.1780521538831 340,698717,1912.6664972668054,2055.1073335596802,1551.413320562825,1009.8797759279687 360,738468,1940.3861933767728,2051.3430577192726,1579.897070517331,1031.4220907479778 380,769673,1808.904535049888,2025.4290025114635,1577.8601886321867,1045.7495364549434 400,807406,1834.34177336605,2018.480153382513,1597.9097843354657,1064.241627735365 420,851009,1918.7250831019285,2026.1694030316874,1634.835168023956,1088.691609058512 440,884427,1854.4469403637247,2010.0049048826818,1637.4121396194184,1101.510206100955 460,927835,1951.4786440268247,2016.9731382075943,1672.0801089577474,1125.0229080682857 480,975961,2076.6338294064494,2033.1853468961958,1719.2462212252974,1153.158261927912 500,1018777,2094.823218886582,2037.4852753404898,1746.4288709310954,1174.8624939692247 520,1063733,2137.998709779158,2045.5028969721004,1778.8271760046641,1198.4702839108422 540,1105407,2121.91619308999,2046.9043296729049,1798.2960826750962,1217.8576817035882 560,1148338,2126.4127152635606,2050.4509561002797,1820.616007231527,1238.2444234074262 580,1189999,2113.61602051879,2051.570290715584,1837.4446097375073,1256.7786337375683 600,1219584,1937.6445710562814,2031.6753069957329,1814.671731059335,1261.7477397513687 620,1237632,1632.4506998235333,1995.2499874637524,1755.4487350018405,1253.8445924435982    Seems very strange value for my example, far from 200,000 events/s/node/stream   Regards,  Matthieu   On Jun 24, 2013, at 11:19 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  I try to use Adapter to send s4 events.", "With metrics report, 20,10462,88.63259092217602,539.6449108859357,18.577650313690874,6.241814566462701 40,36006,417.83633322358764,914.1057643161282,97.55624823196746,33.40088245418529 60,63859,674.1012974987167,1075.2326549158463,176.33878995148274,61.646803531230724 80,97835,953.6282787690939,1232.2934375999696,271.48890371088254,96.56144395108957 100,131535,1162.2060916405578,1323.3704459079934,363.98505627735324,131.98430793014757 120,165282,1327.52314133145,1384.2675551261093,453.5195236495672,167.61679021575551 140,190776,1305.7285112621298,1368.4361242524062,504.7782182758366,191.36049732440895  20,000 events per 20s  = 1000 EVENTS/s  Very slow, I modify the S4_HOME/subprojects/s4-comm/bin/default.s4.comm.properties  s4.comm.emitter.class=org.apache.s4.comm.tcp.TCPEmitter s4.comm.emitter.remote.class=org.apache.s4.comm.tcp.TCPRemoteEmitter s4.comm.listener.class=org.apache.s4.comm.tcp.TCPListener  # I/O channel connection timeout, when applicable (e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Indeed JVMs typically perform better after a while, due to various kinds of dynamic optimizations.", "Do make sure you experiments are long enough.", "Here is metrics report, already run 620s(NO pe logic,) List event-emitted@seacluster1@partition-0.csv<mailto:event-emitted@seacluster1@partition-0.csv file  # time,count,1 min rate,mean rate,5 min rate,15 min rate 20,30482,728.4264567247864,1532.1851387286329,577.7027401449616,550.7086872323928 40,68256,1003.4693068790475,1710.158498360041,651.1838653093574,576.4097533046603 60,126665,1526.423951717675,2114.1420782852447,792.5372602881871,626.2019978354255 80,159222,1631.4770294401224,1992.409242530607,865.8704071266087,654.9705320161032 100,206876,1821.551542703833,2070.5009200329328,958.5809893144597,691.1999843184435 120,245115,1852.9758861695257,2044.0217493195803,1021.5173199955651,718.5310141264627 140,286057,1892.8858688327914,2044.444972093294,1084.2498192357743,746.5688788373484 160,324662,1952.4301928970413,2030.146074235012,1151.037578221836,776.8090098793148 180,371829,1959.4350067303067,2066.6134765239876,1202.907690109737,802.6284106918696 200,433557,2283.734039729985,2168.043356631868,1325.4143582100112,853.1681426416412 220,464815,2165.047817425384,2113.0019773033414,1362.4434184467839,876.2949286924105 240,504620,2065.9291371459913,2102.751240490619,1390.829435765405,896.605433400642 260,558158,2192.29985605397,2146.9020962821564,1462.6363950595012,931.9095985316526 280,595910,2199.1021924478428,2128.3677435991726,1513.937239488627,961.2098181907241 300,627181,1994.0377201896988,2090.694355349902,1511.0411290249551,972.3477079289204 320,664342,1959.8933962067783,2076.1406486970463,1534.4815810411815,992.1780521538831 340,698717,1912.6664972668054,2055.1073335596802,1551.413320562825,1009.8797759279687 360,738468,1940.3861933767728,2051.3430577192726,1579.897070517331,1031.4220907479778 380,769673,1808.904535049888,2025.4290025114635,1577.8601886321867,1045.7495364549434 400,807406,1834.34177336605,2018.480153382513,1597.9097843354657,1064.241627735365 420,851009,1918.7250831019285,2026.1694030316874,1634.835168023956,1088.691609058512 440,884427,1854.4469403637247,2010.0049048826818,1637.4121396194184,1101.510206100955 460,927835,1951.4786440268247,2016.9731382075943,1672.0801089577474,1125.0229080682857 480,975961,2076.6338294064494,2033.1853468961958,1719.2462212252974,1153.158261927912 500,1018777,2094.823218886582,2037.4852753404898,1746.4288709310954,1174.8624939692247 520,1063733,2137.998709779158,2045.5028969721004,1778.8271760046641,1198.4702839108422 540,1105407,2121.91619308999,2046.9043296729049,1798.2960826750962,1217.8576817035882 560,1148338,2126.4127152635606,2050.4509561002797,1820.616007231527,1238.2444234074262 580,1189999,2113.61602051879,2051.570290715584,1837.4446097375073,1256.7786337375683 600,1219584,1937.6445710562814,2031.6753069957329,1814.671731059335,1261.7477397513687 620,1237632,1632.4506998235333,1995.2499874637524,1755.4487350018405,1253.8445924435982    Seems very strange value for my example, far from 200,000 events/s/node/stream   Regards,  Matthieu   On Jun 24, 2013, at 11:19 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  I try to use Adapter to send s4 events.", "With metrics report, 20,10462,88.63259092217602,539.6449108859357,18.577650313690874,6.241814566462701 40,36006,417.83633322358764,914.1057643161282,97.55624823196746,33.40088245418529 60,63859,674.1012974987167,1075.2326549158463,176.33878995148274,61.646803531230724 80,97835,953.6282787690939,1232.2934375999696,271.48890371088254,96.56144395108957 100,131535,1162.2060916405578,1323.3704459079934,363.98505627735324,131.98430793014757 120,165282,1327.52314133145,1384.2675551261093,453.5195236495672,167.61679021575551 140,190776,1305.7285112621298,1368.4361242524062,504.7782182758366,191.36049732440895  20,000 events per 20s  = 1000 EVENTS/s  Very slow, I modify the S4_HOME/subprojects/s4-comm/bin/default.s4.comm.properties  s4.comm.emitter.class=org.apache.s4.comm.tcp.TCPEmitter s4.comm.emitter.remote.class=org.apache.s4.comm.tcp.TCPRemoteEmitter s4.comm.listener.class=org.apache.s4.comm.tcp.TCPListener  # I/O channel connection timeout, when applicable (e.g.", "used by netty) s4.comm.timeout=1000  # NOTE: the following numbers should be tuned according to the application, use case, and infrastructure  # how many threads to use for the sender stage (i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do make sure you experiments are long enough.", "Here is metrics report, already run 620s(NO pe logic,) List event-emitted@seacluster1@partition-0.csv<mailto:event-emitted@seacluster1@partition-0.csv file  # time,count,1 min rate,mean rate,5 min rate,15 min rate 20,30482,728.4264567247864,1532.1851387286329,577.7027401449616,550.7086872323928 40,68256,1003.4693068790475,1710.158498360041,651.1838653093574,576.4097533046603 60,126665,1526.423951717675,2114.1420782852447,792.5372602881871,626.2019978354255 80,159222,1631.4770294401224,1992.409242530607,865.8704071266087,654.9705320161032 100,206876,1821.551542703833,2070.5009200329328,958.5809893144597,691.1999843184435 120,245115,1852.9758861695257,2044.0217493195803,1021.5173199955651,718.5310141264627 140,286057,1892.8858688327914,2044.444972093294,1084.2498192357743,746.5688788373484 160,324662,1952.4301928970413,2030.146074235012,1151.037578221836,776.8090098793148 180,371829,1959.4350067303067,2066.6134765239876,1202.907690109737,802.6284106918696 200,433557,2283.734039729985,2168.043356631868,1325.4143582100112,853.1681426416412 220,464815,2165.047817425384,2113.0019773033414,1362.4434184467839,876.2949286924105 240,504620,2065.9291371459913,2102.751240490619,1390.829435765405,896.605433400642 260,558158,2192.29985605397,2146.9020962821564,1462.6363950595012,931.9095985316526 280,595910,2199.1021924478428,2128.3677435991726,1513.937239488627,961.2098181907241 300,627181,1994.0377201896988,2090.694355349902,1511.0411290249551,972.3477079289204 320,664342,1959.8933962067783,2076.1406486970463,1534.4815810411815,992.1780521538831 340,698717,1912.6664972668054,2055.1073335596802,1551.413320562825,1009.8797759279687 360,738468,1940.3861933767728,2051.3430577192726,1579.897070517331,1031.4220907479778 380,769673,1808.904535049888,2025.4290025114635,1577.8601886321867,1045.7495364549434 400,807406,1834.34177336605,2018.480153382513,1597.9097843354657,1064.241627735365 420,851009,1918.7250831019285,2026.1694030316874,1634.835168023956,1088.691609058512 440,884427,1854.4469403637247,2010.0049048826818,1637.4121396194184,1101.510206100955 460,927835,1951.4786440268247,2016.9731382075943,1672.0801089577474,1125.0229080682857 480,975961,2076.6338294064494,2033.1853468961958,1719.2462212252974,1153.158261927912 500,1018777,2094.823218886582,2037.4852753404898,1746.4288709310954,1174.8624939692247 520,1063733,2137.998709779158,2045.5028969721004,1778.8271760046641,1198.4702839108422 540,1105407,2121.91619308999,2046.9043296729049,1798.2960826750962,1217.8576817035882 560,1148338,2126.4127152635606,2050.4509561002797,1820.616007231527,1238.2444234074262 580,1189999,2113.61602051879,2051.570290715584,1837.4446097375073,1256.7786337375683 600,1219584,1937.6445710562814,2031.6753069957329,1814.671731059335,1261.7477397513687 620,1237632,1632.4506998235333,1995.2499874637524,1755.4487350018405,1253.8445924435982    Seems very strange value for my example, far from 200,000 events/s/node/stream   Regards,  Matthieu   On Jun 24, 2013, at 11:19 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  I try to use Adapter to send s4 events.", "With metrics report, 20,10462,88.63259092217602,539.6449108859357,18.577650313690874,6.241814566462701 40,36006,417.83633322358764,914.1057643161282,97.55624823196746,33.40088245418529 60,63859,674.1012974987167,1075.2326549158463,176.33878995148274,61.646803531230724 80,97835,953.6282787690939,1232.2934375999696,271.48890371088254,96.56144395108957 100,131535,1162.2060916405578,1323.3704459079934,363.98505627735324,131.98430793014757 120,165282,1327.52314133145,1384.2675551261093,453.5195236495672,167.61679021575551 140,190776,1305.7285112621298,1368.4361242524062,504.7782182758366,191.36049732440895  20,000 events per 20s  = 1000 EVENTS/s  Very slow, I modify the S4_HOME/subprojects/s4-comm/bin/default.s4.comm.properties  s4.comm.emitter.class=org.apache.s4.comm.tcp.TCPEmitter s4.comm.emitter.remote.class=org.apache.s4.comm.tcp.TCPRemoteEmitter s4.comm.listener.class=org.apache.s4.comm.tcp.TCPListener  # I/O channel connection timeout, when applicable (e.g.", "used by netty) s4.comm.timeout=1000  # NOTE: the following numbers should be tuned according to the application, use case, and infrastructure  # how many threads to use for the sender stage (i.e.", "serialization) #s4.sender.parallelism=1 s4.sender.parallelism=100 # maximum number of events in the buffer of the sender stage #s4.sender.workQueueSize=10000 s4.sender.workQueueSize=100000 # maximum sending rate from a given node, in events / s (used with throttling sender executors) s4.sender.maxRate=200000  # how many threads to use for the *remote* sender stage (i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Here is metrics report, already run 620s(NO pe logic,) List event-emitted@seacluster1@partition-0.csv<mailto:event-emitted@seacluster1@partition-0.csv file  # time,count,1 min rate,mean rate,5 min rate,15 min rate 20,30482,728.4264567247864,1532.1851387286329,577.7027401449616,550.7086872323928 40,68256,1003.4693068790475,1710.158498360041,651.1838653093574,576.4097533046603 60,126665,1526.423951717675,2114.1420782852447,792.5372602881871,626.2019978354255 80,159222,1631.4770294401224,1992.409242530607,865.8704071266087,654.9705320161032 100,206876,1821.551542703833,2070.5009200329328,958.5809893144597,691.1999843184435 120,245115,1852.9758861695257,2044.0217493195803,1021.5173199955651,718.5310141264627 140,286057,1892.8858688327914,2044.444972093294,1084.2498192357743,746.5688788373484 160,324662,1952.4301928970413,2030.146074235012,1151.037578221836,776.8090098793148 180,371829,1959.4350067303067,2066.6134765239876,1202.907690109737,802.6284106918696 200,433557,2283.734039729985,2168.043356631868,1325.4143582100112,853.1681426416412 220,464815,2165.047817425384,2113.0019773033414,1362.4434184467839,876.2949286924105 240,504620,2065.9291371459913,2102.751240490619,1390.829435765405,896.605433400642 260,558158,2192.29985605397,2146.9020962821564,1462.6363950595012,931.9095985316526 280,595910,2199.1021924478428,2128.3677435991726,1513.937239488627,961.2098181907241 300,627181,1994.0377201896988,2090.694355349902,1511.0411290249551,972.3477079289204 320,664342,1959.8933962067783,2076.1406486970463,1534.4815810411815,992.1780521538831 340,698717,1912.6664972668054,2055.1073335596802,1551.413320562825,1009.8797759279687 360,738468,1940.3861933767728,2051.3430577192726,1579.897070517331,1031.4220907479778 380,769673,1808.904535049888,2025.4290025114635,1577.8601886321867,1045.7495364549434 400,807406,1834.34177336605,2018.480153382513,1597.9097843354657,1064.241627735365 420,851009,1918.7250831019285,2026.1694030316874,1634.835168023956,1088.691609058512 440,884427,1854.4469403637247,2010.0049048826818,1637.4121396194184,1101.510206100955 460,927835,1951.4786440268247,2016.9731382075943,1672.0801089577474,1125.0229080682857 480,975961,2076.6338294064494,2033.1853468961958,1719.2462212252974,1153.158261927912 500,1018777,2094.823218886582,2037.4852753404898,1746.4288709310954,1174.8624939692247 520,1063733,2137.998709779158,2045.5028969721004,1778.8271760046641,1198.4702839108422 540,1105407,2121.91619308999,2046.9043296729049,1798.2960826750962,1217.8576817035882 560,1148338,2126.4127152635606,2050.4509561002797,1820.616007231527,1238.2444234074262 580,1189999,2113.61602051879,2051.570290715584,1837.4446097375073,1256.7786337375683 600,1219584,1937.6445710562814,2031.6753069957329,1814.671731059335,1261.7477397513687 620,1237632,1632.4506998235333,1995.2499874637524,1755.4487350018405,1253.8445924435982    Seems very strange value for my example, far from 200,000 events/s/node/stream   Regards,  Matthieu   On Jun 24, 2013, at 11:19 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  I try to use Adapter to send s4 events.", "With metrics report, 20,10462,88.63259092217602,539.6449108859357,18.577650313690874,6.241814566462701 40,36006,417.83633322358764,914.1057643161282,97.55624823196746,33.40088245418529 60,63859,674.1012974987167,1075.2326549158463,176.33878995148274,61.646803531230724 80,97835,953.6282787690939,1232.2934375999696,271.48890371088254,96.56144395108957 100,131535,1162.2060916405578,1323.3704459079934,363.98505627735324,131.98430793014757 120,165282,1327.52314133145,1384.2675551261093,453.5195236495672,167.61679021575551 140,190776,1305.7285112621298,1368.4361242524062,504.7782182758366,191.36049732440895  20,000 events per 20s  = 1000 EVENTS/s  Very slow, I modify the S4_HOME/subprojects/s4-comm/bin/default.s4.comm.properties  s4.comm.emitter.class=org.apache.s4.comm.tcp.TCPEmitter s4.comm.emitter.remote.class=org.apache.s4.comm.tcp.TCPRemoteEmitter s4.comm.listener.class=org.apache.s4.comm.tcp.TCPListener  # I/O channel connection timeout, when applicable (e.g.", "used by netty) s4.comm.timeout=1000  # NOTE: the following numbers should be tuned according to the application, use case, and infrastructure  # how many threads to use for the sender stage (i.e.", "serialization) #s4.sender.parallelism=1 s4.sender.parallelism=100 # maximum number of events in the buffer of the sender stage #s4.sender.workQueueSize=10000 s4.sender.workQueueSize=100000 # maximum sending rate from a given node, in events / s (used with throttling sender executors) s4.sender.maxRate=200000  # how many threads to use for the *remote* sender stage (i.e.", "serialization) #s4.remoteSender.parallelism=1 s4.remoteSender.parallelism=100 # maximum number of events in the buffer of the *remote* sender stage #s4.remoteSender.workQueueSize=10000 s4.remoteSender.workQueueSize=100000 # maximum *remote* sending rate from a given node, in events / s (used with throttling *remote* sender executors) s4.remoteSender.maxRate=200000  # maximum number of pending writes to a given comm channel #s4.emitter.maxPendingWrites=1000 s4.emitter.maxPendingWrites=10000  # maximum number of events in the buffer of the processing stage #s4.stream.workQueueSize=10000 s4.stream.workQueueSize=100000  only improve from 500 events 1000 events,  I read file 88m only need 8s, but send events cost 620s now for 1,237,632 events, why slow, s4 can trigger 200,000 events/s, how I can do up to this values, pls give me detail instructions.      "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Can you send the whole schema please?", "________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Monday, April 13, 2020 11:11 AM To: users@daffodil.apache.org <users@daffodil.apache.org Subject: Two bugs in Daffodil?", "Hi Folks,  I have a binary data format.", "One field is a 1-byte name representing a person's age.", "I have it defined like so:  <xs:element name=\"age\" type=\"xs:nonNegativeInteger\"     dfdl:representation=\"binary\"     dfdl:lengthKind=\"explicit\"     dfdl:lengthUnits=\"bytes\"     dfdl:length=\"1\"     dfdl:alignment=\"1\"     dfdl:alignmentUnits=\"bytes\"     dfdl:byteOrder=\"littleEndian\"     dfdl:binaryNumberRep=\"binary\" /  Daffodil gives this error message:  [error] Schema Definition Error: Property encoding is not defined."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Monday, April 13, 2020 11:11 AM To: users@daffodil.apache.org <users@daffodil.apache.org Subject: Two bugs in Daffodil?", "Hi Folks,  I have a binary data format.", "One field is a 1-byte name representing a person's age.", "I have it defined like so:  <xs:element name=\"age\" type=\"xs:nonNegativeInteger\"     dfdl:representation=\"binary\"     dfdl:lengthKind=\"explicit\"     dfdl:lengthUnits=\"bytes\"     dfdl:length=\"1\"     dfdl:alignment=\"1\"     dfdl:alignmentUnits=\"bytes\"     dfdl:byteOrder=\"littleEndian\"     dfdl:binaryNumberRep=\"binary\" /  Daffodil gives this error message:  [error] Schema Definition Error: Property encoding is not defined.", "Huh?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Folks,  I have a binary data format.", "One field is a 1-byte name representing a person's age.", "I have it defined like so:  <xs:element name=\"age\" type=\"xs:nonNegativeInteger\"     dfdl:representation=\"binary\"     dfdl:lengthKind=\"explicit\"     dfdl:lengthUnits=\"bytes\"     dfdl:length=\"1\"     dfdl:alignment=\"1\"     dfdl:alignmentUnits=\"bytes\"     dfdl:byteOrder=\"littleEndian\"     dfdl:binaryNumberRep=\"binary\" /  Daffodil gives this error message:  [error] Schema Definition Error: Property encoding is not defined.", "Huh?", "Why do I have to specify a (character) encoding on a binary nonNegativeInteger field?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One field is a 1-byte name representing a person's age.", "I have it defined like so:  <xs:element name=\"age\" type=\"xs:nonNegativeInteger\"     dfdl:representation=\"binary\"     dfdl:lengthKind=\"explicit\"     dfdl:lengthUnits=\"bytes\"     dfdl:length=\"1\"     dfdl:alignment=\"1\"     dfdl:alignmentUnits=\"bytes\"     dfdl:byteOrder=\"littleEndian\"     dfdl:binaryNumberRep=\"binary\" /  Daffodil gives this error message:  [error] Schema Definition Error: Property encoding is not defined.", "Huh?", "Why do I have to specify a (character) encoding on a binary nonNegativeInteger field?", "Next, I thought, \"Okay, that doesn't make sense, but let me put dfdl:encoding=\"ASCII\" on the element declaration.\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have it defined like so:  <xs:element name=\"age\" type=\"xs:nonNegativeInteger\"     dfdl:representation=\"binary\"     dfdl:lengthKind=\"explicit\"     dfdl:lengthUnits=\"bytes\"     dfdl:length=\"1\"     dfdl:alignment=\"1\"     dfdl:alignmentUnits=\"bytes\"     dfdl:byteOrder=\"littleEndian\"     dfdl:binaryNumberRep=\"binary\" /  Daffodil gives this error message:  [error] Schema Definition Error: Property encoding is not defined.", "Huh?", "Why do I have to specify a (character) encoding on a binary nonNegativeInteger field?", "Next, I thought, \"Okay, that doesn't make sense, but let me put dfdl:encoding=\"ASCII\" on the element declaration.\"", "I did so, and yet I got the same error message!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Huh?", "Why do I have to specify a (character) encoding on a binary nonNegativeInteger field?", "Next, I thought, \"Okay, that doesn't make sense, but let me put dfdl:encoding=\"ASCII\" on the element declaration.\"", "I did so, and yet I got the same error message!", "Curiously, when I put the encoding in dfdl:format then the error message went away."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why do I have to specify a (character) encoding on a binary nonNegativeInteger field?", "Next, I thought, \"Okay, that doesn't make sense, but let me put dfdl:encoding=\"ASCII\" on the element declaration.\"", "I did so, and yet I got the same error message!", "Curiously, when I put the encoding in dfdl:format then the error message went away.", "Conclusion: Daffodil has a bug."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Next, I thought, \"Okay, that doesn't make sense, but let me put dfdl:encoding=\"ASCII\" on the element declaration.\"", "I did so, and yet I got the same error message!", "Curiously, when I put the encoding in dfdl:format then the error message went away.", "Conclusion: Daffodil has a bug.", "Daffodil sometimes does not recognize the encoding property on an element declaration."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I did so, and yet I got the same error message!", "Curiously, when I put the encoding in dfdl:format then the error message went away.", "Conclusion: Daffodil has a bug.", "Daffodil sometimes does not recognize the encoding property on an element declaration.", "Do you agree?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Curiously, when I put the encoding in dfdl:format then the error message went away.", "Conclusion: Daffodil has a bug.", "Daffodil sometimes does not recognize the encoding property on an element declaration.", "Do you agree?", "There is the exact same problem with initiator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Conclusion: Daffodil has a bug.", "Daffodil sometimes does not recognize the encoding property on an element declaration.", "Do you agree?", "There is the exact same problem with initiator.", "I put dfdl:initiator=\"\" on the element declaration and Daffodil says there is no initiator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Daffodil sometimes does not recognize the encoding property on an element declaration.", "Do you agree?", "There is the exact same problem with initiator.", "I put dfdl:initiator=\"\" on the element declaration and Daffodil says there is no initiator.", "When I put it on dfdl:format, Daffodil doesn't give an error."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do you agree?", "There is the exact same problem with initiator.", "I put dfdl:initiator=\"\" on the element declaration and Daffodil says there is no initiator.", "When I put it on dfdl:format, Daffodil doesn't give an error.", "I believe this is aother bug."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I put dfdl:initiator=\"\" on the element declaration and Daffodil says there is no initiator.", "When I put it on dfdl:format, Daffodil doesn't give an error.", "I believe this is aother bug.", "Do you agree?", "/Roger       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It should be available if all the infrastructure works as it should.", "-- Adam   On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:  Thanks Adam.", "That was a quick response for a Sunday!", "Will that be  available at:   http://people.apache.org/maven-snapshot-repository/org/apache/myfaces/tr  inidad   tomorrow, or do I have to build from source control.", "Steve Vangasse   www.boardshop.co.uk   0870 0600 688    -----Original Message-----  From: Adam Winer [mailto:awiner@gmail.com]  Sent: 11 February 2007 17:56  To: adffaces-user@incubator.apache.org  Subject: Re: Skinning tr:panelTip   Steve,   I've rectified this just now."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam   On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:  Thanks Adam.", "That was a quick response for a Sunday!", "Will that be  available at:   http://people.apache.org/maven-snapshot-repository/org/apache/myfaces/tr  inidad   tomorrow, or do I have to build from source control.", "Steve Vangasse   www.boardshop.co.uk   0870 0600 688    -----Original Message-----  From: Adam Winer [mailto:awiner@gmail.com]  Sent: 11 February 2007 17:56  To: adffaces-user@incubator.apache.org  Subject: Re: Skinning tr:panelTip   Steve,   I've rectified this just now.", "panelTip has been brought into the modern  era with a real, non-UINode renderer (makes it twice as fast, FWIW), and  three skinning selectors:    af|panelTip    af|panelTip::label    af|panelTip::content   ... and pretty much the obvious meaning for each."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That was a quick response for a Sunday!", "Will that be  available at:   http://people.apache.org/maven-snapshot-repository/org/apache/myfaces/tr  inidad   tomorrow, or do I have to build from source control.", "Steve Vangasse   www.boardshop.co.uk   0870 0600 688    -----Original Message-----  From: Adam Winer [mailto:awiner@gmail.com]  Sent: 11 February 2007 17:56  To: adffaces-user@incubator.apache.org  Subject: Re: Skinning tr:panelTip   Steve,   I've rectified this just now.", "panelTip has been brought into the modern  era with a real, non-UINode renderer (makes it twice as fast, FWIW), and  three skinning selectors:    af|panelTip    af|panelTip::label    af|panelTip::content   ... and pretty much the obvious meaning for each.", "We probably should  also add an optional skinnable icon, which'd bring this back to parity  with the old ADF Faces panelTip."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Will that be  available at:   http://people.apache.org/maven-snapshot-repository/org/apache/myfaces/tr  inidad   tomorrow, or do I have to build from source control.", "Steve Vangasse   www.boardshop.co.uk   0870 0600 688    -----Original Message-----  From: Adam Winer [mailto:awiner@gmail.com]  Sent: 11 February 2007 17:56  To: adffaces-user@incubator.apache.org  Subject: Re: Skinning tr:panelTip   Steve,   I've rectified this just now.", "panelTip has been brought into the modern  era with a real, non-UINode renderer (makes it twice as fast, FWIW), and  three skinning selectors:    af|panelTip    af|panelTip::label    af|panelTip::content   ... and pretty much the obvious meaning for each.", "We probably should  also add an optional skinnable icon, which'd bring this back to parity  with the old ADF Faces panelTip.", "Cheers,  Adam    On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:   Hello,   I've been looking through the Trinidad code for a way to skin the   panelTip component."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Steve Vangasse   www.boardshop.co.uk   0870 0600 688    -----Original Message-----  From: Adam Winer [mailto:awiner@gmail.com]  Sent: 11 February 2007 17:56  To: adffaces-user@incubator.apache.org  Subject: Re: Skinning tr:panelTip   Steve,   I've rectified this just now.", "panelTip has been brought into the modern  era with a real, non-UINode renderer (makes it twice as fast, FWIW), and  three skinning selectors:    af|panelTip    af|panelTip::label    af|panelTip::content   ... and pretty much the obvious meaning for each.", "We probably should  also add an optional skinnable icon, which'd bring this back to parity  with the old ADF Faces panelTip.", "Cheers,  Adam    On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:   Hello,   I've been looking through the Trinidad code for a way to skin the   panelTip component.", "So far the best I have found are OraTipText   OraTipLabel which only skin the text and the label, not the   surrounding panel."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["panelTip has been brought into the modern  era with a real, non-UINode renderer (makes it twice as fast, FWIW), and  three skinning selectors:    af|panelTip    af|panelTip::label    af|panelTip::content   ... and pretty much the obvious meaning for each.", "We probably should  also add an optional skinnable icon, which'd bring this back to parity  with the old ADF Faces panelTip.", "Cheers,  Adam    On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:   Hello,   I've been looking through the Trinidad code for a way to skin the   panelTip component.", "So far the best I have found are OraTipText   OraTipLabel which only skin the text and the label, not the   surrounding panel.", "I've resorted to using inlineStyle but I would much    prefer to keep the styling in the skin file."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers,  Adam    On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:   Hello,   I've been looking through the Trinidad code for a way to skin the   panelTip component.", "So far the best I have found are OraTipText   OraTipLabel which only skin the text and the label, not the   surrounding panel.", "I've resorted to using inlineStyle but I would much    prefer to keep the styling in the skin file.", "Does anyone know how this    can be done?", "Thanks,     Steve Vangasse     www.shopformat.com       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm not sure what the equivalent Trinidad class is to AdfFacesContext.", "Benj  -----Original Message----- From: Chris Gibbons [mailto:cgibbons@solutionstream.com]  Sent: Friday, March 30, 2007 3:33 PM To: adffaces-user@incubator.apache.org Subject: PPR Issue  Hi,        I have a page that has two text boxes and a string of text.", "The text string needs to reflect the values from the inputText boxes.", "I have autoSubmit=true, and have partialListeners on my outputText set to id's of the inputText boxes, and an update does trigger.", "When I debug into my code, the value that is backing the outputText does get updated, but the text on the webpage doesn't update with this new value."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Benj  -----Original Message----- From: Chris Gibbons [mailto:cgibbons@solutionstream.com]  Sent: Friday, March 30, 2007 3:33 PM To: adffaces-user@incubator.apache.org Subject: PPR Issue  Hi,        I have a page that has two text boxes and a string of text.", "The text string needs to reflect the values from the inputText boxes.", "I have autoSubmit=true, and have partialListeners on my outputText set to id's of the inputText boxes, and an update does trigger.", "When I debug into my code, the value that is backing the outputText does get updated, but the text on the webpage doesn't update with this new value.", "Now what?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The text string needs to reflect the values from the inputText boxes.", "I have autoSubmit=true, and have partialListeners on my outputText set to id's of the inputText boxes, and an update does trigger.", "When I debug into my code, the value that is backing the outputText does get updated, but the text on the webpage doesn't update with this new value.", "Now what?", "I'm stuck, and getting rather frustrated with PPR in Trinidad, any and all help would be greatly appreciated."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have autoSubmit=true, and have partialListeners on my outputText set to id's of the inputText boxes, and an update does trigger.", "When I debug into my code, the value that is backing the outputText does get updated, but the text on the webpage doesn't update with this new value.", "Now what?", "I'm stuck, and getting rather frustrated with PPR in Trinidad, any and all help would be greatly appreciated.", "Chris    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But is/will be the ADF Faces Rich Client also under Apache 2.0 license and what about the different ADF Faces downloads offered by Oracle (with JDeveloper or stand alone).", "I have the impression that they are _not_ Apache 2.0 licensed.", "But that means there are at least two different streams of development which have the same origin but was separated in last summer and are now independent from each other.", "Am I right (I hope not!)?", "Michael    -----Urspr\u00fcngliche Nachricht----- Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com] Im Auftrag von Matthias Wessendorf Gesendet: Donnerstag, 15."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But that means there are at least two different streams of development which have the same origin but was separated in last summer and are now independent from each other.", "Am I right (I hope not!)?", "Michael    -----Urspr\u00fcngliche Nachricht----- Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com] Im Auftrag von Matthias Wessendorf Gesendet: Donnerstag, 15.", "M\u00e4rz 2007 10:27 An: adffaces-user@incubator.apache.org Betreff: Re: Relation between Trinidad and ADF Faces  Hello Michael,  there is a ADF Faces Rich Client, currently under development, see here: http://www.oracle.com/technology/products/jdev/viewlets/1013/richclient_viewlet_swf.html  That stuff uses Trinidad features (like framework or build feature).", "Trinidad is Apache 2.0 license based, so you can even fork it and sell the *enhancements* as closed source, no problems with that!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Am I right (I hope not!)?", "Michael    -----Urspr\u00fcngliche Nachricht----- Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com] Im Auftrag von Matthias Wessendorf Gesendet: Donnerstag, 15.", "M\u00e4rz 2007 10:27 An: adffaces-user@incubator.apache.org Betreff: Re: Relation between Trinidad and ADF Faces  Hello Michael,  there is a ADF Faces Rich Client, currently under development, see here: http://www.oracle.com/technology/products/jdev/viewlets/1013/richclient_viewlet_swf.html  That stuff uses Trinidad features (like framework or build feature).", "Trinidad is Apache 2.0 license based, so you can even fork it and sell the *enhancements* as closed source, no problems with that!", "The good on Apache 2.0 license is, it is damn liberal ;)   Does that help ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Michael    -----Urspr\u00fcngliche Nachricht----- Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com] Im Auftrag von Matthias Wessendorf Gesendet: Donnerstag, 15.", "M\u00e4rz 2007 10:27 An: adffaces-user@incubator.apache.org Betreff: Re: Relation between Trinidad and ADF Faces  Hello Michael,  there is a ADF Faces Rich Client, currently under development, see here: http://www.oracle.com/technology/products/jdev/viewlets/1013/richclient_viewlet_swf.html  That stuff uses Trinidad features (like framework or build feature).", "Trinidad is Apache 2.0 license based, so you can even fork it and sell the *enhancements* as closed source, no problems with that!", "The good on Apache 2.0 license is, it is damn liberal ;)   Does that help ?", "-M  On 3/15/07, Michael Trompertz <Michael.Trompertz@feltengmbh.de wrote:  Hello   Can anybody eplain the Relation between Trinidad and ADF Faces."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Trinidad is Apache 2.0 license based, so you can even fork it and sell the *enhancements* as closed source, no problems with that!", "The good on Apache 2.0 license is, it is damn liberal ;)   Does that help ?", "-M  On 3/15/07, Michael Trompertz <Michael.Trompertz@feltengmbh.de wrote:  Hello   Can anybody eplain the Relation between Trinidad and ADF Faces.", "What I found out is that Oracle donated ADF Faces to Apache in summer   2006.", "The project was renamed to Trinidad."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The good on Apache 2.0 license is, it is damn liberal ;)   Does that help ?", "-M  On 3/15/07, Michael Trompertz <Michael.Trompertz@feltengmbh.de wrote:  Hello   Can anybody eplain the Relation between Trinidad and ADF Faces.", "What I found out is that Oracle donated ADF Faces to Apache in summer   2006.", "The project was renamed to Trinidad.", "But it seems that both   projetcs are developed further indepent from each other."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-M  On 3/15/07, Michael Trompertz <Michael.Trompertz@feltengmbh.de wrote:  Hello   Can anybody eplain the Relation between Trinidad and ADF Faces.", "What I found out is that Oracle donated ADF Faces to Apache in summer   2006.", "The project was renamed to Trinidad.", "But it seems that both   projetcs are developed further indepent from each other.", "The ADF Faces needs a license for usage while Trinidad does not."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What I found out is that Oracle donated ADF Faces to Apache in summer   2006.", "The project was renamed to Trinidad.", "But it seems that both   projetcs are developed further indepent from each other.", "The ADF Faces needs a license for usage while Trinidad does not.", "Is this right so far?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The project was renamed to Trinidad.", "But it seems that both   projetcs are developed further indepent from each other.", "The ADF Faces needs a license for usage while Trinidad does not.", "Is this right so far?", "Thanks in advance   Michael     -- Matthias Wessendorf http://tinyurl.com/fmywh  further stuff: blog: http://jroller.com/page/mwessendorf mail: mwessendorf-at-gmail-dot-com    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello,  JobExecutionEntity is afaik not a regal entity because it maps TemporalType to a java.sql.", "* (Timestamp).", "I think java.sql.", "* was legal at one point but not anymore.", "The javadoc for Java EE 6 and Java EE 7 says this about TemporalType:  \"Type used to indicate a specific mapping of java.util.Date or java.util.Calendar.\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["* (Timestamp).", "I think java.sql.", "* was legal at one point but not anymore.", "The javadoc for Java EE 6 and Java EE 7 says this about TemporalType:  \"Type used to indicate a specific mapping of java.util.Date or java.util.Calendar.\"", "cheers / Karl  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Ok.. list didnt return anything because there were no tables present.", "I have another query.", "*blur (default) create -t test -c 10 -l test* *blur (default) mutate test r1 rid1 f1 c1:v1* *blur (default) query test **  *1 results found in [223.357074 ms].", "Row [1] Record [1] Column [1] Data (bytes) [8]* *result# rowid recordid f1.c1* *0       r1    rid1     v1   *   Until this point it is working fine.", "*blur (default) create -t testable -c 10 -l test* *blur (default) mutate testable r2 rid2 f2 c2:f2* *blur (default) query testable **   *2 results found in [36.109771 ms]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have another query.", "*blur (default) create -t test -c 10 -l test* *blur (default) mutate test r1 rid1 f1 c1:v1* *blur (default) query test **  *1 results found in [223.357074 ms].", "Row [1] Record [1] Column [1] Data (bytes) [8]* *result# rowid recordid f1.c1* *0       r1    rid1     v1   *   Until this point it is working fine.", "*blur (default) create -t testable -c 10 -l test* *blur (default) mutate testable r2 rid2 f2 c2:f2* *blur (default) query testable **   *2 results found in [36.109771 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1   *  Now when i am querying table testable why it is returning 2 results (One row from test table) even though it has only one record?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*blur (default) create -t test -c 10 -l test* *blur (default) mutate test r1 rid1 f1 c1:v1* *blur (default) query test **  *1 results found in [223.357074 ms].", "Row [1] Record [1] Column [1] Data (bytes) [8]* *result# rowid recordid f1.c1* *0       r1    rid1     v1   *   Until this point it is working fine.", "*blur (default) create -t testable -c 10 -l test* *blur (default) mutate testable r2 rid2 f2 c2:f2* *blur (default) query testable **   *2 results found in [36.109771 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1   *  Now when i am querying table testable why it is returning 2 results (One row from test table) even though it has only one record?", "*blur (default) disable test* *blur (default) remove test* *blur (default) query testable **   *2 results found in [43.491442 ms]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Row [1] Record [1] Column [1] Data (bytes) [8]* *result# rowid recordid f1.c1* *0       r1    rid1     v1   *   Until this point it is working fine.", "*blur (default) create -t testable -c 10 -l test* *blur (default) mutate testable r2 rid2 f2 c2:f2* *blur (default) query testable **   *2 results found in [36.109771 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1   *  Now when i am querying table testable why it is returning 2 results (One row from test table) even though it has only one record?", "*blur (default) disable test* *blur (default) remove test* *blur (default) query testable **   *2 results found in [43.491442 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1  *  Here even after removing test table, it still returns 2 results , one from test and one from testable tables."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*blur (default) create -t testable -c 10 -l test* *blur (default) mutate testable r2 rid2 f2 c2:f2* *blur (default) query testable **   *2 results found in [36.109771 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1   *  Now when i am querying table testable why it is returning 2 results (One row from test table) even though it has only one record?", "*blur (default) disable test* *blur (default) remove test* *blur (default) query testable **   *2 results found in [43.491442 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1  *  Here even after removing test table, it still returns 2 results , one from test and one from testable tables.", "What is wrong here??"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*blur (default) disable test* *blur (default) remove test* *blur (default) query testable **   *2 results found in [43.491442 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1  *  Here even after removing test table, it still returns 2 results , one from test and one from testable tables.", "What is wrong here??", "Why is this residual data keep coming?", "Any solution on this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1  *  Here even after removing test table, it still returns 2 results , one from test and one from testable tables.", "What is wrong here??", "Why is this residual data keep coming?", "Any solution on this?", "Thanks, Ameya          On Thu, Aug 7, 2014 at 2:35 PM, Aaron McCurry <amccurry@gmail.com wrote:   Unfortunately no."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What is wrong here??", "Why is this residual data keep coming?", "Any solution on this?", "Thanks, Ameya          On Thu, Aug 7, 2014 at 2:35 PM, Aaron McCurry <amccurry@gmail.com wrote:   Unfortunately no.", "The mail lists strip out all attachments."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why is this residual data keep coming?", "Any solution on this?", "Thanks, Ameya          On Thu, Aug 7, 2014 at 2:35 PM, Aaron McCurry <amccurry@gmail.com wrote:   Unfortunately no.", "The mail lists strip out all attachments.", "You could copy  the text from the terminal or find a public image share."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Any solution on this?", "Thanks, Ameya          On Thu, Aug 7, 2014 at 2:35 PM, Aaron McCurry <amccurry@gmail.com wrote:   Unfortunately no.", "The mail lists strip out all attachments.", "You could copy  the text from the terminal or find a public image share.", "Thanks!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks, Ameya          On Thu, Aug 7, 2014 at 2:35 PM, Aaron McCurry <amccurry@gmail.com wrote:   Unfortunately no.", "The mail lists strip out all attachments.", "You could copy  the text from the terminal or find a public image share.", "Thanks!", "Aaron   On Thursday, August 7, 2014, Ameya Aware <ameya.aware@gmail.com wrote:    Can i attach it here in mail?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You could copy  the text from the terminal or find a public image share.", "Thanks!", "Aaron   On Thursday, August 7, 2014, Ameya Aware <ameya.aware@gmail.com wrote:    Can i attach it here in mail?", "Will that work?", "Thanks,   Ameya           On Thu, Aug 7, 2014 at 10:54 AM, Tim Williams <williamstw@gmail.com   <javascript:; wrote:      Hey Ameya,    Your screenshot got stripped by the mail-list software - can you post  to    some public spot?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks!", "Aaron   On Thursday, August 7, 2014, Ameya Aware <ameya.aware@gmail.com wrote:    Can i attach it here in mail?", "Will that work?", "Thanks,   Ameya           On Thu, Aug 7, 2014 at 10:54 AM, Tim Williams <williamstw@gmail.com   <javascript:; wrote:      Hey Ameya,    Your screenshot got stripped by the mail-list software - can you post  to    some public spot?", "The list command doesn't take any parameters so if    you've created a table and list it and it doesn't show, then  something's    gone really wrong."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Aaron   On Thursday, August 7, 2014, Ameya Aware <ameya.aware@gmail.com wrote:    Can i attach it here in mail?", "Will that work?", "Thanks,   Ameya           On Thu, Aug 7, 2014 at 10:54 AM, Tim Williams <williamstw@gmail.com   <javascript:; wrote:      Hey Ameya,    Your screenshot got stripped by the mail-list software - can you post  to    some public spot?", "The list command doesn't take any parameters so if    you've created a table and list it and it doesn't show, then  something's    gone really wrong.", "Thanks,    --tim          On Thu, Aug 7, 2014 at 9:53 AM, Ameya Aware <ameya.aware@gmail.com   <javascript:; wrote:        Any help on this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Will that work?", "Thanks,   Ameya           On Thu, Aug 7, 2014 at 10:54 AM, Tim Williams <williamstw@gmail.com   <javascript:; wrote:      Hey Ameya,    Your screenshot got stripped by the mail-list software - can you post  to    some public spot?", "The list command doesn't take any parameters so if    you've created a table and list it and it doesn't show, then  something's    gone really wrong.", "Thanks,    --tim          On Thu, Aug 7, 2014 at 9:53 AM, Ameya Aware <ameya.aware@gmail.com   <javascript:; wrote:        Any help on this?", "Thanks,     Ameya             On Wed, Aug 6, 2014 at 3:08 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;    wrote:         ok sure.."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,   Ameya           On Thu, Aug 7, 2014 at 10:54 AM, Tim Williams <williamstw@gmail.com   <javascript:; wrote:      Hey Ameya,    Your screenshot got stripped by the mail-list software - can you post  to    some public spot?", "The list command doesn't take any parameters so if    you've created a table and list it and it doesn't show, then  something's    gone really wrong.", "Thanks,    --tim          On Thu, Aug 7, 2014 at 9:53 AM, Ameya Aware <ameya.aware@gmail.com   <javascript:; wrote:        Any help on this?", "Thanks,     Ameya             On Wed, Aug 6, 2014 at 3:08 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;    wrote:         ok sure..", "When i give list commnad it didnt list the tables.. Do i need to  pass    any     parameters to it?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The list command doesn't take any parameters so if    you've created a table and list it and it doesn't show, then  something's    gone really wrong.", "Thanks,    --tim          On Thu, Aug 7, 2014 at 9:53 AM, Ameya Aware <ameya.aware@gmail.com   <javascript:; wrote:        Any help on this?", "Thanks,     Ameya             On Wed, Aug 6, 2014 at 3:08 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;    wrote:         ok sure..", "When i give list commnad it didnt list the tables.. Do i need to  pass    any     parameters to it?", "Though \"help\" doesnt say so."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,    --tim          On Thu, Aug 7, 2014 at 9:53 AM, Ameya Aware <ameya.aware@gmail.com   <javascript:; wrote:        Any help on this?", "Thanks,     Ameya             On Wed, Aug 6, 2014 at 3:08 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;    wrote:         ok sure..", "When i give list commnad it didnt list the tables.. Do i need to  pass    any     parameters to it?", "Though \"help\" doesnt say so.", "Please find snapshot below."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,     Ameya             On Wed, Aug 6, 2014 at 3:08 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;    wrote:         ok sure..", "When i give list commnad it didnt list the tables.. Do i need to  pass    any     parameters to it?", "Though \"help\" doesnt say so.", "Please find snapshot below.", "[image: Inline image 1]             Thanks,     Ameya             On Wed, Aug 6, 2014 at 2:24 PM, Tim Williams <williamstw@gmail.com   <javascript:;     wrote:         On Wed, Aug 6, 2014 at 2:21 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;     wrote:      Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When i give list commnad it didnt list the tables.. Do i need to  pass    any     parameters to it?", "Though \"help\" doesnt say so.", "Please find snapshot below.", "[image: Inline image 1]             Thanks,     Ameya             On Wed, Aug 6, 2014 at 2:24 PM, Tim Williams <williamstw@gmail.com   <javascript:;     wrote:         On Wed, Aug 6, 2014 at 2:21 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;     wrote:      Thanks.", "Also can we check which all tables exist?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Though \"help\" doesnt say so.", "Please find snapshot below.", "[image: Inline image 1]             Thanks,     Ameya             On Wed, Aug 6, 2014 at 2:24 PM, Tim Williams <williamstw@gmail.com   <javascript:;     wrote:         On Wed, Aug 6, 2014 at 2:21 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;     wrote:      Thanks.", "Also can we check which all tables exist?", "Sure, the \"list\" command in the shell will list them."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please find snapshot below.", "[image: Inline image 1]             Thanks,     Ameya             On Wed, Aug 6, 2014 at 2:24 PM, Tim Williams <williamstw@gmail.com   <javascript:;     wrote:         On Wed, Aug 6, 2014 at 2:21 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;     wrote:      Thanks.", "Also can we check which all tables exist?", "Sure, the \"list\" command in the shell will list them.", "You may also     wanna give the \"help\" command a spin:)  If you type 'help' and it     isn't clear what they actually do, let us know so we can add a more     helpful description of the command."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[image: Inline image 1]             Thanks,     Ameya             On Wed, Aug 6, 2014 at 2:24 PM, Tim Williams <williamstw@gmail.com   <javascript:;     wrote:         On Wed, Aug 6, 2014 at 2:21 PM, Ameya Aware <ameya.aware@gmail.com   <javascript:;     wrote:      Thanks.", "Also can we check which all tables exist?", "Sure, the \"list\" command in the shell will list them.", "You may also     wanna give the \"help\" command a spin:)  If you type 'help' and it     isn't clear what they actually do, let us know so we can add a more     helpful description of the command.", "--tim                        "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1  Hello,  The Apache Taverna team are happy to announce the release of Taverna Parent 1 incubating and Taverna Language 0.15.0 incubating.", "Apache Taverna Language is a set of APIs for workflow definitions (SCUFL2) and workflow inputs/outputs/run (DataBundle), as consumed and produced by the Apache Taverna workflow system.", "The API includes support for working with Research Object Bundles, and loading/saving Taverna workflows in different formats.", "The release artifacts are downloadable from:  https://www.apache.org/dyn/closer.cgi/incubator/taverna/  Maven JAR artifacts are available from:  https://repository.apache.org/content/repositories/releases/org/apache/t averna/  Release notes are available from:  https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=1231832 2&version=12332247  https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=1231832 2&version=12332246  More details on Apache Taverna can be found at: http://taverna.incubator.apache.org/  We would like to thank all contributors who made this release possible.", "Thanks,  The Apache Taverna team -----BEGIN PGP SIGNATURE----- Version: GnuPG v1  iQEcBAEBAgAGBQJVy2qXAAoJEPK45GBX+Cy5TmIIAKrz/kVO1FAkRGsg760y4aJp +2W0262HCW+sdMkJauJAjyW7+W8LTBoBr1GquPYEE1wl1KVz6H63J4VFxuUdOyMn 0ujH4Y9Nqw8H76enm+hGba8rUxymfIcXEzWy6fBq7G2ho3rzosEcW+Mco6+Md3up J0fY1mS7+WYjf8X6vrRQDWeX5eWt4tte2kVAFUQeIDrsSj6zc+fJ8sYOylRfHfx1 3tnfpDjBA3QNfmui5WDok/LX/LqI8cPOwyyA9bu8jp+CSlAEB38TBGGPa47+5ZK6 jty9SqMOkOwUwFz2I2aKnYiaD3CErUJqWqPWnT+oGdwi7WfnHqi+2IRrSwfkjbM= =0LG4 -----END PGP SIGNATURE-----  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Aug 5, 2014 at 10:54 AM, Ameya Aware <ameya.aware@gmail.com wrote:  It returns some of the rows of table but not all.. :(  How are you interacting with Blur?", "the Shell?", "the thrift API?", "Console?", "By default Blur limits the number of results returned but those can be adjusted through the BlurQuery[1] object in the API."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["the Shell?", "the thrift API?", "Console?", "By default Blur limits the number of results returned but those can be adjusted through the BlurQuery[1] object in the API.", "Or, if you're using the shell, just adjust it with command line switches (e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["the thrift API?", "Console?", "By default Blur limits the number of results returned but those can be adjusted through the BlurQuery[1] object in the API.", "Or, if you're using the shell, just adjust it with command line switches (e.g.", "query test *:* -min 1000 -fetch 1000 )  --tim  [1] - http://incubator.apache.org/blur/docs/0.2.3/Blur.html#Struct_BlurQuery  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We use the ICU library for parsing numbers based on the textNumberPattern.", "This library has this to say about strict parsing of numbers:    The following conditions cause a parse failure relative to [lax] mode  (examples use the pattern \"#,##0.#\"):    * The presence and position of special symbols, including currency,   must match the pattern.", "'+123' fails (there is no plus sign in the pattern)    * Leading or doubled grouping separators      ',123' and '1,,234\" fail    * Groups of incorrect length when grouping is used      '1,23' and '1234,567' fail, but '1234' passes    * Grouping separators used in numbers followed by exponents      '1,234E5' fails, but '1234E5' and '1,234E' pass ('E' is not an      exponent when not followed by a number)  So bsaed on ICU's description of strict, this is the expected behavior.", "It doesn't say anything about missing grouping separators causing an error.", "Only that if they do exist then they must be in the right spot."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This library has this to say about strict parsing of numbers:    The following conditions cause a parse failure relative to [lax] mode  (examples use the pattern \"#,##0.#\"):    * The presence and position of special symbols, including currency,   must match the pattern.", "'+123' fails (there is no plus sign in the pattern)    * Leading or doubled grouping separators      ',123' and '1,,234\" fail    * Groups of incorrect length when grouping is used      '1,23' and '1234,567' fail, but '1234' passes    * Grouping separators used in numbers followed by exponents      '1,234E5' fails, but '1234E5' and '1,234E' pass ('E' is not an      exponent when not followed by a number)  So bsaed on ICU's description of strict, this is the expected behavior.", "It doesn't say anything about missing grouping separators causing an error.", "Only that if they do exist then they must be in the right spot.", "The only thing the DFDL specification mentions regarding strict numbers is this:   If 'strict' and dfdl:textNumberRep is 'standard' then the data must   follow the pattern with the exceptions that digits 0-9, decimal   separator and exponent separator are always recognised and parsed  To me, that reads like the decimal separator should always be required in strict mode, so this feels like the ICU behavior and the behavior described in the DFDL specification do not match."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["'+123' fails (there is no plus sign in the pattern)    * Leading or doubled grouping separators      ',123' and '1,,234\" fail    * Groups of incorrect length when grouping is used      '1,23' and '1234,567' fail, but '1234' passes    * Grouping separators used in numbers followed by exponents      '1,234E5' fails, but '1234E5' and '1,234E' pass ('E' is not an      exponent when not followed by a number)  So bsaed on ICU's description of strict, this is the expected behavior.", "It doesn't say anything about missing grouping separators causing an error.", "Only that if they do exist then they must be in the right spot.", "The only thing the DFDL specification mentions regarding strict numbers is this:   If 'strict' and dfdl:textNumberRep is 'standard' then the data must   follow the pattern with the exceptions that digits 0-9, decimal   separator and exponent separator are always recognised and parsed  To me, that reads like the decimal separator should always be required in strict mode, so this feels like the ICU behavior and the behavior described in the DFDL specification do not match.", "And I believe the DFDL behavior was intended to match match ICU behavior, so it's possible the DFDL specification needs to be updated."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It doesn't say anything about missing grouping separators causing an error.", "Only that if they do exist then they must be in the right spot.", "The only thing the DFDL specification mentions regarding strict numbers is this:   If 'strict' and dfdl:textNumberRep is 'standard' then the data must   follow the pattern with the exceptions that digits 0-9, decimal   separator and exponent separator are always recognised and parsed  To me, that reads like the decimal separator should always be required in strict mode, so this feels like the ICU behavior and the behavior described in the DFDL specification do not match.", "And I believe the DFDL behavior was intended to match match ICU behavior, so it's possible the DFDL specification needs to be updated.", "I've created DAFFODIL-2384 [1] to track this issue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Only that if they do exist then they must be in the right spot.", "The only thing the DFDL specification mentions regarding strict numbers is this:   If 'strict' and dfdl:textNumberRep is 'standard' then the data must   follow the pattern with the exceptions that digits 0-9, decimal   separator and exponent separator are always recognised and parsed  To me, that reads like the decimal separator should always be required in strict mode, so this feels like the ICU behavior and the behavior described in the DFDL specification do not match.", "And I believe the DFDL behavior was intended to match match ICU behavior, so it's possible the DFDL specification needs to be updated.", "I've created DAFFODIL-2384 [1] to track this issue.", "- Steve  [1] https://issues.apache.org/jira/browse/DAFFODIL-2384  On 8/17/20 11:57 AM, Roger L Costello wrote:  What is an example of input that will cause parsing the fail due to the grouping separators when textNumberCheckPolicy=\"strict\"?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The only thing the DFDL specification mentions regarding strict numbers is this:   If 'strict' and dfdl:textNumberRep is 'standard' then the data must   follow the pattern with the exceptions that digits 0-9, decimal   separator and exponent separator are always recognised and parsed  To me, that reads like the decimal separator should always be required in strict mode, so this feels like the ICU behavior and the behavior described in the DFDL specification do not match.", "And I believe the DFDL behavior was intended to match match ICU behavior, so it's possible the DFDL specification needs to be updated.", "I've created DAFFODIL-2384 [1] to track this issue.", "- Steve  [1] https://issues.apache.org/jira/browse/DAFFODIL-2384  On 8/17/20 11:57 AM, Roger L Costello wrote:  What is an example of input that will cause parsing the fail due to the grouping separators when textNumberCheckPolicy=\"strict\"?", "Why isn't the below an example, i.e., why is no error generated with the below example?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And I believe the DFDL behavior was intended to match match ICU behavior, so it's possible the DFDL specification needs to be updated.", "I've created DAFFODIL-2384 [1] to track this issue.", "- Steve  [1] https://issues.apache.org/jira/browse/DAFFODIL-2384  On 8/17/20 11:57 AM, Roger L Costello wrote:  What is an example of input that will cause parsing the fail due to the grouping separators when textNumberCheckPolicy=\"strict\"?", "Why isn't the below an example, i.e., why is no error generated with the below example?", "Why is it that with this input    1234    No error is raised when textNumberCheckPolicy=\"strict\" and textNumberPattern=\"#,###\" are specified:    <xs:element name=\"SimpleDataFormat\"      <xs:complexType          <xs:sequence              <xs:element name=\"NumStudents\" type=\"xs:nonNegativeInteger\"                   dfdl:textNumberCheckPolicy=\"strict\"                  dfdl:textNumberPattern=\"#,###\"                  dfdl:textStandardGroupingSeparator=\",\"                  dfdl:textStandardDecimalSeparator=\".\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I've created DAFFODIL-2384 [1] to track this issue.", "- Steve  [1] https://issues.apache.org/jira/browse/DAFFODIL-2384  On 8/17/20 11:57 AM, Roger L Costello wrote:  What is an example of input that will cause parsing the fail due to the grouping separators when textNumberCheckPolicy=\"strict\"?", "Why isn't the below an example, i.e., why is no error generated with the below example?", "Why is it that with this input    1234    No error is raised when textNumberCheckPolicy=\"strict\" and textNumberPattern=\"#,###\" are specified:    <xs:element name=\"SimpleDataFormat\"      <xs:complexType          <xs:sequence              <xs:element name=\"NumStudents\" type=\"xs:nonNegativeInteger\"                   dfdl:textNumberCheckPolicy=\"strict\"                  dfdl:textNumberPattern=\"#,###\"                  dfdl:textStandardGroupingSeparator=\",\"                  dfdl:textStandardDecimalSeparator=\".\"", "/          </xs:sequence      </xs:complexType  </xs:element     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Leonidas,  Thank you for your help.", "The newest version fix my original problem.", "The result is now delivered.", "However from a non root user I get a similar exception: [marcos@namenode mrql]$ /home/hadoop/mrql-0.9.4/bin/mrql.spark -dist -nodes 1 /tmp/script_etienne.mrql .", "."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The newest version fix my original problem.", "The result is now delivered.", "However from a non root user I get a similar exception: [marcos@namenode mrql]$ /home/hadoop/mrql-0.9.4/bin/mrql.spark -dist -nodes 1 /tmp/script_etienne.mrql .", ".", "java.io.FileNotFoundException: File does not exist: /user/marcos/tmp/hadoop_data_source_dir.txt  Where hadoop is my hadoop root user and marcos my regular user."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The result is now delivered.", "However from a non root user I get a similar exception: [marcos@namenode mrql]$ /home/hadoop/mrql-0.9.4/bin/mrql.spark -dist -nodes 1 /tmp/script_etienne.mrql .", ".", "java.io.FileNotFoundException: File does not exist: /user/marcos/tmp/hadoop_data_source_dir.txt  Where hadoop is my hadoop root user and marcos my regular user.", "If I *ls* the tmp directory I get: [marcos@namenode mrql]$ hadoop fs -ls /user/marcos/tmp/ Found 10 items -rw-r--r--   2 marcos users        213 2015-01-08 09:51 /user/marcos/tmp/marcos_data_source_dir.txt  Regards,  \u00c9tienne  On 7 January 2015 at 22:03, Leonidas Fegaras <fegaras@cse.uta.edu wrote:    Hi Etienne,  Both problems were fixed after MRQL 0.9.2 was released."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However from a non root user I get a similar exception: [marcos@namenode mrql]$ /home/hadoop/mrql-0.9.4/bin/mrql.spark -dist -nodes 1 /tmp/script_etienne.mrql .", ".", "java.io.FileNotFoundException: File does not exist: /user/marcos/tmp/hadoop_data_source_dir.txt  Where hadoop is my hadoop root user and marcos my regular user.", "If I *ls* the tmp directory I get: [marcos@namenode mrql]$ hadoop fs -ls /user/marcos/tmp/ Found 10 items -rw-r--r--   2 marcos users        213 2015-01-08 09:51 /user/marcos/tmp/marcos_data_source_dir.txt  Regards,  \u00c9tienne  On 7 January 2015 at 22:03, Leonidas Fegaras <fegaras@cse.uta.edu wrote:    Hi Etienne,  Both problems were fixed after MRQL 0.9.2 was released.", "You can get the  latest 0.9.4 src tarball using:  git clone https://git-wip-us.apache.org/repos/asf/incubator-mrql.git  mrql-0.9.4  The EndpointWriter are Spark errors during Spark shutdown; you may simply  ignore them."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [".", "java.io.FileNotFoundException: File does not exist: /user/marcos/tmp/hadoop_data_source_dir.txt  Where hadoop is my hadoop root user and marcos my regular user.", "If I *ls* the tmp directory I get: [marcos@namenode mrql]$ hadoop fs -ls /user/marcos/tmp/ Found 10 items -rw-r--r--   2 marcos users        213 2015-01-08 09:51 /user/marcos/tmp/marcos_data_source_dir.txt  Regards,  \u00c9tienne  On 7 January 2015 at 22:03, Leonidas Fegaras <fegaras@cse.uta.edu wrote:    Hi Etienne,  Both problems were fixed after MRQL 0.9.2 was released.", "You can get the  latest 0.9.4 src tarball using:  git clone https://git-wip-us.apache.org/repos/asf/incubator-mrql.git  mrql-0.9.4  The EndpointWriter are Spark errors during Spark shutdown; you may simply  ignore them.", "Best regards,  Leonidas     On 01/07/2015 06:53 AM, Etienne Dumoulin wrote:     Hi MRQL users,    I am using mrql 0.9.2 and spark 1.0.2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["java.io.FileNotFoundException: File does not exist: /user/marcos/tmp/hadoop_data_source_dir.txt  Where hadoop is my hadoop root user and marcos my regular user.", "If I *ls* the tmp directory I get: [marcos@namenode mrql]$ hadoop fs -ls /user/marcos/tmp/ Found 10 items -rw-r--r--   2 marcos users        213 2015-01-08 09:51 /user/marcos/tmp/marcos_data_source_dir.txt  Regards,  \u00c9tienne  On 7 January 2015 at 22:03, Leonidas Fegaras <fegaras@cse.uta.edu wrote:    Hi Etienne,  Both problems were fixed after MRQL 0.9.2 was released.", "You can get the  latest 0.9.4 src tarball using:  git clone https://git-wip-us.apache.org/repos/asf/incubator-mrql.git  mrql-0.9.4  The EndpointWriter are Spark errors during Spark shutdown; you may simply  ignore them.", "Best regards,  Leonidas     On 01/07/2015 06:53 AM, Etienne Dumoulin wrote:     Hi MRQL users,    I am using mrql 0.9.2 and spark 1.0.2.", "I have a little issue with the spark mode."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If I *ls* the tmp directory I get: [marcos@namenode mrql]$ hadoop fs -ls /user/marcos/tmp/ Found 10 items -rw-r--r--   2 marcos users        213 2015-01-08 09:51 /user/marcos/tmp/marcos_data_source_dir.txt  Regards,  \u00c9tienne  On 7 January 2015 at 22:03, Leonidas Fegaras <fegaras@cse.uta.edu wrote:    Hi Etienne,  Both problems were fixed after MRQL 0.9.2 was released.", "You can get the  latest 0.9.4 src tarball using:  git clone https://git-wip-us.apache.org/repos/asf/incubator-mrql.git  mrql-0.9.4  The EndpointWriter are Spark errors during Spark shutdown; you may simply  ignore them.", "Best regards,  Leonidas     On 01/07/2015 06:53 AM, Etienne Dumoulin wrote:     Hi MRQL users,    I am using mrql 0.9.2 and spark 1.0.2.", "I have a little issue with the spark mode.", "When I try to execute a small test job that is successful in MapReduce  mode I get an error."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Best regards,  Leonidas     On 01/07/2015 06:53 AM, Etienne Dumoulin wrote:     Hi MRQL users,    I am using mrql 0.9.2 and spark 1.0.2.", "I have a little issue with the spark mode.", "When I try to execute a small test job that is successful in MapReduce  mode I get an error.", "RMAT and pagerank examples throw a lot of warnings but I get the result at  the end.", "[hadoop@namenode ~]$  /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/RMAT.mrql 100 1000  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: ( int, int, int, int ) - ( int, int )  Query type: !bag(( int, int ))  Physical plan:  MapReduce:     input: Generator  Run time: 6.165 secs  15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode2:40321]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:40321] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:40321  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have a little issue with the spark mode.", "When I try to execute a small test job that is successful in MapReduce  mode I get an error.", "RMAT and pagerank examples throw a lot of warnings but I get the result at  the end.", "[hadoop@namenode ~]$  /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/RMAT.mrql 100 1000  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: ( int, int, int, int ) - ( int, int )  Query type: !bag(( int, int ))  Physical plan:  MapReduce:     input: Generator  Run time: 6.165 secs  15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode2:40321]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:40321] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:40321  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode3:39739]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:39739] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:39739  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When I try to execute a small test job that is successful in MapReduce  mode I get an error.", "RMAT and pagerank examples throw a lot of warnings but I get the result at  the end.", "[hadoop@namenode ~]$  /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/RMAT.mrql 100 1000  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: ( int, int, int, int ) - ( int, int )  Query type: !bag(( int, int ))  Physical plan:  MapReduce:     input: Generator  Run time: 6.165 secs  15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode2:40321]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:40321] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:40321  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode3:39739]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:39739] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:39739  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "/home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/pagerank.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: long  Physical plan:  Aggregate:     input: MapAggregateReduce:               input: Source (binary): \"tmp/graph.bin\"  Run time: 6.352 secs  Query type: string  Result:  \"*** number of nodes: 76\"  Run time: 0.055 secs  Query type: !list(< node: int, rank: double )  Physical plan:  MapReduce:     input: Repeat (x_55):               init: MapReduce:                        input: Source (binary): \"tmp/graph.bin\"               step: MapReduce:                        input: x_55  Repeat #1: 67 true results  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Lost TID 6 (task 5.0:0)  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  Repeat #2: 61 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 9 (task 9.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 10 (task 9.0:0)  Repeat #3: 41 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 13 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 14 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 15 (task 14.0:0)  Repeat #4: 6 true results  Repeat #5: 0 true results  Result:  [ < node: 0, rank: 0.07872101046749681 , < node: 6, rank:  0.05065660974066882 , < node: 3, rank: 0.0459596116701186 , < node: 25,  rank: 0.04376273127207268 , < node: 12, rank: 0.04188453600736429 , <  node: 1, rank: 0.04040564312366409 , < node: 50, rank:  0.033875663833967604 , < node: 31, rank: 0.023745510555659932 , < node:  28, rank: 0.022415986790041594 , < node: 9, rank: 0.021938982955425616 ,  < node: 75, rank: 0.021586163633336066 , < node: 18, rank:  0.02116068257012926 , < node: 15, rank: 0.02026443639558245 , < node: 62,  rank: 0.017767555008348448 , < node: 53, rank: 0.017410210069182946 , <  node: 7, rank: 0.017195867587074802 , < node: 56, rank:  0.01699938245303271 , < node: 10, rank: 0.01654553114240747 , < node: 26,  rank: 0.01650909559014266 , < node: 37, rank: 0.016466833143024252 , ... ]  Run time: 5.379 secs  15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode2:32794]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:32794] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:32794  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[hadoop@namenode ~]$  /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/RMAT.mrql 100 1000  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: ( int, int, int, int ) - ( int, int )  Query type: !bag(( int, int ))  Physical plan:  MapReduce:     input: Generator  Run time: 6.165 secs  15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode2:40321]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:40321] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:40321  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode3:39739]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:39739] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:39739  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "/home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/pagerank.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: long  Physical plan:  Aggregate:     input: MapAggregateReduce:               input: Source (binary): \"tmp/graph.bin\"  Run time: 6.352 secs  Query type: string  Result:  \"*** number of nodes: 76\"  Run time: 0.055 secs  Query type: !list(< node: int, rank: double )  Physical plan:  MapReduce:     input: Repeat (x_55):               init: MapReduce:                        input: Source (binary): \"tmp/graph.bin\"               step: MapReduce:                        input: x_55  Repeat #1: 67 true results  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Lost TID 6 (task 5.0:0)  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  Repeat #2: 61 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 9 (task 9.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 10 (task 9.0:0)  Repeat #3: 41 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 13 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 14 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 15 (task 14.0:0)  Repeat #4: 6 true results  Repeat #5: 0 true results  Result:  [ < node: 0, rank: 0.07872101046749681 , < node: 6, rank:  0.05065660974066882 , < node: 3, rank: 0.0459596116701186 , < node: 25,  rank: 0.04376273127207268 , < node: 12, rank: 0.04188453600736429 , <  node: 1, rank: 0.04040564312366409 , < node: 50, rank:  0.033875663833967604 , < node: 31, rank: 0.023745510555659932 , < node:  28, rank: 0.022415986790041594 , < node: 9, rank: 0.021938982955425616 ,  < node: 75, rank: 0.021586163633336066 , < node: 18, rank:  0.02116068257012926 , < node: 15, rank: 0.02026443639558245 , < node: 62,  rank: 0.017767555008348448 , < node: 53, rank: 0.017410210069182946 , <  node: 7, rank: 0.017195867587074802 , < node: 56, rank:  0.01699938245303271 , < node: 10, rank: 0.01654553114240747 , < node: 26,  rank: 0.01650909559014266 , < node: 37, rank: 0.016466833143024252 , ... ]  Run time: 5.379 secs  15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode2:32794]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:32794] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:32794  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode3:35978]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:35978] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:35978  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "However when I run a simple select script (that works in MapReduce mode):  ds =  source(line,'/user/marcos/hdfs_file2/Text1Comma',',',type(<city:string,country:string,pop:string));  SELECT (t.city, t.country) FROM t in ds;   /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark -dist -nodes 1  /tmp/script_etienne.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: !bag(( string, string ))  Physical plan:  cMap:     input: Source (line): \"/user/marcos/hdfs_file2/Text1Comma\"  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager:  *Loss was due to java.io.FileNotFoundException  java.io.FileNotFoundException: File does not exist:  /tmp/hadoop_data_source_dir.txt*      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)      at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)      at  org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)      at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)      at  org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)      at  org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)      at org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 2 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 3 (task 0.0:0)  15/01/07 12:32:48 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 4  times; aborting job  *** MRQL System Error at line 8: java.lang.Error:  org.apache.spark.SparkException: Job aborted due to stage failure: Task  0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on  host datanode3: java.io.FileNotFoundException:* File does not exist:  /tmp/hadoop_data_source_dir.txt*   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)          org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)   org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)          org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)   org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)   org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)          org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)          org.apache.spark.rdd.RDD.iterator(RDD.scala:227)          org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)          org.apache.spark.scheduler.Task.run(Task.scala:51)   org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)   java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)          java.lang.Thread.run(Thread.java:662)  Driver stacktrace:  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode2:47194]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:47194] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:47194  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode3:39739]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:39739] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:39739  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "/home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/pagerank.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: long  Physical plan:  Aggregate:     input: MapAggregateReduce:               input: Source (binary): \"tmp/graph.bin\"  Run time: 6.352 secs  Query type: string  Result:  \"*** number of nodes: 76\"  Run time: 0.055 secs  Query type: !list(< node: int, rank: double )  Physical plan:  MapReduce:     input: Repeat (x_55):               init: MapReduce:                        input: Source (binary): \"tmp/graph.bin\"               step: MapReduce:                        input: x_55  Repeat #1: 67 true results  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Lost TID 6 (task 5.0:0)  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  Repeat #2: 61 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 9 (task 9.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 10 (task 9.0:0)  Repeat #3: 41 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 13 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 14 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 15 (task 14.0:0)  Repeat #4: 6 true results  Repeat #5: 0 true results  Result:  [ < node: 0, rank: 0.07872101046749681 , < node: 6, rank:  0.05065660974066882 , < node: 3, rank: 0.0459596116701186 , < node: 25,  rank: 0.04376273127207268 , < node: 12, rank: 0.04188453600736429 , <  node: 1, rank: 0.04040564312366409 , < node: 50, rank:  0.033875663833967604 , < node: 31, rank: 0.023745510555659932 , < node:  28, rank: 0.022415986790041594 , < node: 9, rank: 0.021938982955425616 ,  < node: 75, rank: 0.021586163633336066 , < node: 18, rank:  0.02116068257012926 , < node: 15, rank: 0.02026443639558245 , < node: 62,  rank: 0.017767555008348448 , < node: 53, rank: 0.017410210069182946 , <  node: 7, rank: 0.017195867587074802 , < node: 56, rank:  0.01699938245303271 , < node: 10, rank: 0.01654553114240747 , < node: 26,  rank: 0.01650909559014266 , < node: 37, rank: 0.016466833143024252 , ... ]  Run time: 5.379 secs  15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode2:32794]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:32794] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:32794  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode3:35978]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:35978] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:35978  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "However when I run a simple select script (that works in MapReduce mode):  ds =  source(line,'/user/marcos/hdfs_file2/Text1Comma',',',type(<city:string,country:string,pop:string));  SELECT (t.city, t.country) FROM t in ds;   /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark -dist -nodes 1  /tmp/script_etienne.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: !bag(( string, string ))  Physical plan:  cMap:     input: Source (line): \"/user/marcos/hdfs_file2/Text1Comma\"  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager:  *Loss was due to java.io.FileNotFoundException  java.io.FileNotFoundException: File does not exist:  /tmp/hadoop_data_source_dir.txt*      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)      at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)      at  org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)      at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)      at  org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)      at  org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)      at org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 2 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 3 (task 0.0:0)  15/01/07 12:32:48 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 4  times; aborting job  *** MRQL System Error at line 8: java.lang.Error:  org.apache.spark.SparkException: Job aborted due to stage failure: Task  0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on  host datanode3: java.io.FileNotFoundException:* File does not exist:  /tmp/hadoop_data_source_dir.txt*   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)          org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)   org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)          org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)   org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)   org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)          org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)          org.apache.spark.rdd.RDD.iterator(RDD.scala:227)          org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)          org.apache.spark.scheduler.Task.run(Task.scala:51)   org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)   java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)          java.lang.Thread.run(Thread.java:662)  Driver stacktrace:  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode2:47194]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:47194] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:47194  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode3:60893]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:60893] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:60893  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/pagerank.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: long  Physical plan:  Aggregate:     input: MapAggregateReduce:               input: Source (binary): \"tmp/graph.bin\"  Run time: 6.352 secs  Query type: string  Result:  \"*** number of nodes: 76\"  Run time: 0.055 secs  Query type: !list(< node: int, rank: double )  Physical plan:  MapReduce:     input: Repeat (x_55):               init: MapReduce:                        input: Source (binary): \"tmp/graph.bin\"               step: MapReduce:                        input: x_55  Repeat #1: 67 true results  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Lost TID 6 (task 5.0:0)  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  Repeat #2: 61 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 9 (task 9.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 10 (task 9.0:0)  Repeat #3: 41 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 13 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 14 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 15 (task 14.0:0)  Repeat #4: 6 true results  Repeat #5: 0 true results  Result:  [ < node: 0, rank: 0.07872101046749681 , < node: 6, rank:  0.05065660974066882 , < node: 3, rank: 0.0459596116701186 , < node: 25,  rank: 0.04376273127207268 , < node: 12, rank: 0.04188453600736429 , <  node: 1, rank: 0.04040564312366409 , < node: 50, rank:  0.033875663833967604 , < node: 31, rank: 0.023745510555659932 , < node:  28, rank: 0.022415986790041594 , < node: 9, rank: 0.021938982955425616 ,  < node: 75, rank: 0.021586163633336066 , < node: 18, rank:  0.02116068257012926 , < node: 15, rank: 0.02026443639558245 , < node: 62,  rank: 0.017767555008348448 , < node: 53, rank: 0.017410210069182946 , <  node: 7, rank: 0.017195867587074802 , < node: 56, rank:  0.01699938245303271 , < node: 10, rank: 0.01654553114240747 , < node: 26,  rank: 0.01650909559014266 , < node: 37, rank: 0.016466833143024252 , ... ]  Run time: 5.379 secs  15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode2:32794]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:32794] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:32794  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode3:35978]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:35978] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:35978  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "However when I run a simple select script (that works in MapReduce mode):  ds =  source(line,'/user/marcos/hdfs_file2/Text1Comma',',',type(<city:string,country:string,pop:string));  SELECT (t.city, t.country) FROM t in ds;   /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark -dist -nodes 1  /tmp/script_etienne.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: !bag(( string, string ))  Physical plan:  cMap:     input: Source (line): \"/user/marcos/hdfs_file2/Text1Comma\"  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager:  *Loss was due to java.io.FileNotFoundException  java.io.FileNotFoundException: File does not exist:  /tmp/hadoop_data_source_dir.txt*      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)      at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)      at  org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)      at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)      at  org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)      at  org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)      at org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 2 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 3 (task 0.0:0)  15/01/07 12:32:48 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 4  times; aborting job  *** MRQL System Error at line 8: java.lang.Error:  org.apache.spark.SparkException: Job aborted due to stage failure: Task  0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on  host datanode3: java.io.FileNotFoundException:* File does not exist:  /tmp/hadoop_data_source_dir.txt*   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)          org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)   org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)          org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)   org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)   org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)          org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)          org.apache.spark.rdd.RDD.iterator(RDD.scala:227)          org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)          org.apache.spark.scheduler.Task.run(Task.scala:51)   org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)   java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)          java.lang.Thread.run(Thread.java:662)  Driver stacktrace:  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode2:47194]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:47194] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:47194  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode3:60893]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:60893] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:60893  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]     The spark installation seems fine as I can use the spark shell and  Zeppelin is working."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode3:35978]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:35978] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:35978  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "However when I run a simple select script (that works in MapReduce mode):  ds =  source(line,'/user/marcos/hdfs_file2/Text1Comma',',',type(<city:string,country:string,pop:string));  SELECT (t.city, t.country) FROM t in ds;   /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark -dist -nodes 1  /tmp/script_etienne.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: !bag(( string, string ))  Physical plan:  cMap:     input: Source (line): \"/user/marcos/hdfs_file2/Text1Comma\"  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager:  *Loss was due to java.io.FileNotFoundException  java.io.FileNotFoundException: File does not exist:  /tmp/hadoop_data_source_dir.txt*      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)      at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)      at  org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)      at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)      at  org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)      at  org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)      at org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 2 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 3 (task 0.0:0)  15/01/07 12:32:48 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 4  times; aborting job  *** MRQL System Error at line 8: java.lang.Error:  org.apache.spark.SparkException: Job aborted due to stage failure: Task  0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on  host datanode3: java.io.FileNotFoundException:* File does not exist:  /tmp/hadoop_data_source_dir.txt*   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)          org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)   org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)          org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)   org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)   org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)          org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)          org.apache.spark.rdd.RDD.iterator(RDD.scala:227)          org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)          org.apache.spark.scheduler.Task.run(Task.scala:51)   org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)   java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)          java.lang.Thread.run(Thread.java:662)  Driver stacktrace:  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode2:47194]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:47194] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:47194  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode3:60893]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:60893] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:60893  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]     The spark installation seems fine as I can use the spark shell and  Zeppelin is working.", "There are no useful spark logs and I am not sure where to start the  troubleshooting."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However when I run a simple select script (that works in MapReduce mode):  ds =  source(line,'/user/marcos/hdfs_file2/Text1Comma',',',type(<city:string,country:string,pop:string));  SELECT (t.city, t.country) FROM t in ds;   /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark -dist -nodes 1  /tmp/script_etienne.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: !bag(( string, string ))  Physical plan:  cMap:     input: Source (line): \"/user/marcos/hdfs_file2/Text1Comma\"  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager:  *Loss was due to java.io.FileNotFoundException  java.io.FileNotFoundException: File does not exist:  /tmp/hadoop_data_source_dir.txt*      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)      at  org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)      at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)      at  org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)      at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)      at  org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)      at  org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)      at org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)      at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)      at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 2 (task 0.0:0)  15/01/07 12:32:48 WARN scheduler.TaskSetManager: Lost TID 3 (task 0.0:0)  15/01/07 12:32:48 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 4  times; aborting job  *** MRQL System Error at line 8: java.lang.Error:  org.apache.spark.SparkException: Job aborted due to stage failure: Task  0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on  host datanode3: java.io.FileNotFoundException:* File does not exist:  /tmp/hadoop_data_source_dir.txt*   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1843)   org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init(DFSClient.java:1834)          org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)   org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)          org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)   org.apache.mrql.SparkEvaluator.load_source_dir(SparkEvaluator.java:155)   org.apache.mrql.SparkParsedInputFormat.getRecordReader(SparkParsedInputFormat.java:92)          org.apache.spark.rdd.HadoopRDD$$anon$1.<init(HadoopRDD.scala:193)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)          org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.rdd.RDD.iterator(RDD.scala:229)          org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)          org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)          org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)          org.apache.spark.rdd.RDD.iterator(RDD.scala:227)          org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)          org.apache.spark.scheduler.Task.run(Task.scala:51)   org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)   java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)          java.lang.Thread.run(Thread.java:662)  Driver stacktrace:  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode2:47194]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:47194] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:47194  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] <- [akka.tcp://sparkExecutor@datanode3:60893]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:60893] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:60893  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode2:56494]:  Error [Association failed with [akka.tcp://spark@datanode2:56494]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode2:56494]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode2/192.168.11.90:56494  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]  15/01/07 12:32:48 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43495] - [akka.tcp://spark@datanode3:42480]:  Error [Association failed with [akka.tcp://spark@datanode3:42480]] [  akka.remote.EndpointAssociationException: Association failed with  [akka.tcp://spark@datanode3:42480]  Caused by:  akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2:  Connection refused: datanode3/192.168.11.91:42480  ]     The spark installation seems fine as I can use the spark shell and  Zeppelin is working.", "There are no useful spark logs and I am not sure where to start the  troubleshooting.", "Thanks for your help,   \u00c9tienne   --   \u00c9tienne Dumoulin  Head of product development   Idiro Technologies  Clarendon House,  34-37 Clarendon St,  Dublin 2  Ireland      --  \u00c9tienne Dumoulin Head of product development  Idiro Technologies Clarendon House, 34-37 Clarendon St, Dublin 2 Ireland Email: etienne.dumoulin@idiro.com  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [" Thanks lot Adam!", "Also for the halign in tr:panelButtonBar.", "I owe you a beer for that.", ":)  While the tr:panelButtonBar's halign is doing great, I got a problem with the tr:table's actions facet: Since the navigation bar is only displayed if there are more then \"rows\" entries to display, my action-facet contents disappear if there are to few rows in the table.", "I would generally prefer to have the navigaton bar always displayed when the rows attribute is  0, independent of the current count of rows to display."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also for the halign in tr:panelButtonBar.", "I owe you a beer for that.", ":)  While the tr:panelButtonBar's halign is doing great, I got a problem with the tr:table's actions facet: Since the navigation bar is only displayed if there are more then \"rows\" entries to display, my action-facet contents disappear if there are to few rows in the table.", "I would generally prefer to have the navigaton bar always displayed when the rows attribute is  0, independent of the current count of rows to display.", "Navigation controls could be disabled if the number of rows in the model is less then the \"rows\" attribute, but IMHO they should be displayed to have the page optically more \"stable\" for the user."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I owe you a beer for that.", ":)  While the tr:panelButtonBar's halign is doing great, I got a problem with the tr:table's actions facet: Since the navigation bar is only displayed if there are more then \"rows\" entries to display, my action-facet contents disappear if there are to few rows in the table.", "I would generally prefer to have the navigaton bar always displayed when the rows attribute is  0, independent of the current count of rows to display.", "Navigation controls could be disabled if the number of rows in the model is less then the \"rows\" attribute, but IMHO they should be displayed to have the page optically more \"stable\" for the user.", "Regards, Thorsten  -----Urspr\u00fcngliche Nachricht----- Von: Adam Winer [mailto:awiner@gmail.com]  Gesendet: Donnerstag, 12."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":)  While the tr:panelButtonBar's halign is doing great, I got a problem with the tr:table's actions facet: Since the navigation bar is only displayed if there are more then \"rows\" entries to display, my action-facet contents disappear if there are to few rows in the table.", "I would generally prefer to have the navigaton bar always displayed when the rows attribute is  0, independent of the current count of rows to display.", "Navigation controls could be disabled if the number of rows in the model is less then the \"rows\" attribute, but IMHO they should be displayed to have the page optically more \"stable\" for the user.", "Regards, Thorsten  -----Urspr\u00fcngliche Nachricht----- Von: Adam Winer [mailto:awiner@gmail.com]  Gesendet: Donnerstag, 12.", "Oktober 2006 19:30 An: adffaces-user@incubator.apache.org Betreff: Re: Re: Re: decorateCollection component  It just did."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would generally prefer to have the navigaton bar always displayed when the rows attribute is  0, independent of the current count of rows to display.", "Navigation controls could be disabled if the number of rows in the model is less then the \"rows\" attribute, but IMHO they should be displayed to have the page optically more \"stable\" for the user.", "Regards, Thorsten  -----Urspr\u00fcngliche Nachricht----- Von: Adam Winer [mailto:awiner@gmail.com]  Gesendet: Donnerstag, 12.", "Oktober 2006 19:30 An: adffaces-user@incubator.apache.org Betreff: Re: Re: Re: decorateCollection component  It just did.", ":)  SVN revision 463333."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Navigation controls could be disabled if the number of rows in the model is less then the \"rows\" attribute, but IMHO they should be displayed to have the page optically more \"stable\" for the user.", "Regards, Thorsten  -----Urspr\u00fcngliche Nachricht----- Von: Adam Winer [mailto:awiner@gmail.com]  Gesendet: Donnerstag, 12.", "Oktober 2006 19:30 An: adffaces-user@incubator.apache.org Betreff: Re: Re: Re: decorateCollection component  It just did.", ":)  SVN revision 463333.", "Hrm, it should be at http://incubator.apache.org/adffaces/, but that site is out-of-date, *and* the Javadoc doesn't work..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards, Thorsten  -----Urspr\u00fcngliche Nachricht----- Von: Adam Winer [mailto:awiner@gmail.com]  Gesendet: Donnerstag, 12.", "Oktober 2006 19:30 An: adffaces-user@incubator.apache.org Betreff: Re: Re: Re: decorateCollection component  It just did.", ":)  SVN revision 463333.", "Hrm, it should be at http://incubator.apache.org/adffaces/, but that site is out-of-date, *and* the Javadoc doesn't work...", "I'll ask Matthias what's up with that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Oktober 2006 19:30 An: adffaces-user@incubator.apache.org Betreff: Re: Re: Re: decorateCollection component  It just did.", ":)  SVN revision 463333.", "Hrm, it should be at http://incubator.apache.org/adffaces/, but that site is out-of-date, *and* the Javadoc doesn't work...", "I'll ask Matthias what's up with that.", "-- Adam   On 10/12/06, Brian Smith <unobriani@gmail.com wrote:  Cool thanks Adam, is the new \"actions\" facet going to make it into the Trunk  build?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":)  SVN revision 463333.", "Hrm, it should be at http://incubator.apache.org/adffaces/, but that site is out-of-date, *and* the Javadoc doesn't work...", "I'll ask Matthias what's up with that.", "-- Adam   On 10/12/06, Brian Smith <unobriani@gmail.com wrote:  Cool thanks Adam, is the new \"actions\" facet going to make it into the Trunk  build?", "Also on a side note, do you know where a javadoc for trinidad can be  found (if there is one)?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hrm, it should be at http://incubator.apache.org/adffaces/, but that site is out-of-date, *and* the Javadoc doesn't work...", "I'll ask Matthias what's up with that.", "-- Adam   On 10/12/06, Brian Smith <unobriani@gmail.com wrote:  Cool thanks Adam, is the new \"actions\" facet going to make it into the Trunk  build?", "Also on a side note, do you know where a javadoc for trinidad can be  found (if there is one)?", "Thanks,  -Brian   On 10/12/06, Adam Winer <awiner@gmail.com wrote:     I'm coding this up now as an \"actions\" facet on the table."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'll ask Matthias what's up with that.", "-- Adam   On 10/12/06, Brian Smith <unobriani@gmail.com wrote:  Cool thanks Adam, is the new \"actions\" facet going to make it into the Trunk  build?", "Also on a side note, do you know where a javadoc for trinidad can be  found (if there is one)?", "Thanks,  -Brian   On 10/12/06, Adam Winer <awiner@gmail.com wrote:     I'm coding this up now as an \"actions\" facet on the table.", "We had this   back in ADF Faces, then got rid of it - I forget why - but it's   definitely useful here."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam   On 10/12/06, Brian Smith <unobriani@gmail.com wrote:  Cool thanks Adam, is the new \"actions\" facet going to make it into the Trunk  build?", "Also on a side note, do you know where a javadoc for trinidad can be  found (if there is one)?", "Thanks,  -Brian   On 10/12/06, Adam Winer <awiner@gmail.com wrote:     I'm coding this up now as an \"actions\" facet on the table.", "We had this   back in ADF Faces, then got rid of it - I forget why - but it's   definitely useful here.", "It'll be in there shortly."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also on a side note, do you know where a javadoc for trinidad can be  found (if there is one)?", "Thanks,  -Brian   On 10/12/06, Adam Winer <awiner@gmail.com wrote:     I'm coding this up now as an \"actions\" facet on the table.", "We had this   back in ADF Faces, then got rid of it - I forget why - but it's   definitely useful here.", "It'll be in there shortly.", "-- Adam       On 10/12/06, Brian Smith <unobriani@gmail.com wrote:    Thanks Adam, I am working with a build from the trunk, (built   yesterday)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  -Brian   On 10/12/06, Adam Winer <awiner@gmail.com wrote:     I'm coding this up now as an \"actions\" facet on the table.", "We had this   back in ADF Faces, then got rid of it - I forget why - but it's   definitely useful here.", "It'll be in there shortly.", "-- Adam       On 10/12/06, Brian Smith <unobriani@gmail.com wrote:    Thanks Adam, I am working with a build from the trunk, (built   yesterday).", "Do you know when/if this component is going to make it in the   build?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We had this   back in ADF Faces, then got rid of it - I forget why - but it's   definitely useful here.", "It'll be in there shortly.", "-- Adam       On 10/12/06, Brian Smith <unobriani@gmail.com wrote:    Thanks Adam, I am working with a build from the trunk, (built   yesterday).", "Do you know when/if this component is going to make it in the   build?", "Rough    guess?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It'll be in there shortly.", "-- Adam       On 10/12/06, Brian Smith <unobriani@gmail.com wrote:    Thanks Adam, I am working with a build from the trunk, (built   yesterday).", "Do you know when/if this component is going to make it in the   build?", "Rough    guess?", "Is there an alternative for now?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam       On 10/12/06, Brian Smith <unobriani@gmail.com wrote:    Thanks Adam, I am working with a build from the trunk, (built   yesterday).", "Do you know when/if this component is going to make it in the   build?", "Rough    guess?", "Is there an alternative for now?", "I guess I could just use the   table    header facet but it is not quite what I am wanting."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do you know when/if this component is going to make it in the   build?", "Rough    guess?", "Is there an alternative for now?", "I guess I could just use the   table    header facet but it is not quite what I am wanting.", "I am placing a    selectOneChoice box on the table to select the number of rows visble."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Rough    guess?", "Is there an alternative for now?", "I guess I could just use the   table    header facet but it is not quite what I am wanting.", "I am placing a    selectOneChoice box on the table to select the number of rows visble.", "I    would like it inline with the range navigation stuff."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there an alternative for now?", "I guess I could just use the   table    header facet but it is not quite what I am wanting.", "I am placing a    selectOneChoice box on the table to select the number of rows visble.", "I    would like it inline with the range navigation stuff.", "Thanks,    -Brian       On 10/12/06, Adam Winer <awiner@gmail.com wrote:         Brian,         No, you're not missing anything."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am placing a    selectOneChoice box on the table to select the number of rows visble.", "I    would like it inline with the range navigation stuff.", "Thanks,    -Brian       On 10/12/06, Adam Winer <awiner@gmail.com wrote:         Brian,         No, you're not missing anything.", "It's missing, and needs to be     added;  it looks like it just didn't make it in the open-source drop.", "-- Adam             On 10/11/06, Brian Smith <unobriani@gmail.com wrote:      I am trying to convert some adf faces pages over to trinidad."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I    would like it inline with the range navigation stuff.", "Thanks,    -Brian       On 10/12/06, Adam Winer <awiner@gmail.com wrote:         Brian,         No, you're not missing anything.", "It's missing, and needs to be     added;  it looks like it just didn't make it in the open-source drop.", "-- Adam             On 10/11/06, Brian Smith <unobriani@gmail.com wrote:      I am trying to convert some adf faces pages over to trinidad.", "I was     using      the \"actions\" facet of the af:table."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,    -Brian       On 10/12/06, Adam Winer <awiner@gmail.com wrote:         Brian,         No, you're not missing anything.", "It's missing, and needs to be     added;  it looks like it just didn't make it in the open-source drop.", "-- Adam             On 10/11/06, Brian Smith <unobriani@gmail.com wrote:      I am trying to convert some adf faces pages over to trinidad.", "I was     using      the \"actions\" facet of the af:table.", "In the \"API Changes from   Oracle's     ADF      Faces\" doc here   http://incubator.apache.org/adffaces/api-changes.html       It      says to use the \"toolbar\" facet of the decorateCollection   component."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It's missing, and needs to be     added;  it looks like it just didn't make it in the open-source drop.", "-- Adam             On 10/11/06, Brian Smith <unobriani@gmail.com wrote:      I am trying to convert some adf faces pages over to trinidad.", "I was     using      the \"actions\" facet of the af:table.", "In the \"API Changes from   Oracle's     ADF      Faces\" doc here   http://incubator.apache.org/adffaces/api-changes.html       It      says to use the \"toolbar\" facet of the decorateCollection   component.", "I      can't seem to find this component is the latest build."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam             On 10/11/06, Brian Smith <unobriani@gmail.com wrote:      I am trying to convert some adf faces pages over to trinidad.", "I was     using      the \"actions\" facet of the af:table.", "In the \"API Changes from   Oracle's     ADF      Faces\" doc here   http://incubator.apache.org/adffaces/api-changes.html       It      says to use the \"toolbar\" facet of the decorateCollection   component.", "I      can't seem to find this component is the latest build.", "Has it been     renamed      or am I missing some other way to do what I am wanting?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I was     using      the \"actions\" facet of the af:table.", "In the \"API Changes from   Oracle's     ADF      Faces\" doc here   http://incubator.apache.org/adffaces/api-changes.html       It      says to use the \"toolbar\" facet of the decorateCollection   component.", "I      can't seem to find this component is the latest build.", "Has it been     renamed      or am I missing some other way to do what I am wanting?", "Thanks,      Brian                            "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Oops, I meant what is tempParams['extend']?", "Is it a JS function?", "If so, how did it get into tempParams?", "-- Adam   On 2/15/07, Adam Winer <awiner@gmail.com wrote:  What is tempParams['source'] here?", "A DOM node, etc.?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is it a JS function?", "If so, how did it get into tempParams?", "-- Adam   On 2/15/07, Adam Winer <awiner@gmail.com wrote:  What is tempParams['source'] here?", "A DOM node, etc.?", "-- Adam    On 2/15/07, Christopher Cudennec <SmutjeJim@gmx.net wrote:   Hi..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam   On 2/15/07, Adam Winer <awiner@gmail.com wrote:  What is tempParams['source'] here?", "A DOM node, etc.?", "-- Adam    On 2/15/07, Christopher Cudennec <SmutjeJim@gmx.net wrote:   Hi...", "I'm getting closer to integrating trinidad :).", "My next problem (also already posted some weeks ago) is a JS error that   only occurs in Firefox."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A DOM node, etc.?", "-- Adam    On 2/15/07, Christopher Cudennec <SmutjeJim@gmx.net wrote:   Hi...", "I'm getting closer to integrating trinidad :).", "My next problem (also already posted some weeks ago) is a JS error that   only occurs in Firefox.", "IE does not have any problems."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam    On 2/15/07, Christopher Cudennec <SmutjeJim@gmx.net wrote:   Hi...", "I'm getting closer to integrating trinidad :).", "My next problem (also already posted some weeks ago) is a JS error that   only occurs in Firefox.", "IE does not have any problems.", "I get this   exception after submitting the very first page of my app (login form):     Fehler: uncaught exception: [Exception... \"Node was not found\"  code:   \"8\" nsresult: \"0x80530008 (NS_ERROR_DOM_NOT_FOUND_ERR)\"  location:   \"http://localhost:8080/op/adf/jsLibs/Common11-m7.js Line: 4106\"]     Debugging revealed that tempParams contains to values: \"source\" (the   login button) and \"extend\" (a function ?!)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm getting closer to integrating trinidad :).", "My next problem (also already posted some weeks ago) is a JS error that   only occurs in Firefox.", "IE does not have any problems.", "I get this   exception after submitting the very first page of my app (login form):     Fehler: uncaught exception: [Exception... \"Node was not found\"  code:   \"8\" nsresult: \"0x80530008 (NS_ERROR_DOM_NOT_FOUND_ERR)\"  location:   \"http://localhost:8080/op/adf/jsLibs/Common11-m7.js Line: 4106\"]     Debugging revealed that tempParams contains to values: \"source\" (the   login button) and \"extend\" (a function ?!).", "Trying to remove the second   value causes the exception."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My next problem (also already posted some weeks ago) is a JS error that   only occurs in Firefox.", "IE does not have any problems.", "I get this   exception after submitting the very first page of my app (login form):     Fehler: uncaught exception: [Exception... \"Node was not found\"  code:   \"8\" nsresult: \"0x80530008 (NS_ERROR_DOM_NOT_FOUND_ERR)\"  location:   \"http://localhost:8080/op/adf/jsLibs/Common11-m7.js Line: 4106\"]     Debugging revealed that tempParams contains to values: \"source\" (the   login button) and \"extend\" (a function ?!).", "Trying to remove the second   value causes the exception.", "4103 if(isDOM)   4104 {   4105 for(var paramName in tempParams)   4106 form.removeChild(tempParams[paramName]);   4107 }   4108 }       Sounds familiar or any suggestions?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["IE does not have any problems.", "I get this   exception after submitting the very first page of my app (login form):     Fehler: uncaught exception: [Exception... \"Node was not found\"  code:   \"8\" nsresult: \"0x80530008 (NS_ERROR_DOM_NOT_FOUND_ERR)\"  location:   \"http://localhost:8080/op/adf/jsLibs/Common11-m7.js Line: 4106\"]     Debugging revealed that tempParams contains to values: \"source\" (the   login button) and \"extend\" (a function ?!).", "Trying to remove the second   value causes the exception.", "4103 if(isDOM)   4104 {   4105 for(var paramName in tempParams)   4106 form.removeChild(tempParams[paramName]);   4107 }   4108 }       Sounds familiar or any suggestions?", "Christopher     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That's all very viable indeed.", "1.", "TOSCA is an OASIS standard specification for cloud orchestration, consisting of a rich object-oriented modeling grammar as well as a recommended Simple Profile of base types.", "2.", "ARIA is a straightforward implementation of a TOSCA parser and orchestrator, currently an Apache incubator project."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["1.", "TOSCA is an OASIS standard specification for cloud orchestration, consisting of a rich object-oriented modeling grammar as well as a recommended Simple Profile of base types.", "2.", "ARIA is a straightforward implementation of a TOSCA parser and orchestrator, currently an Apache incubator project.", "3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["TOSCA is an OASIS standard specification for cloud orchestration, consisting of a rich object-oriented modeling grammar as well as a recommended Simple Profile of base types.", "2.", "ARIA is a straightforward implementation of a TOSCA parser and orchestrator, currently an Apache incubator project.", "3.", "Cloudify is a mature and feature-rich cloud orchestrator (Apache-licensed, currently at version 4.1) that uses a TOSCA-inspired language, but not real TOSCA."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["ARIA is a straightforward implementation of a TOSCA parser and orchestrator, currently an Apache incubator project.", "3.", "Cloudify is a mature and feature-rich cloud orchestrator (Apache-licensed, currently at version 4.1) that uses a TOSCA-inspired language, but not real TOSCA.", "Because ARIA is still quite new and missing supporting plugins for various technologies (Docker, Openstack, AWS, Puppet, Chef, Juju, etc.)", "we have created an adapter layer that help us use Cloudify plugins in ARIA."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Because ARIA is still quite new and missing supporting plugins for various technologies (Docker, Openstack, AWS, Puppet, Chef, Juju, etc.)", "we have created an adapter layer that help us use Cloudify plugins in ARIA.", "We consider this a temporary measure and intend to re-implement all these plugins natively for ARIA, as extensions in the repository.", "We're still building up our documentation on the wiki, as well as our list of examples.", "We currently have a Hello World for Openstack, but not one for Docker yet."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We consider this a temporary measure and intend to re-implement all these plugins natively for ARIA, as extensions in the repository.", "We're still building up our documentation on the wiki, as well as our list of examples.", "We currently have a Hello World for Openstack, but not one for Docker yet.", "Have you tried the Openstack example?", "https://cwiki.apache.org/confluence/display/ARIATOSCA/OpenStack+Hello+World  On Fri, Aug 4, 2017 at 4:17 AM, chbndrhnns <code+apache@rueschel.de wrote:   Hi there,   I am just about to start digging into TOSCA, ARIA and Cloudify and stuff  and I am still a bit confused about it all works together (or not)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We're still building up our documentation on the wiki, as well as our list of examples.", "We currently have a Hello World for Openstack, but not one for Docker yet.", "Have you tried the Openstack example?", "https://cwiki.apache.org/confluence/display/ARIATOSCA/OpenStack+Hello+World  On Fri, Aug 4, 2017 at 4:17 AM, chbndrhnns <code+apache@rueschel.de wrote:   Hi there,   I am just about to start digging into TOSCA, ARIA and Cloudify and stuff  and I am still a bit confused about it all works together (or not).", "I can get the `hello-world` example to run locally and now I would like to  connect to a Docker instance, pull an image and run workflows either via  ssh on that container or via the Docker plugin."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We currently have a Hello World for Openstack, but not one for Docker yet.", "Have you tried the Openstack example?", "https://cwiki.apache.org/confluence/display/ARIATOSCA/OpenStack+Hello+World  On Fri, Aug 4, 2017 at 4:17 AM, chbndrhnns <code+apache@rueschel.de wrote:   Hi there,   I am just about to start digging into TOSCA, ARIA and Cloudify and stuff  and I am still a bit confused about it all works together (or not).", "I can get the `hello-world` example to run locally and now I would like to  connect to a Docker instance, pull an image and run workflows either via  ssh on that container or via the Docker plugin.", "Is that a viable use case and can someone provide me with a hello world,  as well?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Have you tried the Openstack example?", "https://cwiki.apache.org/confluence/display/ARIATOSCA/OpenStack+Hello+World  On Fri, Aug 4, 2017 at 4:17 AM, chbndrhnns <code+apache@rueschel.de wrote:   Hi there,   I am just about to start digging into TOSCA, ARIA and Cloudify and stuff  and I am still a bit confused about it all works together (or not).", "I can get the `hello-world` example to run locally and now I would like to  connect to a Docker instance, pull an image and run workflows either via  ssh on that container or via the Docker plugin.", "Is that a viable use case and can someone provide me with a hello world,  as well?", "Bye,  Jo   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*  I hope this will be of interest to people using Taverna in the drug  discovery arena.", "You may be interested to hear that we are organising a session on  workflow tools in the Division of Chemical Information (CINF) at the  American Chemical Society Fall 2015 National Meeting in Boston,  Massachusetts.", "The title of the symposium is \u201cWorkflow Tools & Data  Pipelining in Drug Discovery\u201d and will comprise a series oral  presentations.", "The meeting takes place on August 16-20.", "Full details of  the program can be found here:  http://www.acs.org/content/acs/en/meetings/abstract-submissions/acsnm250/division-of-chemical-information.html   We would are keen to have an exciting set of talks for this session, and  hope you might be interested in participating."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You may be interested to hear that we are organising a session on  workflow tools in the Division of Chemical Information (CINF) at the  American Chemical Society Fall 2015 National Meeting in Boston,  Massachusetts.", "The title of the symposium is \u201cWorkflow Tools & Data  Pipelining in Drug Discovery\u201d and will comprise a series oral  presentations.", "The meeting takes place on August 16-20.", "Full details of  the program can be found here:  http://www.acs.org/content/acs/en/meetings/abstract-submissions/acsnm250/division-of-chemical-information.html   We would are keen to have an exciting set of talks for this session, and  hope you might be interested in participating.", "To do so we ask you need  to submit an abstract by March 12."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The title of the symposium is \u201cWorkflow Tools & Data  Pipelining in Drug Discovery\u201d and will comprise a series oral  presentations.", "The meeting takes place on August 16-20.", "Full details of  the program can be found here:  http://www.acs.org/content/acs/en/meetings/abstract-submissions/acsnm250/division-of-chemical-information.html   We would are keen to have an exciting set of talks for this session, and  hope you might be interested in participating.", "To do so we ask you need  to submit an abstract by March 12.", "We are expecting the session to cover a number of aspects of workflow  tools, such as    *      the types of problem being solved    *      the approaches used    *      the technical solution chosen    *      the hurdles that had to be overcome    *      scalability and performance aspects    *      issues with deploying the solution to others    *      the balance between manual and automated approaches   Please also note that there is a separate lightning session that will  allow products and solutions in this area to be presented."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The meeting takes place on August 16-20.", "Full details of  the program can be found here:  http://www.acs.org/content/acs/en/meetings/abstract-submissions/acsnm250/division-of-chemical-information.html   We would are keen to have an exciting set of talks for this session, and  hope you might be interested in participating.", "To do so we ask you need  to submit an abstract by March 12.", "We are expecting the session to cover a number of aspects of workflow  tools, such as    *      the types of problem being solved    *      the approaches used    *      the technical solution chosen    *      the hurdles that had to be overcome    *      scalability and performance aspects    *      issues with deploying the solution to others    *      the balance between manual and automated approaches   Please also note that there is a separate lightning session that will  allow products and solutions in this area to be presented.", "You might  well want to participate in both sessions."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Full details of  the program can be found here:  http://www.acs.org/content/acs/en/meetings/abstract-submissions/acsnm250/division-of-chemical-information.html   We would are keen to have an exciting set of talks for this session, and  hope you might be interested in participating.", "To do so we ask you need  to submit an abstract by March 12.", "We are expecting the session to cover a number of aspects of workflow  tools, such as    *      the types of problem being solved    *      the approaches used    *      the technical solution chosen    *      the hurdles that had to be overcome    *      scalability and performance aspects    *      issues with deploying the solution to others    *      the balance between manual and automated approaches   Please also note that there is a separate lightning session that will  allow products and solutions in this area to be presented.", "You might  well want to participate in both sessions.", "Finally, if you think this might be of interest to others, then please  pass the message on."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["To do so we ask you need  to submit an abstract by March 12.", "We are expecting the session to cover a number of aspects of workflow  tools, such as    *      the types of problem being solved    *      the approaches used    *      the technical solution chosen    *      the hurdles that had to be overcome    *      scalability and performance aspects    *      issues with deploying the solution to others    *      the balance between manual and automated approaches   Please also note that there is a separate lightning session that will  allow products and solutions in this area to be presented.", "You might  well want to participate in both sessions.", "Finally, if you think this might be of interest to others, then please  pass the message on.", "Hoping we might see you in Boston in August."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We are expecting the session to cover a number of aspects of workflow  tools, such as    *      the types of problem being solved    *      the approaches used    *      the technical solution chosen    *      the hurdles that had to be overcome    *      scalability and performance aspects    *      issues with deploying the solution to others    *      the balance between manual and automated approaches   Please also note that there is a separate lightning session that will  allow products and solutions in this area to be presented.", "You might  well want to participate in both sessions.", "Finally, if you think this might be of interest to others, then please  pass the message on.", "Hoping we might see you in Boston in August.", "Tim Dudgeon  *  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello,  I'm interested in both serialization and deserialization of custom binary data sets.", "My understanding is that daffodil handles deserialization of data into xml - as illustrated in the nice examples at https://daffodil.apache.org/examples/.", "Does anyone know if daffodil handles serialization, as well?", "Thanks,  Amy   "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi All,  I have a hcatalog table \"partitioned by (d string)\".", "I have couple of days worth of data and when i run \"show partitions\" it provides the correct daa.", "d=20111215 d=20111216 d=20111217 d=20111218 d=20111219 d=20111220 d=20111221 d=20111222 d=20111223 d=20111224 d=20111225 d=20120415  However, when I run PIG with \"filter a by d == '20120415'\", it ends up scanning all data.", "Is this a known bug/enhancement in HCatalog?.", "Ideally, shouldn't it scan only the d=20120415 directory?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["d=20111215 d=20111216 d=20111217 d=20111218 d=20111219 d=20111220 d=20111221 d=20111222 d=20111223 d=20111224 d=20111225 d=20120415  However, when I run PIG with \"filter a by d == '20120415'\", it ends up scanning all data.", "Is this a known bug/enhancement in HCatalog?.", "Ideally, shouldn't it scan only the d=20120415 directory?", "Any pointers would be of great help.", "--  ~Rajesh.B  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Davide Ling wrote:   Hi,  I found an error in my wsdef, targetNamespace wasn't correct in types   section.", "So now I made a new step toward invoke... but...  a new strange Exception occurred!", "An html tag in envelope?", "In Italian: \"Dove cavolo lo va a pescare un tag html   nell'envelope??????\"", ";-)  I respond to myself."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So now I made a new step toward invoke... but...  a new strange Exception occurred!", "An html tag in envelope?", "In Italian: \"Dove cavolo lo va a pescare un tag html   nell'envelope??????\"", ";-)  I respond to myself.", "Service alias was different from service name so the bpel process got an html error page instead of a service response."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["An html tag in envelope?", "In Italian: \"Dove cavolo lo va a pescare un tag html   nell'envelope??????\"", ";-)  I respond to myself.", "Service alias was different from service name so the bpel process got an html error page instead of a service response.", "Bye  --  Davide Ling Sito personale - http://davideling.altervista.org   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi *  I'm using olio (PHP) with NetBeans 6.5.", "Setting apache's DocumentRoot to olio/public_html works fine.", "However  given a problem I have configuring the NetBeans project, I need to set  the DocumentRoot one step above (at olio level and not public_html) but  doing so the web app doesn't work anymore  /Looking in the error logs here is what I get:/  /PHP Fatal error: Class 'RequestUrl' not found in  /var/apache2/2.2/htdocs/olio/public_html/index.php on line 38 /  Is it possible to chage the DocumentRoot, and if so is there soem  tweaking to do in the PHP code to comply to the nrw DocumentRoot ?", "Thanks for your help  Amir    "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, Shailendra, I save the data in the PE like this:  private List<Event T_lineitem = new ArrayList<Event();  private long count = 0;  And in the onEvent(Event event){ count=count+1; T_lineitem .add(event) }  I find the count works correct in each same PE, but T_lineitem stores all the data in every PE.", "That means the list T_lineitem  are shared for every PE.", "I don't know what is the problem.", "Thank you!", "Dingyu Yang   2012/10/3 Shailendra Mishra <shailendrah@gmail.com   I am assuming you are planning on doing windowed joins - all you need  to do is keep save the window state and on each insert to the state  data strucuture check if the window has expired."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thank you!", "Dingyu Yang   2012/10/3 Shailendra Mishra <shailendrah@gmail.com   I am assuming you are planning on doing windowed joins - all you need  to do is keep save the window state and on each insert to the state  data strucuture check if the window has expired.", "If the window does  expire then compute the join and output it.", "Now this logic works only  for tumbling windows for sliding windows you have to work harder,  however the logic is kinda similar.", "- Shailendra    On Wed, Oct 3, 2012 at 1:24 AM, \u6768\u5b9a\u88d5 <yangdingyu@gmail.com wrote:   I have read the paper of S4: Distributed Stream Computing Platform."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Dingyu Yang   2012/10/3 Shailendra Mishra <shailendrah@gmail.com   I am assuming you are planning on doing windowed joins - all you need  to do is keep save the window state and on each insert to the state  data strucuture check if the window has expired.", "If the window does  expire then compute the join and output it.", "Now this logic works only  for tumbling windows for sliding windows you have to work harder,  however the logic is kinda similar.", "- Shailendra    On Wed, Oct 3, 2012 at 1:24 AM, \u6768\u5b9a\u88d5 <yangdingyu@gmail.com wrote:   I have read the paper of S4: Distributed Stream Computing Platform.", "There is a example of Joining: Click-through rate."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If the window does  expire then compute the join and output it.", "Now this logic works only  for tumbling windows for sliding windows you have to work harder,  however the logic is kinda similar.", "- Shailendra    On Wed, Oct 3, 2012 at 1:24 AM, \u6768\u5b9a\u88d5 <yangdingyu@gmail.com wrote:   I have read the paper of S4: Distributed Stream Computing Platform.", "There is a example of Joining: Click-through rate.", "Two data tables  RawServe   and RawClick are joined according 'serve' column."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Shailendra    On Wed, Oct 3, 2012 at 1:24 AM, \u6768\u5b9a\u88d5 <yangdingyu@gmail.com wrote:   I have read the paper of S4: Distributed Stream Computing Platform.", "There is a example of Joining: Click-through rate.", "Two data tables  RawServe   and RawClick are joined according 'serve' column.", "The data with same serve in two tables are sent to the same PE.", "The data  are   streaming to the PE and joined."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is a example of Joining: Click-through rate.", "Two data tables  RawServe   and RawClick are joined according 'serve' column.", "The data with same serve in two tables are sent to the same PE.", "The data  are   streaming to the PE and joined.", "I have a question :   While there are new tuples sent to the PE, PE has no previous data and   previous data in the PE are discarded after streaming processing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Two data tables  RawServe   and RawClick are joined according 'serve' column.", "The data with same serve in two tables are sent to the same PE.", "The data  are   streaming to the PE and joined.", "I have a question :   While there are new tuples sent to the PE, PE has no previous data and   previous data in the PE are discarded after streaming processing.", "As I know,the data is not stored in the PE."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  Is it possible to switch off skinning altogether?", "If so, will turning off the skinning prevent the wrapping of various components in SPANs and the download of a ~140k CSS file?", "Cheers,  Chris.  "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Wikipedia says there are 3 implementations:  1.", "Apache Daffodil  2.", "IBM has a DFDL processor included in their products  3.", "The European Space Agency has a DFDL processor  Is that list correct and complete?", "/Roger  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Thank you.", "I will check out the twitter example then.", "Thankyou  "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I was wondering how to modify window options like resizable,scrollbars,..", "I use the dialog framework to open a popup.", "The popup seems to use a frameset.", "Can i disable this behaviour?", "Is a non modal Dialog available?       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is this reproducible?", "Could you collect and share the zookeeper logs?", "-Flavio  On Sep 5, 2012, at 3:52 PM, Davide Simoncelli wrote:   Hello,    I'm trying to running an application on a cluster with 10 nodes.", "There is also an adapter cluster with only one nodes.", "What I noticed is that the node in the adapter cluster sends events and the node on it is running (the top command shows that the java process is using the CPU)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could you collect and share the zookeeper logs?", "-Flavio  On Sep 5, 2012, at 3:52 PM, Davide Simoncelli wrote:   Hello,    I'm trying to running an application on a cluster with 10 nodes.", "There is also an adapter cluster with only one nodes.", "What I noticed is that the node in the adapter cluster sends events and the node on it is running (the top command shows that the java process is using the CPU).", "The other 10 nodes (all of them) don't receive anything and the java process on each node doesn't even use the CPU."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-Flavio  On Sep 5, 2012, at 3:52 PM, Davide Simoncelli wrote:   Hello,    I'm trying to running an application on a cluster with 10 nodes.", "There is also an adapter cluster with only one nodes.", "What I noticed is that the node in the adapter cluster sends events and the node on it is running (the top command shows that the java process is using the CPU).", "The other 10 nodes (all of them) don't receive anything and the java process on each node doesn't even use the CPU.", "After a while the following exception is thrown:    [ZkClient-EventThread-27-localhost:2181] ERROR o.a.s4.comm.topology.ClustersFromZK - Zookeeper session expired, possibly due to a network partition for cluster [cluster1_adapter]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is also an adapter cluster with only one nodes.", "What I noticed is that the node in the adapter cluster sends events and the node on it is running (the top command shows that the java process is using the CPU).", "The other 10 nodes (all of them) don't receive anything and the java process on each node doesn't even use the CPU.", "After a while the following exception is thrown:    [ZkClient-EventThread-27-localhost:2181] ERROR o.a.s4.comm.topology.ClustersFromZK - Zookeeper session expired, possibly due to a network partition for cluster [cluster1_adapter].", "This node is considered as dead by Zookeeper."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What I noticed is that the node in the adapter cluster sends events and the node on it is running (the top command shows that the java process is using the CPU).", "The other 10 nodes (all of them) don't receive anything and the java process on each node doesn't even use the CPU.", "After a while the following exception is thrown:    [ZkClient-EventThread-27-localhost:2181] ERROR o.a.s4.comm.topology.ClustersFromZK - Zookeeper session expired, possibly due to a network partition for cluster [cluster1_adapter].", "This node is considered as dead by Zookeeper.", "Proceeding to stop this node."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The other 10 nodes (all of them) don't receive anything and the java process on each node doesn't even use the CPU.", "After a while the following exception is thrown:    [ZkClient-EventThread-27-localhost:2181] ERROR o.a.s4.comm.topology.ClustersFromZK - Zookeeper session expired, possibly due to a network partition for cluster [cluster1_adapter].", "This node is considered as dead by Zookeeper.", "Proceeding to stop this node.", "There is no error when clusters are created and nodes are started."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["After a while the following exception is thrown:    [ZkClient-EventThread-27-localhost:2181] ERROR o.a.s4.comm.topology.ClustersFromZK - Zookeeper session expired, possibly due to a network partition for cluster [cluster1_adapter].", "This node is considered as dead by Zookeeper.", "Proceeding to stop this node.", "There is no error when clusters are created and nodes are started.", "Also the status command shows the following output that let me to assume everything is ok:  App Status  ----------------------------------------------------------------------------------------------------------------------------------         Name              Cluster                                                  URI                                              ----------------------------------------------------------------------------------------------------------------------------------  testAppAdapter    cluster1_adapter  file:/home/s4-piper/testApp/build/libs/testAppAdapter.s4r                                 testApp                 cluster1      file:/tmp/testApp.s4r                                                                    ----------------------------------------------------------------------------------------------------------------------------------      Cluster Status  ----------------------------------------------------------------------------------------------------------------------------------                                                                                     Active nodes                                           Name                App           Tasks   --------------------------------------------------------------------------------                                                    Number    Task id                         Host                         Port      ----------------------------------------------------------------------------------------------------------------------------------   cluster1_adapter   testAppAdapter    1         1        Task-0                  computer1                   13000          cluster1           testApp                 10        10       Task-6                  computer2                   12006                                                                  Task-7                  computer4                   12007                                                                  Task-4                  computer6                   12004                                                                  Task-5                  computer7                   12005                                                                  Task-2                  computer9                   12002                                                                  Task-3                  computer11                  12003                                                                  Task-0                  computer17                  12000                                                                  Task-1                  computer18                  12001                                                                  Task-9                  computer23                  12009                                                                  Task-8                  computer37                  12008     ----------------------------------------------------------------------------------------------------------------------------------        Stream Status  ----------------------------------------------------------------------------------------------------------------------------------         Name                               Producers                                              Consumers                         ----------------------------------------------------------------------------------------------------------------------------------  RawlData                             cluster1_adapter(testAppAdapter)                            cluster1(testApp)                    ----------------------------------------------------------------------------------------------------------------------------------    Could you help me?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This node is considered as dead by Zookeeper.", "Proceeding to stop this node.", "There is no error when clusters are created and nodes are started.", "Also the status command shows the following output that let me to assume everything is ok:  App Status  ----------------------------------------------------------------------------------------------------------------------------------         Name              Cluster                                                  URI                                              ----------------------------------------------------------------------------------------------------------------------------------  testAppAdapter    cluster1_adapter  file:/home/s4-piper/testApp/build/libs/testAppAdapter.s4r                                 testApp                 cluster1      file:/tmp/testApp.s4r                                                                    ----------------------------------------------------------------------------------------------------------------------------------      Cluster Status  ----------------------------------------------------------------------------------------------------------------------------------                                                                                     Active nodes                                           Name                App           Tasks   --------------------------------------------------------------------------------                                                    Number    Task id                         Host                         Port      ----------------------------------------------------------------------------------------------------------------------------------   cluster1_adapter   testAppAdapter    1         1        Task-0                  computer1                   13000          cluster1           testApp                 10        10       Task-6                  computer2                   12006                                                                  Task-7                  computer4                   12007                                                                  Task-4                  computer6                   12004                                                                  Task-5                  computer7                   12005                                                                  Task-2                  computer9                   12002                                                                  Task-3                  computer11                  12003                                                                  Task-0                  computer17                  12000                                                                  Task-1                  computer18                  12001                                                                  Task-9                  computer23                  12009                                                                  Task-8                  computer37                  12008     ----------------------------------------------------------------------------------------------------------------------------------        Stream Status  ----------------------------------------------------------------------------------------------------------------------------------         Name                               Producers                                              Consumers                         ----------------------------------------------------------------------------------------------------------------------------------  RawlData                             cluster1_adapter(testAppAdapter)                            cluster1(testApp)                    ----------------------------------------------------------------------------------------------------------------------------------    Could you help me?", "Thank you    Regards    - Davide   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That would be 64888 * 0.0948 = 6151  EventDetail (success count)\t64888 % EventDetail views where attendee added\t9.48  Complete report is attached!", "In the summary report, under miscellaneous stats, there's the \"% EventDetail  views where attendee added\" field.", "Use this percentage point and calculate  the added attendees from the success count of EventDetail (first table -  \"Operation Mix.\")", "Thanks,  -Akara   William Voorsluys wrote:   Hello,   Here are some details about a 1 hour run that shows when the bursts  happen.", "The tools I ran were \"mpstat -P ALL 5; vmstat 5; iostat -x 5\"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the summary report, under miscellaneous stats, there's the \"% EventDetail  views where attendee added\" field.", "Use this percentage point and calculate  the added attendees from the success count of EventDetail (first table -  \"Operation Mix.\")", "Thanks,  -Akara   William Voorsluys wrote:   Hello,   Here are some details about a 1 hour run that shows when the bursts  happen.", "The tools I ran were \"mpstat -P ALL 5; vmstat 5; iostat -x 5\".", "However, in this for some reason the Detail Results page was not  generated for this run."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Use this percentage point and calculate  the added attendees from the success count of EventDetail (first table -  \"Operation Mix.\")", "Thanks,  -Akara   William Voorsluys wrote:   Hello,   Here are some details about a 1 hour run that shows when the bursts  happen.", "The tools I ran were \"mpstat -P ALL 5; vmstat 5; iostat -x 5\".", "However, in this for some reason the Detail Results page was not  generated for this run.", "Thus, the graph attached belongs to another  run, which was identical but different tools were running."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  -Akara   William Voorsluys wrote:   Hello,   Here are some details about a 1 hour run that shows when the bursts  happen.", "The tools I ran were \"mpstat -P ALL 5; vmstat 5; iostat -x 5\".", "However, in this for some reason the Detail Results page was not  generated for this run.", "Thus, the graph attached belongs to another  run, which was identical but different tools were running.", "System 'william2' is the web server (Ubuntu server 8.04 64-bit, 2  CPUs, 3GB RAM)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The tools I ran were \"mpstat -P ALL 5; vmstat 5; iostat -x 5\".", "However, in this for some reason the Detail Results page was not  generated for this run.", "Thus, the graph attached belongs to another  run, which was identical but different tools were running.", "System 'william2' is the web server (Ubuntu server 8.04 64-bit, 2  CPUs, 3GB RAM).", "System 'mysql1' runs only the database (Debian Etch 32-bit, 4 CPUS, 1GB  RAM)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, in this for some reason the Detail Results page was not  generated for this run.", "Thus, the graph attached belongs to another  run, which was identical but different tools were running.", "System 'william2' is the web server (Ubuntu server 8.04 64-bit, 2  CPUs, 3GB RAM).", "System 'mysql1' runs only the database (Debian Etch 32-bit, 4 CPUS, 1GB  RAM).", "Both the filestore and the database files are on NFS."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["System 'william2' is the web server (Ubuntu server 8.04 64-bit, 2  CPUs, 3GB RAM).", "System 'mysql1' runs only the database (Debian Etch 32-bit, 4 CPUS, 1GB  RAM).", "Both the filestore and the database files are on NFS.", "Cheers,   William     On Wed, Feb 4, 2009 at 4:45 AM, Shanti Subramanyam  <Shanti.Subramanyam@sun.com wrote:   I have noticed some bursts at 15-20 mins, but I think these are specific  to  our deployment (nfs issues) and we're working on tuning those - we  haven't  identified any Olio issue that will cause this behavior (at least not yet  !)", "I assume you're running with a local filestore ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["System 'mysql1' runs only the database (Debian Etch 32-bit, 4 CPUS, 1GB  RAM).", "Both the filestore and the database files are on NFS.", "Cheers,   William     On Wed, Feb 4, 2009 at 4:45 AM, Shanti Subramanyam  <Shanti.Subramanyam@sun.com wrote:   I have noticed some bursts at 15-20 mins, but I think these are specific  to  our deployment (nfs issues) and we're working on tuning those - we  haven't  identified any Olio issue that will cause this behavior (at least not yet  !)", "I assume you're running with a local filestore ?", "I'd be happy to take a  look  at one of your run outputs (assuming you collect vmstat, iostat etc.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Both the filestore and the database files are on NFS.", "Cheers,   William     On Wed, Feb 4, 2009 at 4:45 AM, Shanti Subramanyam  <Shanti.Subramanyam@sun.com wrote:   I have noticed some bursts at 15-20 mins, but I think these are specific  to  our deployment (nfs issues) and we're working on tuning those - we  haven't  identified any Olio issue that will cause this behavior (at least not yet  !)", "I assume you're running with a local filestore ?", "I'd be happy to take a  look  at one of your run outputs (assuming you collect vmstat, iostat etc.)", "Shanti   William Voorsluys wrote:   Hello,   What is the minimum time I should run Olio's PHP workload to be sure  my system really supports a certain amount of concurrent users?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I assume you're running with a local filestore ?", "I'd be happy to take a  look  at one of your run outputs (assuming you collect vmstat, iostat etc.)", "Shanti   William Voorsluys wrote:   Hello,   What is the minimum time I should run Olio's PHP workload to be sure  my system really supports a certain amount of concurrent users?", "I've  been running for 30 minutes on steady state and all metrics pass, but  extending the run to 1 hour make things fail.", "It seems that at 25  minutes there's a workload burst that destabilize my system."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'd be happy to take a  look  at one of your run outputs (assuming you collect vmstat, iostat etc.)", "Shanti   William Voorsluys wrote:   Hello,   What is the minimum time I should run Olio's PHP workload to be sure  my system really supports a certain amount of concurrent users?", "I've  been running for 30 minutes on steady state and all metrics pass, but  extending the run to 1 hour make things fail.", "It seems that at 25  minutes there's a workload burst that destabilize my system.", "Is there a documentation that describes workload characteristics, such  as when bursts take place?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Shanti   William Voorsluys wrote:   Hello,   What is the minimum time I should run Olio's PHP workload to be sure  my system really supports a certain amount of concurrent users?", "I've  been running for 30 minutes on steady state and all metrics pass, but  extending the run to 1 hour make things fail.", "It seems that at 25  minutes there's a workload burst that destabilize my system.", "Is there a documentation that describes workload characteristics, such  as when bursts take place?", "Thanks,   William       ------------------------------------------------------------------------         System parameters on server william2   #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I've  been running for 30 minutes on steady state and all metrics pass, but  extending the run to 1 hour make things fail.", "It seems that at 25  minutes there's a workload burst that destabilize my system.", "Is there a documentation that describes workload characteristics, such  as when bursts take place?", "Thanks,   William       ------------------------------------------------------------------------         System parameters on server william2   #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information.", "#   #kernel.domainname = example.com   # the following stops low-level messages on console  kernel.printk = 4 4 1 7   # enable /proc/$pid/maps privacy so that memory relocations are not  # visible to other users."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It seems that at 25  minutes there's a workload burst that destabilize my system.", "Is there a documentation that describes workload characteristics, such  as when bursts take place?", "Thanks,   William       ------------------------------------------------------------------------         System parameters on server william2   #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information.", "#   #kernel.domainname = example.com   # the following stops low-level messages on console  kernel.printk = 4 4 1 7   # enable /proc/$pid/maps privacy so that memory relocations are not  # visible to other users.", "(Added in kernel 2.6.22.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,   William       ------------------------------------------------------------------------         System parameters on server william2   #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information.", "#   #kernel.domainname = example.com   # the following stops low-level messages on console  kernel.printk = 4 4 1 7   # enable /proc/$pid/maps privacy so that memory relocations are not  # visible to other users.", "(Added in kernel 2.6.22.)", "kernel.maps_protect = 1   # Increase inotify availability  fs.inotify.max_user_watches = 524288   # protect bottom 64k of memory from mmap to prevent NULL-dereference  # attacks against potential future kernel security vulnerabilities.", "# (Added in kernel 2.6.23.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["#   #kernel.domainname = example.com   # the following stops low-level messages on console  kernel.printk = 4 4 1 7   # enable /proc/$pid/maps privacy so that memory relocations are not  # visible to other users.", "(Added in kernel 2.6.22.)", "kernel.maps_protect = 1   # Increase inotify availability  fs.inotify.max_user_watches = 524288   # protect bottom 64k of memory from mmap to prevent NULL-dereference  # attacks against potential future kernel security vulnerabilities.", "# (Added in kernel 2.6.23.)", "vm.mmap_min_addr = 65536   ##############################################################3  # Functions previously found in netbase  #   # Comment the next two lines to disable Spoof protection (reverse-path  filter)  # Turn on Source Address Verification in all interfaces to  # prevent some spoofing attacks  net.ipv4.conf.default.rp_filter=1  net.ipv4.conf.all.rp_filter=1   # Uncomment the next line to enable TCP/IP SYN cookies  # This disables TCP Window Scaling (http://lkml.org/lkml/2008/2/5/167)  #net.ipv4.tcp_syncookies=1   # Uncomment the next line to enable packet forwarding for IPv4  #net.ipv4.ip_forward=1   # Uncomment the next line to enable packet forwarding for IPv6  #net.ipv6.ip_forward=1    ###################################################################  # Additional settings - these settings can improve the network  # security of the host and prevent against some network attacks  # including spoofing attacks and man in the middle attacks through  # redirection."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(Added in kernel 2.6.22.)", "kernel.maps_protect = 1   # Increase inotify availability  fs.inotify.max_user_watches = 524288   # protect bottom 64k of memory from mmap to prevent NULL-dereference  # attacks against potential future kernel security vulnerabilities.", "# (Added in kernel 2.6.23.)", "vm.mmap_min_addr = 65536   ##############################################################3  # Functions previously found in netbase  #   # Comment the next two lines to disable Spoof protection (reverse-path  filter)  # Turn on Source Address Verification in all interfaces to  # prevent some spoofing attacks  net.ipv4.conf.default.rp_filter=1  net.ipv4.conf.all.rp_filter=1   # Uncomment the next line to enable TCP/IP SYN cookies  # This disables TCP Window Scaling (http://lkml.org/lkml/2008/2/5/167)  #net.ipv4.tcp_syncookies=1   # Uncomment the next line to enable packet forwarding for IPv4  #net.ipv4.ip_forward=1   # Uncomment the next line to enable packet forwarding for IPv6  #net.ipv6.ip_forward=1    ###################################################################  # Additional settings - these settings can improve the network  # security of the host and prevent against some network attacks  # including spoofing attacks and man in the middle attacks through  # redirection.", "Some network environments, however, require that these  # settings are disabled so review and enable them as needed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["kernel.maps_protect = 1   # Increase inotify availability  fs.inotify.max_user_watches = 524288   # protect bottom 64k of memory from mmap to prevent NULL-dereference  # attacks against potential future kernel security vulnerabilities.", "# (Added in kernel 2.6.23.)", "vm.mmap_min_addr = 65536   ##############################################################3  # Functions previously found in netbase  #   # Comment the next two lines to disable Spoof protection (reverse-path  filter)  # Turn on Source Address Verification in all interfaces to  # prevent some spoofing attacks  net.ipv4.conf.default.rp_filter=1  net.ipv4.conf.all.rp_filter=1   # Uncomment the next line to enable TCP/IP SYN cookies  # This disables TCP Window Scaling (http://lkml.org/lkml/2008/2/5/167)  #net.ipv4.tcp_syncookies=1   # Uncomment the next line to enable packet forwarding for IPv4  #net.ipv4.ip_forward=1   # Uncomment the next line to enable packet forwarding for IPv6  #net.ipv6.ip_forward=1    ###################################################################  # Additional settings - these settings can improve the network  # security of the host and prevent against some network attacks  # including spoofing attacks and man in the middle attacks through  # redirection.", "Some network environments, however, require that these  # settings are disabled so review and enable them as needed.", "#  # Ignore ICMP broadcasts  #net/ipv4/icmp_echo_ignore_broadcasts = 1  #  # Ignore bogus ICMP errors  #net/ipv4/icmp_ignore_bogus_error_responses = 1  # # Do not accept ICMP redirects (prevent MITM attacks)  #net/ipv4/conf/all/accept_redirects = 0  # _or_  # Accept ICMP redirects only for gateways listed in our default  # gateway list (enabled by default)  # net/ipv4/conf/all/secure_redirects = 1  #  # Do not send ICMP redirects (we are not a router)  #net/ipv4/conf/all/send_redirects = 0  #  # Do not accept IP source route packets (we are not a router)  #net/ipv4/conf/all/accept_source_route = 0  #  # Log Martian Packets  #net/ipv4/conf/all/log_martians = 1  #  # Always defragment packets  #net/ipv4/ip_always_defrag = 1          Processor info for server william2     processor       : 0  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpu cores       : 1  cpuid level     : 10  cache_alignment : 64  processor       : 0  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpu cores       : 1  cpuid level     : 10  cache_alignment : 64          Memory info for server william2     MemTotal:      3139292 kB  MemFree:       1072572 kB  SwapCached:          0 kB  SwapTotal:      409616 kB  SwapFree:       409548 kB          Kernel on server william2     Linux 2.6.24-23-xen #1 SMP Thu Nov 27 20:14:09 UTC 2008 x86_64     ------------------------------------------------------------------------    ------------------------------------------------------------------------           System parameters on server mysql1     #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["# (Added in kernel 2.6.23.)", "vm.mmap_min_addr = 65536   ##############################################################3  # Functions previously found in netbase  #   # Comment the next two lines to disable Spoof protection (reverse-path  filter)  # Turn on Source Address Verification in all interfaces to  # prevent some spoofing attacks  net.ipv4.conf.default.rp_filter=1  net.ipv4.conf.all.rp_filter=1   # Uncomment the next line to enable TCP/IP SYN cookies  # This disables TCP Window Scaling (http://lkml.org/lkml/2008/2/5/167)  #net.ipv4.tcp_syncookies=1   # Uncomment the next line to enable packet forwarding for IPv4  #net.ipv4.ip_forward=1   # Uncomment the next line to enable packet forwarding for IPv6  #net.ipv6.ip_forward=1    ###################################################################  # Additional settings - these settings can improve the network  # security of the host and prevent against some network attacks  # including spoofing attacks and man in the middle attacks through  # redirection.", "Some network environments, however, require that these  # settings are disabled so review and enable them as needed.", "#  # Ignore ICMP broadcasts  #net/ipv4/icmp_echo_ignore_broadcasts = 1  #  # Ignore bogus ICMP errors  #net/ipv4/icmp_ignore_bogus_error_responses = 1  # # Do not accept ICMP redirects (prevent MITM attacks)  #net/ipv4/conf/all/accept_redirects = 0  # _or_  # Accept ICMP redirects only for gateways listed in our default  # gateway list (enabled by default)  # net/ipv4/conf/all/secure_redirects = 1  #  # Do not send ICMP redirects (we are not a router)  #net/ipv4/conf/all/send_redirects = 0  #  # Do not accept IP source route packets (we are not a router)  #net/ipv4/conf/all/accept_source_route = 0  #  # Log Martian Packets  #net/ipv4/conf/all/log_martians = 1  #  # Always defragment packets  #net/ipv4/ip_always_defrag = 1          Processor info for server william2     processor       : 0  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpu cores       : 1  cpuid level     : 10  cache_alignment : 64  processor       : 0  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpu cores       : 1  cpuid level     : 10  cache_alignment : 64          Memory info for server william2     MemTotal:      3139292 kB  MemFree:       1072572 kB  SwapCached:          0 kB  SwapTotal:      409616 kB  SwapFree:       409548 kB          Kernel on server william2     Linux 2.6.24-23-xen #1 SMP Thu Nov 27 20:14:09 UTC 2008 x86_64     ------------------------------------------------------------------------    ------------------------------------------------------------------------           System parameters on server mysql1     #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information.", "#   #kernel.domainname = example.com  #net/ipv4/icmp_echo_ignore_broadcasts=1   # Uncomment the following to stop low-level messages on console  #kernel.printk = 4 4 1 7   ##############################################################3  # Functions previously found in netbase  #   # Uncomment the next line to enable Spoof protection (reverse-path filter)  #net.ipv4.conf.default.rp_filter=1   # Uncomment the next line to enable TCP/IP SYN cookies  #net.ipv4.tcp_syncookies=1   # Uncomment the next line to enable packet forwarding for IPv4  #net.ipv4.conf.default.forwarding=1   # Uncomment the next line to enable packet forwarding for IPv6  #net.ipv6.conf.default.forwarding=1          Processor info for server mysql1     processor       : 0  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpuid level     : 10  processor       : 1  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpuid level     : 10  processor       : 2  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpuid level     : 10  processor       : 3  cpu family      : 6  model           : 23  model name      : Intel(R) Xeon(R) CPU           E5410  @ 2.33GHz  cpu MHz         : 2327.498  cache size      : 6144 KB  cpuid level     : 10          Memory info for server mysql1     MemTotal:      1048688 kB  MemFree:        499280 kB  SwapCached:          0 kB  SwapTotal:      522072 kB  SwapFree:       522072 kB          Kernel on server mysql1     Linux 2.6.18.8.xs5.0.0.10.439 #1 SMP Wed Aug 6 23:55:12 UTC 2008 i686        --  William Voorsluys  williamvoor.googlepages.com  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["do you have a trace?", "If not, can you try adding the -debug or option to  the gradle command?", "That may help diagnose the issue.", "Thanks,  Matthieu  On 8/14/12 11:22 AM, Qun Huang wrote:  Dear all,   I'm a new user of S4.", "I checked out the latest version from the git and followed the  instruction in the S4 website."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If not, can you try adding the -debug or option to  the gradle command?", "That may help diagnose the issue.", "Thanks,  Matthieu  On 8/14/12 11:22 AM, Qun Huang wrote:  Dear all,   I'm a new user of S4.", "I checked out the latest version from the git and followed the  instruction in the S4 website.", "However, when I type in the command       \"./gradlew install -DskipTests\",  it stops at the point       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \"  and then cannot continue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That may help diagnose the issue.", "Thanks,  Matthieu  On 8/14/12 11:22 AM, Qun Huang wrote:  Dear all,   I'm a new user of S4.", "I checked out the latest version from the git and followed the  instruction in the S4 website.", "However, when I type in the command       \"./gradlew install -DskipTests\",  it stops at the point       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \"  and then cannot continue.", "Similarly, when I use the command       \"./gradlew eclipse\",  it stops at       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Matthieu  On 8/14/12 11:22 AM, Qun Huang wrote:  Dear all,   I'm a new user of S4.", "I checked out the latest version from the git and followed the  instruction in the S4 website.", "However, when I type in the command       \"./gradlew install -DskipTests\",  it stops at the point       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \"  and then cannot continue.", "Similarly, when I use the command       \"./gradlew eclipse\",  it stops at       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \".", "Can anybody help me?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I checked out the latest version from the git and followed the  instruction in the S4 website.", "However, when I type in the command       \"./gradlew install -DskipTests\",  it stops at the point       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \"  and then cannot continue.", "Similarly, when I use the command       \"./gradlew eclipse\",  it stops at       \" Building  :s4-tools:compileJava  Resolving dependencies  ':s4-tools:compile' \".", "Can anybody help me?", "Thanks very much!    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Oh look who it is!", "It just so happens I was Googling around this morning  trying to find out whether Xdebug locks profiling files in append mode  as I vaguely remember reading somewhere it didn't.", "But that's a question for another list (or tracker).", ":)   On 05/06/11 18:09, Derick Rethans wrote:  On Fri, 6 May 2011, Benjamin Eberlei wrote:   Not possible right now, this is why the patch in ZETACOMP-79 exists.", "Derick is  currently reviewing it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It just so happens I was Googling around this morning  trying to find out whether Xdebug locks profiling files in append mode  as I vaguely remember reading somewhere it didn't.", "But that's a question for another list (or tracker).", ":)   On 05/06/11 18:09, Derick Rethans wrote:  On Fri, 6 May 2011, Benjamin Eberlei wrote:   Not possible right now, this is why the patch in ZETACOMP-79 exists.", "Derick is  currently reviewing it.", "I just did!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But that's a question for another list (or tracker).", ":)   On 05/06/11 18:09, Derick Rethans wrote:  On Fri, 6 May 2011, Benjamin Eberlei wrote:   Not possible right now, this is why the patch in ZETACOMP-79 exists.", "Derick is  currently reviewing it.", "I just did!", ":)   Derick   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, the detailed stack trace states that you cannot serialize your  Instance class due to a missing no-args constructor.", "I would recomend to  follow the provided suggestion for GSON, which is : \"Register an  InstanceCreator with Gson for this type to fix this problem\".", "Note that the missing no-arg constructor is also an issue when you  serialize with kryo 1, so you'd have to modify the provided serializer  to include custom serialization for the instance class.", "Please refer to  the kryo documentation for this.", "Regards,  Matthieu."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Note that the missing no-arg constructor is also an issue when you  serialize with kryo 1, so you'd have to modify the provided serializer  to include custom serialization for the instance class.", "Please refer to  the kryo documentation for this.", "Regards,  Matthieu.", "On 9/4/12 9:19 AM, Raghavendar TS wrote:  Hi  In S4 0.3.0 I am trying to dispatch object of Instance.class from one PE  to another.But it is not working.ie <http://working.ie PE2 is not  receiving events from PE1 if the event consist of object of  Instance.class.What may be the problem?", "Also I tried using GSON to serialize objects of Instance.class to string  and vice versa.It is throwing the exception during deserialization   Exception in thread \"main\" java.lang.RuntimeException: No-args  constructor for interface weka.core.Instance does not exist."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please refer to  the kryo documentation for this.", "Regards,  Matthieu.", "On 9/4/12 9:19 AM, Raghavendar TS wrote:  Hi  In S4 0.3.0 I am trying to dispatch object of Instance.class from one PE  to another.But it is not working.ie <http://working.ie PE2 is not  receiving events from PE1 if the event consist of object of  Instance.class.What may be the problem?", "Also I tried using GSON to serialize objects of Instance.class to string  and vice versa.It is throwing the exception during deserialization   Exception in thread \"main\" java.lang.RuntimeException: No-args  constructor for interface weka.core.Instance does not exist.", "Register an  InstanceCreator with Gson for this type to fix this problem."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 9/4/12 9:19 AM, Raghavendar TS wrote:  Hi  In S4 0.3.0 I am trying to dispatch object of Instance.class from one PE  to another.But it is not working.ie <http://working.ie PE2 is not  receiving events from PE1 if the event consist of object of  Instance.class.What may be the problem?", "Also I tried using GSON to serialize objects of Instance.class to string  and vice versa.It is throwing the exception during deserialization   Exception in thread \"main\" java.lang.RuntimeException: No-args  constructor for interface weka.core.Instance does not exist.", "Register an  InstanceCreator with Gson for this type to fix this problem.", "at  com.google.gson.MappedObjectConstructor.constructWithNoArgConstructor(MappedObjectConstructor.java:64)       at  com.google.gson.MappedObjectConstructor.construct(MappedObjectConstructor.java:53)       at  com.google.gson.JsonObjectDeserializationVisitor.constructTarget(JsonObjectDeserializationVisitor.java:41)       at  com.google.gson.JsonDeserializationVisitor.getTarget(JsonDeserializationVisitor.java:56)       at com.google.gson.ObjectNavigator.accept(ObjectNavigator.java:101)       at  com.google.gson.JsonDeserializationContextDefault.fromJsonObject(JsonDeserializationContextDefault.java:73)       at  com.google.gson.JsonDeserializationContextDefault.deserialize(JsonDeserializationContextDefault.java:51)       at com.google.gson.Gson.fromJson(Gson.java:495)       at com.google.gson.Gson.fromJson(Gson.java:444)       at com.google.gson.Gson.fromJson(Gson.java:396)       at com.google.gson.Gson.fromJson(Gson.java:372)       at gson.main(gson.java:62)   What is the problem.", "How can I dispatch such objects in S4 0.3.0.   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello DFDL community,  My input consists of a string containing one to five characters.", "Each character must be a digit character.", "Here is a sample input:  \t99999  My DFDL schema uses the xs:unsignedInt datatype to constrain the string to digits.", "It uses dfdl:length=5 to constrain the length of the string.", "I am trying to be very precise in my wording."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Here is a sample input:  \t99999  My DFDL schema uses the xs:unsignedInt datatype to constrain the string to digits.", "It uses dfdl:length=5 to constrain the length of the string.", "I am trying to be very precise in my wording.", "Is the above wording correct?", "Here is my DFDL schema:  <xs:element name=\"input\"     <xs:complexType         <xs:sequence             <xs:element name=\"value\"   \t\ttype=\"xs:unsignedInt\"   \t\tdfdl:lengthKind=\"explicit\"   \t\tdfdl:length=\"5\"   \t\tdfdl:lengthUnits=\"characters\" /         </xs:sequence     </xs:complexType </xs:element  Parsing the input 99999 produces this XML:  <input   <value99999</value </input  Unparsing produces this error message:  [error] Unparse Error: Data too long by 8 bits."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It uses dfdl:length=5 to constrain the length of the string.", "I am trying to be very precise in my wording.", "Is the above wording correct?", "Here is my DFDL schema:  <xs:element name=\"input\"     <xs:complexType         <xs:sequence             <xs:element name=\"value\"   \t\ttype=\"xs:unsignedInt\"   \t\tdfdl:lengthKind=\"explicit\"   \t\tdfdl:length=\"5\"   \t\tdfdl:lengthUnits=\"characters\" /         </xs:sequence     </xs:complexType </xs:element  Parsing the input 99999 produces this XML:  <input   <value99999</value </input  Unparsing produces this error message:  [error] Unparse Error: Data too long by 8 bits.", "Unable to truncate."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am trying to be very precise in my wording.", "Is the above wording correct?", "Here is my DFDL schema:  <xs:element name=\"input\"     <xs:complexType         <xs:sequence             <xs:element name=\"value\"   \t\ttype=\"xs:unsignedInt\"   \t\tdfdl:lengthKind=\"explicit\"   \t\tdfdl:length=\"5\"   \t\tdfdl:lengthUnits=\"characters\" /         </xs:sequence     </xs:complexType </xs:element  Parsing the input 99999 produces this XML:  <input   <value99999</value </input  Unparsing produces this error message:  [error] Unparse Error: Data too long by 8 bits.", "Unable to truncate.", "Here is the output of unparsing:  99,999  Why does unparsing produce that error message?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is the above wording correct?", "Here is my DFDL schema:  <xs:element name=\"input\"     <xs:complexType         <xs:sequence             <xs:element name=\"value\"   \t\ttype=\"xs:unsignedInt\"   \t\tdfdl:lengthKind=\"explicit\"   \t\tdfdl:length=\"5\"   \t\tdfdl:lengthUnits=\"characters\" /         </xs:sequence     </xs:complexType </xs:element  Parsing the input 99999 produces this XML:  <input   <value99999</value </input  Unparsing produces this error message:  [error] Unparse Error: Data too long by 8 bits.", "Unable to truncate.", "Here is the output of unparsing:  99,999  Why does unparsing produce that error message?", "Is it a bug in Daffodil?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Here is my DFDL schema:  <xs:element name=\"input\"     <xs:complexType         <xs:sequence             <xs:element name=\"value\"   \t\ttype=\"xs:unsignedInt\"   \t\tdfdl:lengthKind=\"explicit\"   \t\tdfdl:length=\"5\"   \t\tdfdl:lengthUnits=\"characters\" /         </xs:sequence     </xs:complexType </xs:element  Parsing the input 99999 produces this XML:  <input   <value99999</value </input  Unparsing produces this error message:  [error] Unparse Error: Data too long by 8 bits.", "Unable to truncate.", "Here is the output of unparsing:  99,999  Why does unparsing produce that error message?", "Is it a bug in Daffodil?", "/Roger  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["<html <head</head <body <div <div <h4 <img height=\"9px\" width=\"9px\" src=\"https://avatars0.githubusercontent.com/u/7692062?v=4&s=60\" / nathantfrank<span style=\"color: ##24229F\" (nathantfrank)</span  </h4 <p <small<b style=\"color: #900C3F\"[2019-01-02T17:08:09.555Z]</b</small  Could someone provide some guidance on what I might be missing?", "I am currently trying to copy a file locally to an Azure DataLakeStore.", "Here is the pull file I have put together so far.<pre<codejob.name=GobblinCopy job.group=Copy job.description=Test Gobblin job <span class=\"keyword\"for</span copy <span class=\"preprocessor\"# job.schedule=<span class=\"number\"0</span <span class=\"number\"0</span/<span class=\"number\"2</span * * * ?</span  source.<span class=\"keyword\"class</span=org.apache.gobblin.data.management.copy.CopySource writer.builder.<span class=\"keyword\"class</span=org.apache.gobblin.data.management.copy.writer.FileAwareInputStreamDataWriterBuilder extract.<span class=\"keyword\"namespace</span=org.apache.gobblin.copy  source.filebased.uri=file:<span class=\"comment\"///</span  <span class=\"preprocessor\"# data.publisher.type=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher</span <span class=\"preprocessor\"# data.publisher.final.dir=file:<span class=\"comment\"///home/ntfrank/Documents/</span</span  writer.fs.uri=adl:<span class=\"comment\"//[ADLSName].azuredatalakestore.net/</span  gobblin.dataset.pattern=file:<span class=\"comment\"///home/ntfrank/copy_data/</span <span class=\"preprocessor\"# converter.classes=org.apache.gobblin.data.management.copy.converter.UnGzipConverter</span  fs.AbstractFileSystem.adl.impl=<span class=\"string\"\"org.apache.hadoop.fs.adl.Adl\"</span dfs.adls.oauth2.access.token.provider.type=ClientCredential dfs.adls.oauth2.refresh.url=https:<span class=\"comment\"//login.microsoftonline.com/[TenantID]/oauth2/v2.0/token</span dfs.adls.oauth2.client.id=[ClientID] writer.encrypted.dfs.adls.oauth2.credential=[ClientSecret] <span class=\"preprocessor\"# encrypt.key.loc=/passphrase/file/path</span  task.maxretries=<span class=\"number\"0</span workunit.retry.enabled=<span class=\"literal\"false</span</code</pre </p <hr </div <div <h4 <img height=\"9px\" width=\"9px\" src=\"https://avatars1.githubusercontent.com/u/2355051?v=4&s=60\" / Zhixiong Chen<span style=\"color: ##24229F\" (zxcware)</span  </h4 <p <small<b style=\"color: #900C3F\"[2019-01-02T17:51:33.691Z]</b</small  <p<span data-link-type=\"mention\" data-screen-name=\"nathantfrank\" class=\"mention\"@nathantfrank</span There are some missing required configurations:<br<codegobblin.dataset.pattern=</code<br<codedata.publisher.type=</code<br<codedata.publisher.final.dir=</code</p<pFor distcp, we have a template available in module <codegobblin-runtime/src/main/resources/templates</code.</p </p <hr </div <div <h4 <img height=\"9px\" width=\"9px\" src=\"https://avatars1.githubusercontent.com/u/2355051?v=4&s=60\" / Zhixiong Chen<span style=\"color: ##24229F\" (zxcware)</span  </h4 <p <small<b style=\"color: #900C3F\"[2019-01-02T17:51:55.511Z]</b</small  Are you running the job in your local machine?", "</p <hr </div <div <h4 <img height=\"9px\" width=\"9px\" src=\"https://avatars0.githubusercontent.com/u/7692062?v=4&s=60\" / nathantfrank<span style=\"color: ##24229F\" (nathantfrank)</span  </h4 <p <small<b style=\"color: #900C3F\"[2019-01-02T19:18:22.573Z]</b</small  <span data-link-type=\"mention\" data-screen-name=\"zxcware\" class=\"mention\"@zxcware</span I am running it locally and in standalone mode </p <hr </div </div </body  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please see inline comments.", "On Sun, October 23, 2011 5:10 pm, Michael Stroucken wrote:  Kushal Dalmia wrote:   Hi,    We have setup Tashi on a local machine and are running the node manager   and cluster manager on the same machine.", "When we use the client to  create a VM, no VM gets instantiated.", "We do not see any errors from the  client, however the NodeManager(running in DEBUG = 1 mode) shows us the  following log:    Hi Kushal,    I have to assume you are running from the latest SVN trunk if I see  messages related to vgscratch here.", "Keep in mind that there is some debug output put in /tmp by the  nodemanager currently (like /tmp/20037.{out|err})."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Sun, October 23, 2011 5:10 pm, Michael Stroucken wrote:  Kushal Dalmia wrote:   Hi,    We have setup Tashi on a local machine and are running the node manager   and cluster manager on the same machine.", "When we use the client to  create a VM, no VM gets instantiated.", "We do not see any errors from the  client, however the NodeManager(running in DEBUG = 1 mode) shows us the  following log:    Hi Kushal,    I have to assume you are running from the latest SVN trunk if I see  messages related to vgscratch here.", "Keep in mind that there is some debug output put in /tmp by the  nodemanager currently (like /tmp/20037.{out|err}).", "It should have something  there, because I see that the nodemanager tries to create VMs."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When we use the client to  create a VM, no VM gets instantiated.", "We do not see any errors from the  client, however the NodeManager(running in DEBUG = 1 mode) shows us the  following log:    Hi Kushal,    I have to assume you are running from the latest SVN trunk if I see  messages related to vgscratch here.", "Keep in mind that there is some debug output put in /tmp by the  nodemanager currently (like /tmp/20037.{out|err}).", "It should have something  there, because I see that the nodemanager tries to create VMs.", "My /tmp/<pid.err file looks like this: /etc/qemu-ifup.1: could not launch network script qemu-system-x86_64: -net tap,ifname=tashi4.0,vlan=1,script=/etc/qemu-ifup.1: Device 'tap' could not be initialized  The /etc/qemu-ifup.1 script looks like this: #!/bin/sh  /sbin/ifconfig $1 0.0.0.0 up /usr/sbin/brctl addif mybridge $1 exit 0     Does the /etc/qemu-ifup.1 script exist?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We do not see any errors from the  client, however the NodeManager(running in DEBUG = 1 mode) shows us the  following log:    Hi Kushal,    I have to assume you are running from the latest SVN trunk if I see  messages related to vgscratch here.", "Keep in mind that there is some debug output put in /tmp by the  nodemanager currently (like /tmp/20037.{out|err}).", "It should have something  there, because I see that the nodemanager tries to create VMs.", "My /tmp/<pid.err file looks like this: /etc/qemu-ifup.1: could not launch network script qemu-system-x86_64: -net tap,ifname=tashi4.0,vlan=1,script=/etc/qemu-ifup.1: Device 'tap' could not be initialized  The /etc/qemu-ifup.1 script looks like this: #!/bin/sh  /sbin/ifconfig $1 0.0.0.0 up /usr/sbin/brctl addif mybridge $1 exit 0     Does the /etc/qemu-ifup.1 script exist?", "Yes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Keep in mind that there is some debug output put in /tmp by the  nodemanager currently (like /tmp/20037.{out|err}).", "It should have something  there, because I see that the nodemanager tries to create VMs.", "My /tmp/<pid.err file looks like this: /etc/qemu-ifup.1: could not launch network script qemu-system-x86_64: -net tap,ifname=tashi4.0,vlan=1,script=/etc/qemu-ifup.1: Device 'tap' could not be initialized  The /etc/qemu-ifup.1 script looks like this: #!/bin/sh  /sbin/ifconfig $1 0.0.0.0 up /usr/sbin/brctl addif mybridge $1 exit 0     Does the /etc/qemu-ifup.1 script exist?", "Yes.", "Does the disk image exist?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It should have something  there, because I see that the nodemanager tries to create VMs.", "My /tmp/<pid.err file looks like this: /etc/qemu-ifup.1: could not launch network script qemu-system-x86_64: -net tap,ifname=tashi4.0,vlan=1,script=/etc/qemu-ifup.1: Device 'tap' could not be initialized  The /etc/qemu-ifup.1 script looks like this: #!/bin/sh  /sbin/ifconfig $1 0.0.0.0 up /usr/sbin/brctl addif mybridge $1 exit 0     Does the /etc/qemu-ifup.1 script exist?", "Yes.", "Does the disk image exist?", "Yes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My /tmp/<pid.err file looks like this: /etc/qemu-ifup.1: could not launch network script qemu-system-x86_64: -net tap,ifname=tashi4.0,vlan=1,script=/etc/qemu-ifup.1: Device 'tap' could not be initialized  The /etc/qemu-ifup.1 script looks like this: #!/bin/sh  /sbin/ifconfig $1 0.0.0.0 up /usr/sbin/brctl addif mybridge $1 exit 0     Does the /etc/qemu-ifup.1 script exist?", "Yes.", "Does the disk image exist?", "Yes.", "Is the name of your emulator really /usr/local/bin/qemu-system-x86_64?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yes.", "Does the disk image exist?", "Yes.", "Is the name of your emulator really /usr/local/bin/qemu-system-x86_64?", "Yes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Does the disk image exist?", "Yes.", "Is the name of your emulator really /usr/local/bin/qemu-system-x86_64?", "Yes.", "Also, do you know what prints those messages about leaking file  descriptors?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is the name of your emulator really /usr/local/bin/qemu-system-x86_64?", "Yes.", "Also, do you know what prints those messages about leaking file  descriptors?", "Probably not important, but I'd like to clean that up.", "Not really sure what prints that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yes.", "Also, do you know what prints those messages about leaking file  descriptors?", "Probably not important, but I'd like to clean that up.", "Not really sure what prints that.", "Will let you know if I find something."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also, do you know what prints those messages about leaking file  descriptors?", "Probably not important, but I'd like to clean that up.", "Not really sure what prints that.", "Will let you know if I find something.", "Thanks, Kushal   I haven't looked at the configuration file yet, please check for the  above issues first."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Probably not important, but I'd like to clean that up.", "Not really sure what prints that.", "Will let you know if I find something.", "Thanks, Kushal   I haven't looked at the configuration file yet, please check for the  above issues first.", "Greetings,  Michael.       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, I already used \"hot deployment\" of CSS files with eclipse / JBoss (JBoss uses Tomcat as web server, but don't know if there are differences when it comes to hot deployment) The only think I needed to do is to disable the browser-cache completely (possible on latest opera versions or on firefox with WebDeveloper plugin [1])  Greetings Markus D\u00f6ring  [1] http://chrispederick.com/work/webdeveloper/    -----Urspr\u00fcngliche Nachricht-----  Von: Felix Gonschorek [mailto:felix@gg-media.biz]  Gesendet: Montag, 19.", "M\u00e4rz 2007 13:44  An: adffaces-user@incubator.apache.org  Betreff: Skinning / Detecting Style-Sheet-Changes    Hello all,    i am trying to implement my own style for trinidad components.", "but for  every change i make to the css, i have to stop tomcat, clear the cache  folder and then start tomcat again.", "if i try to delete the cached css,  the file is locked and can't be deleted.", "Changes to the css are not  recognized immediately."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["M\u00e4rz 2007 13:44  An: adffaces-user@incubator.apache.org  Betreff: Skinning / Detecting Style-Sheet-Changes    Hello all,    i am trying to implement my own style for trinidad components.", "but for  every change i make to the css, i have to stop tomcat, clear the cache  folder and then start tomcat again.", "if i try to delete the cached css,  the file is locked and can't be deleted.", "Changes to the css are not  recognized immediately.", "i have played around with some web.xml  config-settings, but i haven't had success yet."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["if i try to delete the cached css,  the file is locked and can't be deleted.", "Changes to the css are not  recognized immediately.", "i have played around with some web.xml  config-settings, but i haven't had success yet.", "this way it takes a lot of time to create a whole new style, is'nt there  a possibility to see immediately results after changes to the css an a  page reload?", "does sombody have a hint for me?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Changes to the css are not  recognized immediately.", "i have played around with some web.xml  config-settings, but i haven't had success yet.", "this way it takes a lot of time to create a whole new style, is'nt there  a possibility to see immediately results after changes to the css an a  page reload?", "does sombody have a hint for me?", "i would greatly appreciate some help."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Okay, that seems to edit or modify some scripts, but which file should I do this?", "Inside Taverna server directory or in the java client?", "Please explain more   Thanks    -----Original Message----- From: alaninmcr [mailto:alaninmcr@googlemail.com]  Sent: Thursday, March 26, 2015 9:03 PM To: users@taverna.incubator.apache.org Subject: Re: A few questions before choosing Taverna for our project  On 26/03/2015 19:45, Ahmad Aburomman wrote:  Dear Stian,   I'm working on workflow and I ran it perfectly (tomcat6 and RESTful),   I got the output without provenance bundle  Before you started the run, did you set generate-provenance to true?", "Also, you need to make sure you are not sending the outputs to a Baclava file.", "See http://dev.mygrid.org.uk/wiki/display/tav250/REST+API#RESTAPI-Resource:/runs/{id}/generate-provenance   I don't know how to configure Taverna server to enable provenance   data, can you guide me please?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please explain more   Thanks    -----Original Message----- From: alaninmcr [mailto:alaninmcr@googlemail.com]  Sent: Thursday, March 26, 2015 9:03 PM To: users@taverna.incubator.apache.org Subject: Re: A few questions before choosing Taverna for our project  On 26/03/2015 19:45, Ahmad Aburomman wrote:  Dear Stian,   I'm working on workflow and I ran it perfectly (tomcat6 and RESTful),   I got the output without provenance bundle  Before you started the run, did you set generate-provenance to true?", "Also, you need to make sure you are not sending the outputs to a Baclava file.", "See http://dev.mygrid.org.uk/wiki/display/tav250/REST+API#RESTAPI-Resource:/runs/{id}/generate-provenance   I don't know how to configure Taverna server to enable provenance   data, can you guide me please?", "It is not part of the server configuration.", "It is set for each run."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also, you need to make sure you are not sending the outputs to a Baclava file.", "See http://dev.mygrid.org.uk/wiki/display/tav250/REST+API#RESTAPI-Resource:/runs/{id}/generate-provenance   I don't know how to configure Taverna server to enable provenance   data, can you guide me please?", "It is not part of the server configuration.", "It is set for each run.", "Kind regards   Ahmad  Alan   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Any way it sent the important part which is the code.", "I still have the problem and actually the problem is simple.", "I need to  merge a set of text documents into one text document.", "To regenerate my  problem, I modified the demo7  available at http://incubator.apache.org/odftoolkit/simple/demo/demo7.html  This demo is generating a set of documents and I modified it to also merge  these documents into one document.", "I used two methods to do so:  One by using  org.odftoolkit.simple.TextDocument as following:          public static void generateAllOffersLetterTextDocument() throws  Exception {                 TextDocument templateDocument = TextDocument.loadDocument( fRootDirectory + \"/\"+ \"OfferTemplate.odt\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I need to  merge a set of text documents into one text document.", "To regenerate my  problem, I modified the demo7  available at http://incubator.apache.org/odftoolkit/simple/demo/demo7.html  This demo is generating a set of documents and I modified it to also merge  these documents into one document.", "I used two methods to do so:  One by using  org.odftoolkit.simple.TextDocument as following:          public static void generateAllOffersLetterTextDocument() throws  Exception {                 TextDocument templateDocument = TextDocument.loadDocument( fRootDirectory + \"/\"+ \"OfferTemplate.odt\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   TextDocument offerDoc = null;                 for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           offerDoc = TextDocument.loadDocument(fOutDirectory  + \"/\" + name+\"'s offer letter.odt\");                         templateDocument.insertDocument(offerDoc, \"/\" +  name);                 }                   templateDocument.save(fOutDirectory +  \"/AllOffersTextDoc.odt\");         }  And the other way is by extending  org.odftoolkit.odfdom.doc.OdfTextDocument by a new class MyTextDocument to  do exactly the same thing as follows:          public static void generateAllOffersODFDocument () throws  Exception {                 MyTextDocument allDocTextDoc = MyTextDocument.", "newTextWorkProduct(fRootDirectory + \"/\"+ \"OfferTemplate.ott\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   TextDocument offerDoc = null;                 for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           offerDoc = TextDocument.loadDocument(fOutDirectory  + \"/\" + name+\"'s offer letter.odt\");                         templateDocument.insertDocument(offerDoc, \"/\" +  name);                 }                   templateDocument.save(fOutDirectory +  \"/AllOffersTextDoc.odt\");         }  And the other way is by extending  org.odftoolkit.odfdom.doc.OdfTextDocument by a new class MyTextDocument to  do exactly the same thing as follows:          public static void generateAllOffersODFDocument () throws  Exception {                 MyTextDocument allDocTextDoc = MyTextDocument.", "newTextWorkProduct(fRootDirectory + \"/\"+ \"OfferTemplate.ott\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           allDocTextDoc.includeFile(fOutDirectory + \"/\" +  name+\"'s offer letter.odt\", \"/\" + name);                 }                    allDocTextDoc.save (fOutDirectory + \"/AllOffersODFDoc.odt\" );         }  While the includeFile is as follows:          public void includeFile (String filePath, String documentPath)  throws Exception{                 OdfPackageDocument newDocument;                   try {                         newDocument = OdfPackageDocument.loadDocument (filePath);                         insertDocument(newDocument, documentPath);                 } catch (Exception exception) {                         throw new Exception (\"Failed to include the file:  \" + filePath, exception);                 }         }  The results were exactly the same, the generated files didn't show the  agregated documents, only the first contents of the document, however the  conetns of the documents are inserted in their coresponding folders in the  zip file.", "In the attached zip file in the previous message, I included the source  code (FieldsDemo.java and MyTextDocument.java) as well as the source files  and the generated files.", "Sorry again for the previous messgae."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["can't verify this.", "Do you have something special in your settings.xml ?", "-M  On 1/11/07, Pfau, Oliver <oliver.pfau@siemens.com wrote:  Hi,   I tried to build the trunk and stable-06-dec branch but got an  excpetion.", "What's wrong ?", "[INFO]  ------------------------------------------------------------------------  ----  [INFO] Building Maven i18n Plugin  [INFO]    task-segment: [install]  [INFO]  ------------------------------------------------------------------------  ----  [INFO] [plugin:descriptor]  [INFO] Using 2 extractors."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do you have something special in your settings.xml ?", "-M  On 1/11/07, Pfau, Oliver <oliver.pfau@siemens.com wrote:  Hi,   I tried to build the trunk and stable-06-dec branch but got an  excpetion.", "What's wrong ?", "[INFO]  ------------------------------------------------------------------------  ----  [INFO] Building Maven i18n Plugin  [INFO]    task-segment: [install]  [INFO]  ------------------------------------------------------------------------  ----  [INFO] [plugin:descriptor]  [INFO] Using 2 extractors.", "[INFO] Applying extractor for language: java  [INFO]  ------------------------------------------------------------------------  [ERROR] FATAL ERROR  [INFO]  ------------------------------------------------------------------------  [INFO] syntax error @[861,1] in  file:/D:/3rdparty/plugins/maven-i18n-plugin/src/main/java/org/apache/myf  aces/trinidadbuild/plugin/i18n/uixto  ols/JSLocaleElementsGenerator.java  [INFO]  ------------------------------------------------------------------------  [INFO] Trace  com.thoughtworks.qdox.parser.ParseException: syntax error @[861,1] in  file:/D:/3rdparty/plugins/maven-i18n-plugin/src/main/java/org/apache/m  yfaces/trinidadbuild/plugin/i18n/uixtools/JSLocaleElementsGenerator.java          at  com.thoughtworks.qdox.parser.impl.Parser.yyerror(Parser.java:638)          at  com.thoughtworks.qdox.parser.impl.Parser.yyparse(Parser.java:747)          at  com.thoughtworks.qdox.parser.impl.Parser.parse(Parser.java:619)          at  com.thoughtworks.qdox.JavaDocBuilder.addSource(JavaDocBuilder.java:300)          at  com.thoughtworks.qdox.JavaDocBuilder.addSource(JavaDocBuilder.java:316)          at  com.thoughtworks.qdox.JavaDocBuilder.addSource(JavaDocBuilder.java:312)          at  com.thoughtworks.qdox.JavaDocBuilder$1.visitFile(JavaDocBuilder.java:369  )          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:43)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.walk(DirectorySca  nner.java:34)          at  com.thoughtworks.qdox.directorywalker.DirectoryScanner.scan(DirectorySca  nner.java:52)          at  com.thoughtworks.qdox.JavaDocBuilder.addSourceTree(JavaDocBuilder.java:3  66)          at  org.apache.maven.tools.plugin.extractor.java.JavaMojoDescriptorExtractor  .execute(JavaMojoDescriptorExtractor.java:520)          at  org.apache.maven.tools.plugin.scanner.DefaultMojoScanner.populatePluginD  escriptor(DefaultMojoScanner.java:84)          at  org.apache.maven.plugin.plugin.AbstractGeneratorMojo.execute(AbstractGen  eratorMojo.java:135)          at  org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginMa  nager.java:412)          at  org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(Default  LifecycleExecutor.java:534)          at  org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalWithLifec  ycle(DefaultLifecycleExecutor.java:475)          at  org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultL  ifecycleExecutor.java:454)          at  org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandle  Failures(DefaultLifecycleExecutor.java:306)          at  org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(  DefaultLifecycleExecutor.java:273)          at  org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifec  ycleExecutor.java:140)          at  org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:322)          at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:115)          at org.apache.maven.cli.MavenCli.main(MavenCli.java:256)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav  a:39)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor  Impl.java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)          at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)          at  org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)          at org.codehaus.classworlds.Launcher.main(Launcher.java:375)      --  Matthias Wessendorf http://tinyurl.com/fmywh  further stuff: blog: http://jroller.com/page/mwessendorf mail: mwessendorf-at-gmail-dot-com  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks to Daniel and Arthur, we now have a few versions of the docs up on ReadTheDocs.", "https://readthedocs.org/projects/airflow/  I updated the wiki with the links.", "Let me know if you think we need to publish older versions of the docs.", "Max  On Wed, Mar 8, 2017 at 2:16 PM, Maxime Beauchemin < maximebeauchemin@gmail.com wrote:   Thanks for taking that on.", "It's mostly an effort in mocking all the libs  that cannot be built in the readthedocs environment."], "labels": ["1", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://readthedocs.org/projects/airflow/  I updated the wiki with the links.", "Let me know if you think we need to publish older versions of the docs.", "Max  On Wed, Mar 8, 2017 at 2:16 PM, Maxime Beauchemin < maximebeauchemin@gmail.com wrote:   Thanks for taking that on.", "It's mostly an effort in mocking all the libs  that cannot be built in the readthedocs environment.", "This may require a  specific `requirements.txt`-`readthedocs-reqs.txt` for that purpose with  only the core dependencies needed to parse the modules."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max  On Wed, Mar 8, 2017 at 2:16 PM, Maxime Beauchemin < maximebeauchemin@gmail.com wrote:   Thanks for taking that on.", "It's mostly an effort in mocking all the libs  that cannot be built in the readthedocs environment.", "This may require a  specific `requirements.txt`-`readthedocs-reqs.txt` for that purpose with  only the core dependencies needed to parse the modules.", "Max   On Wed, Mar 8, 2017 at 1:40 PM, Daniel Huang <dxhuang@gmail.com wrote:   Just filed https://issues.apache.org/jira/browse/AIRFLOW-956 to setup  readthedocs.", "I'll give it a shot."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This may require a  specific `requirements.txt`-`readthedocs-reqs.txt` for that purpose with  only the core dependencies needed to parse the modules.", "Max   On Wed, Mar 8, 2017 at 1:40 PM, Daniel Huang <dxhuang@gmail.com wrote:   Just filed https://issues.apache.org/jira/browse/AIRFLOW-956 to setup  readthedocs.", "I'll give it a shot.", "On Wed, Mar 8, 2017 at 12:29 PM, Jake Gysland <jgysland@optoro.com  wrote:    I'm glad there's new documentation available for 1.8.0, but replacing  the   documentation for the latest stable release with the documentation for a   future release candidate can lead to experiences that are ...  frustrating,   as I've discovered this afternoon while trying to help a colleague QA a  DAG   and finding documentation for CLI flags that don't exist.", "Can we at least get a version number (or... a commit hash?)"], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["I'll give it a shot.", "On Wed, Mar 8, 2017 at 12:29 PM, Jake Gysland <jgysland@optoro.com  wrote:    I'm glad there's new documentation available for 1.8.0, but replacing  the   documentation for the latest stable release with the documentation for a   future release candidate can lead to experiences that are ...  frustrating,   as I've discovered this afternoon while trying to help a colleague QA a  DAG   and finding documentation for CLI flags that don't exist.", "Can we at least get a version number (or... a commit hash?)", "into the   documentation somewhere that's reasonably visible?", "On Mon, Mar 6, 2017 at 12:20 PM, Maxime Beauchemin <   maximebeauchemin@gmail.com wrote:      I just pushed a new version of the docs out."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Mar 8, 2017 at 12:29 PM, Jake Gysland <jgysland@optoro.com  wrote:    I'm glad there's new documentation available for 1.8.0, but replacing  the   documentation for the latest stable release with the documentation for a   future release candidate can lead to experiences that are ...  frustrating,   as I've discovered this afternoon while trying to help a colleague QA a  DAG   and finding documentation for CLI flags that don't exist.", "Can we at least get a version number (or... a commit hash?)", "into the   documentation somewhere that's reasonably visible?", "On Mon, Mar 6, 2017 at 12:20 PM, Maxime Beauchemin <   maximebeauchemin@gmail.com wrote:      I just pushed a new version of the docs out.", "We may want to set up a service like https://readthedocs.org/ to  offer    versioned docs."], "labels": ["1", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Can we at least get a version number (or... a commit hash?)", "into the   documentation somewhere that's reasonably visible?", "On Mon, Mar 6, 2017 at 12:20 PM, Maxime Beauchemin <   maximebeauchemin@gmail.com wrote:      I just pushed a new version of the docs out.", "We may want to set up a service like https://readthedocs.org/ to  offer    versioned docs.", "It would require setting up some \"mocking\" of external   libs    that can't be installed on the RTD build nodes and that actually   shouldn't    be required to build the docs."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["into the   documentation somewhere that's reasonably visible?", "On Mon, Mar 6, 2017 at 12:20 PM, Maxime Beauchemin <   maximebeauchemin@gmail.com wrote:      I just pushed a new version of the docs out.", "We may want to set up a service like https://readthedocs.org/ to  offer    versioned docs.", "It would require setting up some \"mocking\" of external   libs    that can't be installed on the RTD build nodes and that actually   shouldn't    be required to build the docs.", "From my understanding the mocking of  libs    would have to be done here in this file:    https://github.com/airbnb/superset/blob/master/docs/conf.py       It would be great if someone wanted to take that on."], "labels": ["0", "0", "1", "0", "1"]}
{"abstract_id": 0, "sentences": ["On Mon, Mar 6, 2017 at 12:20 PM, Maxime Beauchemin <   maximebeauchemin@gmail.com wrote:      I just pushed a new version of the docs out.", "We may want to set up a service like https://readthedocs.org/ to  offer    versioned docs.", "It would require setting up some \"mocking\" of external   libs    that can't be installed on the RTD build nodes and that actually   shouldn't    be required to build the docs.", "From my understanding the mocking of  libs    would have to be done here in this file:    https://github.com/airbnb/superset/blob/master/docs/conf.py       It would be great if someone wanted to take that on.", "Max       On Mon, Mar 6, 2017 at 8:46 AM, Michael Gong <gonwg@hotmail.com  wrote:        Hi,             Does anyone know where I can find the documentation for the coming    airflow     1.8.0 release ?"], "labels": ["0", "1", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["We may want to set up a service like https://readthedocs.org/ to  offer    versioned docs.", "It would require setting up some \"mocking\" of external   libs    that can't be installed on the RTD build nodes and that actually   shouldn't    be required to build the docs.", "From my understanding the mocking of  libs    would have to be done here in this file:    https://github.com/airbnb/superset/blob/master/docs/conf.py       It would be great if someone wanted to take that on.", "Max       On Mon, Mar 6, 2017 at 8:46 AM, Michael Gong <gonwg@hotmail.com  wrote:        Hi,             Does anyone know where I can find the documentation for the coming    airflow     1.8.0 release ?", "thanks."], "labels": ["1", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["It would require setting up some \"mocking\" of external   libs    that can't be installed on the RTD build nodes and that actually   shouldn't    be required to build the docs.", "From my understanding the mocking of  libs    would have to be done here in this file:    https://github.com/airbnb/superset/blob/master/docs/conf.py       It would be great if someone wanted to take that on.", "Max       On Mon, Mar 6, 2017 at 8:46 AM, Michael Gong <gonwg@hotmail.com  wrote:        Hi,             Does anyone know where I can find the documentation for the coming    airflow     1.8.0 release ?", "thanks.", "Michael             ________________________________     From: Michael Gong <gonwg@hotmail.com     Sent: Monday, March 6, 2017 2:37 PM     To: dev; Michael Gong     Subject: Re: Airflow running different with different user id ?"], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From my understanding the mocking of  libs    would have to be done here in this file:    https://github.com/airbnb/superset/blob/master/docs/conf.py       It would be great if someone wanted to take that on.", "Max       On Mon, Mar 6, 2017 at 8:46 AM, Michael Gong <gonwg@hotmail.com  wrote:        Hi,             Does anyone know where I can find the documentation for the coming    airflow     1.8.0 release ?", "thanks.", "Michael             ________________________________     From: Michael Gong <gonwg@hotmail.com     Sent: Monday, March 6, 2017 2:37 PM     To: dev; Michael Gong     Subject: Re: Airflow running different with different user id ?", "Hi, Dan,         The only doc mentioning \"run_as\" was in the airflow.operators."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max       On Mon, Mar 6, 2017 at 8:46 AM, Michael Gong <gonwg@hotmail.com  wrote:        Hi,             Does anyone know where I can find the documentation for the coming    airflow     1.8.0 release ?", "thanks.", "Michael             ________________________________     From: Michael Gong <gonwg@hotmail.com     Sent: Monday, March 6, 2017 2:37 PM     To: dev; Michael Gong     Subject: Re: Airflow running different with different user id ?", "Hi, Dan,         The only doc mentioning \"run_as\" was in the airflow.operators.", "HiveOperator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["thanks.", "Michael             ________________________________     From: Michael Gong <gonwg@hotmail.com     Sent: Monday, March 6, 2017 2:37 PM     To: dev; Michael Gong     Subject: Re: Airflow running different with different user id ?", "Hi, Dan,         The only doc mentioning \"run_as\" was in the airflow.operators.", "HiveOperator.", "Is this what you mean \"run_as\"?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Michael             ________________________________     From: Michael Gong <gonwg@hotmail.com     Sent: Monday, March 6, 2017 2:37 PM     To: dev; Michael Gong     Subject: Re: Airflow running different with different user id ?", "Hi, Dan,         The only doc mentioning \"run_as\" was in the airflow.operators.", "HiveOperator.", "Is this what you mean \"run_as\"?", "If not, can you provide more information about it?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, Dan,         The only doc mentioning \"run_as\" was in the airflow.operators.", "HiveOperator.", "Is this what you mean \"run_as\"?", "If not, can you provide more information about it?", "We are very   interested     to know more ."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["HiveOperator.", "Is this what you mean \"run_as\"?", "If not, can you provide more information about it?", "We are very   interested     to know more .", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is this what you mean \"run_as\"?", "If not, can you provide more information about it?", "We are very   interested     to know more .", "Thanks.", "Michael         Sent from my PP\u2022KING\u2122 smartphone         On Mar 3, 2017 3:57 PM, Michael Gong <gonwg@hotmail.com wrote:     Hi, Dan,             Thanks for the encouraging news."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If not, can you provide more information about it?", "We are very   interested     to know more .", "Thanks.", "Michael         Sent from my PP\u2022KING\u2122 smartphone         On Mar 3, 2017 3:57 PM, Michael Gong <gonwg@hotmail.com wrote:     Hi, Dan,             Thanks for the encouraging news.", "Maybe you could direct me to the documentation about \"run_as\" so I   could     see whether it is for my purpose."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We are very   interested     to know more .", "Thanks.", "Michael         Sent from my PP\u2022KING\u2122 smartphone         On Mar 3, 2017 3:57 PM, Michael Gong <gonwg@hotmail.com wrote:     Hi, Dan,             Thanks for the encouraging news.", "Maybe you could direct me to the documentation about \"run_as\" so I   could     see whether it is for my purpose.", "I guess it will be passed as an argument for the DAG object , right  ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks.", "Michael         Sent from my PP\u2022KING\u2122 smartphone         On Mar 3, 2017 3:57 PM, Michael Gong <gonwg@hotmail.com wrote:     Hi, Dan,             Thanks for the encouraging news.", "Maybe you could direct me to the documentation about \"run_as\" so I   could     see whether it is for my purpose.", "I guess it will be passed as an argument for the DAG object , right  ?", "Thanks again."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Michael         Sent from my PP\u2022KING\u2122 smartphone         On Mar 3, 2017 3:57 PM, Michael Gong <gonwg@hotmail.com wrote:     Hi, Dan,             Thanks for the encouraging news.", "Maybe you could direct me to the documentation about \"run_as\" so I   could     see whether it is for my purpose.", "I guess it will be passed as an argument for the DAG object , right  ?", "Thanks again.", "Michael                     ________________________________     From: Dan Davydov <dan.davydov@airbnb.com.INVALID     Sent: Friday, March 3, 2017 8:44 PM     To: dev@airflow.incubator.apache.org     Subject: Re: Airflow running different with different user id ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Maybe you could direct me to the documentation about \"run_as\" so I   could     see whether it is for my purpose.", "I guess it will be passed as an argument for the DAG object , right  ?", "Thanks again.", "Michael                     ________________________________     From: Dan Davydov <dan.davydov@airbnb.com.INVALID     Sent: Friday, March 3, 2017 8:44 PM     To: dev@airflow.incubator.apache.org     Subject: Re: Airflow running different with different user id ?", "Within a couple of weeks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I guess it will be passed as an argument for the DAG object , right  ?", "Thanks again.", "Michael                     ________________________________     From: Dan Davydov <dan.davydov@airbnb.com.INVALID     Sent: Friday, March 3, 2017 8:44 PM     To: dev@airflow.incubator.apache.org     Subject: Re: Airflow running different with different user id ?", "Within a couple of weeks.", "On Fri, Mar 3, 2017 at 12:34 PM, Michael Gong <gonwg@hotmail.com   wrote:          When approximately will it be released?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Michael                     ________________________________     From: Dan Davydov <dan.davydov@airbnb.com.INVALID     Sent: Friday, March 3, 2017 8:44 PM     To: dev@airflow.incubator.apache.org     Subject: Re: Airflow running different with different user id ?", "Within a couple of weeks.", "On Fri, Mar 3, 2017 at 12:34 PM, Michael Gong <gonwg@hotmail.com   wrote:          When approximately will it be released?", "Sent from my PP\u2022KING\u2122 smartphone           On Mar 3, 2017 1:42 PM, Dan Davydov <dan.davydov@airbnb.com  .INVALID      wrote:      Yes it is starting on 1.8.0 which will be released soon, you can  look    in      the documentation/grep for \"run_as\".", "On Mar 3, 2017 8:50 AM, \"Michael Gong\" <gonwg@hotmail.com wrote:            Hi,                   Suppose I have 1 airflow instance running 2 different DAGs, is  it      possible       to specify the 2 DAGs running under 2 different ids ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Mar 3, 2017 at 12:34 PM, Michael Gong <gonwg@hotmail.com   wrote:          When approximately will it be released?", "Sent from my PP\u2022KING\u2122 smartphone           On Mar 3, 2017 1:42 PM, Dan Davydov <dan.davydov@airbnb.com  .INVALID      wrote:      Yes it is starting on 1.8.0 which will be released soon, you can  look    in      the documentation/grep for \"run_as\".", "On Mar 3, 2017 8:50 AM, \"Michael Gong\" <gonwg@hotmail.com wrote:            Hi,                   Suppose I have 1 airflow instance running 2 different DAGs, is  it      possible       to specify the 2 DAGs running under 2 different ids ?", "Any advises are welcomed.", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sent from my PP\u2022KING\u2122 smartphone           On Mar 3, 2017 1:42 PM, Dan Davydov <dan.davydov@airbnb.com  .INVALID      wrote:      Yes it is starting on 1.8.0 which will be released soon, you can  look    in      the documentation/grep for \"run_as\".", "On Mar 3, 2017 8:50 AM, \"Michael Gong\" <gonwg@hotmail.com wrote:            Hi,                   Suppose I have 1 airflow instance running 2 different DAGs, is  it      possible       to specify the 2 DAGs running under 2 different ids ?", "Any advises are welcomed.", "Thanks.", "Michael                                                   --     Jake Gysland     Data Pipeline Engineer     www.optoro.com     [image: Optoro, Inc.] <http://www.optoro.com/       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It starts its own actor system.", "The fey-test-actor.jar is included in fey-core.", "It is only a performer test used to make sure fey handles its performers in the right way.", "For now I would proceed removing the tests from assembly so you can generate the executable and proceed with the steps.", "I will have to do some investigation and see if I figure out why the tests breaks for you."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It is only a performer test used to make sure fey handles its performers in the right way.", "For now I would proceed removing the tests from assembly so you can generate the executable and proceed with the steps.", "I will have to do some investigation and see if I figure out why the tests breaks for you.", "Regards  Barbara Gomes +1 (650) 713-6092   On Feb 19, 2017, at 11:41 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi Barbara,    The same five tests fail with this change.", "Doesn't the iota-fey-core.jar file have to be created before running tests?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I will have to do some investigation and see if I figure out why the tests breaks for you.", "Regards  Barbara Gomes +1 (650) 713-6092   On Feb 19, 2017, at 11:41 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi Barbara,    The same five tests fail with this change.", "Doesn't the iota-fey-core.jar file have to be created before running tests?", "[gunnar@localhost incubator-iota]$ find fey-core/target/scala-2.11 | grep jar  fey-core/target/scala-2.11/test-classes/fey-test-actor.jar    Thanks,    Gunnar      On Sun, Feb 19, 2017 at 11:21 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    I have noticed that when assembly runs the test, in some cases, some of the tests fails.", "I will need sometime to investigate why it does not work 100% when assembly calls it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards  Barbara Gomes +1 (650) 713-6092   On Feb 19, 2017, at 11:41 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi Barbara,    The same five tests fail with this change.", "Doesn't the iota-fey-core.jar file have to be created before running tests?", "[gunnar@localhost incubator-iota]$ find fey-core/target/scala-2.11 | grep jar  fey-core/target/scala-2.11/test-classes/fey-test-actor.jar    Thanks,    Gunnar      On Sun, Feb 19, 2017 at 11:21 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    I have noticed that when assembly runs the test, in some cases, some of the tests fails.", "I will need sometime to investigate why it does not work 100% when assembly calls it.", "For now, you can run the tests separately:     sbt   projetc fey-core   test    and for now add the following line to the fey project in  project/Build.scala    test in assembly := {}    it should look like:  <Screen Shot 2017-02-19 at 10.18.47 AM.png    Please, let me know if that works for you    Regards    On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  HI Gunnar,     Thanks for the info."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[gunnar@localhost incubator-iota]$ find fey-core/target/scala-2.11 | grep jar  fey-core/target/scala-2.11/test-classes/fey-test-actor.jar    Thanks,    Gunnar      On Sun, Feb 19, 2017 at 11:21 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    I have noticed that when assembly runs the test, in some cases, some of the tests fails.", "I will need sometime to investigate why it does not work 100% when assembly calls it.", "For now, you can run the tests separately:     sbt   projetc fey-core   test    and for now add the following line to the fey project in  project/Build.scala    test in assembly := {}    it should look like:  <Screen Shot 2017-02-19 at 10.18.47 AM.png    Please, let me know if that works for you    Regards    On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  HI Gunnar,     Thanks for the info.", "I will try to reproduce the issue on CentOS.", "If you re-run the tests, do you still get the SAME failed tests or it fails but in different tests?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I will need sometime to investigate why it does not work 100% when assembly calls it.", "For now, you can run the tests separately:     sbt   projetc fey-core   test    and for now add the following line to the fey project in  project/Build.scala    test in assembly := {}    it should look like:  <Screen Shot 2017-02-19 at 10.18.47 AM.png    Please, let me know if that works for you    Regards    On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  HI Gunnar,     Thanks for the info.", "I will try to reproduce the issue on CentOS.", "If you re-run the tests, do you still get the SAME failed tests or it fails but in different tests?", "Regards    On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:  Hi Barbara:    I'm on an x86 box with:    [gunnar@localhost ~]$ lsb_release -a  LSB Version:    :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch  Distributor ID: CentOS  Description:    CentOS release 6.8 (Final)  Release:        6.8  Codename:       Final    It may be too weak for what's needed here."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For now, you can run the tests separately:     sbt   projetc fey-core   test    and for now add the following line to the fey project in  project/Build.scala    test in assembly := {}    it should look like:  <Screen Shot 2017-02-19 at 10.18.47 AM.png    Please, let me know if that works for you    Regards    On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  HI Gunnar,     Thanks for the info.", "I will try to reproduce the issue on CentOS.", "If you re-run the tests, do you still get the SAME failed tests or it fails but in different tests?", "Regards    On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:  Hi Barbara:    I'm on an x86 box with:    [gunnar@localhost ~]$ lsb_release -a  LSB Version:    :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch  Distributor ID: CentOS  Description:    CentOS release 6.8 (Final)  Release:        6.8  Codename:       Final    It may be too weak for what's needed here.", "I can spin up a AWS micro instance once I know what OS to install."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I will try to reproduce the issue on CentOS.", "If you re-run the tests, do you still get the SAME failed tests or it fails but in different tests?", "Regards    On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:  Hi Barbara:    I'm on an x86 box with:    [gunnar@localhost ~]$ lsb_release -a  LSB Version:    :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch  Distributor ID: CentOS  Description:    CentOS release 6.8 (Final)  Release:        6.8  Codename:       Final    It may be too weak for what's needed here.", "I can spin up a AWS micro instance once I know what OS to install.", "Thanks,    Gunnar    On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    Which Operational System are you using?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards    On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:  Hi Barbara:    I'm on an x86 box with:    [gunnar@localhost ~]$ lsb_release -a  LSB Version:    :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch  Distributor ID: CentOS  Description:    CentOS release 6.8 (Final)  Release:        6.8  Codename:       Final    It may be too weak for what's needed here.", "I can spin up a AWS micro instance once I know what OS to install.", "Thanks,    Gunnar    On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    Which Operational System are you using?", "sbt picks the order of the tests execution and it might be the problem on your machine.", "As you can see in the picture bellow, I pulled the repo from github and run the tests and all of them succeeded        I'm using Mac OS    About the ERROR messages during the tests, those are fey logs caused intentionally by the tests."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can spin up a AWS micro instance once I know what OS to install.", "Thanks,    Gunnar    On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    Which Operational System are you using?", "sbt picks the order of the tests execution and it might be the problem on your machine.", "As you can see in the picture bellow, I pulled the repo from github and run the tests and all of them succeeded        I'm using Mac OS    About the ERROR messages during the tests, those are fey logs caused intentionally by the tests.", "Also, try to run the tests separated:     sbt   project fey-core   test    Regards,    On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com wrote:  Thanks for pointing this out will take a look."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,    Gunnar    On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    Which Operational System are you using?", "sbt picks the order of the tests execution and it might be the problem on your machine.", "As you can see in the picture bellow, I pulled the repo from github and run the tests and all of them succeeded        I'm using Mac OS    About the ERROR messages during the tests, those are fey logs caused intentionally by the tests.", "Also, try to run the tests separated:     sbt   project fey-core   test    Regards,    On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com wrote:  Thanks for pointing this out will take a look.", "-Tony      On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi,    I verified the required versions and then I ran assembly of the fey-core project."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["sbt picks the order of the tests execution and it might be the problem on your machine.", "As you can see in the picture bellow, I pulled the repo from github and run the tests and all of them succeeded        I'm using Mac OS    About the ERROR messages during the tests, those are fey logs caused intentionally by the tests.", "Also, try to run the tests separated:     sbt   project fey-core   test    Regards,    On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com wrote:  Thanks for pointing this out will take a look.", "-Tony      On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi,    I verified the required versions and then I ran assembly of the fey-core project.", "5 failures."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As you can see in the picture bellow, I pulled the repo from github and run the tests and all of them succeeded        I'm using Mac OS    About the ERROR messages during the tests, those are fey logs caused intentionally by the tests.", "Also, try to run the tests separated:     sbt   project fey-core   test    Regards,    On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com wrote:  Thanks for pointing this out will take a look.", "-Tony      On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi,    I verified the required versions and then I ran assembly of the fey-core project.", "5 failures.", "[info] - should result in creating a global Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in one global actor created for orchestration  [info] - should result in right number of running actors  [info] Stopping performer inside ensemble  [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer PERFORMER-SCHEDULER  org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER          at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(Ensemble.scala:60)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should Send stop message to monitor  [info] Stopping ensemble  [info] - should Send stop message to monitor  [info] - should result in no orchestration running  [info] - should not affect global performer  [info] Stopping global performer  [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestActorRef.scala:60)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:250)          at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:283)          at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:191)          at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch.scala:147)          at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)          at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:402)          at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:213)          at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)          at akka.actor.ActorCell.terminate(ActorCell.scala:374)          at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)          at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)          at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in restart the orchestration  [info] - should all previous actors restarted  [info] Stopping orchestration  [info] - should result in empty global  [info] EnsembleSpec:  [info] Creating a simple Ensemble MY-ENSEMBLE-0005  [info] - should result in creation of Ensemble actor 'akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'  [info] - should result in sending START to monitor actor  [info] - should result in creation of Performer 'TEST-0004'  [info] - should result in Empty state variable Ensemble.connectors  [info] - should result in one entry added to state variable Ensemble.performer  [info] - should result in one right entry to state variable Ensemble.performers_metadata  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending Ensemble.STOP_PERFORMERS to Ensemble  [info] - should result in Terminate message of actor 'TEST-0004' and throw RestartEnsemble Exception  [info] - should result in Performer 'TEST-0004' restarted  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending PoisonPill to Ensemble  [info] - should result in termination of actor 'MY-ENSEMBLE-0005'  [info] - should result in sending TERMINATE to monitor actor  [info] - should result in termination of ensemble and performer  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating more detailed Ensemble  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create connection PERFORMER-SCHEDULER - PERFORMER-PARAMS  [info] - should create 'PERFORMER-SCHEDULER' with schedule time equal to 200ms  [info] - should create 'PERFORMER-SCHEDULER' with connection to 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with no connections  [info] - should create 'PERFORMER-PARAMS' with specified params  [info] 'PERFORMER-SCHEDULER'  [info] - should produce 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] 'PERFORMER-PARAMS'  [info] - should process 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] Stopping any Performer that belongs to the Ensemble  [info] - should force restart of entire Ensemble  [info] - should result in sending STOP - RESTART to monitor actor  [info] - should keep ensemble actorRef when restarted  [info] - should stop and start the performer with a new reference  [info] Restarting an Ensemble  [info] - should Consuming left messages on Process  [info] - should Cleanup TestProbs  [info] Redefining TestProbe for performers  [info] - should start receiving messages  [info] Sending PoisonPill to detailed Ensemble  [info] - should result in termination of Ensemble  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating Ensemble with Backoff performer  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with backoff time equal to 1 second  [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal to true  [info] Performer with backoff enabled  [info] - should not process messages during the backoff period  [info] Performer with autoScale  [info] - should result in router and routees created  [info] IdentifyFeyActorsSpec:  [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] Creating a new actor in the system and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] - should result in matching paths  [info] Stopping previous added actor and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in going back to have just one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] WatchServiceReceiverSpec:  [info] Creating WatchServiceReceiver  [info] - should process initial files in the JSON repository  [info] Start a Thread with WatchServiceReceiver  [info] - should Start Thread  [info] Start watching directory  [info] - should Starting receiving CREATED event  [info] - should Starting receiving UPDATE event  [info] processJson  [info] - should log to warn level when json has invalid schema  [info] interrupt watchservice  [info] - should interrupt thread  [info] FeyCoreSpec:  [info] Creating FeyCore  [info] - should result in creating a child actor with the name 'FEY_IDENTIFIER'  [info] - should result in sending START message to Monitor actor  [info] Sending FeyCore.START to FeyCore  [info] - should result in creating a child actor with the name 'JSON_RECEIVER'  [info] - should result in starting FeyWatchService Thread  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'TEST-ACTOR'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'TEST-ACTOR'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command to FeyCore  [info] - should result in creating a new Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command and DELETE ensemble to FeyCore  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command and same Timestamp to FeyCore  [info] - should result in logging a 'not recreated' message at Warn  [info] Sending FeyCore.JSON_TREE to FeyCore  [info] - should result in logging a 6 path messages at Info  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command to FeyCore  [info] - should result in termination of Orchestration with the name 'TEST-ACTOR'  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in removing key 'TEST-ACTOR' at FEY_CACHE.activeOrchestrations  [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore  [info] - should result in termination of 'TEST-ORCH-2' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) had size 2 instead of expected size 1 (FeyCoreSpec.scala:144)  [info] - should result in sending Terminate message to Monitor actor *** FAILED ***  [info]   java.lang.AssertionError: assertion failed: timeout (1 second) during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TERMINATE  [info]   at scala.Predef$.assert(Predef.scala:170)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass_internal(TestKit.scala:435)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass(TestKit.scala:431)  [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)  [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)  [info]   ...  [info] - should result in empty FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) was not empty (FeyCoreSpec.scala:153)  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore of a GenericReceiverActor  [info] - should result in creating an Orchestration child actor with the name 'RECEIVER_ORCHESTRATION'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'  [info] - should result in creating a Performer child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVER_ORCHESTRATION'  [info] Sending PROCESS message to the Receiver Performer  [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'  [info] - should result in one new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVED-BY-ACTOR-RECEIVER' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"RECEIVED-BY-ACTOR-RECEIVER\" - (213263914979,Actor[akka://FEY-MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 4 instead of expected size 2 (FeyCoreSpec.scala:200)  [info] Sending PROCESS message to the Receiver Performer with command DELETE  [info] - should STOP running orchestration  [info] - should result in one entry in FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 3 instead of expected size 1 (FeyCoreSpec.scala:213)  [info] Sending PROCESS message to Receiver with checkpoint enabled  [info] - should Save received JSON to checkpoint dir  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND GLOBAL performer command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'GLOBAL-PERFORMER'  [info] - should result in creating an Ensemble child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL'  [info] - should result in creating a global Performer child actor with the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'GLOBAL-PERFORMER'  [info] - should result in one global actor created for orchestration  [info] - should result in globa metadata add to table  [info] - should result in right running actors  [info] Stopping Global actor  [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3] [akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in sending logging error  [info] - should result in orchestration restarted  [info] - should all previous actors restarted  [info] Stopping orchestration with global performer  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in no global actors for orchestration  [info] Stopping FeyCore  [info] - should result in sending STOP message to Monitor actor  [info] BaseAkkaSpec:  [info] JsonReceiverSpec:  [info] Executing validJson in JsonReceiver  [info] - should return false when json schema is not right  [info] - should log message to Error  [info] - should return true when Json schema is valid  [info] Executing checkForLocation in JsonReceiver  [info] - should log message at Debug level  [info] - should download jar dynamically from URL  [info] Start a Thread with the JSON receiver  [info] - should Start Thread  [info] - should execute execute() method inside run  [info] Interrupting the receiver Thread  [info] - should Throw Interrupted exception  [info] - should execute exceptionOnRun method  [info] FeyGenericActorSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] Backoff of GenericActor  [info] - should be zero until the first PROCESS message  [info] - should change when first PROCESS message was received  [info] Sending PROCESS message to GenericActor  [info] - should call processMessage method  [info] customReceive method  [info] - should process any non treated message  [info] Sending PROCESS message to GenericActor  [info] - should be discarded when backoff is enabled  [info] - should be processed when backoff has finished  [info] Calling startBackoff  [info] - should set endBackoff with time now  [info] Calling propagateMessage  [info] - should send message to connectTo actors  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors  [info] Creating GenericActor with schedule anc backoff equal to zero  [info] - should not start a scheduler  [info] - should result in one active actor  [info] - should result in no discarded PROCESS messages  [info] FeyGenericActorReceiverSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] - should result in normal functioning of GenericActor  [info] Sending PROCESS message to GenericReceiver  [info] - should log message to Warn saying that the JSON could not be forwarded to FeyCore when JSON is invalid  [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON to be processed has a valid schema  [info] - should Download jar from location and send ORCHESTRATION_RECEIVED to FeyCore when JSON has a location defined  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors    CLeaning up[info] Run completed in 44 seconds, 724 milliseconds."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also, try to run the tests separated:     sbt   project fey-core   test    Regards,    On Sat, Feb 18, 2017 at 11:43 PM, Tony Faustini <tony@litbit.com wrote:  Thanks for pointing this out will take a look.", "-Tony      On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi,    I verified the required versions and then I ran assembly of the fey-core project.", "5 failures.", "[info] - should result in creating a global Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in one global actor created for orchestration  [info] - should result in right number of running actors  [info] Stopping performer inside ensemble  [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer PERFORMER-SCHEDULER  org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER          at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(Ensemble.scala:60)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should Send stop message to monitor  [info] Stopping ensemble  [info] - should Send stop message to monitor  [info] - should result in no orchestration running  [info] - should not affect global performer  [info] Stopping global performer  [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestActorRef.scala:60)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:250)          at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:283)          at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:191)          at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch.scala:147)          at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)          at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:402)          at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:213)          at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)          at akka.actor.ActorCell.terminate(ActorCell.scala:374)          at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)          at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)          at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in restart the orchestration  [info] - should all previous actors restarted  [info] Stopping orchestration  [info] - should result in empty global  [info] EnsembleSpec:  [info] Creating a simple Ensemble MY-ENSEMBLE-0005  [info] - should result in creation of Ensemble actor 'akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'  [info] - should result in sending START to monitor actor  [info] - should result in creation of Performer 'TEST-0004'  [info] - should result in Empty state variable Ensemble.connectors  [info] - should result in one entry added to state variable Ensemble.performer  [info] - should result in one right entry to state variable Ensemble.performers_metadata  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending Ensemble.STOP_PERFORMERS to Ensemble  [info] - should result in Terminate message of actor 'TEST-0004' and throw RestartEnsemble Exception  [info] - should result in Performer 'TEST-0004' restarted  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending PoisonPill to Ensemble  [info] - should result in termination of actor 'MY-ENSEMBLE-0005'  [info] - should result in sending TERMINATE to monitor actor  [info] - should result in termination of ensemble and performer  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating more detailed Ensemble  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create connection PERFORMER-SCHEDULER - PERFORMER-PARAMS  [info] - should create 'PERFORMER-SCHEDULER' with schedule time equal to 200ms  [info] - should create 'PERFORMER-SCHEDULER' with connection to 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with no connections  [info] - should create 'PERFORMER-PARAMS' with specified params  [info] 'PERFORMER-SCHEDULER'  [info] - should produce 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] 'PERFORMER-PARAMS'  [info] - should process 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] Stopping any Performer that belongs to the Ensemble  [info] - should force restart of entire Ensemble  [info] - should result in sending STOP - RESTART to monitor actor  [info] - should keep ensemble actorRef when restarted  [info] - should stop and start the performer with a new reference  [info] Restarting an Ensemble  [info] - should Consuming left messages on Process  [info] - should Cleanup TestProbs  [info] Redefining TestProbe for performers  [info] - should start receiving messages  [info] Sending PoisonPill to detailed Ensemble  [info] - should result in termination of Ensemble  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating Ensemble with Backoff performer  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with backoff time equal to 1 second  [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal to true  [info] Performer with backoff enabled  [info] - should not process messages during the backoff period  [info] Performer with autoScale  [info] - should result in router and routees created  [info] IdentifyFeyActorsSpec:  [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] Creating a new actor in the system and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] - should result in matching paths  [info] Stopping previous added actor and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in going back to have just one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] WatchServiceReceiverSpec:  [info] Creating WatchServiceReceiver  [info] - should process initial files in the JSON repository  [info] Start a Thread with WatchServiceReceiver  [info] - should Start Thread  [info] Start watching directory  [info] - should Starting receiving CREATED event  [info] - should Starting receiving UPDATE event  [info] processJson  [info] - should log to warn level when json has invalid schema  [info] interrupt watchservice  [info] - should interrupt thread  [info] FeyCoreSpec:  [info] Creating FeyCore  [info] - should result in creating a child actor with the name 'FEY_IDENTIFIER'  [info] - should result in sending START message to Monitor actor  [info] Sending FeyCore.START to FeyCore  [info] - should result in creating a child actor with the name 'JSON_RECEIVER'  [info] - should result in starting FeyWatchService Thread  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'TEST-ACTOR'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'TEST-ACTOR'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command to FeyCore  [info] - should result in creating a new Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command and DELETE ensemble to FeyCore  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command and same Timestamp to FeyCore  [info] - should result in logging a 'not recreated' message at Warn  [info] Sending FeyCore.JSON_TREE to FeyCore  [info] - should result in logging a 6 path messages at Info  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command to FeyCore  [info] - should result in termination of Orchestration with the name 'TEST-ACTOR'  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in removing key 'TEST-ACTOR' at FEY_CACHE.activeOrchestrations  [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore  [info] - should result in termination of 'TEST-ORCH-2' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) had size 2 instead of expected size 1 (FeyCoreSpec.scala:144)  [info] - should result in sending Terminate message to Monitor actor *** FAILED ***  [info]   java.lang.AssertionError: assertion failed: timeout (1 second) during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TERMINATE  [info]   at scala.Predef$.assert(Predef.scala:170)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass_internal(TestKit.scala:435)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass(TestKit.scala:431)  [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)  [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)  [info]   ...  [info] - should result in empty FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) was not empty (FeyCoreSpec.scala:153)  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore of a GenericReceiverActor  [info] - should result in creating an Orchestration child actor with the name 'RECEIVER_ORCHESTRATION'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'  [info] - should result in creating a Performer child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVER_ORCHESTRATION'  [info] Sending PROCESS message to the Receiver Performer  [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'  [info] - should result in one new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVED-BY-ACTOR-RECEIVER' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"RECEIVED-BY-ACTOR-RECEIVER\" - (213263914979,Actor[akka://FEY-MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 4 instead of expected size 2 (FeyCoreSpec.scala:200)  [info] Sending PROCESS message to the Receiver Performer with command DELETE  [info] - should STOP running orchestration  [info] - should result in one entry in FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 3 instead of expected size 1 (FeyCoreSpec.scala:213)  [info] Sending PROCESS message to Receiver with checkpoint enabled  [info] - should Save received JSON to checkpoint dir  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND GLOBAL performer command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'GLOBAL-PERFORMER'  [info] - should result in creating an Ensemble child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL'  [info] - should result in creating a global Performer child actor with the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'GLOBAL-PERFORMER'  [info] - should result in one global actor created for orchestration  [info] - should result in globa metadata add to table  [info] - should result in right running actors  [info] Stopping Global actor  [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3] [akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in sending logging error  [info] - should result in orchestration restarted  [info] - should all previous actors restarted  [info] Stopping orchestration with global performer  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in no global actors for orchestration  [info] Stopping FeyCore  [info] - should result in sending STOP message to Monitor actor  [info] BaseAkkaSpec:  [info] JsonReceiverSpec:  [info] Executing validJson in JsonReceiver  [info] - should return false when json schema is not right  [info] - should log message to Error  [info] - should return true when Json schema is valid  [info] Executing checkForLocation in JsonReceiver  [info] - should log message at Debug level  [info] - should download jar dynamically from URL  [info] Start a Thread with the JSON receiver  [info] - should Start Thread  [info] - should execute execute() method inside run  [info] Interrupting the receiver Thread  [info] - should Throw Interrupted exception  [info] - should execute exceptionOnRun method  [info] FeyGenericActorSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] Backoff of GenericActor  [info] - should be zero until the first PROCESS message  [info] - should change when first PROCESS message was received  [info] Sending PROCESS message to GenericActor  [info] - should call processMessage method  [info] customReceive method  [info] - should process any non treated message  [info] Sending PROCESS message to GenericActor  [info] - should be discarded when backoff is enabled  [info] - should be processed when backoff has finished  [info] Calling startBackoff  [info] - should set endBackoff with time now  [info] Calling propagateMessage  [info] - should send message to connectTo actors  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors  [info] Creating GenericActor with schedule anc backoff equal to zero  [info] - should not start a scheduler  [info] - should result in one active actor  [info] - should result in no discarded PROCESS messages  [info] FeyGenericActorReceiverSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] - should result in normal functioning of GenericActor  [info] Sending PROCESS message to GenericReceiver  [info] - should log message to Warn saying that the JSON could not be forwarded to FeyCore when JSON is invalid  [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON to be processed has a valid schema  [info] - should Download jar from location and send ORCHESTRATION_RECEIVED to FeyCore when JSON has a location defined  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors    CLeaning up[info] Run completed in 44 seconds, 724 milliseconds.", "[info] Total number of tests run: 243  [info] Suites: completed 12, aborted 0  [info] Tests: succeeded 238, failed 5, canceled 0, ignored 0, pending 0  [info] *** 5 TESTS FAILED ***  [error] Failed tests:  [error]         org.apache.iota.fey.FeyCoreSpec  [error] (fey-core/test:test) sbt.TestsFailedException: Tests unsuccessful  [error] Total time: 46 s, completed Feb 19, 2017 12:36:25 AM      --   Thanks,    Gunnar  If you think you can you can, if you think you can't you're right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-Tony      On Feb 18, 2017, at 11:39 PM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi,    I verified the required versions and then I ran assembly of the fey-core project.", "5 failures.", "[info] - should result in creating a global Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in one global actor created for orchestration  [info] - should result in right number of running actors  [info] Stopping performer inside ensemble  [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer PERFORMER-SCHEDULER  org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER          at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(Ensemble.scala:60)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should Send stop message to monitor  [info] Stopping ensemble  [info] - should Send stop message to monitor  [info] - should result in no orchestration running  [info] - should not affect global performer  [info] Stopping global performer  [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestActorRef.scala:60)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:250)          at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:283)          at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:191)          at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch.scala:147)          at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)          at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:402)          at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:213)          at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)          at akka.actor.ActorCell.terminate(ActorCell.scala:374)          at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)          at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)          at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in restart the orchestration  [info] - should all previous actors restarted  [info] Stopping orchestration  [info] - should result in empty global  [info] EnsembleSpec:  [info] Creating a simple Ensemble MY-ENSEMBLE-0005  [info] - should result in creation of Ensemble actor 'akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'  [info] - should result in sending START to monitor actor  [info] - should result in creation of Performer 'TEST-0004'  [info] - should result in Empty state variable Ensemble.connectors  [info] - should result in one entry added to state variable Ensemble.performer  [info] - should result in one right entry to state variable Ensemble.performers_metadata  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending Ensemble.STOP_PERFORMERS to Ensemble  [info] - should result in Terminate message of actor 'TEST-0004' and throw RestartEnsemble Exception  [info] - should result in Performer 'TEST-0004' restarted  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending PoisonPill to Ensemble  [info] - should result in termination of actor 'MY-ENSEMBLE-0005'  [info] - should result in sending TERMINATE to monitor actor  [info] - should result in termination of ensemble and performer  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating more detailed Ensemble  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create connection PERFORMER-SCHEDULER - PERFORMER-PARAMS  [info] - should create 'PERFORMER-SCHEDULER' with schedule time equal to 200ms  [info] - should create 'PERFORMER-SCHEDULER' with connection to 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with no connections  [info] - should create 'PERFORMER-PARAMS' with specified params  [info] 'PERFORMER-SCHEDULER'  [info] - should produce 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] 'PERFORMER-PARAMS'  [info] - should process 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] Stopping any Performer that belongs to the Ensemble  [info] - should force restart of entire Ensemble  [info] - should result in sending STOP - RESTART to monitor actor  [info] - should keep ensemble actorRef when restarted  [info] - should stop and start the performer with a new reference  [info] Restarting an Ensemble  [info] - should Consuming left messages on Process  [info] - should Cleanup TestProbs  [info] Redefining TestProbe for performers  [info] - should start receiving messages  [info] Sending PoisonPill to detailed Ensemble  [info] - should result in termination of Ensemble  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating Ensemble with Backoff performer  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with backoff time equal to 1 second  [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal to true  [info] Performer with backoff enabled  [info] - should not process messages during the backoff period  [info] Performer with autoScale  [info] - should result in router and routees created  [info] IdentifyFeyActorsSpec:  [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] Creating a new actor in the system and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] - should result in matching paths  [info] Stopping previous added actor and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in going back to have just one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] WatchServiceReceiverSpec:  [info] Creating WatchServiceReceiver  [info] - should process initial files in the JSON repository  [info] Start a Thread with WatchServiceReceiver  [info] - should Start Thread  [info] Start watching directory  [info] - should Starting receiving CREATED event  [info] - should Starting receiving UPDATE event  [info] processJson  [info] - should log to warn level when json has invalid schema  [info] interrupt watchservice  [info] - should interrupt thread  [info] FeyCoreSpec:  [info] Creating FeyCore  [info] - should result in creating a child actor with the name 'FEY_IDENTIFIER'  [info] - should result in sending START message to Monitor actor  [info] Sending FeyCore.START to FeyCore  [info] - should result in creating a child actor with the name 'JSON_RECEIVER'  [info] - should result in starting FeyWatchService Thread  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'TEST-ACTOR'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'TEST-ACTOR'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command to FeyCore  [info] - should result in creating a new Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command and DELETE ensemble to FeyCore  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command and same Timestamp to FeyCore  [info] - should result in logging a 'not recreated' message at Warn  [info] Sending FeyCore.JSON_TREE to FeyCore  [info] - should result in logging a 6 path messages at Info  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command to FeyCore  [info] - should result in termination of Orchestration with the name 'TEST-ACTOR'  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in removing key 'TEST-ACTOR' at FEY_CACHE.activeOrchestrations  [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore  [info] - should result in termination of 'TEST-ORCH-2' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) had size 2 instead of expected size 1 (FeyCoreSpec.scala:144)  [info] - should result in sending Terminate message to Monitor actor *** FAILED ***  [info]   java.lang.AssertionError: assertion failed: timeout (1 second) during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TERMINATE  [info]   at scala.Predef$.assert(Predef.scala:170)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass_internal(TestKit.scala:435)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass(TestKit.scala:431)  [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)  [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)  [info]   ...  [info] - should result in empty FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) was not empty (FeyCoreSpec.scala:153)  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore of a GenericReceiverActor  [info] - should result in creating an Orchestration child actor with the name 'RECEIVER_ORCHESTRATION'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'  [info] - should result in creating a Performer child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVER_ORCHESTRATION'  [info] Sending PROCESS message to the Receiver Performer  [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'  [info] - should result in one new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVED-BY-ACTOR-RECEIVER' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"RECEIVED-BY-ACTOR-RECEIVER\" - (213263914979,Actor[akka://FEY-MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 4 instead of expected size 2 (FeyCoreSpec.scala:200)  [info] Sending PROCESS message to the Receiver Performer with command DELETE  [info] - should STOP running orchestration  [info] - should result in one entry in FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 3 instead of expected size 1 (FeyCoreSpec.scala:213)  [info] Sending PROCESS message to Receiver with checkpoint enabled  [info] - should Save received JSON to checkpoint dir  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND GLOBAL performer command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'GLOBAL-PERFORMER'  [info] - should result in creating an Ensemble child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL'  [info] - should result in creating a global Performer child actor with the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'GLOBAL-PERFORMER'  [info] - should result in one global actor created for orchestration  [info] - should result in globa metadata add to table  [info] - should result in right running actors  [info] Stopping Global actor  [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3] [akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in sending logging error  [info] - should result in orchestration restarted  [info] - should all previous actors restarted  [info] Stopping orchestration with global performer  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in no global actors for orchestration  [info] Stopping FeyCore  [info] - should result in sending STOP message to Monitor actor  [info] BaseAkkaSpec:  [info] JsonReceiverSpec:  [info] Executing validJson in JsonReceiver  [info] - should return false when json schema is not right  [info] - should log message to Error  [info] - should return true when Json schema is valid  [info] Executing checkForLocation in JsonReceiver  [info] - should log message at Debug level  [info] - should download jar dynamically from URL  [info] Start a Thread with the JSON receiver  [info] - should Start Thread  [info] - should execute execute() method inside run  [info] Interrupting the receiver Thread  [info] - should Throw Interrupted exception  [info] - should execute exceptionOnRun method  [info] FeyGenericActorSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] Backoff of GenericActor  [info] - should be zero until the first PROCESS message  [info] - should change when first PROCESS message was received  [info] Sending PROCESS message to GenericActor  [info] - should call processMessage method  [info] customReceive method  [info] - should process any non treated message  [info] Sending PROCESS message to GenericActor  [info] - should be discarded when backoff is enabled  [info] - should be processed when backoff has finished  [info] Calling startBackoff  [info] - should set endBackoff with time now  [info] Calling propagateMessage  [info] - should send message to connectTo actors  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors  [info] Creating GenericActor with schedule anc backoff equal to zero  [info] - should not start a scheduler  [info] - should result in one active actor  [info] - should result in no discarded PROCESS messages  [info] FeyGenericActorReceiverSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] - should result in normal functioning of GenericActor  [info] Sending PROCESS message to GenericReceiver  [info] - should log message to Warn saying that the JSON could not be forwarded to FeyCore when JSON is invalid  [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON to be processed has a valid schema  [info] - should Download jar from location and send ORCHESTRATION_RECEIVED to FeyCore when JSON has a location defined  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors    CLeaning up[info] Run completed in 44 seconds, 724 milliseconds.", "[info] Total number of tests run: 243  [info] Suites: completed 12, aborted 0  [info] Tests: succeeded 238, failed 5, canceled 0, ignored 0, pending 0  [info] *** 5 TESTS FAILED ***  [error] Failed tests:  [error]         org.apache.iota.fey.FeyCoreSpec  [error] (fey-core/test:test) sbt.TestsFailedException: Tests unsuccessful  [error] Total time: 46 s, completed Feb 19, 2017 12:36:25 AM      --   Thanks,    Gunnar  If you think you can you can, if you think you can't you're right.", "--   Barbara Gomes  Computer Engineer  San Jose, CA        --   Thanks,    Gunnar  If you think you can you can, if you think you can't you're right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["5 failures.", "[info] - should result in creating a global Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in one global actor created for orchestration  [info] - should result in right number of running actors  [info] Stopping performer inside ensemble  [ERROR] [02/19/2017 00:35:47.820] [FEY-TEST-akka.actor.default-dispatcher-8] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH/ENS-GLOBAL] DEAD Performer PERFORMER-SCHEDULER  org.apache.iota.fey.RestartEnsemble: DEAD Performer PERFORMER-SCHEDULER          at org.apache.iota.fey.Ensemble$$anonfun$receive$1.applyOrElse(Ensemble.scala:60)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.Ensemble.aroundReceive(Ensemble.scala:29)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should Send stop message to monitor  [info] Stopping ensemble  [info] - should Send stop message to monitor  [info] - should result in no orchestration running  [info] - should not affect global performer  [info] Stopping global performer  [ERROR] [02/19/2017 00:35:49.023] [FEY-TEST-akka.actor.default-dispatcher-7] [akka://FEY-TEST/system/CORE-27/GLOBAL-ORCH] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.testkit.TestActorRef$$anon$1.autoReceiveMessage(TestActorRef.scala:60)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:250)          at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:283)          at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:191)          at akka.actor.dungeon.Dispatch$class.sendSystemMessage(Dispatch.scala:147)          at akka.actor.ActorCell.sendSystemMessage(ActorCell.scala:374)          at akka.actor.LocalActorRef.sendSystemMessage(ActorRef.scala:402)          at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:213)          at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)          at akka.actor.ActorCell.terminate(ActorCell.scala:374)          at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:467)          at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483)          at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:260)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in restart the orchestration  [info] - should all previous actors restarted  [info] Stopping orchestration  [info] - should result in empty global  [info] EnsembleSpec:  [info] Creating a simple Ensemble MY-ENSEMBLE-0005  [info] - should result in creation of Ensemble actor 'akka://FEY-TEST/system/ENSEMBLE-34/MY-ENSEMBLE-0005'  [info] - should result in sending START to monitor actor  [info] - should result in creation of Performer 'TEST-0004'  [info] - should result in Empty state variable Ensemble.connectors  [info] - should result in one entry added to state variable Ensemble.performer  [info] - should result in one right entry to state variable Ensemble.performers_metadata  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending Ensemble.STOP_PERFORMERS to Ensemble  [info] - should result in Terminate message of actor 'TEST-0004' and throw RestartEnsemble Exception  [info] - should result in Performer 'TEST-0004' restarted  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] Sending PoisonPill to Ensemble  [info] - should result in termination of actor 'MY-ENSEMBLE-0005'  [info] - should result in sending TERMINATE to monitor actor  [info] - should result in termination of ensemble and performer  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating more detailed Ensemble  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create connection PERFORMER-SCHEDULER - PERFORMER-PARAMS  [info] - should create 'PERFORMER-SCHEDULER' with schedule time equal to 200ms  [info] - should create 'PERFORMER-SCHEDULER' with connection to 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with no connections  [info] - should create 'PERFORMER-PARAMS' with specified params  [info] 'PERFORMER-SCHEDULER'  [info] - should produce 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] 'PERFORMER-PARAMS'  [info] - should process 5 messages in 1 seconds  [info] - should produce 10 messages in 2 seconds  [info] Stopping any Performer that belongs to the Ensemble  [info] - should force restart of entire Ensemble  [info] - should result in sending STOP - RESTART to monitor actor  [info] - should keep ensemble actorRef when restarted  [info] - should stop and start the performer with a new reference  [info] Restarting an Ensemble  [info] - should Consuming left messages on Process  [info] - should Cleanup TestProbs  [info] Redefining TestProbe for performers  [info] - should start receiving messages  [info] Sending PoisonPill to detailed Ensemble  [info] - should result in termination of Ensemble  [info] - should result in empty IdentifyFeyActors.actorsPath  [info] creating Ensemble with Backoff performer  [info] - should result in creation of Ensemble actor  [info] - should result in creation of Performer 'PERFORMER-SCHEDULER'  [info] - should result in creation of Performer 'PERFORMER-PARAMS'  [info] - should create 'PERFORMER-PARAMS' with backoff time equal to 1 second  [info] - should create 'PERFORMER-SCHEDULER' with autoScale equal to true  [info] Performer with backoff enabled  [info] - should not process messages during the backoff period  [info] Performer with autoScale  [info] - should result in router and routees created  [info] IdentifyFeyActorsSpec:  [info] Sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] Creating a new actor in the system and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in two paths added to IdentifyFeyActors.actorsPath  [info] - should result in matching paths  [info] Stopping previous added actor and sending IdentifyFeyActors.IDENTIFY_TREE to IdentifyFeyActors  [info] - should result in going back to have just one path added to IdentifyFeyActors.actorsPath  [info] - should result in path 'akka://FEY-TEST/user/GLOBAL-IDENTIFIER'  [info] WatchServiceReceiverSpec:  [info] Creating WatchServiceReceiver  [info] - should process initial files in the JSON repository  [info] Start a Thread with WatchServiceReceiver  [info] - should Start Thread  [info] Start watching directory  [info] - should Starting receiving CREATED event  [info] - should Starting receiving UPDATE event  [info] processJson  [info] - should log to warn level when json has invalid schema  [info] interrupt watchservice  [info] - should interrupt thread  [info] FeyCoreSpec:  [info] Creating FeyCore  [info] - should result in creating a child actor with the name 'FEY_IDENTIFIER'  [info] - should result in sending START message to Monitor actor  [info] Sending FeyCore.START to FeyCore  [info] - should result in creating a child actor with the name 'JSON_RECEIVER'  [info] - should result in starting FeyWatchService Thread  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'TEST-ACTOR'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in creating an Ensemble child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'TEST-ACTOR'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command to FeyCore  [info] - should result in creating a new Performer child actor with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with UPDATE command and DELETE ensemble to FeyCore  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0001'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0001/TEST-0002'  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with RECREATE command and same Timestamp to FeyCore  [info] - should result in logging a 'not recreated' message at Warn  [info] Sending FeyCore.JSON_TREE to FeyCore  [info] - should result in logging a 6 path messages at Info  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with DELETE command to FeyCore  [info] - should result in termination of Orchestration with the name 'TEST-ACTOR'  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in termination of Ensemble with the name 'TEST-ACTOR/MY-ENSEMBLE-0002'  [info] - should result in termination of Performer with the name 'TEST-ACTOR/MY-ENSEMBLE-0002/TEST-0001'  [info] - should result in removing key 'TEST-ACTOR' at FEY_CACHE.activeOrchestrations  [info] Sending FeyCore.STOP_EMPTY_ORCHESTRATION to FeyCore  [info] - should result in termination of 'TEST-ORCH-2' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) had size 2 instead of expected size 1 (FeyCoreSpec.scala:144)  [info] - should result in sending Terminate message to Monitor actor *** FAILED ***  [info]   java.lang.AssertionError: assertion failed: timeout (1 second) during expectMsgClass waiting for class org.apache.iota.fey.Monitor$TERMINATE  [info]   at scala.Predef$.assert(Predef.scala:170)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass_internal(TestKit.scala:435)  [info]   at akka.testkit.TestKitBase$class.expectMsgClass(TestKit.scala:431)  [info]   at akka.testkit.TestKit.expectMsgClass(TestKit.scala:737)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.apache.iota.fey.FeyCoreSpec$$anonfun$9$$anonfun$apply$mcV$sp$37.apply(FeyCoreSpec.scala:150)  [info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)  [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)  [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)  [info]   ...  [info] - should result in empty FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067])) was not empty (FeyCoreSpec.scala:153)  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE command to FeyCore of a GenericReceiverActor  [info] - should result in creating an Orchestration child actor with the name 'RECEIVER_ORCHESTRATION'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE'  [info] - should result in creating a Performer child actor with the name 'RECEIVER_ORCHESTRATION/RECEIVER-ENSEMBLE/MY_RECEIVER_PERFORMER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVER_ORCHESTRATION'  [info] Sending PROCESS message to the Receiver Performer  [info] - should Send FeyCore.ORCHESTRATION_RECEIVED to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001'  [info] - should result in creating an Ensemble child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0002/TEST-0001'  [info] - should result in creating a Performer child actor with the name 'RECEIVED-BY-ACTOR-RECEIVER/MY-ENSEMBLE-REC-0001/TEST-0001'  [info] - should result in one new entry to FEY_CACHE.activeOrchestrations with key 'RECEIVED-BY-ACTOR-RECEIVER' *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"RECEIVED-BY-ACTOR-RECEIVER\" - (213263914979,Actor[akka://FEY-MANAGEMENT-SYSTEM/user/FEY-CORE/RECEIVED-BY-ACTOR-RECEIVER#1213682574]), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 4 instead of expected size 2 (FeyCoreSpec.scala:200)  [info] Sending PROCESS message to the Receiver Performer with command DELETE  [info] - should STOP running orchestration  [info] - should result in one entry in FEY_CACHE.activeOrchestrations *** FAILED ***  [info]   Map(\"TEST_ORCHESTRATION_FOR_UTILS\" - (,null), \"TEST-ORCH-2\" - (213263914979,Actor[akka://FEY-TEST/user/FEY-CORE/TEST-ORCH-2#-2041630067]), \"RECEIVER_ORCHESTRATION\" - (591997890,Actor[akka://FEY-TEST/user/FEY-CORE/RECEIVER_ORCHESTRATION#-560956299])) had size 3 instead of expected size 1 (FeyCoreSpec.scala:213)  [info] Sending PROCESS message to Receiver with checkpoint enabled  [info] - should Save received JSON to checkpoint dir  [info] Sending FeyCore.ORCHESTRATION_RECEIVED with CREATE AND GLOBAL performer command to FeyCore  [info] - should result in creating an Orchestration child actor with the name 'GLOBAL-PERFORMER'  [info] - should result in creating an Ensemble child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL'  [info] - should result in creating a global Performer child actor with the name 'GLOBAL-PERFORMER/GLOBAL_MANAGER/GLOBAL-TEST'  [info] - should result in creating a Performer child actor with the name 'GLOBAL-PERFORMER/ENS-GLOBAL/PERFORMER-SCHEDULER'  [info] - should result in new entry to FEY_CACHE.activeOrchestrations with key 'GLOBAL-PERFORMER'  [info] - should result in one global actor created for orchestration  [info] - should result in globa metadata add to table  [info] - should result in right running actors  [info] Stopping Global actor  [ERROR] [02/19/2017 00:36:09.279] [FEY-TEST-akka.actor.default-dispatcher-3] [akka://FEY-TEST/user/FEY-CORE/GLOBAL-PERFORMER] DEAD Global Performer GLOBAL-TEST  org.apache.iota.fey.RestartGlobalPerformers: DEAD Global Performer GLOBAL-TEST          at org.apache.iota.fey.GlobalPerformer$$anonfun$receive$1.applyOrElse(GlobalPerformer.scala:49)          at akka.actor.Actor$class.aroundReceive(Actor.scala:484)          at org.apache.iota.fey.GlobalPerformer.aroundReceive(GlobalPerformer.scala:28)          at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)          at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:44)          at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:374)          at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:511)          at akka.actor.ActorCell.invoke(ActorCell.scala:494)          at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)          at akka.dispatch.Mailbox.run(Mailbox.scala:224)          at akka.dispatch.Mailbox.exec(Mailbox.scala:234)          at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)          at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)          at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)          at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)    [info] - should result in sending logging error  [info] - should result in orchestration restarted  [info] - should all previous actors restarted  [info] Stopping orchestration with global performer  [info] - should result in sending TERMINATE message to Monitor actor  [info] - should result in no global actors for orchestration  [info] Stopping FeyCore  [info] - should result in sending STOP message to Monitor actor  [info] BaseAkkaSpec:  [info] JsonReceiverSpec:  [info] Executing validJson in JsonReceiver  [info] - should return false when json schema is not right  [info] - should log message to Error  [info] - should return true when Json schema is valid  [info] Executing checkForLocation in JsonReceiver  [info] - should log message at Debug level  [info] - should download jar dynamically from URL  [info] Start a Thread with the JSON receiver  [info] - should Start Thread  [info] - should execute execute() method inside run  [info] Interrupting the receiver Thread  [info] - should Throw Interrupted exception  [info] - should execute exceptionOnRun method  [info] FeyGenericActorSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] Backoff of GenericActor  [info] - should be zero until the first PROCESS message  [info] - should change when first PROCESS message was received  [info] Sending PROCESS message to GenericActor  [info] - should call processMessage method  [info] customReceive method  [info] - should process any non treated message  [info] Sending PROCESS message to GenericActor  [info] - should be discarded when backoff is enabled  [info] - should be processed when backoff has finished  [info] Calling startBackoff  [info] - should set endBackoff with time now  [info] Calling propagateMessage  [info] - should send message to connectTo actors  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors  [info] Creating GenericActor with schedule anc backoff equal to zero  [info] - should not start a scheduler  [info] - should result in one active actor  [info] - should result in no discarded PROCESS messages  [info] FeyGenericActorReceiverSpec:  [info] Creating a GenericActor with Schedule time defined  [info] - should result in scheduler started  [info] - should result in onStart method called  [info] - should result in START message sent to Monitor  [info] - should result in one active actor  [info] - should result in normal functioning of GenericActor  [info] Sending PROCESS message to GenericReceiver  [info] - should log message to Warn saying that the JSON could not be forwarded to FeyCore when JSON is invalid  [info] - should send ORCHESTRATION_RECEIVED to FeyCore when JSON to be processed has a valid schema  [info] - should Download jar from location and send ORCHESTRATION_RECEIVED to FeyCore when JSON has a location defined  [info] Scheduler component  [info] - should call execute() method  [info] Sending EXCEPTION(IllegalArgumentException) message to GenericActor  [info] - should Throw IllegalArgumentException  [info] - should Result in restart of the actor with sequence of Monitoring: STOP - RESTART - START  [info] - should call onStart method  [info] - should call onRestart method  [info] - should restart scheduler  [info] Sending STOP to GenericActor  [info] - should terminate GenericActor  [info] - should call onStop method  [info] - should cancel scheduler  [info] - should send STOP - TERMINATE message to Monitor  [info] - should result in no active actors    CLeaning up[info] Run completed in 44 seconds, 724 milliseconds.", "[info] Total number of tests run: 243  [info] Suites: completed 12, aborted 0  [info] Tests: succeeded 238, failed 5, canceled 0, ignored 0, pending 0  [info] *** 5 TESTS FAILED ***  [error] Failed tests:  [error]         org.apache.iota.fey.FeyCoreSpec  [error] (fey-core/test:test) sbt.TestsFailedException: Tests unsuccessful  [error] Total time: 46 s, completed Feb 19, 2017 12:36:25 AM      --   Thanks,    Gunnar  If you think you can you can, if you think you can't you're right.", "--   Barbara Gomes  Computer Engineer  San Jose, CA        --   Thanks,    Gunnar  If you think you can you can, if you think you can't you're right.", "--   Barbara Gomes  Computer Engineer  San Jose, CA        --   Barbara Gomes  Computer Engineer  San Jose, CA        --   Thanks,    Gunnar  If you think you can you can, if you think you can't you're right.  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Nice!", "Why don\u2019t we also merge 1 into master then and fix issues from there?", "It will get wider exposure (We run master in our dev environments for example), it won\u2019t put everything into the lab or Airbnb.", "Master hardly should be considered production so imho it is allowed some time to settle before it is stable again.", "Does someone want to pick up 2 say by the end of week or next week to fix all headers?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why don\u2019t we also merge 1 into master then and fix issues from there?", "It will get wider exposure (We run master in our dev environments for example), it won\u2019t put everything into the lab or Airbnb.", "Master hardly should be considered production so imho it is allowed some time to settle before it is stable again.", "Does someone want to pick up 2 say by the end of week or next week to fix all headers?", "- Bolke    Op 14 jun."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It will get wider exposure (We run master in our dev environments for example), it won\u2019t put everything into the lab or Airbnb.", "Master hardly should be considered production so imho it is allowed some time to settle before it is stable again.", "Does someone want to pick up 2 say by the end of week or next week to fix all headers?", "- Bolke    Op 14 jun.", "2016, om 20:01 heeft Maxime Beauchemin <maximebeauchemin@gmail.com het volgende geschreven:    I think (1) is good to go."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Master hardly should be considered production so imho it is allowed some time to settle before it is stable again.", "Does someone want to pick up 2 say by the end of week or next week to fix all headers?", "- Bolke    Op 14 jun.", "2016, om 20:01 heeft Maxime Beauchemin <maximebeauchemin@gmail.com het volgende geschreven:    I think (1) is good to go.", "I can cherry pick it into our production to make  sure it's ready for release."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Does someone want to pick up 2 say by the end of week or next week to fix all headers?", "- Bolke    Op 14 jun.", "2016, om 20:01 heeft Maxime Beauchemin <maximebeauchemin@gmail.com het volgende geschreven:    I think (1) is good to go.", "I can cherry pick it into our production to make  sure it's ready for release.", "(2) should be fine, git should be able to merge easily, otherwise it's  super easy to resolve any conflicts    Max    On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <criccomini@apache.org  wrote:    Hey Bolke,    I think your list is good."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Bolke    Op 14 jun.", "2016, om 20:01 heeft Maxime Beauchemin <maximebeauchemin@gmail.com het volgende geschreven:    I think (1) is good to go.", "I can cherry pick it into our production to make  sure it's ready for release.", "(2) should be fine, git should be able to merge easily, otherwise it's  super easy to resolve any conflicts    Max    On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <criccomini@apache.org  wrote:    Hey Bolke,    I think your list is good.", "(1) is what I'm most concerned about, as it  requires actually touching the code, and is blocking on graduation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2016, om 20:01 heeft Maxime Beauchemin <maximebeauchemin@gmail.com het volgende geschreven:    I think (1) is good to go.", "I can cherry pick it into our production to make  sure it's ready for release.", "(2) should be fine, git should be able to merge easily, otherwise it's  super easy to resolve any conflicts    Max    On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <criccomini@apache.org  wrote:    Hey Bolke,    I think your list is good.", "(1) is what I'm most concerned about, as it  requires actually touching the code, and is blocking on graduation.", "I  *think* Max had a partial PR on that, but don't know the current state."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can cherry pick it into our production to make  sure it's ready for release.", "(2) should be fine, git should be able to merge easily, otherwise it's  super easy to resolve any conflicts    Max    On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <criccomini@apache.org  wrote:    Hey Bolke,    I think your list is good.", "(1) is what I'm most concerned about, as it  requires actually touching the code, and is blocking on graduation.", "I  *think* Max had a partial PR on that, but don't know the current state.", "Re: (2), agree."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(2) should be fine, git should be able to merge easily, otherwise it's  super easy to resolve any conflicts    Max    On Tue, Jun 14, 2016 at 9:04 AM, Chris Riccomini <criccomini@apache.org  wrote:    Hey Bolke,    I think your list is good.", "(1) is what I'm most concerned about, as it  requires actually touching the code, and is blocking on graduation.", "I  *think* Max had a partial PR on that, but don't know the current state.", "Re: (2), agree.", "Should just do a bulk PR for it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(1) is what I'm most concerned about, as it  requires actually touching the code, and is blocking on graduation.", "I  *think* Max had a partial PR on that, but don't know the current state.", "Re: (2), agree.", "Should just do a bulk PR for it.", "Cheers,  Chris    On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com wrote:    Hi,    I am wondering what needs to be done to get to an Apache release?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Re: (2), agree.", "Should just do a bulk PR for it.", "Cheers,  Chris    On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com wrote:    Hi,    I am wondering what needs to be done to get to an Apache release?", "I think  now 1.7.1.3 is out we should be focused on getting one out as we are kind  of half way the incubation process.", "What comes to my mind is:    1."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["Cheers,  Chris    On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com wrote:    Hi,    I am wondering what needs to be done to get to an Apache release?", "I think  now 1.7.1.3 is out we should be focused on getting one out as we are kind  of half way the incubation process.", "What comes to my mind is:    1.", "Replace highcharts by D3 (WIP:  https://github.com/apache/incubator-airflow/pull/1469)  2.", "Add license headers everywhere (TM) (Sucks, as it will break many PRs  -  but lets do it quickly)  3."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["I think  now 1.7.1.3 is out we should be focused on getting one out as we are kind  of half way the incubation process.", "What comes to my mind is:    1.", "Replace highcharts by D3 (WIP:  https://github.com/apache/incubator-airflow/pull/1469)  2.", "Add license headers everywhere (TM) (Sucks, as it will break many PRs  -  but lets do it quickly)  3.", "Have a review by Apache    Anything I am missing?"], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["If so, I can ship to our environments and vote accordingly.", "On Mon, Jul 11, 2016 at 3:28 PM, Chris Riccomini <criccomini@apache.org wrote:  If we need to update this branch I prefer very much to have commits that  are also applied against master.", "Absolutely +1 to this.", "Everything must go through master first.", "On Mon, Jul 11, 2016 at 7:06 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Ok."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Absolutely +1 to this.", "Everything must go through master first.", "On Mon, Jul 11, 2016 at 7:06 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Ok.", "I created a \u201cbranch-1.7.2-apache\u201d branch based on airbnb_rb1.7.1_4 to  which I added cherry-picked commits to establish Apache compliancy (I  think!).", "If we need to update this branch I prefer very much to have commits  that are also applied against master."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Everything must go through master first.", "On Mon, Jul 11, 2016 at 7:06 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Ok.", "I created a \u201cbranch-1.7.2-apache\u201d branch based on airbnb_rb1.7.1_4 to  which I added cherry-picked commits to establish Apache compliancy (I  think!).", "If we need to update this branch I prefer very much to have commits  that are also applied against master.", "- Bolke     Op 11 jul."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Mon, Jul 11, 2016 at 7:06 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Ok.", "I created a \u201cbranch-1.7.2-apache\u201d branch based on airbnb_rb1.7.1_4 to  which I added cherry-picked commits to establish Apache compliancy (I  think!).", "If we need to update this branch I prefer very much to have commits  that are also applied against master.", "- Bolke     Op 11 jul.", "2016, om 15:16 heeft Bolke de Bruin <bdbruin@gmail.com het   volgende geschreven:     Ok."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I created a \u201cbranch-1.7.2-apache\u201d branch based on airbnb_rb1.7.1_4 to  which I added cherry-picked commits to establish Apache compliancy (I  think!).", "If we need to update this branch I prefer very much to have commits  that are also applied against master.", "- Bolke     Op 11 jul.", "2016, om 15:16 heeft Bolke de Bruin <bdbruin@gmail.com het   volgende geschreven:     Ok.", "I am seeing a \u201cairbnb_rb1.7.1_4\u201d branch, I am assuming this is the   branch to start from (?)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Bolke     Op 11 jul.", "2016, om 15:16 heeft Bolke de Bruin <bdbruin@gmail.com het   volgende geschreven:     Ok.", "I am seeing a \u201cairbnb_rb1.7.1_4\u201d branch, I am assuming this is the   branch to start from (?).", "I will work on applying the commits that are required to become   compliant.", "- Bolke       Op 11 jul."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["I am seeing a \u201cairbnb_rb1.7.1_4\u201d branch, I am assuming this is the   branch to start from (?).", "I will work on applying the commits that are required to become   compliant.", "- Bolke       Op 11 jul.", "2016, om 05:22 heeft Maxime Beauchemin   <maximebeauchemin@gmail.com het volgende geschreven:     +1     On Fri, Jul 8, 2016 at 5:16 PM, Dan Davydov   <dan.davydov@airbnb.com.invalid   wrote:     +1 the minimal apache cherrypick release makes sense to me.", "On Fri, Jul 8, 2016 at 1:14 PM, Chris Riccomini   <criccomini@apache.org   wrote:     Hey Bolke,     A fast release with the 1.7.1.3 + cherry picks listed above sounds   like   the   way to go."], "labels": ["0", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["- Bolke       Op 11 jul.", "2016, om 05:22 heeft Maxime Beauchemin   <maximebeauchemin@gmail.com het volgende geschreven:     +1     On Fri, Jul 8, 2016 at 5:16 PM, Dan Davydov   <dan.davydov@airbnb.com.invalid   wrote:     +1 the minimal apache cherrypick release makes sense to me.", "On Fri, Jul 8, 2016 at 1:14 PM, Chris Riccomini   <criccomini@apache.org   wrote:     Hey Bolke,     A fast release with the 1.7.1.3 + cherry picks listed above sounds   like   the   way to go.", "Then, a second release in sept where we just cut from   master.", "I'm +1 on this."], "labels": ["0", "0", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["2016, om 05:22 heeft Maxime Beauchemin   <maximebeauchemin@gmail.com het volgende geschreven:     +1     On Fri, Jul 8, 2016 at 5:16 PM, Dan Davydov   <dan.davydov@airbnb.com.invalid   wrote:     +1 the minimal apache cherrypick release makes sense to me.", "On Fri, Jul 8, 2016 at 1:14 PM, Chris Riccomini   <criccomini@apache.org   wrote:     Hey Bolke,     A fast release with the 1.7.1.3 + cherry picks listed above sounds   like   the   way to go.", "Then, a second release in sept where we just cut from   master.", "I'm +1 on this.", "Lets us get our Apache ducks in a row without   worrying   about stabilizing everything simultaneously."], "labels": ["0", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Jul 8, 2016 at 1:14 PM, Chris Riccomini   <criccomini@apache.org   wrote:     Hey Bolke,     A fast release with the 1.7.1.3 + cherry picks listed above sounds   like   the   way to go.", "Then, a second release in sept where we just cut from   master.", "I'm +1 on this.", "Lets us get our Apache ducks in a row without   worrying   about stabilizing everything simultaneously.", "Cheers,   Chris     On Fri, Jul 8, 2016 at 12:32 AM, Bolke de Bruin <bdbruin@gmail.com   wrote:     This was my assessment as well, thus I agree."], "labels": ["1", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Then, a second release in sept where we just cut from   master.", "I'm +1 on this.", "Lets us get our Apache ducks in a row without   worrying   about stabilizing everything simultaneously.", "Cheers,   Chris     On Fri, Jul 8, 2016 at 12:32 AM, Bolke de Bruin <bdbruin@gmail.com   wrote:     This was my assessment as well, thus I agree.", "My suggestion is to   start   the process and see if we get questions about this that require us   the   change our point of view."], "labels": ["1", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["I'm +1 on this.", "Lets us get our Apache ducks in a row without   worrying   about stabilizing everything simultaneously.", "Cheers,   Chris     On Fri, Jul 8, 2016 at 12:32 AM, Bolke de Bruin <bdbruin@gmail.com   wrote:     This was my assessment as well, thus I agree.", "My suggestion is to   start   the process and see if we get questions about this that require us   the   change our point of view.", "If we do an earlier release I would like to aim for July 19, but   that   might be a bit short notice."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Lets us get our Apache ducks in a row without   worrying   about stabilizing everything simultaneously.", "Cheers,   Chris     On Fri, Jul 8, 2016 at 12:32 AM, Bolke de Bruin <bdbruin@gmail.com   wrote:     This was my assessment as well, thus I agree.", "My suggestion is to   start   the process and see if we get questions about this that require us   the   change our point of view.", "If we do an earlier release I would like to aim for July 19, but   that   might be a bit short notice.", "If needed I can put myself up as   release   manager till the 21st."], "labels": ["0", "0", "1", "0", "1"]}
{"abstract_id": 0, "sentences": ["Cheers,   Chris     On Fri, Jul 8, 2016 at 12:32 AM, Bolke de Bruin <bdbruin@gmail.com   wrote:     This was my assessment as well, thus I agree.", "My suggestion is to   start   the process and see if we get questions about this that require us   the   change our point of view.", "If we do an earlier release I would like to aim for July 19, but   that   might be a bit short notice.", "If needed I can put myself up as   release   manager till the 21st.", "If we do 1.7.1.3 + cherry picks I would say     * Licenses   * Notices   * Disclaimer   * Highcharts - d3     Makes sense?"], "labels": ["0", "1", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["If we do an earlier release I would like to aim for July 19, but   that   might be a bit short notice.", "If needed I can put myself up as   release   manager till the 21st.", "If we do 1.7.1.3 + cherry picks I would say     * Licenses   * Notices   * Disclaimer   * Highcharts - d3     Makes sense?", "Anything missing here?", "- Bolke     Op 7 jul."], "labels": ["0", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["If needed I can put myself up as   release   manager till the 21st.", "If we do 1.7.1.3 + cherry picks I would say     * Licenses   * Notices   * Disclaimer   * Highcharts - d3     Makes sense?", "Anything missing here?", "- Bolke     Op 7 jul.", "2016, om 18:04 heeft Chris Riccomini <   criccomini@apache.org   het volgende geschreven:     but it's acceptable to have a soft dependency on an LGPL   component,   such   that a user could deploy the LGPL component separately to enable   additional   optional features     This is precisely what I believe is going on with Airflow."], "labels": ["1", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["If we do 1.7.1.3 + cherry picks I would say     * Licenses   * Notices   * Disclaimer   * Highcharts - d3     Makes sense?", "Anything missing here?", "- Bolke     Op 7 jul.", "2016, om 18:04 heeft Chris Riccomini <   criccomini@apache.org   het volgende geschreven:     but it's acceptable to have a soft dependency on an LGPL   component,   such   that a user could deploy the LGPL component separately to enable   additional   optional features     This is precisely what I believe is going on with Airflow.", "It's   under   an   airflow[postgres] package (so `pip install airflow` doesn't even   install   it)."], "labels": ["1", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Anything missing here?", "- Bolke     Op 7 jul.", "2016, om 18:04 heeft Chris Riccomini <   criccomini@apache.org   het volgende geschreven:     but it's acceptable to have a soft dependency on an LGPL   component,   such   that a user could deploy the LGPL component separately to enable   additional   optional features     This is precisely what I believe is going on with Airflow.", "It's   under   an   airflow[postgres] package (so `pip install airflow` doesn't even   install   it).", "We went through a very similar exercise with Samza, where we   had a   dependency on Paramiko (also LGPL [1]), and our (LinkedIn) lawyers   talked   to Apache, and agreed that it was fine."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Bolke     Op 7 jul.", "2016, om 18:04 heeft Chris Riccomini <   criccomini@apache.org   het volgende geschreven:     but it's acceptable to have a soft dependency on an LGPL   component,   such   that a user could deploy the LGPL component separately to enable   additional   optional features     This is precisely what I believe is going on with Airflow.", "It's   under   an   airflow[postgres] package (so `pip install airflow` doesn't even   install   it).", "We went through a very similar exercise with Samza, where we   had a   dependency on Paramiko (also LGPL [1]), and our (LinkedIn) lawyers   talked   to Apache, and agreed that it was fine.", "[1] https://github.com/paramiko/paramiko/blob/master/LICENSE     On Wed, Jul 6, 2016 at 10:16 PM, Chris Nauroth <   cnauroth@hortonworks.com     wrote:     Here are more details on Apache release requirements:     http://www.apache.org/dev/release-publishing.html       http://www.apache.org/dev/release       To summarize, it's much more focused on compliance with licensing,   signing   and Apache infrastructure requirements."], "labels": ["0", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["2016, om 18:04 heeft Chris Riccomini <   criccomini@apache.org   het volgende geschreven:     but it's acceptable to have a soft dependency on an LGPL   component,   such   that a user could deploy the LGPL component separately to enable   additional   optional features     This is precisely what I believe is going on with Airflow.", "It's   under   an   airflow[postgres] package (so `pip install airflow` doesn't even   install   it).", "We went through a very similar exercise with Samza, where we   had a   dependency on Paramiko (also LGPL [1]), and our (LinkedIn) lawyers   talked   to Apache, and agreed that it was fine.", "[1] https://github.com/paramiko/paramiko/blob/master/LICENSE     On Wed, Jul 6, 2016 at 10:16 PM, Chris Nauroth <   cnauroth@hortonworks.com     wrote:     Here are more details on Apache release requirements:     http://www.apache.org/dev/release-publishing.html       http://www.apache.org/dev/release       To summarize, it's much more focused on compliance with licensing,   signing   and Apache infrastructure requirements.", "That's the kind of   scrutiny   that   a release candidate will get from the Incubator PMC rather than   deep   testing for verification of new features or bug fixes."], "labels": ["1", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["It's   under   an   airflow[postgres] package (so `pip install airflow` doesn't even   install   it).", "We went through a very similar exercise with Samza, where we   had a   dependency on Paramiko (also LGPL [1]), and our (LinkedIn) lawyers   talked   to Apache, and agreed that it was fine.", "[1] https://github.com/paramiko/paramiko/blob/master/LICENSE     On Wed, Jul 6, 2016 at 10:16 PM, Chris Nauroth <   cnauroth@hortonworks.com     wrote:     Here are more details on Apache release requirements:     http://www.apache.org/dev/release-publishing.html       http://www.apache.org/dev/release       To summarize, it's much more focused on compliance with licensing,   signing   and Apache infrastructure requirements.", "That's the kind of   scrutiny   that   a release candidate will get from the Incubator PMC rather than   deep   testing for verification of new features or bug fixes.", "For that reason, I think it makes sense for a podling's first   Apache   release to focus on nothing but those ASF policy requirements."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["We went through a very similar exercise with Samza, where we   had a   dependency on Paramiko (also LGPL [1]), and our (LinkedIn) lawyers   talked   to Apache, and agreed that it was fine.", "[1] https://github.com/paramiko/paramiko/blob/master/LICENSE     On Wed, Jul 6, 2016 at 10:16 PM, Chris Nauroth <   cnauroth@hortonworks.com     wrote:     Here are more details on Apache release requirements:     http://www.apache.org/dev/release-publishing.html       http://www.apache.org/dev/release       To summarize, it's much more focused on compliance with licensing,   signing   and Apache infrastructure requirements.", "That's the kind of   scrutiny   that   a release candidate will get from the Incubator PMC rather than   deep   testing for verification of new features or bug fixes.", "For that reason, I think it makes sense for a podling's first   Apache   release to focus on nothing but those ASF policy requirements.", "It's   completely normal for a podling's early release candidates to have   a   few   false starts that get voted down, because the policies are complex   the   first time around."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["That's the kind of   scrutiny   that   a release candidate will get from the Incubator PMC rather than   deep   testing for verification of new features or bug fixes.", "For that reason, I think it makes sense for a podling's first   Apache   release to focus on nothing but those ASF policy requirements.", "It's   completely normal for a podling's early release candidates to have   a   few   false starts that get voted down, because the policies are complex   the   first time around.", "Some projects have found it helpful to write a   \"How   to   Release\" web page during the first release, so that they have   step-by-step   notes to follow during subsequent releases.", "Focusing on \"latest   stable\"   with a few additional patches sounds like a great plan to me,   because   it   decouples the challenges of your first ASF release from other   software   development pressures, such as pressure from a user base to ship a   new   feature quickly."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["It's   completely normal for a podling's early release candidates to have   a   few   false starts that get voted down, because the policies are complex   the   first time around.", "Some projects have found it helpful to write a   \"How   to   Release\" web page during the first release, so that they have   step-by-step   notes to follow during subsequent releases.", "Focusing on \"latest   stable\"   with a few additional patches sounds like a great plan to me,   because   it   decouples the challenges of your first ASF release from other   software   development pressures, such as pressure from a user base to ship a   new   feature quickly.", "Regarding the LGPL question, in general, the answer is that we are   prohibited from redistributing any LGPL component, but it's   acceptable   to   have a soft dependency on an LGPL component, such that a user   could   deploy   the LGPL component separately to enable additional optional   features.", "More details are here:     http://www.apache.org/legal/resolved.html#prohibited       A specific example of this is Apache Hadoop's integration with LZO   compression, which uses a GPL license."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Some projects have found it helpful to write a   \"How   to   Release\" web page during the first release, so that they have   step-by-step   notes to follow during subsequent releases.", "Focusing on \"latest   stable\"   with a few additional patches sounds like a great plan to me,   because   it   decouples the challenges of your first ASF release from other   software   development pressures, such as pressure from a user base to ship a   new   feature quickly.", "Regarding the LGPL question, in general, the answer is that we are   prohibited from redistributing any LGPL component, but it's   acceptable   to   have a soft dependency on an LGPL component, such that a user   could   deploy   the LGPL component separately to enable additional optional   features.", "More details are here:     http://www.apache.org/legal/resolved.html#prohibited       A specific example of this is Apache Hadoop's integration with LZO   compression, which uses a GPL license.", "Hadoop does not   redistribute   LZO   or include any code that is tightly coupled to it, but the Hadoop   codebase   does have a notion of a pluggable CompressionCodec, with   implementations   of the interface discoverable at runtime."], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["Focusing on \"latest   stable\"   with a few additional patches sounds like a great plan to me,   because   it   decouples the challenges of your first ASF release from other   software   development pressures, such as pressure from a user base to ship a   new   feature quickly.", "Regarding the LGPL question, in general, the answer is that we are   prohibited from redistributing any LGPL component, but it's   acceptable   to   have a soft dependency on an LGPL component, such that a user   could   deploy   the LGPL component separately to enable additional optional   features.", "More details are here:     http://www.apache.org/legal/resolved.html#prohibited       A specific example of this is Apache Hadoop's integration with LZO   compression, which uses a GPL license.", "Hadoop does not   redistribute   LZO   or include any code that is tightly coupled to it, but the Hadoop   codebase   does have a notion of a pluggable CompressionCodec, with   implementations   of the interface discoverable at runtime.", "This setup supports   users   downloading and installing a separate LZO integration library onto   Hadoop's classpath."], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regarding the LGPL question, in general, the answer is that we are   prohibited from redistributing any LGPL component, but it's   acceptable   to   have a soft dependency on an LGPL component, such that a user   could   deploy   the LGPL component separately to enable additional optional   features.", "More details are here:     http://www.apache.org/legal/resolved.html#prohibited       A specific example of this is Apache Hadoop's integration with LZO   compression, which uses a GPL license.", "Hadoop does not   redistribute   LZO   or include any code that is tightly coupled to it, but the Hadoop   codebase   does have a notion of a pluggable CompressionCodec, with   implementations   of the interface discoverable at runtime.", "This setup supports   users   downloading and installing a separate LZO integration library onto   Hadoop's classpath.", "--Chris Nauroth           On 7/6/16, 9:36 PM, \"Maxime Beauchemin\"   <maximebeauchemin@gmail.com     wrote:     Hi,     This sounds very reasonable to me, though we may be able to do an   earlier   release as a practice run for an Apache release with a snapshot   of   our   production which would consists of the latest release plus a set   of   cherry   picked PRs."], "labels": ["1", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["More details are here:     http://www.apache.org/legal/resolved.html#prohibited       A specific example of this is Apache Hadoop's integration with LZO   compression, which uses a GPL license.", "Hadoop does not   redistribute   LZO   or include any code that is tightly coupled to it, but the Hadoop   codebase   does have a notion of a pluggable CompressionCodec, with   implementations   of the interface discoverable at runtime.", "This setup supports   users   downloading and installing a separate LZO integration library onto   Hadoop's classpath.", "--Chris Nauroth           On 7/6/16, 9:36 PM, \"Maxime Beauchemin\"   <maximebeauchemin@gmail.com     wrote:     Hi,     This sounds very reasonable to me, though we may be able to do an   earlier   release as a practice run for an Apache release with a snapshot   of   our   production which would consists of the latest release plus a set   of   cherry   picked PRs.", "How does an Apache release differ from a standard release again?"], "labels": ["1", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Hadoop does not   redistribute   LZO   or include any code that is tightly coupled to it, but the Hadoop   codebase   does have a notion of a pluggable CompressionCodec, with   implementations   of the interface discoverable at runtime.", "This setup supports   users   downloading and installing a separate LZO integration library onto   Hadoop's classpath.", "--Chris Nauroth           On 7/6/16, 9:36 PM, \"Maxime Beauchemin\"   <maximebeauchemin@gmail.com     wrote:     Hi,     This sounds very reasonable to me, though we may be able to do an   earlier   release as a practice run for an Apache release with a snapshot   of   our   production which would consists of the latest release plus a set   of   cherry   picked PRs.", "How does an Apache release differ from a standard release again?", "Max     On Wed, Jul 6, 2016 at 7:59 PM, Chris Riccomini <   criccomini@apache.org     wrote:     One other thing to note is that I'm planning to run the RCs in   all   of   our   environments to exercise things."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["This setup supports   users   downloading and installing a separate LZO integration library onto   Hadoop's classpath.", "--Chris Nauroth           On 7/6/16, 9:36 PM, \"Maxime Beauchemin\"   <maximebeauchemin@gmail.com     wrote:     Hi,     This sounds very reasonable to me, though we may be able to do an   earlier   release as a practice run for an Apache release with a snapshot   of   our   production which would consists of the latest release plus a set   of   cherry   picked PRs.", "How does an Apache release differ from a standard release again?", "Max     On Wed, Jul 6, 2016 at 7:59 PM, Chris Riccomini <   criccomini@apache.org     wrote:     One other thing to note is that I'm planning to run the RCs in   all   of   our   environments to exercise things.", "We should make sure that we're   all   committed (as well as the community) to burning the RCs in on   our   respective environments for a few weeks."], "labels": ["0", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["--Chris Nauroth           On 7/6/16, 9:36 PM, \"Maxime Beauchemin\"   <maximebeauchemin@gmail.com     wrote:     Hi,     This sounds very reasonable to me, though we may be able to do an   earlier   release as a practice run for an Apache release with a snapshot   of   our   production which would consists of the latest release plus a set   of   cherry   picked PRs.", "How does an Apache release differ from a standard release again?", "Max     On Wed, Jul 6, 2016 at 7:59 PM, Chris Riccomini <   criccomini@apache.org     wrote:     One other thing to note is that I'm planning to run the RCs in   all   of   our   environments to exercise things.", "We should make sure that we're   all   committed (as well as the community) to burning the RCs in on   our   respective environments for a few weeks.", "On Wed, Jul 6, 2016 at 7:58 PM, Chris Riccomini <   criccomini@apache.org   wrote:     Hey Bolke,     should we aim for a release 1st week of September     Sounds good to me!"], "labels": ["1", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["How does an Apache release differ from a standard release again?", "Max     On Wed, Jul 6, 2016 at 7:59 PM, Chris Riccomini <   criccomini@apache.org     wrote:     One other thing to note is that I'm planning to run the RCs in   all   of   our   environments to exercise things.", "We should make sure that we're   all   committed (as well as the community) to burning the RCs in on   our   respective environments for a few weeks.", "On Wed, Jul 6, 2016 at 7:58 PM, Chris Riccomini <   criccomini@apache.org   wrote:     Hey Bolke,     should we aim for a release 1st week of September     Sounds good to me!", "I would want to aim earlier, but due to holidays I guess it   might   be   smarter to schedule it a bit after?"], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max     On Wed, Jul 6, 2016 at 7:59 PM, Chris Riccomini <   criccomini@apache.org     wrote:     One other thing to note is that I'm planning to run the RCs in   all   of   our   environments to exercise things.", "We should make sure that we're   all   committed (as well as the community) to burning the RCs in on   our   respective environments for a few weeks.", "On Wed, Jul 6, 2016 at 7:58 PM, Chris Riccomini <   criccomini@apache.org   wrote:     Hey Bolke,     should we aim for a release 1st week of September     Sounds good to me!", "I would want to aim earlier, but due to holidays I guess it   might   be   smarter to schedule it a bit after?", "So would I, personally."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We should make sure that we're   all   committed (as well as the community) to burning the RCs in on   our   respective environments for a few weeks.", "On Wed, Jul 6, 2016 at 7:58 PM, Chris Riccomini <   criccomini@apache.org   wrote:     Hey Bolke,     should we aim for a release 1st week of September     Sounds good to me!", "I would want to aim earlier, but due to holidays I guess it   might   be   smarter to schedule it a bit after?", "So would I, personally.", "I'd be OK with starting RCs now, to be   frank."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Jul 6, 2016 at 7:58 PM, Chris Riccomini <   criccomini@apache.org   wrote:     Hey Bolke,     should we aim for a release 1st week of September     Sounds good to me!", "I would want to aim earlier, but due to holidays I guess it   might   be   smarter to schedule it a bit after?", "So would I, personally.", "I'd be OK with starting RCs now, to be   frank.", "What   do others think?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'd be OK with starting RCs now, to be   frank.", "What   do others think?", "Should we vote on the above?", "No need, IMO.", "A discuss like this is enough, assuming there are   no   objections."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["What   do others think?", "Should we vote on the above?", "No need, IMO.", "A discuss like this is enough, assuming there are   no   objections.", "Re: psycopg2, I don't think it's an issue, but we should   probably   file a   LEGAL [1] just to be sure."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["Should we vote on the above?", "No need, IMO.", "A discuss like this is enough, assuming there are   no   objections.", "Re: psycopg2, I don't think it's an issue, but we should   probably   file a   LEGAL [1] just to be sure.", "In any case, we don't have to be   compliant   for a   release in incubator, I believe."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["No need, IMO.", "A discuss like this is enough, assuming there are   no   objections.", "Re: psycopg2, I don't think it's an issue, but we should   probably   file a   LEGAL [1] just to be sure.", "In any case, we don't have to be   compliant   for a   release in incubator, I believe.", "We just need to be moving in   that   direction."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["A discuss like this is enough, assuming there are   no   objections.", "Re: psycopg2, I don't think it's an issue, but we should   probably   file a   LEGAL [1] just to be sure.", "In any case, we don't have to be   compliant   for a   release in incubator, I believe.", "We just need to be moving in   that   direction.", "Cheers,   Chris     [1] https://issues.apache.org/jira/browse/LEGAL     On Wed, Jul 6, 2016 at 1:24 PM, Bolke de Bruin <   bdbruin@gmail.com   wrote:     Hi,     As I don\u00b9t think there are any show stoppers to have an Apache   release   should we aim for a release 1st week of September?"], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["Re: psycopg2, I don't think it's an issue, but we should   probably   file a   LEGAL [1] just to be sure.", "In any case, we don't have to be   compliant   for a   release in incubator, I believe.", "We just need to be moving in   that   direction.", "Cheers,   Chris     [1] https://issues.apache.org/jira/browse/LEGAL     On Wed, Jul 6, 2016 at 1:24 PM, Bolke de Bruin <   bdbruin@gmail.com   wrote:     Hi,     As I don\u00b9t think there are any show stoppers to have an Apache   release   should we aim for a release 1st week of September?", "I would   want   to   aim   earlier, but due to holidays I guess it might be smarter to   schedule   it   a   bit after?"], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["In any case, we don't have to be   compliant   for a   release in incubator, I believe.", "We just need to be moving in   that   direction.", "Cheers,   Chris     [1] https://issues.apache.org/jira/browse/LEGAL     On Wed, Jul 6, 2016 at 1:24 PM, Bolke de Bruin <   bdbruin@gmail.com   wrote:     Hi,     As I don\u00b9t think there are any show stoppers to have an Apache   release   should we aim for a release 1st week of September?", "I would   want   to   aim   earlier, but due to holidays I guess it might be smarter to   schedule   it   a   bit after?", "- RC last week of August giving about two weeks to have it run   in   production in our environments   - Guess voting needs to happen at the IPMC   - Release (Champagne!)"], "labels": ["1", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Cheers,   Chris     [1] https://issues.apache.org/jira/browse/LEGAL     On Wed, Jul 6, 2016 at 1:24 PM, Bolke de Bruin <   bdbruin@gmail.com   wrote:     Hi,     As I don\u00b9t think there are any show stoppers to have an Apache   release   should we aim for a release 1st week of September?", "I would   want   to   aim   earlier, but due to holidays I guess it might be smarter to   schedule   it   a   bit after?", "- RC last week of August giving about two weeks to have it run   in   production in our environments   - Guess voting needs to happen at the IPMC   - Release (Champagne!)", "Earlier there was some discussion about psycopg2 / postgres   interoperability (psycopg2 being LGPL).", "Personally I don\u00b9t   think   there   is   an issue as we are not redistributing psycopg2 ourselves and   we   are   not   dependent on it to function."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would   want   to   aim   earlier, but due to holidays I guess it might be smarter to   schedule   it   a   bit after?", "- RC last week of August giving about two weeks to have it run   in   production in our environments   - Guess voting needs to happen at the IPMC   - Release (Champagne!)", "Earlier there was some discussion about psycopg2 / postgres   interoperability (psycopg2 being LGPL).", "Personally I don\u00b9t   think   there   is   an issue as we are not redistributing psycopg2 ourselves and   we   are   not   dependent on it to function.", "An company can choose thus what   they   want   without being `tainted` by the LGPL."], "labels": ["0", "1", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["- RC last week of August giving about two weeks to have it run   in   production in our environments   - Guess voting needs to happen at the IPMC   - Release (Champagne!)", "Earlier there was some discussion about psycopg2 / postgres   interoperability (psycopg2 being LGPL).", "Personally I don\u00b9t   think   there   is   an issue as we are not redistributing psycopg2 ourselves and   we   are   not   dependent on it to function.", "An company can choose thus what   they   want   without being `tainted` by the LGPL.", "Any remarks on this?"], "labels": ["1", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Earlier there was some discussion about psycopg2 / postgres   interoperability (psycopg2 being LGPL).", "Personally I don\u00b9t   think   there   is   an issue as we are not redistributing psycopg2 ourselves and   we   are   not   dependent on it to function.", "An company can choose thus what   they   want   without being `tainted` by the LGPL.", "Any remarks on this?", "Should we vote on the above?"], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Personally I don\u00b9t   think   there   is   an issue as we are not redistributing psycopg2 ourselves and   we   are   not   dependent on it to function.", "An company can choose thus what   they   want   without being `tainted` by the LGPL.", "Any remarks on this?", "Should we vote on the above?", "- Bolke                              "], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You are also welcome to tweet your slides out as well (and mention @ApacheAirflow) so we can retweet via the Apache Airflow twitter account.", "-s  On Thu, Nov 17, 2016 at 2:04 PM, Rob Froetscher <rfroetscher@lumoslabs.com wrote:   Thanks, I've added the slides to the links page.", "I didn't see a specific  page for that meetup for slides in general.", "On Thu, Nov 17, 2016 at 11:39 AM, siddharth anand <sanand@apache.org  wrote:    Rob,   Wiki Access granted.", "-s     On Thu, Nov 17, 2016 at 10:49 AM, Rob Froetscher <   rfroetscher@lumoslabs.com   wrote:      Here are our slides:    https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYd    Ohq_np90VlbVRc/edit?usp=sharing       I don't think I have permissions to edit the wiki       On Thu, Nov 17, 2016 at 10:41 AM, Chris Riccomini <  criccomini@apache.org       wrote:        We'll be posting the video recording shortly."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I didn't see a specific  page for that meetup for slides in general.", "On Thu, Nov 17, 2016 at 11:39 AM, siddharth anand <sanand@apache.org  wrote:    Rob,   Wiki Access granted.", "-s     On Thu, Nov 17, 2016 at 10:49 AM, Rob Froetscher <   rfroetscher@lumoslabs.com   wrote:      Here are our slides:    https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYd    Ohq_np90VlbVRc/edit?usp=sharing       I don't think I have permissions to edit the wiki       On Thu, Nov 17, 2016 at 10:41 AM, Chris Riccomini <  criccomini@apache.org       wrote:        We'll be posting the video recording shortly.", "IT is working on it.", ":)         Will post link on the meetup and mailing list."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["On Thu, Nov 17, 2016 at 11:39 AM, siddharth anand <sanand@apache.org  wrote:    Rob,   Wiki Access granted.", "-s     On Thu, Nov 17, 2016 at 10:49 AM, Rob Froetscher <   rfroetscher@lumoslabs.com   wrote:      Here are our slides:    https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYd    Ohq_np90VlbVRc/edit?usp=sharing       I don't think I have permissions to edit the wiki       On Thu, Nov 17, 2016 at 10:41 AM, Chris Riccomini <  criccomini@apache.org       wrote:        We'll be posting the video recording shortly.", "IT is working on it.", ":)         Will post link on the meetup and mailing list.", "On Thu, Nov 17, 2016 at 10:12 AM, Siddharth Anand     <sanand@agari.com.invalid wrote:      Chris and WePayEng,      Thanks for hosting another great Airflow meet-up."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Hi Folks,Interested in the latest news about the Apache Airflow (Incubating) project, follow our newly-minted twitter account!", "ApacheAirflow (@ApacheAirflow) | Twitter     |   |    |    |   |    |     |    | |   |    |   ApacheAirflow (@ApacheAirflow) | Twitter  The latest Tweets from ApacheAirflow (@ApacheAirflow)  |   |    |    |     "], "labels": ["1", "1"]}
{"abstract_id": 0, "sentences": ["depends_on_past is looking at previous task instance which sounds the same as \"latestonly\" but the difference becomes apparent if you look at this example.", "Let's say you have a dag, scheduled to run every day and it has been failing for the past 3 days.", "The whole purpose of that dag is to populate snapshot table or do a daily backup.", "If you use depends on past, you would have to rerun all missed runs or mark them as successful eventually doing useless work (3 daily snapshots or backups for the same data).", "LatestOnly allows you to bypass missed runs and just do it once for most recent instance."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let's say you have a dag, scheduled to run every day and it has been failing for the past 3 days.", "The whole purpose of that dag is to populate snapshot table or do a daily backup.", "If you use depends on past, you would have to rerun all missed runs or mark them as successful eventually doing useless work (3 daily snapshots or backups for the same data).", "LatestOnly allows you to bypass missed runs and just do it once for most recent instance.", "Another difference, depends on past is tricky if you use BranchOperator because some branches may not run one day and run another - it will really mess up your logic."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The whole purpose of that dag is to populate snapshot table or do a daily backup.", "If you use depends on past, you would have to rerun all missed runs or mark them as successful eventually doing useless work (3 daily snapshots or backups for the same data).", "LatestOnly allows you to bypass missed runs and just do it once for most recent instance.", "Another difference, depends on past is tricky if you use BranchOperator because some branches may not run one day and run another - it will really mess up your logic.", "On Mon, Mar 20, 2017 at 12:45 PM, Ruslan Dautkhanov <dautkhanov@gmail.com wrote:   Thanks Boris."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you use depends on past, you would have to rerun all missed runs or mark them as successful eventually doing useless work (3 daily snapshots or backups for the same data).", "LatestOnly allows you to bypass missed runs and just do it once for most recent instance.", "Another difference, depends on past is tricky if you use BranchOperator because some branches may not run one day and run another - it will really mess up your logic.", "On Mon, Mar 20, 2017 at 12:45 PM, Ruslan Dautkhanov <dautkhanov@gmail.com wrote:   Thanks Boris.", "It does make sense."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Another difference, depends on past is tricky if you use BranchOperator because some branches may not run one day and run another - it will really mess up your logic.", "On Mon, Mar 20, 2017 at 12:45 PM, Ruslan Dautkhanov <dautkhanov@gmail.com wrote:   Thanks Boris.", "It does make sense.", "Although how it's different from depends_on_past task-level parameter?", "In both cases, a task will be skipped if there is another TI of this task  is still running (from a previous dagrun), right?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Mon, Mar 20, 2017 at 12:45 PM, Ruslan Dautkhanov <dautkhanov@gmail.com wrote:   Thanks Boris.", "It does make sense.", "Although how it's different from depends_on_past task-level parameter?", "In both cases, a task will be skipped if there is another TI of this task  is still running (from a previous dagrun), right?", "Thanks,  Ruslan    On Sat, Mar 18, 2017 at 7:11 PM, Boris Tyukin <boris@boristyukin.com  wrote:    you would just chain them - there is an example that came with airflow  1.8   https://github.com/apache/incubator-airflow/blob/master/   airflow/example_dags/example_latest_only.py     so in your case, instead of dummy operator, you would use your Oracle   operator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It does make sense.", "Although how it's different from depends_on_past task-level parameter?", "In both cases, a task will be skipped if there is another TI of this task  is still running (from a previous dagrun), right?", "Thanks,  Ruslan    On Sat, Mar 18, 2017 at 7:11 PM, Boris Tyukin <boris@boristyukin.com  wrote:    you would just chain them - there is an example that came with airflow  1.8   https://github.com/apache/incubator-airflow/blob/master/   airflow/example_dags/example_latest_only.py     so in your case, instead of dummy operator, you would use your Oracle   operator.", "Does it make sense?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Although how it's different from depends_on_past task-level parameter?", "In both cases, a task will be skipped if there is another TI of this task  is still running (from a previous dagrun), right?", "Thanks,  Ruslan    On Sat, Mar 18, 2017 at 7:11 PM, Boris Tyukin <boris@boristyukin.com  wrote:    you would just chain them - there is an example that came with airflow  1.8   https://github.com/apache/incubator-airflow/blob/master/   airflow/example_dags/example_latest_only.py     so in your case, instead of dummy operator, you would use your Oracle   operator.", "Does it make sense?", "On Sat, Mar 18, 2017 at 7:12 PM, Ruslan Dautkhanov <dautkhanov@gmail.com     wrote:      Is there is a way to combine scheduling behavior operators  (like this    LatestOnlyOperator)    with a functional operator (like Oracle_Operator)?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In both cases, a task will be skipped if there is another TI of this task  is still running (from a previous dagrun), right?", "Thanks,  Ruslan    On Sat, Mar 18, 2017 at 7:11 PM, Boris Tyukin <boris@boristyukin.com  wrote:    you would just chain them - there is an example that came with airflow  1.8   https://github.com/apache/incubator-airflow/blob/master/   airflow/example_dags/example_latest_only.py     so in your case, instead of dummy operator, you would use your Oracle   operator.", "Does it make sense?", "On Sat, Mar 18, 2017 at 7:12 PM, Ruslan Dautkhanov <dautkhanov@gmail.com     wrote:      Is there is a way to combine scheduling behavior operators  (like this    LatestOnlyOperator)    with a functional operator (like Oracle_Operator)?", "I was thinking   multiple    inheritance would do,like        class Oracle_LatestOnly_Operator (Oracle_Operator,  LatestOnlyOperator):     ..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Ruslan    On Sat, Mar 18, 2017 at 7:11 PM, Boris Tyukin <boris@boristyukin.com  wrote:    you would just chain them - there is an example that came with airflow  1.8   https://github.com/apache/incubator-airflow/blob/master/   airflow/example_dags/example_latest_only.py     so in your case, instead of dummy operator, you would use your Oracle   operator.", "Does it make sense?", "On Sat, Mar 18, 2017 at 7:12 PM, Ruslan Dautkhanov <dautkhanov@gmail.com     wrote:      Is there is a way to combine scheduling behavior operators  (like this    LatestOnlyOperator)    with a functional operator (like Oracle_Operator)?", "I was thinking   multiple    inheritance would do,like        class Oracle_LatestOnly_Operator (Oracle_Operator,  LatestOnlyOperator):     ...", "I might be overthinking this and there could be a simpler way?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Does it make sense?", "On Sat, Mar 18, 2017 at 7:12 PM, Ruslan Dautkhanov <dautkhanov@gmail.com     wrote:      Is there is a way to combine scheduling behavior operators  (like this    LatestOnlyOperator)    with a functional operator (like Oracle_Operator)?", "I was thinking   multiple    inheritance would do,like        class Oracle_LatestOnly_Operator (Oracle_Operator,  LatestOnlyOperator):     ...", "I might be overthinking this and there could be a simpler way?", "Sorry, I am still learning Airflow concepts..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Sat, Mar 18, 2017 at 7:12 PM, Ruslan Dautkhanov <dautkhanov@gmail.com     wrote:      Is there is a way to combine scheduling behavior operators  (like this    LatestOnlyOperator)    with a functional operator (like Oracle_Operator)?", "I was thinking   multiple    inheritance would do,like        class Oracle_LatestOnly_Operator (Oracle_Operator,  LatestOnlyOperator):     ...", "I might be overthinking this and there could be a simpler way?", "Sorry, I am still learning Airflow concepts...", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I was thinking   multiple    inheritance would do,like        class Oracle_LatestOnly_Operator (Oracle_Operator,  LatestOnlyOperator):     ...", "I might be overthinking this and there could be a simpler way?", "Sorry, I am still learning Airflow concepts...", "Thanks.", "--    Ruslan Dautkhanov       On Sat, Mar 18, 2017 at 2:15 PM, Boris Tyukin <boris@boristyukin.com    wrote:        Thanks George for that feature!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I might be overthinking this and there could be a simpler way?", "Sorry, I am still learning Airflow concepts...", "Thanks.", "--    Ruslan Dautkhanov       On Sat, Mar 18, 2017 at 2:15 PM, Boris Tyukin <boris@boristyukin.com    wrote:        Thanks George for that feature!", "sure, just created a jira on this     https://issues.apache.org/jira/browse/AIRFLOW-1008             On Sat, Mar 18, 2017 at 12:05 PM, siddharth anand <sanand@apache.org       wrote:          Thx Boris ."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sorry, I am still learning Airflow concepts...", "Thanks.", "--    Ruslan Dautkhanov       On Sat, Mar 18, 2017 at 2:15 PM, Boris Tyukin <boris@boristyukin.com    wrote:        Thanks George for that feature!", "sure, just created a jira on this     https://issues.apache.org/jira/browse/AIRFLOW-1008             On Sat, Mar 18, 2017 at 12:05 PM, siddharth anand <sanand@apache.org       wrote:          Thx Boris .", "Credit goes to George (gwax) for the implementation of   the      LatestOnlyOperator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks.", "--    Ruslan Dautkhanov       On Sat, Mar 18, 2017 at 2:15 PM, Boris Tyukin <boris@boristyukin.com    wrote:        Thanks George for that feature!", "sure, just created a jira on this     https://issues.apache.org/jira/browse/AIRFLOW-1008             On Sat, Mar 18, 2017 at 12:05 PM, siddharth anand <sanand@apache.org       wrote:          Thx Boris .", "Credit goes to George (gwax) for the implementation of   the      LatestOnlyOperator.", "Boris,      Can you describe what you mean in a Jira?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["--    Ruslan Dautkhanov       On Sat, Mar 18, 2017 at 2:15 PM, Boris Tyukin <boris@boristyukin.com    wrote:        Thanks George for that feature!", "sure, just created a jira on this     https://issues.apache.org/jira/browse/AIRFLOW-1008             On Sat, Mar 18, 2017 at 12:05 PM, siddharth anand <sanand@apache.org       wrote:          Thx Boris .", "Credit goes to George (gwax) for the implementation of   the      LatestOnlyOperator.", "Boris,      Can you describe what you mean in a Jira?", "-s           On Fri, Mar 17, 2017 at 6:02 PM, Boris Tyukin <  boris@boristyukin.com         wrote:            this is nice indeed along with the new catchup option       https://airflow.incubator.apache.org/scheduler.html#     backfill-and-catchup             Thanks Sid and Ben for adding these new options!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["sure, just created a jira on this     https://issues.apache.org/jira/browse/AIRFLOW-1008             On Sat, Mar 18, 2017 at 12:05 PM, siddharth anand <sanand@apache.org       wrote:          Thx Boris .", "Credit goes to George (gwax) for the implementation of   the      LatestOnlyOperator.", "Boris,      Can you describe what you mean in a Jira?", "-s           On Fri, Mar 17, 2017 at 6:02 PM, Boris Tyukin <  boris@boristyukin.com         wrote:            this is nice indeed along with the new catchup option       https://airflow.incubator.apache.org/scheduler.html#     backfill-and-catchup             Thanks Sid and Ben for adding these new options!", "for a complete picture, it would be nice to force only one dag  run   at     the       time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Credit goes to George (gwax) for the implementation of   the      LatestOnlyOperator.", "Boris,      Can you describe what you mean in a Jira?", "-s           On Fri, Mar 17, 2017 at 6:02 PM, Boris Tyukin <  boris@boristyukin.com         wrote:            this is nice indeed along with the new catchup option       https://airflow.incubator.apache.org/scheduler.html#     backfill-and-catchup             Thanks Sid and Ben for adding these new options!", "for a complete picture, it would be nice to force only one dag  run   at     the       time.", "On Fri, Mar 17, 2017 at 7:33 PM, siddharth anand <   sanand@apache.org       wrote:              With the Apache Airflow 1.8 release imminent, you may want to  try    out      the               *LatestOnlyOperator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Boris,      Can you describe what you mean in a Jira?", "-s           On Fri, Mar 17, 2017 at 6:02 PM, Boris Tyukin <  boris@boristyukin.com         wrote:            this is nice indeed along with the new catchup option       https://airflow.incubator.apache.org/scheduler.html#     backfill-and-catchup             Thanks Sid and Ben for adding these new options!", "for a complete picture, it would be nice to force only one dag  run   at     the       time.", "On Fri, Mar 17, 2017 at 7:33 PM, siddharth anand <   sanand@apache.org       wrote:              With the Apache Airflow 1.8 release imminent, you may want to  try    out      the               *LatestOnlyOperator.", "*               If you want your DAG to only run on the most recent scheduled   slot,        regardless of backlog, this operator will skip running  downstream     tasks       for        all DAG Runs prior to the current time slot."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-s           On Fri, Mar 17, 2017 at 6:02 PM, Boris Tyukin <  boris@boristyukin.com         wrote:            this is nice indeed along with the new catchup option       https://airflow.incubator.apache.org/scheduler.html#     backfill-and-catchup             Thanks Sid and Ben for adding these new options!", "for a complete picture, it would be nice to force only one dag  run   at     the       time.", "On Fri, Mar 17, 2017 at 7:33 PM, siddharth anand <   sanand@apache.org       wrote:              With the Apache Airflow 1.8 release imminent, you may want to  try    out      the               *LatestOnlyOperator.", "*               If you want your DAG to only run on the most recent scheduled   slot,        regardless of backlog, this operator will skip running  downstream     tasks       for        all DAG Runs prior to the current time slot.", "For example, I might have a DAG that takes a DB snapshot once a    day."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["for a complete picture, it would be nice to force only one dag  run   at     the       time.", "On Fri, Mar 17, 2017 at 7:33 PM, siddharth anand <   sanand@apache.org       wrote:              With the Apache Airflow 1.8 release imminent, you may want to  try    out      the               *LatestOnlyOperator.", "*               If you want your DAG to only run on the most recent scheduled   slot,        regardless of backlog, this operator will skip running  downstream     tasks       for        all DAG Runs prior to the current time slot.", "For example, I might have a DAG that takes a DB snapshot once a    day.", "It        might be that I paused that DAG for 2 weeks or that I had set  the     start        date to a fixed data 2 weeks in the past."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Mar 17, 2017 at 7:33 PM, siddharth anand <   sanand@apache.org       wrote:              With the Apache Airflow 1.8 release imminent, you may want to  try    out      the               *LatestOnlyOperator.", "*               If you want your DAG to only run on the most recent scheduled   slot,        regardless of backlog, this operator will skip running  downstream     tasks       for        all DAG Runs prior to the current time slot.", "For example, I might have a DAG that takes a DB snapshot once a    day.", "It        might be that I paused that DAG for 2 weeks or that I had set  the     start        date to a fixed data 2 weeks in the past.", "When I enable my  DAG, I     don't        want it to run 14 days' worth of snapshots for the current  state   of     the       DB        -- that's unnecessary work."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*               If you want your DAG to only run on the most recent scheduled   slot,        regardless of backlog, this operator will skip running  downstream     tasks       for        all DAG Runs prior to the current time slot.", "For example, I might have a DAG that takes a DB snapshot once a    day.", "It        might be that I paused that DAG for 2 weeks or that I had set  the     start        date to a fixed data 2 weeks in the past.", "When I enable my  DAG, I     don't        want it to run 14 days' worth of snapshots for the current  state   of     the       DB        -- that's unnecessary work.", "The LatestOnlyOperator avoids that work."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For example, I might have a DAG that takes a DB snapshot once a    day.", "It        might be that I paused that DAG for 2 weeks or that I had set  the     start        date to a fixed data 2 weeks in the past.", "When I enable my  DAG, I     don't        want it to run 14 days' worth of snapshots for the current  state   of     the       DB        -- that's unnecessary work.", "The LatestOnlyOperator avoids that work.", "https://github.com/apache/incubator-airflow/commit/        edf033be65b575f44aa221d5d0ec9ecb6b32c67a               With it, you can simply use        latest_only = LatestOnlyOperator(task_id='latest_only',  dag=dag)               instead of        def skip_to_current_job(ds, **kwargs):            now = datetime.now()            left_window = kwargs['dag'].following_     schedule(kwargs['execution_        date'])            right_window = kwargs['dag'].following_  schedule(left_window)            logging.info(('Left Window {}, Now {}, Right Window        {}').format(left_window,now,right_window))            if not now <= right_window:                logging.info('Not latest execution, skipping   downstream.')"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It        might be that I paused that DAG for 2 weeks or that I had set  the     start        date to a fixed data 2 weeks in the past.", "When I enable my  DAG, I     don't        want it to run 14 days' worth of snapshots for the current  state   of     the       DB        -- that's unnecessary work.", "The LatestOnlyOperator avoids that work.", "https://github.com/apache/incubator-airflow/commit/        edf033be65b575f44aa221d5d0ec9ecb6b32c67a               With it, you can simply use        latest_only = LatestOnlyOperator(task_id='latest_only',  dag=dag)               instead of        def skip_to_current_job(ds, **kwargs):            now = datetime.now()            left_window = kwargs['dag'].following_     schedule(kwargs['execution_        date'])            right_window = kwargs['dag'].following_  schedule(left_window)            logging.info(('Left Window {}, Now {}, Right Window        {}').format(left_window,now,right_window))            if not now <= right_window:                logging.info('Not latest execution, skipping   downstream.')", "return False            return True               short_circuit = ShortCircuitOperator(          task_id         = 'short_circuit_if_not_current_job',          provide_context = True,          python_callable = skip_to_current_job,          dag             = dag        )               -s                              "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Awesome, thanks Jeremiah!", "On Fri, Jan 20, 2017 at 8:20 AM, Jeremiah Lowin <jlowin@apache.org wrote:   Hi Laura,   The error is raised if an unused argument is passed to BaseOperator --  basically if there is anything in either args or kwargs.", "The original issue  was that in a number of cases arguments were misspelled or misused by  Operator subclasses and instead of raising an error, they were just passed  up the inheritance chain and finally (silently) absorbed by BaseOperator,  so there was no warning.", "I think a workaround should be straightforward -- when you call  super().__init__ for the BaseOperator, just pass arguments explicitly  rather than with args/kwargs, or (alternatively), pop arguments out of  kwargs when you use them ahead of calling that __init__.", "On Thu, Jan 19, 2017 at 10:23 AM Laura Lorenz <llorenz@industrydive.com  wrote:    Hi!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The original issue  was that in a number of cases arguments were misspelled or misused by  Operator subclasses and instead of raising an error, they were just passed  up the inheritance chain and finally (silently) absorbed by BaseOperator,  so there was no warning.", "I think a workaround should be straightforward -- when you call  super().__init__ for the BaseOperator, just pass arguments explicitly  rather than with args/kwargs, or (alternatively), pop arguments out of  kwargs when you use them ahead of calling that __init__.", "On Thu, Jan 19, 2017 at 10:23 AM Laura Lorenz <llorenz@industrydive.com  wrote:    Hi!", "Is there a way to determine the rationale behind deprecation  warnings?", "In particular I'm interested in the following:       /Users/llorenz/Envs/fileflow/lib/python2.7/site-packages/  airflow/models.py:1719:    PendingDeprecationWarning: Invalid arguments were passed to    DivePythonOperator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think a workaround should be straightforward -- when you call  super().__init__ for the BaseOperator, just pass arguments explicitly  rather than with args/kwargs, or (alternatively), pop arguments out of  kwargs when you use them ahead of calling that __init__.", "On Thu, Jan 19, 2017 at 10:23 AM Laura Lorenz <llorenz@industrydive.com  wrote:    Hi!", "Is there a way to determine the rationale behind deprecation  warnings?", "In particular I'm interested in the following:       /Users/llorenz/Envs/fileflow/lib/python2.7/site-packages/  airflow/models.py:1719:    PendingDeprecationWarning: Invalid arguments were passed to    DivePythonOperator.", "Support for passing such arguments will be dropped  in    Airflow 2.0."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Jan 19, 2017 at 10:23 AM Laura Lorenz <llorenz@industrydive.com  wrote:    Hi!", "Is there a way to determine the rationale behind deprecation  warnings?", "In particular I'm interested in the following:       /Users/llorenz/Envs/fileflow/lib/python2.7/site-packages/  airflow/models.py:1719:    PendingDeprecationWarning: Invalid arguments were passed to    DivePythonOperator.", "Support for passing such arguments will be dropped  in    Airflow 2.0.", "Invalid arguments were:       *args: ()       **kwargs: {'data_dependencies': {'something': 'write_a_file'}}         category=PendingDeprecationWarning        Our home grown plugin fileflow depends on this capability so I'd like to   get more information about how it will be changing to see if I can   anticipate a workaround to support airflow 2.0."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think you just need to fill in the report like one of the examples, and post it to that wiki.", "On Fri, Apr 29, 2016 at 1:36 PM, Siddharth Anand < siddharthanand@yahoo.com.invalid wrote:   Jacob,What's the process?", "-s       On Friday, April 29, 2016 1:26 PM, Jakob Homan <jghoman@gmail.com  wrote:     Here are the examples from last month:  https://wiki.apache.org/incubator/April2016   Here's where to edit it for May: https://wiki.apache.org/incubator/May2016   Don't feel pressure to fill in a lot; the project is just starting,  which is a fine response.", "-jg    On 29 April 2016 at 13:23, Siddharth Anand  <siddharthanand@yahoo.com.invalid wrote:   I can if someone wants to send me pointers.", "-s        On Friday, April 29, 2016 1:18 PM, Chris Riccomini <  criccomini@apache.org wrote:        So, who wants to write the board report?"], "labels": ["1", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["      [ https://issues.apache.org/jira/browse/KAFKA-46?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Neha Narkhede reassigned KAFKA-46: ----------------------------------      Assignee: Neha Narkhede       Commit thread, ReplicaFetcherThread for intra-cluster replication  -----------------------------------------------------------------                   Key: KAFKA-46                  URL: https://issues.apache.org/jira/browse/KAFKA-46              Project: Kafka           Issue Type: Bug             Reporter: Jun Rao             Assignee: Neha Narkhede   We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader.", "-- This message is automatically generated by JIRA.", "If you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa For more information on JIRA, see: http://www.atlassian.com/software/jira            "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From memory, I think this is related to having the wrong authentication method.", "https://github.com/apache/incubator-airflow/blob/master/airflow/hooks/hive_hooks.py#L578  You may want to try NOSASL.", "To do that i think you have to put something like `{ \"authMechanism\": \"NOSASL\" }` in your Connection's extra params.", "On Wed, Oct 26, 2016 at 1:50 AM, twinkle sachdeva < twinkle.sachdeva@gmail.com wrote:   Hi,   I am trying to use HiveToMySqlTransfer operator, but I am not able to read  any data with the following configuration:   TSocket.py\", line 120, in read       message='TSocket read 0 bytes')   thrift.transport.TTransport.TTransportException: TSocket read 0 bytes   It seems to happen due to some mismatch in thrift protocol etc  specification.", "Please help me on what can be done."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://github.com/apache/incubator-airflow/blob/master/airflow/hooks/hive_hooks.py#L578  You may want to try NOSASL.", "To do that i think you have to put something like `{ \"authMechanism\": \"NOSASL\" }` in your Connection's extra params.", "On Wed, Oct 26, 2016 at 1:50 AM, twinkle sachdeva < twinkle.sachdeva@gmail.com wrote:   Hi,   I am trying to use HiveToMySqlTransfer operator, but I am not able to read  any data with the following configuration:   TSocket.py\", line 120, in read       message='TSocket read 0 bytes')   thrift.transport.TTransport.TTransportException: TSocket read 0 bytes   It seems to happen due to some mismatch in thrift protocol etc  specification.", "Please help me on what can be done.", "Regards,   Twinkle   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Bolke,  Here is the JIRA issue, https://issues.apache.org/jira/browse/AIRFLOW-747.", "Harvey Xia | Software Engineer harveyxia@spotify.com +1 (339) 225 1875  On Wed, Jan 11, 2017 at 11:32 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Harvey,   Thanks for reporting!", "Can you create a lira for this?", "I\u2019ll have a look if  I can reproduce it.", "- Bolke    On 11 Jan 2017, at 16:06, Harvey Xia <harveyxia@spotify.com.INVALID  wrote:     Hi all,     In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor  the   retry_delay parameter, i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Harvey Xia | Software Engineer harveyxia@spotify.com +1 (339) 225 1875  On Wed, Jan 11, 2017 at 11:32 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Harvey,   Thanks for reporting!", "Can you create a lira for this?", "I\u2019ll have a look if  I can reproduce it.", "- Bolke    On 11 Jan 2017, at 16:06, Harvey Xia <harveyxia@spotify.com.INVALID  wrote:     Hi all,     In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor  the   retry_delay parameter, i.e.", "the retries happen immediately one after the   other without waiting the specific retry_delay time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Can you create a lira for this?", "I\u2019ll have a look if  I can reproduce it.", "- Bolke    On 11 Jan 2017, at 16:06, Harvey Xia <harveyxia@spotify.com.INVALID  wrote:     Hi all,     In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor  the   retry_delay parameter, i.e.", "the retries happen immediately one after the   other without waiting the specific retry_delay time.", "However, the  *number*   of retries is honored."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I\u2019ll have a look if  I can reproduce it.", "- Bolke    On 11 Jan 2017, at 16:06, Harvey Xia <harveyxia@spotify.com.INVALID  wrote:     Hi all,     In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor  the   retry_delay parameter, i.e.", "the retries happen immediately one after the   other without waiting the specific retry_delay time.", "However, the  *number*   of retries is honored.", "I am testing with the following code:     from airflow import DAG   from airflow.operators.bash_operator import BashOperator   from datetime import datetime, timedelta     default_args = {   'owner': 'airflow',   'depends_on_past': False,   'start_date': datetime(2016, 10, 5, 19),   'end_date': datetime(2016, 10, 6, 19),   'email': ['airflow@airflow.com'],   'email_on_failure': False,   'email_on_retry': False,   'retries': 10,   'retry_delay': timedelta(0, 500)   }     dag = DAG('test_retry_handling_job', default_args=default_args,   schedule_interval='@once')     task1 = BashOperator(   task_id='test_retry_handling_op1',   bash_command='exit 1',   dag=dag)     task2 = BashOperator(   task_id='test_retry_handling_op2',   bash_command='exit 1',   dag=dag)     task2.set_upstream(task1)     Let me know if anyone has any ideas about this issue, thanks!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Bolke    On 11 Jan 2017, at 16:06, Harvey Xia <harveyxia@spotify.com.INVALID  wrote:     Hi all,     In Airflow 1.8 alpha 2, using LocalExecutor, DAGs do not seem to honor  the   retry_delay parameter, i.e.", "the retries happen immediately one after the   other without waiting the specific retry_delay time.", "However, the  *number*   of retries is honored.", "I am testing with the following code:     from airflow import DAG   from airflow.operators.bash_operator import BashOperator   from datetime import datetime, timedelta     default_args = {   'owner': 'airflow',   'depends_on_past': False,   'start_date': datetime(2016, 10, 5, 19),   'end_date': datetime(2016, 10, 6, 19),   'email': ['airflow@airflow.com'],   'email_on_failure': False,   'email_on_retry': False,   'retries': 10,   'retry_delay': timedelta(0, 500)   }     dag = DAG('test_retry_handling_job', default_args=default_args,   schedule_interval='@once')     task1 = BashOperator(   task_id='test_retry_handling_op1',   bash_command='exit 1',   dag=dag)     task2 = BashOperator(   task_id='test_retry_handling_op2',   bash_command='exit 1',   dag=dag)     task2.set_upstream(task1)     Let me know if anyone has any ideas about this issue, thanks!", "Harvey Xia | Software Engineer   harveyxia@spotify.com   +1 (339) 225 1875    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I just wrote an answer on stack overflow.", "Thanks,  Max  On Wed, Sep 21, 2016 at 3:47 AM, Jiacai Liu <jiacai2050@gmail.com wrote:   http://stackoverflow.com/questions/39612488/airflow-  triggle-dag-execution-date-is-the-next-day-why    Don't know it is right to ask question in this email list, If I go wrong,  please let me know.", "Thanks.   "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*Today* You would need to add 2 new tasks in a chain to the end of your current dag, your_dag -- ShortCircuitOperator -- PythonOperator.", "The ShortCircuitOperator will need to specify a trigger rule of *all_done *to ensure it's always called.", "Then, specify a python callable that checks the status of upstream tasks for success.", "If all direct upstream tasks are successful, skip the last task.", "If not, then execute the last task which would carry out the failure notification that you desire."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Then, specify a python callable that checks the status of upstream tasks for success.", "If all direct upstream tasks are successful, skip the last task.", "If not, then execute the last task which would carry out the failure notification that you desire.", "*Future* Implement an on-failure callback.", "Have a look at how dag.sla_miss_callback is implemented!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If all direct upstream tasks are successful, skip the last task.", "If not, then execute the last task which would carry out the failure notification that you desire.", "*Future* Implement an on-failure callback.", "Have a look at how dag.sla_miss_callback is implemented!", "-s  On Mon, Oct 31, 2016 at 3:24 PM, Casey Ching <casey@eazeup.com wrote:   Hello,   I\u2019d like the ability to send notifications when a DAG fails."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*Future* Implement an on-failure callback.", "Have a look at how dag.sla_miss_callback is implemented!", "-s  On Mon, Oct 31, 2016 at 3:24 PM, Casey Ching <casey@eazeup.com wrote:   Hello,   I\u2019d like the ability to send notifications when a DAG fails.", "Basically the  same as on_failure_callback that is available for operators but for a DAG  instead.", "Is there a way to do that now?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Have a look at how dag.sla_miss_callback is implemented!", "-s  On Mon, Oct 31, 2016 at 3:24 PM, Casey Ching <casey@eazeup.com wrote:   Hello,   I\u2019d like the ability to send notifications when a DAG fails.", "Basically the  same as on_failure_callback that is available for operators but for a DAG  instead.", "Is there a way to do that now?", "Thanks,  Casey   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["      [ https://issues.apache.org/jira/browse/LENS-63?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Amareshwari Sriramadasu updated LENS-63: ----------------------------------------     Priority: Critical  (was: Major)   fact query on jdbc is failing with IllegalArgumentException  -----------------------------------------------------------                   Key: LENS-63                  URL: https://issues.apache.org/jira/browse/LENS-63              Project: Apache Lens           Issue Type: Bug     Affects Versions: 2.0             Reporter: Raghavendra Singh             Assignee: Sushil Mohanty             Priority: Critical              Fix For: 2.0           Attachments: LENS-63.patch    {CODE}  ERROR org.apache.lens.driver.jdbc.JDBCResultSet  - Error getting JDBC type information: Decimal precision out of allowed range [1,38]  java.lang.IllegalArgumentException: Decimal precision out of allowed range [1,38]  \tat org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.validateParameter(HiveDecimalUtils.java:73)  \tat org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:432)  \tat org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:305)  \tat org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromTypeString(TypeInfoUtils.java:759)  \tat org.apache.lens.driver.jdbc.JDBCResultSet$1.getColumns(JDBCResultSet.java:113)  \tat org.apache.lens.server.api.driver.LensResultSetMetadata.toQueryResultSetMetadata(LensResultSetMetadata.java:46)  \tat org.apache.lens.server.query.QueryExecutionServiceImpl.getResultSetMetadata(QueryExecutionServiceImpl.java:1528)  \tat org.apache.lens.server.query.QueryServiceResource.getResultSetMetadata(QueryServiceResource.java:714)  \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  \tat java.lang.reflect.Method.invoke(Method.java:622)  \tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)  \tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)  \tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)  \tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)  \tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)  \tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)  \tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)  \tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)  \tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)  \tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)  \tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)  \tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)  \tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)  \tat org.glassfish.jersey.internal.Errors.process(Errors.java:267)  \tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)  \tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)  \tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)  \tat org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)  \tat org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)  \tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)  \tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)  \tat java.lang.Thread.run(Thread.java:701)  24 Nov 2014 10:22:54,367 [Grizzly-worker(5)] WARN  org.glassfish.jersey.server.ServerRuntime$Responder  - WebApplicationException cause:  java.lang.NullPointerException  \tat org.apache.lens.server.api.driver.LensResultSetMetadata.toQueryResultSetMetadata(LensResultSetMetadata.java:46)  \tat org.apache.lens.server.query.QueryExecutionServiceImpl.getResultSetMetadata(QueryExecutionServiceImpl.java:1528)  \tat org.apache.lens.server.query.QueryServiceResource.getResultSetMetadata(QueryServiceResource.java:714)  \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  \tat java.lang.reflect.Method.invoke(Method.java:622)  \tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)  \tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)  \tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)  \tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)  \tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)  \tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)  \tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)  \tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)  \tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)  \tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)  \tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)  \tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)  \tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)  \tat org.glassfish.jersey.internal.Errors.process(Errors.java:267)  \tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)  \tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)  \tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)  \tat org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)  \tat org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)  \tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)  \tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)  \tat java.lang.Thread.run(Thread.java:701)  {CODE}    -- This message was sent by Atlassian JIRA (v6.3.4#6332)  "], "labels": ["0"]}
{"abstract_id": 0, "sentences": ["      [ https://issues.apache.org/jira/browse/ISIS-110?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Dan Haywood closed ISIS-110.", "----------------------------        Minor edits to pom.xml's to improve operation  ---------------------------------------------                   Key: ISIS-110                  URL: https://issues.apache.org/jira/browse/ISIS-110              Project: Isis           Issue Type: Sub-task             Reporter: Kevin Meyer             Priority: Trivial               Labels: maven              Fix For: 0.2.0-incubating    Minor changes to Isis pom.xml files to improve behaviour.", "Overall behaviour is unaffected by these changes, e.g.", "moving a dependency version definition higher up the build path, etc.", "-- This message is automatically generated by JIRA."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Github user minahlee commented on the pull request:      https://github.com/apache/incubator-zeppelin/pull/526#issuecomment-163819332        LGTM   --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well.", "If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA.", "---  "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hey Rob,  That would be great!", "Could you please send me a title/abstract similar to the others listed on the meetup?", "Cheers, Chris  On Thu, Oct 20, 2016 at 6:24 PM, Rob Froetscher <rfroetscher@lumoslabs.com wrote:  Hi Chris,   If you guys think it would be helpful/interesting to the meetup audience,  my colleague Tim and I would be interested in talking about how we use  Airflow to coordinate our Spark jobs on EMR.", "I wrote the EMR tooling  <https://github.com/apache/incubator-airflow/pull/1630 (hook, operators,  sensors) for Airflow so hopefully this could be a good video documentation  of how to use it.", "Tim wrote many of our Spark jobs and has more context on  how/why we use Airflow in this way for it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could you please send me a title/abstract similar to the others listed on the meetup?", "Cheers, Chris  On Thu, Oct 20, 2016 at 6:24 PM, Rob Froetscher <rfroetscher@lumoslabs.com wrote:  Hi Chris,   If you guys think it would be helpful/interesting to the meetup audience,  my colleague Tim and I would be interested in talking about how we use  Airflow to coordinate our Spark jobs on EMR.", "I wrote the EMR tooling  <https://github.com/apache/incubator-airflow/pull/1630 (hook, operators,  sensors) for Airflow so hopefully this could be a good video documentation  of how to use it.", "Tim wrote many of our Spark jobs and has more context on  how/why we use Airflow in this way for it.", "Let me know if you think this would be good."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers, Chris  On Thu, Oct 20, 2016 at 6:24 PM, Rob Froetscher <rfroetscher@lumoslabs.com wrote:  Hi Chris,   If you guys think it would be helpful/interesting to the meetup audience,  my colleague Tim and I would be interested in talking about how we use  Airflow to coordinate our Spark jobs on EMR.", "I wrote the EMR tooling  <https://github.com/apache/incubator-airflow/pull/1630 (hook, operators,  sensors) for Airflow so hopefully this could be a good video documentation  of how to use it.", "Tim wrote many of our Spark jobs and has more context on  how/why we use Airflow in this way for it.", "Let me know if you think this would be good.", "Happy to put together a short  abstract."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Tim wrote many of our Spark jobs and has more context on  how/why we use Airflow in this way for it.", "Let me know if you think this would be good.", "Happy to put together a short  abstract.", "Thanks,   Rob   On Thu, Oct 20, 2016 at 8:43 AM, Chris Riccomini <criccomini@apache.org  wrote:   Hey all,   We have one more slot to fill for the next meetup at WePay on November 16:     http://www.meetup.com/Bay-Area-Apache-Airflow-Incubating-Meetup/events/  234778571/   The slot is for 15m + 5m Q&A, so it's not too much of a commitment.", "Would you like to speak?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let me know if you think this would be good.", "Happy to put together a short  abstract.", "Thanks,   Rob   On Thu, Oct 20, 2016 at 8:43 AM, Chris Riccomini <criccomini@apache.org  wrote:   Hey all,   We have one more slot to fill for the next meetup at WePay on November 16:     http://www.meetup.com/Bay-Area-Apache-Airflow-Incubating-Meetup/events/  234778571/   The slot is for 15m + 5m Q&A, so it's not too much of a commitment.", "Would you like to speak?", "If so, let me know!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Happy to put together a short  abstract.", "Thanks,   Rob   On Thu, Oct 20, 2016 at 8:43 AM, Chris Riccomini <criccomini@apache.org  wrote:   Hey all,   We have one more slot to fill for the next meetup at WePay on November 16:     http://www.meetup.com/Bay-Area-Apache-Airflow-Incubating-Meetup/events/  234778571/   The slot is for 15m + 5m Q&A, so it's not too much of a commitment.", "Would you like to speak?", "If so, let me know!", "Cheers,  Chris   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Dmitry,  Please provide more information, such as logs and the DAG definition itself.", "This is very little to go on unfortunately.", "Bolke   On 3 May 2017, at 10:22, Dmitry Smirnov <dmi.smirnov07@gmail.com wrote:    Hi everyone,    I'm using Airflow version 1.8.0, just upgraded from 1.7.1.3.", "The issue that  I'm going to describe started already in 1.7.1.3, I upgraded hoping it  might help resolve it.", "I have several DAGs for which the *last* task is not moving from queued to  running."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is very little to go on unfortunately.", "Bolke   On 3 May 2017, at 10:22, Dmitry Smirnov <dmi.smirnov07@gmail.com wrote:    Hi everyone,    I'm using Airflow version 1.8.0, just upgraded from 1.7.1.3.", "The issue that  I'm going to describe started already in 1.7.1.3, I upgraded hoping it  might help resolve it.", "I have several DAGs for which the *last* task is not moving from queued to  running.", "These DAGs used to run fine some time ago, but then we had issues with  rabbitmq cluster we use, and after resetting it up, the problem emerged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Bolke   On 3 May 2017, at 10:22, Dmitry Smirnov <dmi.smirnov07@gmail.com wrote:    Hi everyone,    I'm using Airflow version 1.8.0, just upgraded from 1.7.1.3.", "The issue that  I'm going to describe started already in 1.7.1.3, I upgraded hoping it  might help resolve it.", "I have several DAGs for which the *last* task is not moving from queued to  running.", "These DAGs used to run fine some time ago, but then we had issues with  rabbitmq cluster we use, and after resetting it up, the problem emerged.", "I'm pretty sure the queue is working fine, since all the tasks except the  very last one are queued automatically and run fine."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The issue that  I'm going to describe started already in 1.7.1.3, I upgraded hoping it  might help resolve it.", "I have several DAGs for which the *last* task is not moving from queued to  running.", "These DAGs used to run fine some time ago, but then we had issues with  rabbitmq cluster we use, and after resetting it up, the problem emerged.", "I'm pretty sure the queue is working fine, since all the tasks except the  very last one are queued automatically and run fine.", "For the sake of testing, I added a copy of the last task to the DAG, and  interestingly, the task that used to be the last and did not run, now  started to run normally, but the new last task is stuck."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These DAGs used to run fine some time ago, but then we had issues with  rabbitmq cluster we use, and after resetting it up, the problem emerged.", "I'm pretty sure the queue is working fine, since all the tasks except the  very last one are queued automatically and run fine.", "For the sake of testing, I added a copy of the last task to the DAG, and  interestingly, the task that used to be the last and did not run, now  started to run normally, but the new last task is stuck.", "I checked logs at the DEBUG level and I could see that scheduler queues the  tasks, but those tasks don't show up in the Celery/Flower dashboard in the  corresponding queue.", "When I run the task that is stuck from the webserver interface, they show  up in the queue in Flower dashboard and run successfully."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm pretty sure the queue is working fine, since all the tasks except the  very last one are queued automatically and run fine.", "For the sake of testing, I added a copy of the last task to the DAG, and  interestingly, the task that used to be the last and did not run, now  started to run normally, but the new last task is stuck.", "I checked logs at the DEBUG level and I could see that scheduler queues the  tasks, but those tasks don't show up in the Celery/Flower dashboard in the  corresponding queue.", "When I run the task that is stuck from the webserver interface, they show  up in the queue in Flower dashboard and run successfully.", "So, overall, it seems that the issue is present with the scheduler but not  with webserver, and that this issue is only related to the very last task  in the DAG."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When I run the task that is stuck from the webserver interface, they show  up in the queue in Flower dashboard and run successfully.", "So, overall, it seems that the issue is present with the scheduler but not  with webserver, and that this issue is only related to the very last task  in the DAG.", "I'm really stuck now, I would welcome any suggestions / ideas on what can  be done.", "Thank you in advance!", "BR, Dima    --     Dmitry Smirnov (MSc.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So, overall, it seems that the issue is present with the scheduler but not  with webserver, and that this issue is only related to the very last task  in the DAG.", "I'm really stuck now, I would welcome any suggestions / ideas on what can  be done.", "Thank you in advance!", "BR, Dima    --     Dmitry Smirnov (MSc.)", "Data Engineer @ Yousician  mobile: +358 50 3015072   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Here is an example for 1, you can see that there are some white tasks that should have been run.", "I don't have time to create a skeleton DAG at the moment unfortunately because of release-related firefighting.", "Will hopefully post back here later once firefighting is done.", "[image: Inline image 1]  On Thu, Feb 23, 2017 at 12:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hey Dan, Alex,   Indeed #1 seems serious, specifically the the second part - skipping the  root task (root task of the whole DAG?).", "Do you have a skeleton DAG that  exposes the issue?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't have time to create a skeleton DAG at the moment unfortunately because of release-related firefighting.", "Will hopefully post back here later once firefighting is done.", "[image: Inline image 1]  On Thu, Feb 23, 2017 at 12:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hey Dan, Alex,   Indeed #1 seems serious, specifically the the second part - skipping the  root task (root task of the whole DAG?).", "Do you have a skeleton DAG that  exposes the issue?", "Is there a root cause analysis?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Will hopefully post back here later once firefighting is done.", "[image: Inline image 1]  On Thu, Feb 23, 2017 at 12:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hey Dan, Alex,   Indeed #1 seems serious, specifically the the second part - skipping the  root task (root task of the whole DAG?).", "Do you have a skeleton DAG that  exposes the issue?", "Is there a root cause analysis?", "When was the issue  introduced?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[image: Inline image 1]  On Thu, Feb 23, 2017 at 12:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hey Dan, Alex,   Indeed #1 seems serious, specifically the the second part - skipping the  root task (root task of the whole DAG?).", "Do you have a skeleton DAG that  exposes the issue?", "Is there a root cause analysis?", "When was the issue  introduced?", "On the the issue Alex mentioned, we don\u2019t see that and I cannot  really align the description of the issue with the PR yet, ie."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do you have a skeleton DAG that  exposes the issue?", "Is there a root cause analysis?", "When was the issue  introduced?", "On the the issue Alex mentioned, we don\u2019t see that and I cannot  really align the description of the issue with the PR yet, ie.", "I need  clarification."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there a root cause analysis?", "When was the issue  introduced?", "On the the issue Alex mentioned, we don\u2019t see that and I cannot  really align the description of the issue with the PR yet, ie.", "I need  clarification.", "Obviously, I\u2019m not very happy if we indeed need to retract the release as  we are ~12 hours away from closing of the vote at the IPMC mailinglist  (strangely enough no one has voted yet)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I need  clarification.", "Obviously, I\u2019m not very happy if we indeed need to retract the release as  we are ~12 hours away from closing of the vote at the IPMC mailinglist  (strangely enough no one has voted yet).", "However, if it is that serious  that it cannot wait for 1.8.1 then we need to do it.", "I would define  \u201cserious\u201d as many people are going to be affected by it and they will not  have a workaround available to them (ie.", "patching code or database), but  the opinion of the community might differ."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Obviously, I\u2019m not very happy if we indeed need to retract the release as  we are ~12 hours away from closing of the vote at the IPMC mailinglist  (strangely enough no one has voted yet).", "However, if it is that serious  that it cannot wait for 1.8.1 then we need to do it.", "I would define  \u201cserious\u201d as many people are going to be affected by it and they will not  have a workaround available to them (ie.", "patching code or database), but  the opinion of the community might differ.", "Cheers  Bolke   P.S."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, if it is that serious  that it cannot wait for 1.8.1 then we need to do it.", "I would define  \u201cserious\u201d as many people are going to be affected by it and they will not  have a workaround available to them (ie.", "patching code or database), but  the opinion of the community might differ.", "Cheers  Bolke   P.S.", "I am also interested in #3, as it sounds like a integrity issue  (which verify_integrity should catch) but also maybe too strong a  assumption that such a task should exist (ie."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would define  \u201cserious\u201d as many people are going to be affected by it and they will not  have a workaround available to them (ie.", "patching code or database), but  the opinion of the community might differ.", "Cheers  Bolke   P.S.", "I am also interested in #3, as it sounds like a integrity issue  (which verify_integrity should catch) but also maybe too strong a  assumption that such a task should exist (ie.", "a task was added to a Dag in  a later stage)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["patching code or database), but  the opinion of the community might differ.", "Cheers  Bolke   P.S.", "I am also interested in #3, as it sounds like a integrity issue  (which verify_integrity should catch) but also maybe too strong a  assumption that such a task should exist (ie.", "a task was added to a Dag in  a later stage).", "On 23 Feb 2017, at 20:15, Dan Davydov <dan.davydov@airbnb.com.INVALID  wrote:     Some more issues found by our users in addition to the one Alex reported   and the UI issue when a dagrun doesn't have a start date:   1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers  Bolke   P.S.", "I am also interested in #3, as it sounds like a integrity issue  (which verify_integrity should catch) but also maybe too strong a  assumption that such a task should exist (ie.", "a task was added to a Dag in  a later stage).", "On 23 Feb 2017, at 20:15, Dan Davydov <dan.davydov@airbnb.com.INVALID  wrote:     Some more issues found by our users in addition to the one Alex reported   and the UI issue when a dagrun doesn't have a start date:   1.", "If a task fails it fails the whole dagrun immediately fails, this is a   very large change to how control flow works as the rest of the tasks in  the   DAG are not run (even e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am also interested in #3, as it sounds like a integrity issue  (which verify_integrity should catch) but also maybe too strong a  assumption that such a task should exist (ie.", "a task was added to a Dag in  a later stage).", "On 23 Feb 2017, at 20:15, Dan Davydov <dan.davydov@airbnb.com.INVALID  wrote:     Some more issues found by our users in addition to the one Alex reported   and the UI issue when a dagrun doesn't have a start date:   1.", "If a task fails it fails the whole dagrun immediately fails, this is a   very large change to how control flow works as the rest of the tasks in  the   DAG are not run (even e.g.", "leaf tasks)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 23 Feb 2017, at 20:15, Dan Davydov <dan.davydov@airbnb.com.INVALID  wrote:     Some more issues found by our users in addition to the one Alex reported   and the UI issue when a dagrun doesn't have a start date:   1.", "If a task fails it fails the whole dagrun immediately fails, this is a   very large change to how control flow works as the rest of the tasks in  the   DAG are not run (even e.g.", "leaf tasks).", "The same is true of the skipped   status (if a leaf task is skipped then the root task for the DAG will get   skipped and none of the other tasks in the DAG will run).", "2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If a task fails it fails the whole dagrun immediately fails, this is a   very large change to how control flow works as the rest of the tasks in  the   DAG are not run (even e.g.", "leaf tasks).", "The same is true of the skipped   status (if a leaf task is skipped then the root task for the DAG will get   skipped and none of the other tasks in the DAG will run).", "2.", "The black squares in the UI for tasks that aren't ready to run yet are   confusing and make it hard for users to see which tasks haven't run yet   (lower contrast)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The same is true of the skipped   status (if a leaf task is skipped then the root task for the DAG will get   skipped and none of the other tasks in the DAG will run).", "2.", "The black squares in the UI for tasks that aren't ready to run yet are   confusing and make it hard for users to see which tasks haven't run yet   (lower contrast).", "We should never initialize tasks in the DB that do not   have a state (or at the least these should be white).", "3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2.", "The black squares in the UI for tasks that aren't ready to run yet are   confusing and make it hard for users to see which tasks haven't run yet   (lower contrast).", "We should never initialize tasks in the DB that do not   have a state (or at the least these should be white).", "3.", "The Dagrun has a get_task_instance method that will fail if a dagrun   doesn't have a copy of a task instance created which we have seen happen   for some DAGs."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The black squares in the UI for tasks that aren't ready to run yet are   confusing and make it hard for users to see which tasks haven't run yet   (lower contrast).", "We should never initialize tasks in the DB that do not   have a state (or at the least these should be white).", "3.", "The Dagrun has a get_task_instance method that will fail if a dagrun   doesn't have a copy of a task instance created which we have seen happen   for some DAGs.", "This prevents those tasks from getting scheduled."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["3.", "The Dagrun has a get_task_instance method that will fail if a dagrun   doesn't have a copy of a task instance created which we have seen happen   for some DAGs.", "This prevents those tasks from getting scheduled.", "I already patched 3 (and have a PR in flight for open source), and am   working on a patch for 1 internally.", "1 should be a blocker for releasing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The Dagrun has a get_task_instance method that will fail if a dagrun   doesn't have a copy of a task instance created which we have seen happen   for some DAGs.", "This prevents those tasks from getting scheduled.", "I already patched 3 (and have a PR in flight for open source), and am   working on a patch for 1 internally.", "1 should be a blocker for releasing.", "On Wed, Feb 22, 2017 at 4:38 PM, Alex Guziel <alex.guziel@airbnb.com."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This prevents those tasks from getting scheduled.", "I already patched 3 (and have a PR in flight for open source), and am   working on a patch for 1 internally.", "1 should be a blocker for releasing.", "On Wed, Feb 22, 2017 at 4:38 PM, Alex Guziel <alex.guziel@airbnb.com.", "invalid   wrote:     I have some concern that this change   https://github.com/apache/incubator-airflow/pull/1939   [AIRFLOW-679] may be having issues because we are seeing lots of double   triggers   of tasks and tasks being killed as a result."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I already patched 3 (and have a PR in flight for open source), and am   working on a patch for 1 internally.", "1 should be a blocker for releasing.", "On Wed, Feb 22, 2017 at 4:38 PM, Alex Guziel <alex.guziel@airbnb.com.", "invalid   wrote:     I have some concern that this change   https://github.com/apache/incubator-airflow/pull/1939   [AIRFLOW-679] may be having issues because we are seeing lots of double   triggers   of tasks and tasks being killed as a result.", "On Wed, Feb 22, 2017 4:35 PM, Dan Davydov dan.davydov@airbnb.com.INVALID   wrote:   Bumping the thread so another user can comment."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["1 should be a blocker for releasing.", "On Wed, Feb 22, 2017 at 4:38 PM, Alex Guziel <alex.guziel@airbnb.com.", "invalid   wrote:     I have some concern that this change   https://github.com/apache/incubator-airflow/pull/1939   [AIRFLOW-679] may be having issues because we are seeing lots of double   triggers   of tasks and tasks being killed as a result.", "On Wed, Feb 22, 2017 4:35 PM, Dan Davydov dan.davydov@airbnb.com.INVALID   wrote:   Bumping the thread so another user can comment.", "On Wed, Feb 22, 2017 at 3:12 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:           What I meant to ask is \"how much engineering effort it takes to bake a     single RC?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Feb 22, 2017 at 4:38 PM, Alex Guziel <alex.guziel@airbnb.com.", "invalid   wrote:     I have some concern that this change   https://github.com/apache/incubator-airflow/pull/1939   [AIRFLOW-679] may be having issues because we are seeing lots of double   triggers   of tasks and tasks being killed as a result.", "On Wed, Feb 22, 2017 4:35 PM, Dan Davydov dan.davydov@airbnb.com.INVALID   wrote:   Bumping the thread so another user can comment.", "On Wed, Feb 22, 2017 at 3:12 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:           What I meant to ask is \"how much engineering effort it takes to bake a     single RC?", "\", I guess it depends on how much git-fu is necessary plus  some     overhead cost of doing the series of actions/commands/emails/jira."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["invalid   wrote:     I have some concern that this change   https://github.com/apache/incubator-airflow/pull/1939   [AIRFLOW-679] may be having issues because we are seeing lots of double   triggers   of tasks and tasks being killed as a result.", "On Wed, Feb 22, 2017 4:35 PM, Dan Davydov dan.davydov@airbnb.com.INVALID   wrote:   Bumping the thread so another user can comment.", "On Wed, Feb 22, 2017 at 3:12 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:           What I meant to ask is \"how much engineering effort it takes to bake a     single RC?", "\", I guess it depends on how much git-fu is necessary plus  some     overhead cost of doing the series of actions/commands/emails/jira.", "I can volunteer for 1.8.1 (hopefully I can get do it along another  Airbnb     engineer/volunteer to tag along) and will try to document/automate     everything I can as I go through the process."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Feb 22, 2017 4:35 PM, Dan Davydov dan.davydov@airbnb.com.INVALID   wrote:   Bumping the thread so another user can comment.", "On Wed, Feb 22, 2017 at 3:12 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:           What I meant to ask is \"how much engineering effort it takes to bake a     single RC?", "\", I guess it depends on how much git-fu is necessary plus  some     overhead cost of doing the series of actions/commands/emails/jira.", "I can volunteer for 1.8.1 (hopefully I can get do it along another  Airbnb     engineer/volunteer to tag along) and will try to document/automate     everything I can as I go through the process.", "The goal of 1.8.1 could  be   to     basically package 1.8.0 + Dan's bugfix, and for Airbnb to get familiar   with     the process."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Feb 22, 2017 at 3:12 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:           What I meant to ask is \"how much engineering effort it takes to bake a     single RC?", "\", I guess it depends on how much git-fu is necessary plus  some     overhead cost of doing the series of actions/commands/emails/jira.", "I can volunteer for 1.8.1 (hopefully I can get do it along another  Airbnb     engineer/volunteer to tag along) and will try to document/automate     everything I can as I go through the process.", "The goal of 1.8.1 could  be   to     basically package 1.8.0 + Dan's bugfix, and for Airbnb to get familiar   with     the process.", "It'd be great if you can dump your whole process on the wiki, and we'll     improve it on this next pass."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\", I guess it depends on how much git-fu is necessary plus  some     overhead cost of doing the series of actions/commands/emails/jira.", "I can volunteer for 1.8.1 (hopefully I can get do it along another  Airbnb     engineer/volunteer to tag along) and will try to document/automate     everything I can as I go through the process.", "The goal of 1.8.1 could  be   to     basically package 1.8.0 + Dan's bugfix, and for Airbnb to get familiar   with     the process.", "It'd be great if you can dump your whole process on the wiki, and we'll     improve it on this next pass.", "Thanks again for the mountain of work that went into packaging this     release."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can volunteer for 1.8.1 (hopefully I can get do it along another  Airbnb     engineer/volunteer to tag along) and will try to document/automate     everything I can as I go through the process.", "The goal of 1.8.1 could  be   to     basically package 1.8.0 + Dan's bugfix, and for Airbnb to get familiar   with     the process.", "It'd be great if you can dump your whole process on the wiki, and we'll     improve it on this next pass.", "Thanks again for the mountain of work that went into packaging this     release.", "Max         On Wed, Feb 22, 2017 at 2:44 PM, Bolke de Bruin <bdbruin@gmail.com   wrote:         I thought you volunteered to baby sit 1.8.1 Chris ;-)?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The goal of 1.8.1 could  be   to     basically package 1.8.0 + Dan's bugfix, and for Airbnb to get familiar   with     the process.", "It'd be great if you can dump your whole process on the wiki, and we'll     improve it on this next pass.", "Thanks again for the mountain of work that went into packaging this     release.", "Max         On Wed, Feb 22, 2017 at 2:44 PM, Bolke de Bruin <bdbruin@gmail.com   wrote:         I thought you volunteered to baby sit 1.8.1 Chris ;-)?", "Sent from my iPhone         On 22 Feb 2017, at 23:31, Chris Riccomini <criccomini@apache.org     wrote:         I'm +1 for doing a 1.8.1 fast follow-on         On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:         Our database may have edge cases that could be associated with   running     any     previous version that may or may not have been part of an official     release."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It'd be great if you can dump your whole process on the wiki, and we'll     improve it on this next pass.", "Thanks again for the mountain of work that went into packaging this     release.", "Max         On Wed, Feb 22, 2017 at 2:44 PM, Bolke de Bruin <bdbruin@gmail.com   wrote:         I thought you volunteered to baby sit 1.8.1 Chris ;-)?", "Sent from my iPhone         On 22 Feb 2017, at 23:31, Chris Riccomini <criccomini@apache.org     wrote:         I'm +1 for doing a 1.8.1 fast follow-on         On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:         Our database may have edge cases that could be associated with   running     any     previous version that may or may not have been part of an official     release.", "Let's see if anyone else reports the issue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max         On Wed, Feb 22, 2017 at 2:44 PM, Bolke de Bruin <bdbruin@gmail.com   wrote:         I thought you volunteered to baby sit 1.8.1 Chris ;-)?", "Sent from my iPhone         On 22 Feb 2017, at 23:31, Chris Riccomini <criccomini@apache.org     wrote:         I'm +1 for doing a 1.8.1 fast follow-on         On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:         Our database may have edge cases that could be associated with   running     any     previous version that may or may not have been part of an official     release.", "Let's see if anyone else reports the issue.", "If no one does, one   option     is     to release 1.8.0 as is with a comment in the release notes, and   have a     future official minor apache release 1.8.1 that would fix these   minor     issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one     release     cycle?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sent from my iPhone         On 22 Feb 2017, at 23:31, Chris Riccomini <criccomini@apache.org     wrote:         I'm +1 for doing a 1.8.1 fast follow-on         On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:         Our database may have edge cases that could be associated with   running     any     previous version that may or may not have been part of an official     release.", "Let's see if anyone else reports the issue.", "If no one does, one   option     is     to release 1.8.0 as is with a comment in the release notes, and   have a     future official minor apache release 1.8.1 that would fix these   minor     issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one     release     cycle?", "Oh, and do you have a documented step by step process for     releasing?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let's see if anyone else reports the issue.", "If no one does, one   option     is     to release 1.8.0 as is with a comment in the release notes, and   have a     future official minor apache release 1.8.1 that would fix these   minor     issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one     release     cycle?", "Oh, and do you have a documented step by step process for     releasing?", "I'd like to add the Pypi part to this doc and add committers that   are     interested to have rights on the project on Pypi."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If no one does, one   option     is     to release 1.8.0 as is with a comment in the release notes, and   have a     future official minor apache release 1.8.1 that would fix these   minor     issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one     release     cycle?", "Oh, and do you have a documented step by step process for     releasing?", "I'd like to add the Pypi part to this doc and add committers that   are     interested to have rights on the project on Pypi.", "Max         On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com       wrote:         So it is a database integrity issue?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["@bolke, I'm curious, how long does it take you to go through one     release     cycle?", "Oh, and do you have a documented step by step process for     releasing?", "I'd like to add the Pypi part to this doc and add committers that   are     interested to have rights on the project on Pypi.", "Max         On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com       wrote:         So it is a database integrity issue?", "Afaik a start_date should   always     be     set for a DagRun (create_dagrun) does so I didn't check the code     though."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Oh, and do you have a documented step by step process for     releasing?", "I'd like to add the Pypi part to this doc and add committers that   are     interested to have rights on the project on Pypi.", "Max         On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com       wrote:         So it is a database integrity issue?", "Afaik a start_date should   always     be     set for a DagRun (create_dagrun) does so I didn't check the code     though.", "Sent from my iPhone         On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max         On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com       wrote:         So it is a database integrity issue?", "Afaik a start_date should   always     be     set for a DagRun (create_dagrun) does so I didn't check the code     though.", "Sent from my iPhone         On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.", "INVALID     wrote:         Should clarify this occurs when a dagrun does not have a start   date,     not     a     dag (which makes it even less likely to happen).", "I don't think   this     is     a     blocker for releasing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Afaik a start_date should   always     be     set for a DagRun (create_dagrun) does so I didn't check the code     though.", "Sent from my iPhone         On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.", "INVALID     wrote:         Should clarify this occurs when a dagrun does not have a start   date,     not     a     dag (which makes it even less likely to happen).", "I don't think   this     is     a     blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <     dan.davydov@airbnb.com         wrote:         I rolled this out in our prod and the webservers failed to load   due     to     this commit:         [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag     7c94d81c390881643f94d5e3d7d6fb351a445b72         This fixed it:     - </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"     title=\"Start     Date:     {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span     + </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span         This is caused by assuming that all DAGs have start dates set,   so a     broken     DAG will take down the whole UI."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sent from my iPhone         On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.", "INVALID     wrote:         Should clarify this occurs when a dagrun does not have a start   date,     not     a     dag (which makes it even less likely to happen).", "I don't think   this     is     a     blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <     dan.davydov@airbnb.com         wrote:         I rolled this out in our prod and the webservers failed to load   due     to     this commit:         [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag     7c94d81c390881643f94d5e3d7d6fb351a445b72         This fixed it:     - </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"     title=\"Start     Date:     {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span     + </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span         This is caused by assuming that all DAGs have start dates set,   so a     broken     DAG will take down the whole UI.", "Not sure if we want to make   this a     blocker     for the release or not, I'm guessing for most deployments this     would     occur     pretty rarely."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["INVALID     wrote:         Should clarify this occurs when a dagrun does not have a start   date,     not     a     dag (which makes it even less likely to happen).", "I don't think   this     is     a     blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <     dan.davydov@airbnb.com         wrote:         I rolled this out in our prod and the webservers failed to load   due     to     this commit:         [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag     7c94d81c390881643f94d5e3d7d6fb351a445b72         This fixed it:     - </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"     title=\"Start     Date:     {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span     + </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span         This is caused by assuming that all DAGs have start dates set,   so a     broken     DAG will take down the whole UI.", "Not sure if we want to make   this a     blocker     for the release or not, I'm guessing for most deployments this     would     occur     pretty rarely.", "I'll submit a PR to fix it soon."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't think   this     is     a     blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <     dan.davydov@airbnb.com         wrote:         I rolled this out in our prod and the webservers failed to load   due     to     this commit:         [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag     7c94d81c390881643f94d5e3d7d6fb351a445b72         This fixed it:     - </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"     title=\"Start     Date:     {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span     + </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span         This is caused by assuming that all DAGs have start dates set,   so a     broken     DAG will take down the whole UI.", "Not sure if we want to make   this a     blocker     for the release or not, I'm guessing for most deployments this     would     occur     pretty rarely.", "I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <     criccomini@apache.org         wrote:         Ack that the vote has already passed, but belated +1 (binding)         On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <     bdbruin@gmail.com     wrote:         IPMC Voting can be found here:         http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <     http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E         Kind regards,     Bolke         On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com     wrote:         Hello,         Apache Airflow (incubating) 1.8.0 (based on RC4) has been     accepted."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <     dan.davydov@airbnb.com         wrote:         I rolled this out in our prod and the webservers failed to load   due     to     this commit:         [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag     7c94d81c390881643f94d5e3d7d6fb351a445b72         This fixed it:     - </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"     title=\"Start     Date:     {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span     + </a <span id=\"statuses_info\"     class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span         This is caused by assuming that all DAGs have start dates set,   so a     broken     DAG will take down the whole UI.", "Not sure if we want to make   this a     blocker     for the release or not, I'm guessing for most deployments this     would     occur     pretty rarely.", "I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <     criccomini@apache.org         wrote:         Ack that the vote has already passed, but belated +1 (binding)         On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <     bdbruin@gmail.com     wrote:         IPMC Voting can be found here:         http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <     http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E         Kind regards,     Bolke         On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com     wrote:         Hello,         Apache Airflow (incubating) 1.8.0 (based on RC4) has been     accepted.", "9 \u201c+1\u201d votes received:         - Maxime Beauchemin (binding)     - Arthur Wiedmer (binding)     - Dan Davydov (binding)     - Jeremiah Lowin (binding)     - Siddharth Anand (binding)     - Alex van Boxel (binding)     - Bolke de Bruin (binding)         - Jayesh Senjaliya (non-binding)     - Yi (non-binding)         Vote thread (start):     http://mail-archives.apache.org/mod_mbox/incubator-     airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-     6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Not sure if we want to make   this a     blocker     for the release or not, I'm guessing for most deployments this     would     occur     pretty rarely.", "I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <     criccomini@apache.org         wrote:         Ack that the vote has already passed, but belated +1 (binding)         On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <     bdbruin@gmail.com     wrote:         IPMC Voting can be found here:         http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <     http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E         Kind regards,     Bolke         On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com     wrote:         Hello,         Apache Airflow (incubating) 1.8.0 (based on RC4) has been     accepted.", "9 \u201c+1\u201d votes received:         - Maxime Beauchemin (binding)     - Arthur Wiedmer (binding)     - Dan Davydov (binding)     - Jeremiah Lowin (binding)     - Siddharth Anand (binding)     - Alex van Boxel (binding)     - Bolke de Bruin (binding)         - Jayesh Senjaliya (non-binding)     - Yi (non-binding)         Vote thread (start):     http://mail-archives.apache.org/mod_mbox/incubator-     airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-     6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-     092E-48D2-AA0F-     15F44376A8FF@gmail.com%3E         Next steps:     1) will start the voting process at the IPMC mailinglist."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <     criccomini@apache.org         wrote:         Ack that the vote has already passed, but belated +1 (binding)         On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <     bdbruin@gmail.com     wrote:         IPMC Voting can be found here:         http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <     http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E         Kind regards,     Bolke         On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com     wrote:         Hello,         Apache Airflow (incubating) 1.8.0 (based on RC4) has been     accepted.", "9 \u201c+1\u201d votes received:         - Maxime Beauchemin (binding)     - Arthur Wiedmer (binding)     - Dan Davydov (binding)     - Jeremiah Lowin (binding)     - Siddharth Anand (binding)     - Alex van Boxel (binding)     - Bolke de Bruin (binding)         - Jayesh Senjaliya (non-binding)     - Yi (non-binding)         Vote thread (start):     http://mail-archives.apache.org/mod_mbox/incubator-     airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-     6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-     092E-48D2-AA0F-     15F44376A8FF@gmail.com%3E         Next steps:     1) will start the voting process at the IPMC mailinglist.", "I do     expect     some changes to be required mostly in documentation maybe a     license     here     and there."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <     criccomini@apache.org         wrote:         Ack that the vote has already passed, but belated +1 (binding)         On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <     bdbruin@gmail.com     wrote:         IPMC Voting can be found here:         http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <     http://mail-archives.apache.org/mod_mbox/incubator-general/     201702.mbox/%     3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E         Kind regards,     Bolke         On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com     wrote:         Hello,         Apache Airflow (incubating) 1.8.0 (based on RC4) has been     accepted.", "9 \u201c+1\u201d votes received:         - Maxime Beauchemin (binding)     - Arthur Wiedmer (binding)     - Dan Davydov (binding)     - Jeremiah Lowin (binding)     - Siddharth Anand (binding)     - Alex van Boxel (binding)     - Bolke de Bruin (binding)         - Jayesh Senjaliya (non-binding)     - Yi (non-binding)         Vote thread (start):     http://mail-archives.apache.org/mod_mbox/incubator-     airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-     6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-     092E-48D2-AA0F-     15F44376A8FF@gmail.com%3E         Next steps:     1) will start the voting process at the IPMC mailinglist.", "I do     expect     some changes to be required mostly in documentation maybe a     license     here     and there.", "So, we might end up with changes to stable."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["9 \u201c+1\u201d votes received:         - Maxime Beauchemin (binding)     - Arthur Wiedmer (binding)     - Dan Davydov (binding)     - Jeremiah Lowin (binding)     - Siddharth Anand (binding)     - Alex van Boxel (binding)     - Bolke de Bruin (binding)         - Jayesh Senjaliya (non-binding)     - Yi (non-binding)         Vote thread (start):     http://mail-archives.apache.org/mod_mbox/incubator-     airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-     6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-     092E-48D2-AA0F-     15F44376A8FF@gmail.com%3E         Next steps:     1) will start the voting process at the IPMC mailinglist.", "I do     expect     some changes to be required mostly in documentation maybe a     license     here     and there.", "So, we might end up with changes to stable.", "As long   as     these     are     not (significant) code changes I will not re-raise the vote."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-     092E-48D2-AA0F-     15F44376A8FF@gmail.com%3E         Next steps:     1) will start the voting process at the IPMC mailinglist.", "I do     expect     some changes to be required mostly in documentation maybe a     license     here     and there.", "So, we might end up with changes to stable.", "As long   as     these     are     not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and   finalisation I     will     rebrand the RC to Release."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I do     expect     some changes to be required mostly in documentation maybe a     license     here     and there.", "So, we might end up with changes to stable.", "As long   as     these     are     not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and   finalisation I     will     rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the   tar     ball     needs to propagate to the mirrors."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So, we might end up with changes to stable.", "As long   as     these     are     not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and   finalisation I     will     rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the   tar     ball     needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As long   as     these     are     not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and   finalisation I     will     rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the   tar     ball     needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2) Only after the positive voting on the IPMC and   finalisation I     will     rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the   tar     ball     needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi.", "It seems   we     can     keep     the apache branding as lib cloud is doing this as well (     https://libcloud.apache.org/downloads.html#pypi-package <     https://libcloud.apache.org/downloads.html#pypi-package)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi.", "It seems   we     can     keep     the apache branding as lib cloud is doing this as well (     https://libcloud.apache.org/downloads.html#pypi-package <     https://libcloud.apache.org/downloads.html#pypi-package).", "Jippie!", "Bolke                                          "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, the grouping should follow the sorting order.", "At the moment, it starts at version-1, which results e.g.", "in only displayed version on page 1 if there are 20 versions available (see attached screenshot).", "--  This message is automatically generated by JIRA.", "- You can reply to this email to add a comment to the issue online.   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI.", "If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis.", "was (Author: chrisalbright): [~rdblue], I was not able to find a way to the path prefixes without converting using String#startsWith."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Pull request is updated, and passing in Travis.", "was (Author: chrisalbright): [~rdblue], I was not able to find a way to the path prefixes without converting using String#startsWith.", "It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI.", "If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["was (Author: chrisalbright): [~rdblue], I was not able to find a way to the path prefixes without converting using String#startsWith.", "It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI.", "If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI.", "If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis.", "parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException  --------------------------------------------------------------------------------------------                   Key: PARQUET-124                  URL: https://issues.apache.org/jira/browse/PARQUET-124              Project: Parquet           Issue Type: Bug           Components: parquet-mr     Affects Versions: parquet-mr_1.6.0, 1.6.0rc2             Reporter: Chris Albright             Priority: Minor          Attachments: PARQUET-124-test    I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example), and in the process of updating the library versions, am getting the warning below."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis.", "parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException  --------------------------------------------------------------------------------------------                   Key: PARQUET-124                  URL: https://issues.apache.org/jira/browse/PARQUET-124              Project: Parquet           Issue Type: Bug           Components: parquet-mr     Affects Versions: parquet-mr_1.6.0, 1.6.0rc2             Reporter: Chris Albright             Priority: Minor          Attachments: PARQUET-124-test    I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example), and in the process of updating the library versions, am getting the warning below.", "The version of Parquet-Hadoop in the original example is 1.0.0."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Pull request is updated, and passing in Travis.", "parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException  --------------------------------------------------------------------------------------------                   Key: PARQUET-124                  URL: https://issues.apache.org/jira/browse/PARQUET-124              Project: Parquet           Issue Type: Bug           Components: parquet-mr     Affects Versions: parquet-mr_1.6.0, 1.6.0rc2             Reporter: Chris Albright             Priority: Minor          Attachments: PARQUET-124-test    I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example), and in the process of updating the library versions, am getting the warning below.", "The version of Parquet-Hadoop in the original example is 1.0.0.", "I am using 1.6.0rc3  The ParquetFileWriter.mergeFooters(Path, List<Footer) method is performing a check to ensure the footers are all for files in the output directory.", "The output directory is supplied by ParquetFileWriter.writeMetadataFile; in 1.0.0, the output path was converted to a fully qualified output path before the call to mergeFooters, but in 1.6.0rc[2,3] that conversion happens after the call to mergeFooters."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException  --------------------------------------------------------------------------------------------                   Key: PARQUET-124                  URL: https://issues.apache.org/jira/browse/PARQUET-124              Project: Parquet           Issue Type: Bug           Components: parquet-mr     Affects Versions: parquet-mr_1.6.0, 1.6.0rc2             Reporter: Chris Albright             Priority: Minor          Attachments: PARQUET-124-test    I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example), and in the process of updating the library versions, am getting the warning below.", "The version of Parquet-Hadoop in the original example is 1.0.0.", "I am using 1.6.0rc3  The ParquetFileWriter.mergeFooters(Path, List<Footer) method is performing a check to ensure the footers are all for files in the output directory.", "The output directory is supplied by ParquetFileWriter.writeMetadataFile; in 1.0.0, the output path was converted to a fully qualified output path before the call to mergeFooters, but in 1.6.0rc[2,3] that conversion happens after the call to mergeFooters.", "Because of this, the check within merge footers is failing (the URI for the footers starts with file:, but not the URI for the root path does not)  Here is the warning message and stacktrace."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi All,   We are running master in our environment and have noticed something new (that wasn't present in release 1.7.1.3).", "I have the following DAG, which I ran a backfill on:  #                                                     ------ BuildTask6 ------ BuildTask7 ------ BuildTask8 ------ BuildTask9 #                                                    /                                     / #   InitBuildTask ------ BuildTask1 ------ BuildTask2 ------ BuildTask3                   / #                \\                                                     \\                 / #                 ------ BuildTask4 ------------------------------------------ BuildTask5 #   When the backfill begins, the outputs gives warnings for all tasks that have not yet had their dependencies met (full output attached) i.e.", "all tasks except the one task that has no dependencies.", "This continues until the backfill has completed.", "Is this expected behaviour?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have the following DAG, which I ran a backfill on:  #                                                     ------ BuildTask6 ------ BuildTask7 ------ BuildTask8 ------ BuildTask9 #                                                    /                                     / #   InitBuildTask ------ BuildTask1 ------ BuildTask2 ------ BuildTask3                   / #                \\                                                     \\                 / #                 ------ BuildTask4 ------------------------------------------ BuildTask5 #   When the backfill begins, the outputs gives warnings for all tasks that have not yet had their dependencies met (full output attached) i.e.", "all tasks except the one task that has no dependencies.", "This continues until the backfill has completed.", "Is this expected behaviour?", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/   ________________________________ From: Chris Riccomini <criccomini@apache.org Sent: 29 September 2016 18:14:02 To: dev@airflow.incubator.apache.org Subject: Re: Airflow Releases  Hey Luke,   Is there anything we can do to help get the next release in place?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["all tasks except the one task that has no dependencies.", "This continues until the backfill has completed.", "Is this expected behaviour?", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/   ________________________________ From: Chris Riccomini <criccomini@apache.org Sent: 29 September 2016 18:14:02 To: dev@airflow.incubator.apache.org Subject: Re: Airflow Releases  Hey Luke,   Is there anything we can do to help get the next release in place?", "One thing that would definitely help is running master somewhere in your environment, and reporting any issues that see."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This continues until the backfill has completed.", "Is this expected behaviour?", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/   ________________________________ From: Chris Riccomini <criccomini@apache.org Sent: 29 September 2016 18:14:02 To: dev@airflow.incubator.apache.org Subject: Re: Airflow Releases  Hey Luke,   Is there anything we can do to help get the next release in place?", "One thing that would definitely help is running master somewhere in your environment, and reporting any issues that see.", "Over the next few weeks, AirBNB and a few other folks will be doing the same in an effort to harden the 1.8 release."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is this expected behaviour?", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/   ________________________________ From: Chris Riccomini <criccomini@apache.org Sent: 29 September 2016 18:14:02 To: dev@airflow.incubator.apache.org Subject: Re: Airflow Releases  Hey Luke,   Is there anything we can do to help get the next release in place?", "One thing that would definitely help is running master somewhere in your environment, and reporting any issues that see.", "Over the next few weeks, AirBNB and a few other folks will be doing the same in an effort to harden the 1.8 release.", "Cheers, Chris  On Thu, Sep 29, 2016 at 3:08 AM, Maycock, Luke <luke.maycock@affiliate.oliverwyman.com wrote:  Airflow Developers,    We were looking at writing a workflow framework in Python when we found Airflow."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One thing that would definitely help is running master somewhere in your environment, and reporting any issues that see.", "Over the next few weeks, AirBNB and a few other folks will be doing the same in an effort to harden the 1.8 release.", "Cheers, Chris  On Thu, Sep 29, 2016 at 3:08 AM, Maycock, Luke <luke.maycock@affiliate.oliverwyman.com wrote:  Airflow Developers,    We were looking at writing a workflow framework in Python when we found Airflow.", "We have carried out some proof of concept work for using Airflow and wish to continue using it as it comes with lots of great features out-of-the-box.", "We have created our own fork here:   https://github.com/owlabs/incubator-airflow    So far, the only thing we have committed back to the main repository is the following fix to the mssql_hook:   https://github.com/apache/incubator-airflow/pull/1626    Among other types of tasks, we wish to be able to run mssql tasks using Airflow."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Over the next few weeks, AirBNB and a few other folks will be doing the same in an effort to harden the 1.8 release.", "Cheers, Chris  On Thu, Sep 29, 2016 at 3:08 AM, Maycock, Luke <luke.maycock@affiliate.oliverwyman.com wrote:  Airflow Developers,    We were looking at writing a workflow framework in Python when we found Airflow.", "We have carried out some proof of concept work for using Airflow and wish to continue using it as it comes with lots of great features out-of-the-box.", "We have created our own fork here:   https://github.com/owlabs/incubator-airflow    So far, the only thing we have committed back to the main repository is the following fix to the mssql_hook:   https://github.com/apache/incubator-airflow/pull/1626    Among other types of tasks, we wish to be able to run mssql tasks using Airflow.", "In order to do so, the above and below fixes are required:   https://github.com/apache/incubator-airflow/pull/1458<https://github.com/apache/incubator-airflow/pull/1458/commits/e7e655fde3c29742149d047028cbb21aecba86ed    We have created a Chef cookbook for configuring VMs with Airflow and its prerequisites."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers, Chris  On Thu, Sep 29, 2016 at 3:08 AM, Maycock, Luke <luke.maycock@affiliate.oliverwyman.com wrote:  Airflow Developers,    We were looking at writing a workflow framework in Python when we found Airflow.", "We have carried out some proof of concept work for using Airflow and wish to continue using it as it comes with lots of great features out-of-the-box.", "We have created our own fork here:   https://github.com/owlabs/incubator-airflow    So far, the only thing we have committed back to the main repository is the following fix to the mssql_hook:   https://github.com/apache/incubator-airflow/pull/1626    Among other types of tasks, we wish to be able to run mssql tasks using Airflow.", "In order to do so, the above and below fixes are required:   https://github.com/apache/incubator-airflow/pull/1458<https://github.com/apache/incubator-airflow/pull/1458/commits/e7e655fde3c29742149d047028cbb21aecba86ed    We have created a Chef cookbook for configuring VMs with Airflow and its prerequisites.", "As part of this cookbook, we are installing the latest release of Airflow."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We have carried out some proof of concept work for using Airflow and wish to continue using it as it comes with lots of great features out-of-the-box.", "We have created our own fork here:   https://github.com/owlabs/incubator-airflow    So far, the only thing we have committed back to the main repository is the following fix to the mssql_hook:   https://github.com/apache/incubator-airflow/pull/1626    Among other types of tasks, we wish to be able to run mssql tasks using Airflow.", "In order to do so, the above and below fixes are required:   https://github.com/apache/incubator-airflow/pull/1458<https://github.com/apache/incubator-airflow/pull/1458/commits/e7e655fde3c29742149d047028cbb21aecba86ed    We have created a Chef cookbook for configuring VMs with Airflow and its prerequisites.", "As part of this cookbook, we are installing the latest release of Airflow.", "However, it appears that the latest release does not have the aforementioned fixes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We have created our own fork here:   https://github.com/owlabs/incubator-airflow    So far, the only thing we have committed back to the main repository is the following fix to the mssql_hook:   https://github.com/apache/incubator-airflow/pull/1626    Among other types of tasks, we wish to be able to run mssql tasks using Airflow.", "In order to do so, the above and below fixes are required:   https://github.com/apache/incubator-airflow/pull/1458<https://github.com/apache/incubator-airflow/pull/1458/commits/e7e655fde3c29742149d047028cbb21aecba86ed    We have created a Chef cookbook for configuring VMs with Airflow and its prerequisites.", "As part of this cookbook, we are installing the latest release of Airflow.", "However, it appears that the latest release does not have the aforementioned fixes.", "Do you know when the next release of Airflow is expected?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In order to do so, the above and below fixes are required:   https://github.com/apache/incubator-airflow/pull/1458<https://github.com/apache/incubator-airflow/pull/1458/commits/e7e655fde3c29742149d047028cbb21aecba86ed    We have created a Chef cookbook for configuring VMs with Airflow and its prerequisites.", "As part of this cookbook, we are installing the latest release of Airflow.", "However, it appears that the latest release does not have the aforementioned fixes.", "Do you know when the next release of Airflow is expected?", "Is there anything we can do to help get the next release in place?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As part of this cookbook, we are installing the latest release of Airflow.", "However, it appears that the latest release does not have the aforementioned fixes.", "Do you know when the next release of Airflow is expected?", "Is there anything we can do to help get the next release in place?", "Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________  This e-mail and any attachments may be confidential or legally privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there anything we can do to help get the next release in place?", "Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail.", "Thank you for your cooperation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail.", "Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please inform us of the erroneous delivery by return e-mail.", "Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail.", "Thank you for your cooperation. "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Right..", "I'm \"sanand\" in JIRA.", "Assigned to self.", "-s  On Sat, May 28, 2016 at 8:28 PM, Cyril Scetbon <cyril.scetbon@free.fr wrote:   Hey,   I've created the JIRA https://issues.apache.org/jira/browse/AIRFLOW-188,  but Jira can't find your user id when I try to assign it to you.", "Did you  make  a mistake ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm \"sanand\" in JIRA.", "Assigned to self.", "-s  On Sat, May 28, 2016 at 8:28 PM, Cyril Scetbon <cyril.scetbon@free.fr wrote:   Hey,   I've created the JIRA https://issues.apache.org/jira/browse/AIRFLOW-188,  but Jira can't find your user id when I try to assign it to you.", "Did you  make  a mistake ?", "On May 27, 2016, at 23:24, siddharth anand <sanand@apache.org wrote:     Cyril,   Can you create a Jira for this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Assigned to self.", "-s  On Sat, May 28, 2016 at 8:28 PM, Cyril Scetbon <cyril.scetbon@free.fr wrote:   Hey,   I've created the JIRA https://issues.apache.org/jira/browse/AIRFLOW-188,  but Jira can't find your user id when I try to assign it to you.", "Did you  make  a mistake ?", "On May 27, 2016, at 23:24, siddharth anand <sanand@apache.org wrote:     Cyril,   Can you create a Jira for this?", "We need a way to reproduce this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-s  On Sat, May 28, 2016 at 8:28 PM, Cyril Scetbon <cyril.scetbon@free.fr wrote:   Hey,   I've created the JIRA https://issues.apache.org/jira/browse/AIRFLOW-188,  but Jira can't find your user id when I try to assign it to you.", "Did you  make  a mistake ?", "On May 27, 2016, at 23:24, siddharth anand <sanand@apache.org wrote:     Cyril,   Can you create a Jira for this?", "We need a way to reproduce this.", "Please  add   your backend db (e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Did you  make  a mistake ?", "On May 27, 2016, at 23:24, siddharth anand <sanand@apache.org wrote:     Cyril,   Can you create a Jira for this?", "We need a way to reproduce this.", "Please  add   your backend db (e.g.", "sqlite or something else) and an example of your  code   -- the one you are trying to clear."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On May 27, 2016, at 23:24, siddharth anand <sanand@apache.org wrote:     Cyril,   Can you create a Jira for this?", "We need a way to reproduce this.", "Please  add   your backend db (e.g.", "sqlite or something else) and an example of your  code   -- the one you are trying to clear.", "Then assign the bug to me (r39132)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please  add   your backend db (e.g.", "sqlite or something else) and an example of your  code   -- the one you are trying to clear.", "Then assign the bug to me (r39132).", "This may be related to the recursive clear of subdags, which was merged  in   https://github.com/apache/incubator-airflow/pull/1478     I mention in that PR the need to implement the CLI as well and this might   be a result of a half-implemented feature (i.e.", "works for UI, but not for   CLI)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["sqlite or something else) and an example of your  code   -- the one you are trying to clear.", "Then assign the bug to me (r39132).", "This may be related to the recursive clear of subdags, which was merged  in   https://github.com/apache/incubator-airflow/pull/1478     I mention in that PR the need to implement the CLI as well and this might   be a result of a half-implemented feature (i.e.", "works for UI, but not for   CLI).", "If you provide your code example, I can verify if this is the case."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Then assign the bug to me (r39132).", "This may be related to the recursive clear of subdags, which was merged  in   https://github.com/apache/incubator-airflow/pull/1478     I mention in that PR the need to implement the CLI as well and this might   be a result of a half-implemented feature (i.e.", "works for UI, but not for   CLI).", "If you provide your code example, I can verify if this is the case.", "-s     On Fri, May 27, 2016 at 3:23 PM, Cyril Scetbon <cyril.scetbon@free.fr   wrote:     Hi,     When I try to clear the past of a task I get a Oops."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This may be related to the recursive clear of subdags, which was merged  in   https://github.com/apache/incubator-airflow/pull/1478     I mention in that PR the need to implement the CLI as well and this might   be a result of a half-implemented feature (i.e.", "works for UI, but not for   CLI).", "If you provide your code example, I can verify if this is the case.", "-s     On Fri, May 27, 2016 at 3:23 PM, Cyril Scetbon <cyril.scetbon@free.fr   wrote:     Hi,     When I try to clear the past of a task I get a Oops.", "Here is the output  I   get : http://pastebin.com/yTYsekyB     I'm using Airflow 1.7.0 and my dag contains a SubDagOperator with a  subdag   containing a BashOperator     Is it a known issue ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["works for UI, but not for   CLI).", "If you provide your code example, I can verify if this is the case.", "-s     On Fri, May 27, 2016 at 3:23 PM, Cyril Scetbon <cyril.scetbon@free.fr   wrote:     Hi,     When I try to clear the past of a task I get a Oops.", "Here is the output  I   get : http://pastebin.com/yTYsekyB     I'm using Airflow 1.7.0 and my dag contains a SubDagOperator with a  subdag   containing a BashOperator     Is it a known issue ?", "Thanks    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In Airflow 1.6.2, all of the concurrency controls are sometimes ignored and many tasks are scheduled simultaneously.", "I don't know if this has been completely fixed.", "You can rely on them to separate your task runs *most* of the time, but not *all* of the time- so don't write code that depends on exclusive operation.", "Lance  On Thu, Aug 11, 2016 at 1:15 PM, Kurt Muehlner <kmuehlner@connexity.com wrote:   I\u2019m not aware of a concurrency limit at task granularity, however, one  available option is the \u2018max_active_runs\u2019 parameter in the DAG class.", "max_active_runs (int) \u2013 maximum number of active DAG runs, beyond this  number of DAG runs in a running state, the scheduler won\u2019t create new  active DAG runs   I\u2019ve used the \u2018pool size of 1\u2019 option you mention as a very simple way to  ensure two DAGs run in serial."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You can rely on them to separate your task runs *most* of the time, but not *all* of the time- so don't write code that depends on exclusive operation.", "Lance  On Thu, Aug 11, 2016 at 1:15 PM, Kurt Muehlner <kmuehlner@connexity.com wrote:   I\u2019m not aware of a concurrency limit at task granularity, however, one  available option is the \u2018max_active_runs\u2019 parameter in the DAG class.", "max_active_runs (int) \u2013 maximum number of active DAG runs, beyond this  number of DAG runs in a running state, the scheduler won\u2019t create new  active DAG runs   I\u2019ve used the \u2018pool size of 1\u2019 option you mention as a very simple way to  ensure two DAGs run in serial.", "Kurt   On 8/11/16, 6:57 AM, \"\u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df\" <hilaviz@gmail.com wrote:       should I use pool of size 1?", "On Thu, Aug 11, 2016 at 4:46 PM, \u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df <hilaviz@gmail.com wrote:        Hi,       I searched in the documentation for a way to limit a specific task       concurrency to 1,       but didn't find a way."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Lance  On Thu, Aug 11, 2016 at 1:15 PM, Kurt Muehlner <kmuehlner@connexity.com wrote:   I\u2019m not aware of a concurrency limit at task granularity, however, one  available option is the \u2018max_active_runs\u2019 parameter in the DAG class.", "max_active_runs (int) \u2013 maximum number of active DAG runs, beyond this  number of DAG runs in a running state, the scheduler won\u2019t create new  active DAG runs   I\u2019ve used the \u2018pool size of 1\u2019 option you mention as a very simple way to  ensure two DAGs run in serial.", "Kurt   On 8/11/16, 6:57 AM, \"\u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df\" <hilaviz@gmail.com wrote:       should I use pool of size 1?", "On Thu, Aug 11, 2016 at 4:46 PM, \u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df <hilaviz@gmail.com wrote:        Hi,       I searched in the documentation for a way to limit a specific task       concurrency to 1,       but didn't find a way.", "I thought that 'depends_on_past' should achieve this goal, but I  want the       task to run even if the previous task failed - just to be sure the  they       don't run in parallel."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["max_active_runs (int) \u2013 maximum number of active DAG runs, beyond this  number of DAG runs in a running state, the scheduler won\u2019t create new  active DAG runs   I\u2019ve used the \u2018pool size of 1\u2019 option you mention as a very simple way to  ensure two DAGs run in serial.", "Kurt   On 8/11/16, 6:57 AM, \"\u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df\" <hilaviz@gmail.com wrote:       should I use pool of size 1?", "On Thu, Aug 11, 2016 at 4:46 PM, \u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df <hilaviz@gmail.com wrote:        Hi,       I searched in the documentation for a way to limit a specific task       concurrency to 1,       but didn't find a way.", "I thought that 'depends_on_past' should achieve this goal, but I  want the       task to run even if the previous task failed - just to be sure the  they       don't run in parallel.", "The task doesn't have a downstream task, so I can't use       'wait_for_downstream'."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Kurt   On 8/11/16, 6:57 AM, \"\u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df\" <hilaviz@gmail.com wrote:       should I use pool of size 1?", "On Thu, Aug 11, 2016 at 4:46 PM, \u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df <hilaviz@gmail.com wrote:        Hi,       I searched in the documentation for a way to limit a specific task       concurrency to 1,       but didn't find a way.", "I thought that 'depends_on_past' should achieve this goal, but I  want the       task to run even if the previous task failed - just to be sure the  they       don't run in parallel.", "The task doesn't have a downstream task, so I can't use       'wait_for_downstream'.", "Am I Missing something?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Aug 11, 2016 at 4:46 PM, \u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df <hilaviz@gmail.com wrote:        Hi,       I searched in the documentation for a way to limit a specific task       concurrency to 1,       but didn't find a way.", "I thought that 'depends_on_past' should achieve this goal, but I  want the       task to run even if the previous task failed - just to be sure the  they       don't run in parallel.", "The task doesn't have a downstream task, so I can't use       'wait_for_downstream'.", "Am I Missing something?", "Thanks,       Hila                  --  Lance Norskog lance.norskog@gmail.com Redwood City, CA  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Maybe worth to think about moving parts of the kalumet code into plugins for devops tools like chef or puppets...   ________________________________ Von: Andreas Pieber <anpieber@gmail.com Gesendet: Mittwoch, 2.", "September 2015 15:00 An: kalumet-user@incubator.apache.org Cc: kalumet-dev@incubator.apache.org Betreff: Re: [VOTE] Retire Kalumet from the incubator  +1  Same problem here.", "Simply not enough free timeslots as I had hoped for :-(  Kind regards, Andreas  On Wed, Sep 2, 2015 at 2:11 PM Jamie G. <jamie.goodyear@gmail.com<mailto:jamie.goodyear@gmail.com wrote: +1  Same feelings as Achim here, just not enough time to devote to this project :(  On Wed, Sep 2, 2015 at 9:07 AM, Achim Nierbeck <bcanhome@googlemail.com<mailto:bcanhome@googlemail.com wrote:  +1   ...", "I'm very sorry but I couldn't spent as much time on this project then  what I wished for.", "regards, Achim   2015-09-02 13:35 GMT+02:00 Jean-Baptiste Onofr\u00e9 <jb@nanthrax.net<mailto:jb@nanthrax.net:   Hi all,   Regarding the Kalumet community activity, I think it's a fair question: do  we want to continue the incubation process ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["September 2015 15:00 An: kalumet-user@incubator.apache.org Cc: kalumet-dev@incubator.apache.org Betreff: Re: [VOTE] Retire Kalumet from the incubator  +1  Same problem here.", "Simply not enough free timeslots as I had hoped for :-(  Kind regards, Andreas  On Wed, Sep 2, 2015 at 2:11 PM Jamie G. <jamie.goodyear@gmail.com<mailto:jamie.goodyear@gmail.com wrote: +1  Same feelings as Achim here, just not enough time to devote to this project :(  On Wed, Sep 2, 2015 at 9:07 AM, Achim Nierbeck <bcanhome@googlemail.com<mailto:bcanhome@googlemail.com wrote:  +1   ...", "I'm very sorry but I couldn't spent as much time on this project then  what I wished for.", "regards, Achim   2015-09-02 13:35 GMT+02:00 Jean-Baptiste Onofr\u00e9 <jb@nanthrax.net<mailto:jb@nanthrax.net:   Hi all,   Regarding the Kalumet community activity, I think it's a fair question: do  we want to continue the incubation process ?", "Even if I'm still thinking that Kalumet has lot of values, I propose to  retire Kalumet from the incubator:   [ ] +1 to retire Kalumet from the incubator  [ ] -1 to keep Kalumet in the incubator   The vote is open for 48 hours."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Simply not enough free timeslots as I had hoped for :-(  Kind regards, Andreas  On Wed, Sep 2, 2015 at 2:11 PM Jamie G. <jamie.goodyear@gmail.com<mailto:jamie.goodyear@gmail.com wrote: +1  Same feelings as Achim here, just not enough time to devote to this project :(  On Wed, Sep 2, 2015 at 9:07 AM, Achim Nierbeck <bcanhome@googlemail.com<mailto:bcanhome@googlemail.com wrote:  +1   ...", "I'm very sorry but I couldn't spent as much time on this project then  what I wished for.", "regards, Achim   2015-09-02 13:35 GMT+02:00 Jean-Baptiste Onofr\u00e9 <jb@nanthrax.net<mailto:jb@nanthrax.net:   Hi all,   Regarding the Kalumet community activity, I think it's a fair question: do  we want to continue the incubation process ?", "Even if I'm still thinking that Kalumet has lot of values, I propose to  retire Kalumet from the incubator:   [ ] +1 to retire Kalumet from the incubator  [ ] -1 to keep Kalumet in the incubator   The vote is open for 48 hours.", "Thanks,  Regards  JB  --  Jean-Baptiste Onofr\u00e9  jbonofre@apache.org<mailto:jbonofre@apache.org  http://blog.nanthrax.net  Talend - http://www.talend.com      --   Apache Member  Apache Karaf <http://karaf.apache.org/ Committer & PMC  OPS4J Pax Web <http://wiki.ops4j.org/display/paxweb/Pax+Web/ Committer &  Project Lead  blog <http://notizblog.nierbeck.de/  Co-Author of Apache Karaf Cookbook <http://bit.ly/1ps9rkS   Software Architect / Project Manager / Scrum Master  --  -- Sent from my mobile."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["I'm very sorry but I couldn't spent as much time on this project then  what I wished for.", "regards, Achim   2015-09-02 13:35 GMT+02:00 Jean-Baptiste Onofr\u00e9 <jb@nanthrax.net<mailto:jb@nanthrax.net:   Hi all,   Regarding the Kalumet community activity, I think it's a fair question: do  we want to continue the incubation process ?", "Even if I'm still thinking that Kalumet has lot of values, I propose to  retire Kalumet from the incubator:   [ ] +1 to retire Kalumet from the incubator  [ ] -1 to keep Kalumet in the incubator   The vote is open for 48 hours.", "Thanks,  Regards  JB  --  Jean-Baptiste Onofr\u00e9  jbonofre@apache.org<mailto:jbonofre@apache.org  http://blog.nanthrax.net  Talend - http://www.talend.com      --   Apache Member  Apache Karaf <http://karaf.apache.org/ Committer & PMC  OPS4J Pax Web <http://wiki.ops4j.org/display/paxweb/Pax+Web/ Committer &  Project Lead  blog <http://notizblog.nierbeck.de/  Co-Author of Apache Karaf Cookbook <http://bit.ly/1ps9rkS   Software Architect / Project Manager / Scrum Master  --  -- Sent from my mobile.", "Forgive the brevity, the typos and the lack of nuance.  "], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["The link in the file RELEASE_NOTES.txt - http://incubator.apache.org/hcatalog/docs/r0.1.0/install.html doesn't work.", "I guess this will start working at/after the release..  Other than that +1  (Downloaded the tar ball, and ran unit tests).", "On Jul 12, 2011, at 5:05 PM, Ashutosh Chauhan wrote:    Hi,       I have created a candidate build for HCatalog 0.1.0-incubating.", "This is the initial release of HCatalog.", "Keys used to sign the release are available at  http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.1/KEYS?revision=1145809&view=markup."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I guess this will start working at/after the release..  Other than that +1  (Downloaded the tar ball, and ran unit tests).", "On Jul 12, 2011, at 5:05 PM, Ashutosh Chauhan wrote:    Hi,       I have created a candidate build for HCatalog 0.1.0-incubating.", "This is the initial release of HCatalog.", "Keys used to sign the release are available at  http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.1/KEYS?revision=1145809&view=markup.", "Please download, test, and try it out:       http://people.apache.org/~hashutosh/hcatalog-0.1.0-incubating-candidate-1/       The release, md5 signature, gpg signature, and rat report can all  be found at the above address."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Jul 12, 2011, at 5:05 PM, Ashutosh Chauhan wrote:    Hi,       I have created a candidate build for HCatalog 0.1.0-incubating.", "This is the initial release of HCatalog.", "Keys used to sign the release are available at  http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.1/KEYS?revision=1145809&view=markup.", "Please download, test, and try it out:       http://people.apache.org/~hashutosh/hcatalog-0.1.0-incubating-candidate-1/       The release, md5 signature, gpg signature, and rat report can all  be found at the above address.", "Should we release this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is the initial release of HCatalog.", "Keys used to sign the release are available at  http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.1/KEYS?revision=1145809&view=markup.", "Please download, test, and try it out:       http://people.apache.org/~hashutosh/hcatalog-0.1.0-incubating-candidate-1/       The release, md5 signature, gpg signature, and rat report can all  be found at the above address.", "Should we release this?", "Vote closes on Thursday, July 15th."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am a Blur newbie and just heard about it and the recent 0.2 release from the latest Hadoop Weekly email.", "I checked out the site and there is a great documentation for getting started, but didn't mention a couple questions I have.", "We are running our Hadoop clusters as a mix of persistent and transient EMR clusters in Amazon.", "It is running Hadoop 1.0.3.", "We are also using S3 instead of HDFS to store our data."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I checked out the site and there is a great documentation for getting started, but didn't mention a couple questions I have.", "We are running our Hadoop clusters as a mix of persistent and transient EMR clusters in Amazon.", "It is running Hadoop 1.0.3.", "We are also using S3 instead of HDFS to store our data.", "So does anyone have experience running Blur in an Amazon environment?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We are running our Hadoop clusters as a mix of persistent and transient EMR clusters in Amazon.", "It is running Hadoop 1.0.3.", "We are also using S3 instead of HDFS to store our data.", "So does anyone have experience running Blur in an Amazon environment?", "Does S3 vs HDFS present any problems to the Blur architecture?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It is running Hadoop 1.0.3.", "We are also using S3 instead of HDFS to store our data.", "So does anyone have experience running Blur in an Amazon environment?", "Does S3 vs HDFS present any problems to the Blur architecture?", "Thanks in advance!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We are also using S3 instead of HDFS to store our data.", "So does anyone have experience running Blur in an Amazon environment?", "Does S3 vs HDFS present any problems to the Blur architecture?", "Thanks in advance!", "-Jonathan  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tuesday, October 21, 2019 Mike Beckerle wrote:     *   ... an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving the \"data archaeology\" problem.", "I read the article.", "It provides a nice example of trying to figure out the structure of a file that was poorly documented.", "Assertion: that example has nothing to do with DFDL.", "DFDL says nothing about how to figure out the structure of files."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It provides a nice example of trying to figure out the structure of a file that was poorly documented.", "Assertion: that example has nothing to do with DFDL.", "DFDL says nothing about how to figure out the structure of files.", "What DFDL does do is it tells you, once you have figured out the structure, then here's a way to describe the structure.", "Do you agree with my assertion?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Assertion: that example has nothing to do with DFDL.", "DFDL says nothing about how to figure out the structure of files.", "What DFDL does do is it tells you, once you have figured out the structure, then here's a way to describe the structure.", "Do you agree with my assertion?", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com Sent: Tuesday, October 22, 2019 7:44 PM To: users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["DFDL says nothing about how to figure out the structure of files.", "What DFDL does do is it tells you, once you have figured out the structure, then here's a way to describe the structure.", "Do you agree with my assertion?", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com Sent: Tuesday, October 22, 2019 7:44 PM To: users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What DFDL does do is it tells you, once you have figured out the structure, then here's a way to describe the structure.", "Do you agree with my assertion?", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com Sent: Tuesday, October 22, 2019 7:44 PM To: users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?", "Roger,  I think you may have been blessed not have run into the need to deal with describing data formats as much as is commonplace."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do you agree with my assertion?", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com Sent: Tuesday, October 22, 2019 7:44 PM To: users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?", "Roger,  I think you may have been blessed not have run into the need to deal with describing data formats as much as is commonplace.", "Dealing with them is day-to-day work for many engineers."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger    From: Beckerle, Mike <mbeckerle@tresys.com Sent: Tuesday, October 22, 2019 7:44 PM To: users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?", "Roger,  I think you may have been blessed not have run into the need to deal with describing data formats as much as is commonplace.", "Dealing with them is day-to-day work for many engineers.", "This is a link to an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving the \"data archaeology\" problem."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why parse and unparse data formats?", "Roger,  I think you may have been blessed not have run into the need to deal with describing data formats as much as is commonplace.", "Dealing with them is day-to-day work for many engineers.", "This is a link to an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving the \"data archaeology\" problem.", "It gives an example of an absolutely mundane day-to-day data problem that was typical of what I and many many other people were constantly solving for customers."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Roger,  I think you may have been blessed not have run into the need to deal with describing data formats as much as is commonplace.", "Dealing with them is day-to-day work for many engineers.", "This is a link to an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving the \"data archaeology\" problem.", "It gives an example of an absolutely mundane day-to-day data problem that was typical of what I and many many other people were constantly solving for customers.", "It was some COBOL-ish data records that the customer couldn't figure out how to export from one system to get into some new system."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is a link to an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving the \"data archaeology\" problem.", "It gives an example of an absolutely mundane day-to-day data problem that was typical of what I and many many other people were constantly solving for customers.", "It was some COBOL-ish data records that the customer couldn't figure out how to export from one system to get into some new system.", "They didn't have good configuration management, so the exact software that created the data wasn't clear.", "Absolutely typical stuff."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It gives an example of an absolutely mundane day-to-day data problem that was typical of what I and many many other people were constantly solving for customers.", "It was some COBOL-ish data records that the customer couldn't figure out how to export from one system to get into some new system.", "They didn't have good configuration management, so the exact software that created the data wasn't clear.", "Absolutely typical stuff.", "https://cboblog.typepad.com/cboblog/2008/07/dfdl-data-forma.html  I hope that helps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It was some COBOL-ish data records that the customer couldn't figure out how to export from one system to get into some new system.", "They didn't have good configuration management, so the exact software that created the data wasn't clear.", "Absolutely typical stuff.", "https://cboblog.typepad.com/cboblog/2008/07/dfdl-data-forma.html  I hope that helps.", "...mikeb     ________________________________ From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Tuesday, October 22, 2019 1:03 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: RE: Why use DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["They didn't have good configuration management, so the exact software that created the data wasn't clear.", "Absolutely typical stuff.", "https://cboblog.typepad.com/cboblog/2008/07/dfdl-data-forma.html  I hope that helps.", "...mikeb     ________________________________ From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Tuesday, October 22, 2019 1:03 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: RE: Why use DFDL?", "Why parse and unparse data formats?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Absolutely typical stuff.", "https://cboblog.typepad.com/cboblog/2008/07/dfdl-data-forma.html  I hope that helps.", "...mikeb     ________________________________ From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Tuesday, October 22, 2019 1:03 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: RE: Why use DFDL?", "Why parse and unparse data formats?", "Hi Folks,    I want to elaborate on my question, as I feel it is a very important question."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://cboblog.typepad.com/cboblog/2008/07/dfdl-data-forma.html  I hope that helps.", "...mikeb     ________________________________ From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Tuesday, October 22, 2019 1:03 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: RE: Why use DFDL?", "Why parse and unparse data formats?", "Hi Folks,    I want to elaborate on my question, as I feel it is a very important question.", "Does your average software engineer need to parse data formats?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["...mikeb     ________________________________ From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Tuesday, October 22, 2019 1:03 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: RE: Why use DFDL?", "Why parse and unparse data formats?", "Hi Folks,    I want to elaborate on my question, as I feel it is a very important question.", "Does your average software engineer need to parse data formats?", "I can't think of any time, when implementing some software application, that I have needed to write a parser for a data format."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Folks,    I want to elaborate on my question, as I feel it is a very important question.", "Does your average software engineer need to parse data formats?", "I can't think of any time, when implementing some software application, that I have needed to write a parser for a data format.", "The parsing is already done under the hood by some library.", "For example, when I write code in XSLT I don't ever have to parse the input because there is a parser under the hood that does the parsing for me."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Does your average software engineer need to parse data formats?", "I can't think of any time, when implementing some software application, that I have needed to write a parser for a data format.", "The parsing is already done under the hood by some library.", "For example, when I write code in XSLT I don't ever have to parse the input because there is a parser under the hood that does the parsing for me.", "I can understand why cybersecurity people want to parse data formats - they need to inspect the parsed data for malicious stuff."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can't think of any time, when implementing some software application, that I have needed to write a parser for a data format.", "The parsing is already done under the hood by some library.", "For example, when I write code in XSLT I don't ever have to parse the input because there is a parser under the hood that does the parsing for me.", "I can understand why cybersecurity people want to parse data formats - they need to inspect the parsed data for malicious stuff.", "But as Mike noted, that's a tiny niche."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The parsing is already done under the hood by some library.", "For example, when I write code in XSLT I don't ever have to parse the input because there is a parser under the hood that does the parsing for me.", "I can understand why cybersecurity people want to parse data formats - they need to inspect the parsed data for malicious stuff.", "But as Mike noted, that's a tiny niche.", "Is DFDL relevant only to that tiny niche community?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But as Mike noted, that's a tiny niche.", "Is DFDL relevant only to that tiny niche community?", "Hopefully it is relevant to other communities.", "What communities?", "Who needs to parse?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What communities?", "Who needs to parse?", "Who needs DFDL?", "/Roger      From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Monday, October 21, 2019 5:36 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Re: Why use DFDL?", "Why parse and unparse data formats?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Who needs DFDL?", "/Roger      From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Monday, October 21, 2019 5:36 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Re: Why use DFDL?", "Why parse and unparse data formats?", "Hi Mike,    Thank you for your great feedback.", "I think that I have everything (or almost everything) you said already in my tutorial."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger      From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Monday, October 21, 2019 5:36 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Re: Why use DFDL?", "Why parse and unparse data formats?", "Hi Mike,    Thank you for your great feedback.", "I think that I have everything (or almost everything) you said already in my tutorial.", "I am trying to address a different concern: is parsing and unparsing applicable to the everyday software engineer?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why parse and unparse data formats?", "Hi Mike,    Thank you for your great feedback.", "I think that I have everything (or almost everything) you said already in my tutorial.", "I am trying to address a different concern: is parsing and unparsing applicable to the everyday software engineer?", "Where is parsing and unparsing needed?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Mike,    Thank you for your great feedback.", "I think that I have everything (or almost everything) you said already in my tutorial.", "I am trying to address a different concern: is parsing and unparsing applicable to the everyday software engineer?", "Where is parsing and unparsing needed?", "Why do I (the everyday software engineer) need parsing and unparsing, i.e., why do I need DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am trying to address a different concern: is parsing and unparsing applicable to the everyday software engineer?", "Where is parsing and unparsing needed?", "Why do I (the everyday software engineer) need parsing and unparsing, i.e., why do I need DFDL?", "As you point out, the cybercommunity needs to do parsing and unparsing, but the cybercommunity is a very small niche.", "Are there other niches that need to do parsing and unparsing?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Where is parsing and unparsing needed?", "Why do I (the everyday software engineer) need parsing and unparsing, i.e., why do I need DFDL?", "As you point out, the cybercommunity needs to do parsing and unparsing, but the cybercommunity is a very small niche.", "Are there other niches that need to do parsing and unparsing?", "Ideally most or all software engineers need to do parsing and unparsing; then DFDL would/should be in high demand."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why do I (the everyday software engineer) need parsing and unparsing, i.e., why do I need DFDL?", "As you point out, the cybercommunity needs to do parsing and unparsing, but the cybercommunity is a very small niche.", "Are there other niches that need to do parsing and unparsing?", "Ideally most or all software engineers need to do parsing and unparsing; then DFDL would/should be in high demand.", "Again, I think the answer to these questions is supremely important."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Are there other niches that need to do parsing and unparsing?", "Ideally most or all software engineers need to do parsing and unparsing; then DFDL would/should be in high demand.", "Again, I think the answer to these questions is supremely important.", "If I can't tell people why DFDL is important to them, well, then it's gonna be tough getting them interested in learning DFDL.", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com<mailto:mbeckerle@tresys.com Sent: Monday, October 21, 2019 3:11 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Again, I think the answer to these questions is supremely important.", "If I can't tell people why DFDL is important to them, well, then it's gonna be tough getting them interested in learning DFDL.", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com<mailto:mbeckerle@tresys.com Sent: Monday, October 21, 2019 3:11 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?", "The first 3 points are about the \"Cybersecurity Use Case\" specifically."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If I can't tell people why DFDL is important to them, well, then it's gonna be tough getting them interested in learning DFDL.", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com<mailto:mbeckerle@tresys.com Sent: Monday, October 21, 2019 3:11 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?", "The first 3 points are about the \"Cybersecurity Use Case\" specifically.", "This is one small slice of computerdom."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger    From: Beckerle, Mike <mbeckerle@tresys.com<mailto:mbeckerle@tresys.com Sent: Monday, October 21, 2019 3:11 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?", "The first 3 points are about the \"Cybersecurity Use Case\" specifically.", "This is one small slice of computerdom.", "Important one, but a niche area that is beside the point for most people who want to get on with using the data for some actual end purpose."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why parse and unparse data formats?", "The first 3 points are about the \"Cybersecurity Use Case\" specifically.", "This is one small slice of computerdom.", "Important one, but a niche area that is beside the point for most people who want to get on with using the data for some actual end purpose.", "Your last sentence on the slide wants to end with \"data\" not \"tool\"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The first 3 points are about the \"Cybersecurity Use Case\" specifically.", "This is one small slice of computerdom.", "Important one, but a niche area that is beside the point for most people who want to get on with using the data for some actual end purpose.", "Your last sentence on the slide wants to end with \"data\" not \"tool\".", "Your phrase \"Why parse and unparse data formats?\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is one small slice of computerdom.", "Important one, but a niche area that is beside the point for most people who want to get on with using the data for some actual end purpose.", "Your last sentence on the slide wants to end with \"data\" not \"tool\".", "Your phrase \"Why parse and unparse data formats?\"", "gives me some concern."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Important one, but a niche area that is beside the point for most people who want to get on with using the data for some actual end purpose.", "Your last sentence on the slide wants to end with \"data\" not \"tool\".", "Your phrase \"Why parse and unparse data formats?\"", "gives me some concern.", "I mean do you have a choice?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["gives me some concern.", "I mean do you have a choice?", "So I'm assuming you have to use the data, so the issue is why use DFDL vs. other ways of using the data:    The reasons to use DFDL are:    *   it is an emerging open standard.", "In the long run standards give users power over vendors, reduce costs, increase skills leverage, etc.", "*   it is comprehensive - can handle everything from military messaging formats to COBOL, binary and text and mixtures thereof."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I mean do you have a choice?", "So I'm assuming you have to use the data, so the issue is why use DFDL vs. other ways of using the data:    The reasons to use DFDL are:    *   it is an emerging open standard.", "In the long run standards give users power over vendors, reduce costs, increase skills leverage, etc.", "*   it is comprehensive - can handle everything from military messaging formats to COBOL, binary and text and mixtures thereof.", "There are a few things it cannot describe as yet (TIFF for example), but it will evolve to cover those as well."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the long run standards give users power over vendors, reduce costs, increase skills leverage, etc.", "*   it is comprehensive - can handle everything from military messaging formats to COBOL, binary and text and mixtures thereof.", "There are a few things it cannot describe as yet (TIFF for example), but it will evolve to cover those as well.", "*   it has superior unparsing capability to any existing data format description system - this is one area where the DFDL standard advances the state-of-the-art.", "*   there are multiple implementations including open-source and commercial."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There are a few things it cannot describe as yet (TIFF for example), but it will evolve to cover those as well.", "*   it has superior unparsing capability to any existing data format description system - this is one area where the DFDL standard advances the state-of-the-art.", "*   there are multiple implementations including open-source and commercial.", "Some additional related points:    *   why use DFDL to parse/unparse  to/from an alternative standard textual form such as JSON or XML ?", "*   Note that DFDL doesn't per-se require this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   there are multiple implementations including open-source and commercial.", "Some additional related points:    *   why use DFDL to parse/unparse  to/from an alternative standard textual form such as JSON or XML ?", "*   Note that DFDL doesn't per-se require this.", "It is one common way to use the Daffodil implementation      *   Note that not all DFDL implementations even support this.", "E.g., the ESA's DFDL4Space tool doesn't convert to/from JSON or XML."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Some additional related points:    *   why use DFDL to parse/unparse  to/from an alternative standard textual form such as JSON or XML ?", "*   Note that DFDL doesn't per-se require this.", "It is one common way to use the Daffodil implementation      *   Note that not all DFDL implementations even support this.", "E.g., the ESA's DFDL4Space tool doesn't convert to/from JSON or XML.", "IBM DFDL can be used to convert data to XML, but when used in the most common ways, it takes data directly to/from the native internal data format used by that particular data-handling product/system."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   Note that DFDL doesn't per-se require this.", "It is one common way to use the Daffodil implementation      *   Note that not all DFDL implementations even support this.", "E.g., the ESA's DFDL4Space tool doesn't convert to/from JSON or XML.", "IBM DFDL can be used to convert data to XML, but when used in the most common ways, it takes data directly to/from the native internal data format used by that particular data-handling product/system.", "No intermediate step of XML/JSON is used."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["E.g., the ESA's DFDL4Space tool doesn't convert to/from JSON or XML.", "IBM DFDL can be used to convert data to XML, but when used in the most common ways, it takes data directly to/from the native internal data format used by that particular data-handling product/system.", "No intermediate step of XML/JSON is used.", "So I'd say the above point is really about Daffodil, not DFDL generally.", "The above is about skills leverage and tools leverage."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["IBM DFDL can be used to convert data to XML, but when used in the most common ways, it takes data directly to/from the native internal data format used by that particular data-handling product/system.", "No intermediate step of XML/JSON is used.", "So I'd say the above point is really about Daffodil, not DFDL generally.", "The above is about skills leverage and tools leverage.", "JSON support is built into all javascript based platforms such as web browsers, NODE.js, etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["No intermediate step of XML/JSON is used.", "So I'd say the above point is really about Daffodil, not DFDL generally.", "The above is about skills leverage and tools leverage.", "JSON support is built into all javascript based platforms such as web browsers, NODE.js, etc.", "XML has standard tools available from vendors."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So I'd say the above point is really about Daffodil, not DFDL generally.", "The above is about skills leverage and tools leverage.", "JSON support is built into all javascript based platforms such as web browsers, NODE.js, etc.", "XML has standard tools available from vendors.", "DFDL is another standard that adds capability to people with JSON/XML skills and/or tools already."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["JSON support is built into all javascript based platforms such as web browsers, NODE.js, etc.", "XML has standard tools available from vendors.", "DFDL is another standard that adds capability to people with JSON/XML skills and/or tools already.", "Use of a textual format as an intermediate form has significant QA benefits for most systems.", "*   Why learn and use the DFDL standard vs. some other approach?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["XML has standard tools available from vendors.", "DFDL is another standard that adds capability to people with JSON/XML skills and/or tools already.", "Use of a textual format as an intermediate form has significant QA benefits for most systems.", "*   Why learn and use the DFDL standard vs. some other approach?", "*   Such as any of the hundreds of data description tools/systems in the marketplace - one of which commonly comes with any given enterprise software package."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["DFDL is another standard that adds capability to people with JSON/XML skills and/or tools already.", "Use of a textual format as an intermediate form has significant QA benefits for most systems.", "*   Why learn and use the DFDL standard vs. some other approach?", "*   Such as any of the hundreds of data description tools/systems in the marketplace - one of which commonly comes with any given enterprise software package.", "*   Note that most of these tools are quite declarative, so the \"be declarative\" argument is orthogonal to the \"why DFDL\" argument."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Use of a textual format as an intermediate form has significant QA benefits for most systems.", "*   Why learn and use the DFDL standard vs. some other approach?", "*   Such as any of the hundreds of data description tools/systems in the marketplace - one of which commonly comes with any given enterprise software package.", "*   Note that most of these tools are quite declarative, so the \"be declarative\" argument is orthogonal to the \"why DFDL\" argument.", "*   Note that many of these tools have much better user interfaces than Daffodil (today)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   Why learn and use the DFDL standard vs. some other approach?", "*   Such as any of the hundreds of data description tools/systems in the marketplace - one of which commonly comes with any given enterprise software package.", "*   Note that most of these tools are quite declarative, so the \"be declarative\" argument is orthogonal to the \"why DFDL\" argument.", "*   Note that many of these tools have much better user interfaces than Daffodil (today).", "In the short run this may be a good reason not to use DFDL."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   Such as any of the hundreds of data description tools/systems in the marketplace - one of which commonly comes with any given enterprise software package.", "*   Note that most of these tools are quite declarative, so the \"be declarative\" argument is orthogonal to the \"why DFDL\" argument.", "*   Note that many of these tools have much better user interfaces than Daffodil (today).", "In the short run this may be a good reason not to use DFDL.", "*    In the long run we expect the power of open-source and standards to address this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   Note that most of these tools are quite declarative, so the \"be declarative\" argument is orthogonal to the \"why DFDL\" argument.", "*   Note that many of these tools have much better user interfaces than Daffodil (today).", "In the short run this may be a good reason not to use DFDL.", "*    In the long run we expect the power of open-source and standards to address this.", "*   Such as just writing software code to handle/parse the data."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   Note that many of these tools have much better user interfaces than Daffodil (today).", "In the short run this may be a good reason not to use DFDL.", "*    In the long run we expect the power of open-source and standards to address this.", "*   Such as just writing software code to handle/parse the data.", "The problem with this is it is typically procedural, not declarative."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the short run this may be a good reason not to use DFDL.", "*    In the long run we expect the power of open-source and standards to address this.", "*   Such as just writing software code to handle/parse the data.", "The problem with this is it is typically procedural, not declarative.", "*   DFDL is also not turing complete."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*    In the long run we expect the power of open-source and standards to address this.", "*   Such as just writing software code to handle/parse the data.", "The problem with this is it is typically procedural, not declarative.", "*   DFDL is also not turing complete.", "It is far easier to show correctness of a schema than a program."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   Such as just writing software code to handle/parse the data.", "The problem with this is it is typically procedural, not declarative.", "*   DFDL is also not turing complete.", "It is far easier to show correctness of a schema than a program.", "Point (a) above is just the standards vs. non-standards argument."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Point (a) above is just the standards vs. non-standards argument.", "At this point, I am digressing into lots of points made in this slideshare deck in slides 3, 4, 5:  https://www.slideshare.net/mbeckerle/tresys-dfdl-data-format-description-language-daffodil-open-source-public-overview-100432615    Another point is about the benefits and power of standards.", "DFDL is simply better than existing ad-hoc data format description languages in that it is far more comprehensive than most commercial and other open-source systems, and it is an emerging open standard, with multiple implementations with a good deal of demonstrated interoperability: https://cwiki.apache.org/confluence/display/DAFFODIL/Daffodil+Compatibility+with+IBM+DFDL    DFDL is still quite new, and I would expect some users to choose other things until Daffodil gets out of Apache Incubator status, and the DFDL standard is fully ratified by Open Grid Forum, and is proposed as a standard by a larger/more-recognized body.", "The fact that IBM has DFDL in multiple products now is a strong statement of support going forward.", "________________________________  From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Sunday, October 20, 2019 8:08 AM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Why use DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["DFDL is simply better than existing ad-hoc data format description languages in that it is far more comprehensive than most commercial and other open-source systems, and it is an emerging open standard, with multiple implementations with a good deal of demonstrated interoperability: https://cwiki.apache.org/confluence/display/DAFFODIL/Daffodil+Compatibility+with+IBM+DFDL    DFDL is still quite new, and I would expect some users to choose other things until Daffodil gets out of Apache Incubator status, and the DFDL standard is fully ratified by Open Grid Forum, and is proposed as a standard by a larger/more-recognized body.", "The fact that IBM has DFDL in multiple products now is a strong statement of support going forward.", "________________________________  From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Sunday, October 20, 2019 8:08 AM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Why use DFDL?", "Why parse and unparse data formats?", "Hi Folks,    It occurred to me that in my tutorial I have explained what DFDL is and how to use DFDL, but I never explained why DFDL should be used."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The fact that IBM has DFDL in multiple products now is a strong statement of support going forward.", "________________________________  From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Sunday, October 20, 2019 8:08 AM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Why use DFDL?", "Why parse and unparse data formats?", "Hi Folks,    It occurred to me that in my tutorial I have explained what DFDL is and how to use DFDL, but I never explained why DFDL should be used.", "Below is a slide that takes a stab at why."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["________________________________  From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Sunday, October 20, 2019 8:08 AM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Why use DFDL?", "Why parse and unparse data formats?", "Hi Folks,    It occurred to me that in my tutorial I have explained what DFDL is and how to use DFDL, but I never explained why DFDL should be used.", "Below is a slide that takes a stab at why.", "I am sure there are other reasons."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Folks,    It occurred to me that in my tutorial I have explained what DFDL is and how to use DFDL, but I never explained why DFDL should be used.", "Below is a slide that takes a stab at why.", "I am sure there are other reasons.", "Would you provide other reasons, please?", "I think answering why is a crucial thing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Below is a slide that takes a stab at why.", "I am sure there are other reasons.", "Would you provide other reasons, please?", "I think answering why is a crucial thing.", "I consider this slide to be very important."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am sure there are other reasons.", "Would you provide other reasons, please?", "I think answering why is a crucial thing.", "I consider this slide to be very important.", "/Roger    [cid:image001.png@01D58B3B.21404890]  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Steve,  I see that my defaults.dfdl.xsd file has these:  fillByte=\"f\" textPadKind=\"none\"  Will those produce the behavior you describe?", "/Roger   -----Original Message----- From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:34 PM To: users@daffodil.apache.org Subject: [EXT] Re: 0,100 -- parse -- 100 -- unparse -- 100ff ... Huh?", "There are no defaults value for these properties.", "And actually, dfdl:textNumberPadKind isn't a thing, it should be dfdl:textPadKind.", "But if textPadKind=\"none\" then no padding will be added."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger   -----Original Message----- From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:34 PM To: users@daffodil.apache.org Subject: [EXT] Re: 0,100 -- parse -- 100 -- unparse -- 100ff ... Huh?", "There are no defaults value for these properties.", "And actually, dfdl:textNumberPadKind isn't a thing, it should be dfdl:textPadKind.", "But if textPadKind=\"none\" then no padding will be added.", "But another option is that dfdl:fillByte is being used to fill those extra bytes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There are no defaults value for these properties.", "And actually, dfdl:textNumberPadKind isn't a thing, it should be dfdl:textPadKind.", "But if textPadKind=\"none\" then no padding will be added.", "But another option is that dfdl:fillByte is being used to fill those extra bytes.", "Perhaps dfdl:fillByte=\"f\"?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But if textPadKind=\"none\" then no padding will be added.", "But another option is that dfdl:fillByte is being used to fill those extra bytes.", "Perhaps dfdl:fillByte=\"f\"?", "- Steve  On 6/25/19 2:28 PM, Costello, Roger L. wrote:  Hi Steve,     I would guess that you have:    textNumberPadKind=\"padChar\",   textNumberPadCharacter=\"f\", and   textNumberJustification=\"left\"    Actually, I looked at my defaults.dfdl.xsd file and it doesn't mention any of those properties.", "If those properties are not specified, do they default to the values you list?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But another option is that dfdl:fillByte is being used to fill those extra bytes.", "Perhaps dfdl:fillByte=\"f\"?", "- Steve  On 6/25/19 2:28 PM, Costello, Roger L. wrote:  Hi Steve,     I would guess that you have:    textNumberPadKind=\"padChar\",   textNumberPadCharacter=\"f\", and   textNumberJustification=\"left\"    Actually, I looked at my defaults.dfdl.xsd file and it doesn't mention any of those properties.", "If those properties are not specified, do they default to the values you list?", "/Roger    -----Original Message-----  From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:14 PM  To: users@daffodil.apache.org  Subject: [EXT] Re: 0,100 -- parse -- 100 -- 100ff ... Huh?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Perhaps dfdl:fillByte=\"f\"?", "- Steve  On 6/25/19 2:28 PM, Costello, Roger L. wrote:  Hi Steve,     I would guess that you have:    textNumberPadKind=\"padChar\",   textNumberPadCharacter=\"f\", and   textNumberJustification=\"left\"    Actually, I looked at my defaults.dfdl.xsd file and it doesn't mention any of those properties.", "If those properties are not specified, do they default to the values you list?", "/Roger    -----Original Message-----  From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:14 PM  To: users@daffodil.apache.org  Subject: [EXT] Re: 0,100 -- parse -- 100 -- 100ff ... Huh?", "This is a good example of the difference between # and 0 pattern characters when unparsing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Steve  On 6/25/19 2:28 PM, Costello, Roger L. wrote:  Hi Steve,     I would guess that you have:    textNumberPadKind=\"padChar\",   textNumberPadCharacter=\"f\", and   textNumberJustification=\"left\"    Actually, I looked at my defaults.dfdl.xsd file and it doesn't mention any of those properties.", "If those properties are not specified, do they default to the values you list?", "/Roger    -----Original Message-----  From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:14 PM  To: users@daffodil.apache.org  Subject: [EXT] Re: 0,100 -- parse -- 100 -- 100ff ... Huh?", "This is a good example of the difference between # and 0 pattern characters when unparsing.", "With the pattern \"0,000\", the value \"100\" will be padded with zero's and so will unparse to \"0,100\", which matches the expected length of 5."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If those properties are not specified, do they default to the values you list?", "/Roger    -----Original Message-----  From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:14 PM  To: users@daffodil.apache.org  Subject: [EXT] Re: 0,100 -- parse -- 100 -- 100ff ... Huh?", "This is a good example of the difference between # and 0 pattern characters when unparsing.", "With the pattern \"0,000\", the value \"100\" will be padded with zero's and so will unparse to \"0,100\", which matches the expected length of 5.", "However, with the pattern \"#,###\", the value \"100\" will unparse to   \"100\"--no comma is needed and it will not zero pad."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger    -----Original Message-----  From: Steve Lawrence <slawrence@apache.org  Sent: Tuesday, June 25, 2019 2:14 PM  To: users@daffodil.apache.org  Subject: [EXT] Re: 0,100 -- parse -- 100 -- 100ff ... Huh?", "This is a good example of the difference between # and 0 pattern characters when unparsing.", "With the pattern \"0,000\", the value \"100\" will be padded with zero's and so will unparse to \"0,100\", which matches the expected length of 5.", "However, with the pattern \"#,###\", the value \"100\" will unparse to   \"100\"--no comma is needed and it will not zero pad.", "But your test1   element is defined as having a length of 5 and the the unparsed value   has a length of 3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is a good example of the difference between # and 0 pattern characters when unparsing.", "With the pattern \"0,000\", the value \"100\" will be padded with zero's and so will unparse to \"0,100\", which matches the expected length of 5.", "However, with the pattern \"#,###\", the value \"100\" will unparse to   \"100\"--no comma is needed and it will not zero pad.", "But your test1   element is defined as having a length of 5 and the the unparsed value   has a length of 3.", "In this case, Daffodil uses textNumberPadKind and   related properties (textNumberPadCharacter, textNumberJustification,  etc.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["With the pattern \"0,000\", the value \"100\" will be padded with zero's and so will unparse to \"0,100\", which matches the expected length of 5.", "However, with the pattern \"#,###\", the value \"100\" will unparse to   \"100\"--no comma is needed and it will not zero pad.", "But your test1   element is defined as having a length of 5 and the the unparsed value   has a length of 3.", "In this case, Daffodil uses textNumberPadKind and   related properties (textNumberPadCharacter, textNumberJustification,  etc.)", "to pad the unparsed value up to the needed 5 characters."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But your test1   element is defined as having a length of 5 and the the unparsed value   has a length of 3.", "In this case, Daffodil uses textNumberPadKind and   related properties (textNumberPadCharacter, textNumberJustification,  etc.)", "to pad the unparsed value up to the needed 5 characters.", "So I would guess that you have textNumberPadKind=\"padChar\", textNumberPadCharacter=\"f\", and textNumberJustification=\"left\".", "Those three properties will cause Daffodil to add extra \"f\" characters as padding to the right of the string."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In this case, Daffodil uses textNumberPadKind and   related properties (textNumberPadCharacter, textNumberJustification,  etc.)", "to pad the unparsed value up to the needed 5 characters.", "So I would guess that you have textNumberPadKind=\"padChar\", textNumberPadCharacter=\"f\", and textNumberJustification=\"left\".", "Those three properties will cause Daffodil to add extra \"f\" characters as padding to the right of the string.", "I don't think I would say to never use the '#' character."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["to pad the unparsed value up to the needed 5 characters.", "So I would guess that you have textNumberPadKind=\"padChar\", textNumberPadCharacter=\"f\", and textNumberJustification=\"left\".", "Those three properties will cause Daffodil to add extra \"f\" characters as padding to the right of the string.", "I don't think I would say to never use the '#' character.", "There are certainly going to be times where you don't want extra padding characters, like in some delimited formats where numbers do not have explicit lengths."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So I would guess that you have textNumberPadKind=\"padChar\", textNumberPadCharacter=\"f\", and textNumberJustification=\"left\".", "Those three properties will cause Daffodil to add extra \"f\" characters as padding to the right of the string.", "I don't think I would say to never use the '#' character.", "There are certainly going to be times where you don't want extra padding characters, like in some delimited formats where numbers do not have explicit lengths.", "- Steve    On 6/25/19 1:32 PM, Costello, Roger L. wrote:  Hello DFDL community,   My input file has this:   0,100   0,100   My DFDL schema is this:   <xs:elementname=\"input\"  <xs:complexType  <xs:sequencedfdl:separator=\"%NL;\"dfdl:separatorPosition=\"infix\"  <xs:elementname=\"test1\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"#,###\"/   <xs:elementname=\"test2\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"0,000\"/ </xs:sequence   </xs:complexType </xs:element   The output of parsing is this:   <input  <test1100</test1  <test2100</test2  </input   The output of unparsing is this:   100ff  0,100   Huh?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Those three properties will cause Daffodil to add extra \"f\" characters as padding to the right of the string.", "I don't think I would say to never use the '#' character.", "There are certainly going to be times where you don't want extra padding characters, like in some delimited formats where numbers do not have explicit lengths.", "- Steve    On 6/25/19 1:32 PM, Costello, Roger L. wrote:  Hello DFDL community,   My input file has this:   0,100   0,100   My DFDL schema is this:   <xs:elementname=\"input\"  <xs:complexType  <xs:sequencedfdl:separator=\"%NL;\"dfdl:separatorPosition=\"infix\"  <xs:elementname=\"test1\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"#,###\"/   <xs:elementname=\"test2\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"0,000\"/ </xs:sequence   </xs:complexType </xs:element   The output of parsing is this:   <input  <test1100</test1  <test2100</test2  </input   The output of unparsing is this:   100ff  0,100   Huh?", "Why am I getting 100ff?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There are certainly going to be times where you don't want extra padding characters, like in some delimited formats where numbers do not have explicit lengths.", "- Steve    On 6/25/19 1:32 PM, Costello, Roger L. wrote:  Hello DFDL community,   My input file has this:   0,100   0,100   My DFDL schema is this:   <xs:elementname=\"input\"  <xs:complexType  <xs:sequencedfdl:separator=\"%NL;\"dfdl:separatorPosition=\"infix\"  <xs:elementname=\"test1\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"#,###\"/   <xs:elementname=\"test2\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"0,000\"/ </xs:sequence   </xs:complexType </xs:element   The output of parsing is this:   <input  <test1100</test1  <test2100</test2  </input   The output of unparsing is this:   100ff  0,100   Huh?", "Why am I getting 100ff?", "I think the lesson learned is never use the pound (#) symbol in   dfdl:textNumberPattern.", "Do you agree?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Steve    On 6/25/19 1:32 PM, Costello, Roger L. wrote:  Hello DFDL community,   My input file has this:   0,100   0,100   My DFDL schema is this:   <xs:elementname=\"input\"  <xs:complexType  <xs:sequencedfdl:separator=\"%NL;\"dfdl:separatorPosition=\"infix\"  <xs:elementname=\"test1\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"#,###\"/   <xs:elementname=\"test2\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"0,000\"/ </xs:sequence   </xs:complexType </xs:element   The output of parsing is this:   <input  <test1100</test1  <test2100</test2  </input   The output of unparsing is this:   100ff  0,100   Huh?", "Why am I getting 100ff?", "I think the lesson learned is never use the pound (#) symbol in   dfdl:textNumberPattern.", "Do you agree?", "/Roger      "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That worked.", "-Harold  --- On Thu, 10/22/09, Kim LiChong <Kim.Lichong@Sun.COM wrote:   From: Kim LiChong <Kim.Lichong@Sun.COM  Subject: Re: cannot Find Symbol: com.sun.faban.driver.transport.hc3.ApacheHC3Transport  To: olio-user@incubator.apache.org  Date: Thursday, October 22, 2009, 1:10 AM  Hi Harold,    Recent checkins for the workload driver for Java now depend  on a newer   version of Faban which is now available.", "Can you please get Faban version 1.0RC2 from the faban  website   <http://faban.sunsource.net/index.html#Downloads and  use this as your   FABAN_HOME when compiling?", "thanks,    Kim   Hi All,     When I try to compile the workload driver of Olio, I'm  getting the following errors.", "/mount/olio/workload/java/trunk# ant deploy.jar     Buildfile: build.xml     init:     compile:  \u00a0\u00a0\u00a0[javac] Compiling 25 source files to  /mount/olio/workload/java/trunk/build/classes  \u00a0\u00a0\u00a0[javac]  /mount/olio/workload/java/trunk/src/org/apache/olio/workload/driver/UIDriver.java:541:  cannot find symbol  \u00a0\u00a0\u00a0[javac] symbol\u00a0 : method  readURL(java.lang.String,java.util.ArrayList<org.apache.commons.httpclient.methods.multipart.Part)    \u00a0\u00a0\u00a0[javac] location: class  com.sun.faban.driver.transport.hc3.ApacheHC3Transport  \u00a0\u00a0\u00a0[javac]\u00a0 \u00a0 \u00a0 \u00a0  \u00a0 \u00a0\u00a0\u00a0((ApacheHC3Transport)  http).readURL(fileUploadEventURL, params);     Am I missing a library/JAR file?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-Harold  --- On Thu, 10/22/09, Kim LiChong <Kim.Lichong@Sun.COM wrote:   From: Kim LiChong <Kim.Lichong@Sun.COM  Subject: Re: cannot Find Symbol: com.sun.faban.driver.transport.hc3.ApacheHC3Transport  To: olio-user@incubator.apache.org  Date: Thursday, October 22, 2009, 1:10 AM  Hi Harold,    Recent checkins for the workload driver for Java now depend  on a newer   version of Faban which is now available.", "Can you please get Faban version 1.0RC2 from the faban  website   <http://faban.sunsource.net/index.html#Downloads and  use this as your   FABAN_HOME when compiling?", "thanks,    Kim   Hi All,     When I try to compile the workload driver of Olio, I'm  getting the following errors.", "/mount/olio/workload/java/trunk# ant deploy.jar     Buildfile: build.xml     init:     compile:  \u00a0\u00a0\u00a0[javac] Compiling 25 source files to  /mount/olio/workload/java/trunk/build/classes  \u00a0\u00a0\u00a0[javac]  /mount/olio/workload/java/trunk/src/org/apache/olio/workload/driver/UIDriver.java:541:  cannot find symbol  \u00a0\u00a0\u00a0[javac] symbol\u00a0 : method  readURL(java.lang.String,java.util.ArrayList<org.apache.commons.httpclient.methods.multipart.Part)    \u00a0\u00a0\u00a0[javac] location: class  com.sun.faban.driver.transport.hc3.ApacheHC3Transport  \u00a0\u00a0\u00a0[javac]\u00a0 \u00a0 \u00a0 \u00a0  \u00a0 \u00a0\u00a0\u00a0((ApacheHC3Transport)  http).readURL(fileUploadEventURL, params);     Am I missing a library/JAR file?", "-Harold      \u00a0 \u00a0 \u00a0\u00a0\u00a0  \u00a0\u00a0\u00a0               "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Qi,  I don't see anything suspicious in the log.", "Could you try out the https://github.com/gearpump/gearpump-java-example/tree/master/src/main/java/kafka2kafka example to see whether it's a framework bug ?", "\"group-id\" is set to \"gearpump\" if not configured by user.", "If you want to configure \"group-id\", you may create KafkaSource like  *Properties properties = new Properties();* *properties.put(\"group-id\", \"my-group\");* *properties.put(\"zookeeper.servers\", \"localhost:2181\");* *KafkaSource source = new KafkaSource(\"topic\", properties, storageFactory);*   On Tue, May 3, 2016 at 3:13 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Could you also help me to check the log in the attachment.", "How can I specify a group id when using Kafka Source, now I just set \u201c  group.id=XXX\u201d in UserConfig."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could you try out the https://github.com/gearpump/gearpump-java-example/tree/master/src/main/java/kafka2kafka example to see whether it's a framework bug ?", "\"group-id\" is set to \"gearpump\" if not configured by user.", "If you want to configure \"group-id\", you may create KafkaSource like  *Properties properties = new Properties();* *properties.put(\"group-id\", \"my-group\");* *properties.put(\"zookeeper.servers\", \"localhost:2181\");* *KafkaSource source = new KafkaSource(\"topic\", properties, storageFactory);*   On Tue, May 3, 2016 at 3:13 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Could you also help me to check the log in the attachment.", "How can I specify a group id when using Kafka Source, now I just set \u201c  group.id=XXX\u201d in UserConfig.", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\"group-id\" is set to \"gearpump\" if not configured by user.", "If you want to configure \"group-id\", you may create KafkaSource like  *Properties properties = new Properties();* *properties.put(\"group-id\", \"my-group\");* *properties.put(\"zookeeper.servers\", \"localhost:2181\");* *KafkaSource source = new KafkaSource(\"topic\", properties, storageFactory);*   On Tue, May 3, 2016 at 3:13 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Could you also help me to check the log in the attachment.", "How can I specify a group id when using Kafka Source, now I just set \u201c  group.id=XXX\u201d in UserConfig.", "Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* \u8212\u7426<shuqi@eefung.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:19  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Manu,    Gearpump: 0.7.6_2.11   Kafka: 0.8.2.1_2.10."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you want to configure \"group-id\", you may create KafkaSource like  *Properties properties = new Properties();* *properties.put(\"group-id\", \"my-group\");* *properties.put(\"zookeeper.servers\", \"localhost:2181\");* *KafkaSource source = new KafkaSource(\"topic\", properties, storageFactory);*   On Tue, May 3, 2016 at 3:13 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Could you also help me to check the log in the attachment.", "How can I specify a group id when using Kafka Source, now I just set \u201c  group.id=XXX\u201d in UserConfig.", "Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* \u8212\u7426<shuqi@eefung.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:19  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Manu,    Gearpump: 0.7.6_2.11   Kafka: 0.8.2.1_2.10.", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* \u8212\u7426<shuqi@eefung.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:19  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Manu,    Gearpump: 0.7.6_2.11   Kafka: 0.8.2.1_2.10.", "Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:13  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Your code looks right.", "Which gearpump version and kafka version have you  used ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* \u8212\u7426<shuqi@eefung.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:19  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Manu,    Gearpump: 0.7.6_2.11   Kafka: 0.8.2.1_2.10.", "Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:13  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Your code looks right.", "Which gearpump version and kafka version have you  used ?", "On Tue, May 3, 2016 at 1:37 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Thanks for your help."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:13  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Your code looks right.", "Which gearpump version and kafka version have you  used ?", "On Tue, May 3, 2016 at 1:37 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Thanks for your help.", "I used the kafka-console-consumer with the same zks and topic, and it can  consume messages."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:13  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Your code looks right.", "Which gearpump version and kafka version have you  used ?", "On Tue, May 3, 2016 at 1:37 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Thanks for your help.", "I used the kafka-console-consumer with the same zks and topic, and it can  consume messages.", "There is still lots of messages in that topic."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, May 3, 2016 at 1:37 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Thanks for your help.", "I used the kafka-console-consumer with the same zks and topic, and it can  consume messages.", "There is still lots of messages in that topic.", "Belowing is the function I used to get Kafka Soruce, could you please  help to check if it is ok, thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200712:40  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Neither the red ballon nor the message receive message throughput means  any message has been consumed by KafkaSource."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I used the kafka-console-consumer with the same zks and topic, and it can  consume messages.", "There is still lots of messages in that topic.", "Belowing is the function I used to get Kafka Soruce, could you please  help to check if it is ok, thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200712:40  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Neither the red ballon nor the message receive message throughput means  any message has been consumed by KafkaSource.", "Those are messages source  send to itself to trigger next Task execution."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is still lots of messages in that topic.", "Belowing is the function I used to get Kafka Soruce, could you please  help to check if it is ok, thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200712:40  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Neither the red ballon nor the message receive message throughput means  any message has been consumed by KafkaSource.", "Those are messages source  send to itself to trigger next Task execution.", "The metrics is a bit  confusing and I think we need to fix this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Belowing is the function I used to get Kafka Soruce, could you please  help to check if it is ok, thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200712:40  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Neither the red ballon nor the message receive message throughput means  any message has been consumed by KafkaSource.", "Those are messages source  send to itself to trigger next Task execution.", "The metrics is a bit  confusing and I think we need to fix this.", "Yes, both zookeeper servers and kafka brokers configs are comma-separated  list strings."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200712:40  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Neither the red ballon nor the message receive message throughput means  any message has been consumed by KafkaSource.", "Those are messages source  send to itself to trigger next Task execution.", "The metrics is a bit  confusing and I think we need to fix this.", "Yes, both zookeeper servers and kafka brokers configs are comma-separated  list strings.", "One way to check whether your configurations is correct it to  consume from the topic using kafka-console-consumer."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Those are messages source  send to itself to trigger next Task execution.", "The metrics is a bit  confusing and I think we need to fix this.", "Yes, both zookeeper servers and kafka brokers configs are comma-separated  list strings.", "One way to check whether your configurations is correct it to  consume from the topic using kafka-console-consumer.", "This also makes sure  the topic has data to consume."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The metrics is a bit  confusing and I think we need to fix this.", "Yes, both zookeeper servers and kafka brokers configs are comma-separated  list strings.", "One way to check whether your configurations is correct it to  consume from the topic using kafka-console-consumer.", "This also makes sure  the topic has data to consume.", "Hope this helps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One way to check whether your configurations is correct it to  consume from the topic using kafka-console-consumer.", "This also makes sure  the topic has data to consume.", "Hope this helps.", "Thanks,  Manu   On Tue, May 3, 2016 at 12:07 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi,    I constructed a DAG as show blowing, \u201ckafka source\u201dconsumes messages  from kafka topic \u201cwebs\u201d, its metrics shows  that it consumes lots of  messages, but actually there is no messages handled and I also can\u2019t find  active group under topic \u201cwebs\u201d, the log is ok too.", "I just wonder the properties of kafka for zks and brokers, if there is a  list of zookeeper servers, should I use comma to separate?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 12/15/11 11:09 AM, raghavendhra rahul wrote:  How to add those classes.", "Also how to run other examples described with s4.", "I am really interested in knowing about s4.", "I would really suggest that you follow the manual.", "You probably forgot  to checkout tag 0.3."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["I guess instance-wise l2 normalization is mandatory for FFM.", "https://github.com/guestwalk/libffm/blob/master/ffm.cpp#L688 https://github.com/CNevd/libffm-ftrl/blob/4247440cc190346daa0b675135e0542e4933cb0f/ffm.cpp#L310  Makoto  2017-10-18 21:27 GMT+09:00 Makoto Yui <myui@apache.org:  At the first update, loss is large but average loss for each update is  very small using your test.", "https://github.com/apache/incubator-hivemall/blob/master/core/src/test/java/hivemall/fm/FieldAwareFactorizationMachineUDTFTest.java#L85   It might better to implement instance-wise l2 normalization to reduce  initial losses.", "Further investigation is required but I need to focus on the first  Apache release for this month.", "GA of FFM will be v0.5.1 release scheduled on Dec.   Makoto   2017-10-18 1:36 GMT+09:00 Makoto Yui <myui@apache.org:  Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://github.com/guestwalk/libffm/blob/master/ffm.cpp#L688 https://github.com/CNevd/libffm-ftrl/blob/4247440cc190346daa0b675135e0542e4933cb0f/ffm.cpp#L310  Makoto  2017-10-18 21:27 GMT+09:00 Makoto Yui <myui@apache.org:  At the first update, loss is large but average loss for each update is  very small using your test.", "https://github.com/apache/incubator-hivemall/blob/master/core/src/test/java/hivemall/fm/FieldAwareFactorizationMachineUDTFTest.java#L85   It might better to implement instance-wise l2 normalization to reduce  initial losses.", "Further investigation is required but I need to focus on the first  Apache release for this month.", "GA of FFM will be v0.5.1 release scheduled on Dec.   Makoto   2017-10-18 1:36 GMT+09:00 Makoto Yui <myui@apache.org:  Thanks.", "I'll test FFM with it tomorrow."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://github.com/apache/incubator-hivemall/blob/master/core/src/test/java/hivemall/fm/FieldAwareFactorizationMachineUDTFTest.java#L85   It might better to implement instance-wise l2 normalization to reduce  initial losses.", "Further investigation is required but I need to focus on the first  Apache release for this month.", "GA of FFM will be v0.5.1 release scheduled on Dec.   Makoto   2017-10-18 1:36 GMT+09:00 Makoto Yui <myui@apache.org:  Thanks.", "I'll test FFM with it tomorrow.", "Makoto   2017-10-18 1:19 GMT+09:00 Shadi Mari <shadimari@gmail.com:  Attached us a sample of 500 examples from my training set represented as  vector of features."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["GA of FFM will be v0.5.1 release scheduled on Dec.   Makoto   2017-10-18 1:36 GMT+09:00 Makoto Yui <myui@apache.org:  Thanks.", "I'll test FFM with it tomorrow.", "Makoto   2017-10-18 1:19 GMT+09:00 Shadi Mari <shadimari@gmail.com:  Attached us a sample of 500 examples from my training set represented as  vector of features.", "Regards,    On Tue, Oct 17, 2017 at 7:08 PM, Makoto Yui <myui@apache.org wrote:   I need to reproduce your test.", "Could you give me the sample (100~500 examples are enough) of your  training input in gzipped tsv/csv?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'll test FFM with it tomorrow.", "Makoto   2017-10-18 1:19 GMT+09:00 Shadi Mari <shadimari@gmail.com:  Attached us a sample of 500 examples from my training set represented as  vector of features.", "Regards,    On Tue, Oct 17, 2017 at 7:08 PM, Makoto Yui <myui@apache.org wrote:   I need to reproduce your test.", "Could you give me the sample (100~500 examples are enough) of your  training input in gzipped tsv/csv?", "FFM input format is <field:<index:<value."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Makoto   2017-10-18 1:19 GMT+09:00 Shadi Mari <shadimari@gmail.com:  Attached us a sample of 500 examples from my training set represented as  vector of features.", "Regards,    On Tue, Oct 17, 2017 at 7:08 PM, Makoto Yui <myui@apache.org wrote:   I need to reproduce your test.", "Could you give me the sample (100~500 examples are enough) of your  training input in gzipped tsv/csv?", "FFM input format is <field:<index:<value.", "Thanks,  Makoto   2017-10-18 0:59 GMT+09:00 Shadi Mari <shadimari@gmail.com:   Makoto,     I am using the default hyper-parameters in addition to the following   settings:     feature_hashing: 20   classification is enabled   Iterations = 10   K = 2, another test using K = 4   Opt: FTRL (default)     I tried setting the initial learning to 0.2 and optimizer to AdaGrad   with no   significant changes on the empirical loss."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards,    On Tue, Oct 17, 2017 at 7:08 PM, Makoto Yui <myui@apache.org wrote:   I need to reproduce your test.", "Could you give me the sample (100~500 examples are enough) of your  training input in gzipped tsv/csv?", "FFM input format is <field:<index:<value.", "Thanks,  Makoto   2017-10-18 0:59 GMT+09:00 Shadi Mari <shadimari@gmail.com:   Makoto,     I am using the default hyper-parameters in addition to the following   settings:     feature_hashing: 20   classification is enabled   Iterations = 10   K = 2, another test using K = 4   Opt: FTRL (default)     I tried setting the initial learning to 0.2 and optimizer to AdaGrad   with no   significant changes on the empirical loss.", "Thanks               On Tue, Oct 17, 2017 at 6:51 PM, Makoto Yui <myui@apache.org wrote:     The empirical loss (cumulative logloss) is too large."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could you give me the sample (100~500 examples are enough) of your  training input in gzipped tsv/csv?", "FFM input format is <field:<index:<value.", "Thanks,  Makoto   2017-10-18 0:59 GMT+09:00 Shadi Mari <shadimari@gmail.com:   Makoto,     I am using the default hyper-parameters in addition to the following   settings:     feature_hashing: 20   classification is enabled   Iterations = 10   K = 2, another test using K = 4   Opt: FTRL (default)     I tried setting the initial learning to 0.2 and optimizer to AdaGrad   with no   significant changes on the empirical loss.", "Thanks               On Tue, Oct 17, 2017 at 6:51 PM, Makoto Yui <myui@apache.org wrote:     The empirical loss (cumulative logloss) is too large.", "The simple test in FieldAwareFactorizationMachineUDTFTest shows that   empirical loss is decreasing properly but it seems optimization is not   working correctly in your case."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["FFM input format is <field:<index:<value.", "Thanks,  Makoto   2017-10-18 0:59 GMT+09:00 Shadi Mari <shadimari@gmail.com:   Makoto,     I am using the default hyper-parameters in addition to the following   settings:     feature_hashing: 20   classification is enabled   Iterations = 10   K = 2, another test using K = 4   Opt: FTRL (default)     I tried setting the initial learning to 0.2 and optimizer to AdaGrad   with no   significant changes on the empirical loss.", "Thanks               On Tue, Oct 17, 2017 at 6:51 PM, Makoto Yui <myui@apache.org wrote:     The empirical loss (cumulative logloss) is too large.", "The simple test in FieldAwareFactorizationMachineUDTFTest shows that   empirical loss is decreasing properly but it seems optimization is not   working correctly in your case.", "Could you show me the training hyperparameters?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Makoto   2017-10-18 0:59 GMT+09:00 Shadi Mari <shadimari@gmail.com:   Makoto,     I am using the default hyper-parameters in addition to the following   settings:     feature_hashing: 20   classification is enabled   Iterations = 10   K = 2, another test using K = 4   Opt: FTRL (default)     I tried setting the initial learning to 0.2 and optimizer to AdaGrad   with no   significant changes on the empirical loss.", "Thanks               On Tue, Oct 17, 2017 at 6:51 PM, Makoto Yui <myui@apache.org wrote:     The empirical loss (cumulative logloss) is too large.", "The simple test in FieldAwareFactorizationMachineUDTFTest shows that   empirical loss is decreasing properly but it seems optimization is not   working correctly in your case.", "Could you show me the training hyperparameters?", "Makoto     2017-10-17 19:01 GMT+09:00 Shadi Mari <shadimari@gmail.com:    Hello,       I am trying to understand the results produced by FFM on each    iteration    during the training of Criteo 2014 dataset."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The simple test in FieldAwareFactorizationMachineUDTFTest shows that   empirical loss is decreasing properly but it seems optimization is not   working correctly in your case.", "Could you show me the training hyperparameters?", "Makoto     2017-10-17 19:01 GMT+09:00 Shadi Mari <shadimari@gmail.com:    Hello,       I am trying to understand the results produced by FFM on each    iteration    during the training of Criteo 2014 dataset.", "Basically, I have 10 mappers running concurrently (each has ~4.5M    records),    and follows is an output by one of the mappers:       -----------------------------       fm.FactorizationMachineUDTF|: Wrote 4479491 records to a temporary    file    for    iterative training: hivemall_fm392724107368114556.sgmt (2.02 GiB)    Iteration #2 [curLosses=1.5967339372694769E10,    prevLosses=4.182558816480771E10, changeRate=0.6182399322209704,    #trainingExamples=4479491]       -----------------------------       Looking at the source code, FFM implementation uses LogLess    performance    metric when classification is specified, however the curLossess    counter    is    very high 1.5967339372694769E10          What does this mean?", "Regards               --   Makoto YUI <myui AT apache.org   Research Engineer, Treasure Data, Inc.   http://myui.github.io/         --  Makoto YUI <myui AT apache.org  Research Engineer, Treasure Data, Inc.  http://myui.github.io/       --  Makoto YUI <myui AT apache.org  Research Engineer, Treasure Data, Inc.  http://myui.github.io/     --  Makoto YUI <myui AT apache.org  Research Engineer, Treasure Data, Inc.  http://myui.github.io/    --  Makoto YUI <myui AT apache.org Research Engineer, Treasure Data, Inc. http://myui.github.io/  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, I set the 2 PEs(logic is very simple, just compare the values) threads is 1,200,300,400, the execution time is 400s,161s,163s,171s, why I increase threads but the time get slower?", "How I can improve the speed?", "/Sky  "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am trying to get started with tashi following the \"setting tashi up on  a single test machine\" instructions, but the nodemanager is failing to  start complaining that it is unable to load VM info from  /var/tmp/nm.dat.", "How does that file get created and initialized?   "], "labels": ["0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  I had a look and indeed could reproduce that issue.", "In order to customize the log config, we need to include an arbitrary directory in the classpath of s4 nodes, so we can place logback.xml there.", "The classpath for s4 nodes is given by a generated script (check s4 file and linked s4-tools file), and apparently (maybe due to some gradle update) the generated classpath does not include the lib directory anymore, only the jars it contains.", "So that's a bug.", "Workaround would be to manually modify the s4-tools script and include an arbitrary directory in the CLASSPATH variable."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In order to customize the log config, we need to include an arbitrary directory in the classpath of s4 nodes, so we can place logback.xml there.", "The classpath for s4 nodes is given by a generated script (check s4 file and linked s4-tools file), and apparently (maybe due to some gradle update) the generated classpath does not include the lib directory anymore, only the jars it contains.", "So that's a bug.", "Workaround would be to manually modify the s4-tools script and include an arbitrary directory in the CLASSPATH variable.", "You could do that from a generated binary distribution so you don't have to worry about getting the s4-tools file rewritten."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The classpath for s4 nodes is given by a generated script (check s4 file and linked s4-tools file), and apparently (maybe due to some gradle update) the generated classpath does not include the lib directory anymore, only the jars it contains.", "So that's a bug.", "Workaround would be to manually modify the s4-tools script and include an arbitrary directory in the CLASSPATH variable.", "You could do that from a generated binary distribution so you don't have to worry about getting the s4-tools file rewritten.", "Hope this helps,  Matthieu  On Aug 30, 2013, at 17:19 , Yago <iamxami@gmail.com wrote:   Hello all    I'm having problems with log configuration in my s4 apps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So that's a bug.", "Workaround would be to manually modify the s4-tools script and include an arbitrary directory in the CLASSPATH variable.", "You could do that from a generated binary distribution so you don't have to worry about getting the s4-tools file rewritten.", "Hope this helps,  Matthieu  On Aug 30, 2013, at 17:19 , Yago <iamxami@gmail.com wrote:   Hello all    I'm having problems with log configuration in my s4 apps.", "Have tried a lot of things in order to change the default behaviour of logback, but none of them worked."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Workaround would be to manually modify the s4-tools script and include an arbitrary directory in the CLASSPATH variable.", "You could do that from a generated binary distribution so you don't have to worry about getting the s4-tools file rewritten.", "Hope this helps,  Matthieu  On Aug 30, 2013, at 17:19 , Yago <iamxami@gmail.com wrote:   Hello all    I'm having problems with log configuration in my s4 apps.", "Have tried a lot of things in order to change the default behaviour of logback, but none of them worked.", "These were my attempts (some of them might look a bit strange or desperate):    - Put logback.xml in subprojects/s4-tools/build/install/s4-tools/lib/ directory as suggested in doc, and even in lib/ directory."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You could do that from a generated binary distribution so you don't have to worry about getting the s4-tools file rewritten.", "Hope this helps,  Matthieu  On Aug 30, 2013, at 17:19 , Yago <iamxami@gmail.com wrote:   Hello all    I'm having problems with log configuration in my s4 apps.", "Have tried a lot of things in order to change the default behaviour of logback, but none of them worked.", "These were my attempts (some of them might look a bit strange or desperate):    - Put logback.xml in subprojects/s4-tools/build/install/s4-tools/lib/ directory as suggested in doc, and even in lib/ directory.", "- Edited all the logback.xml files in my s4 directory, changing level and/or pattern  - Edited all the default.s4.core.properties files, changing level  - Added both files to my application in /src/main/resources/ directory, so they are included in the built jar (checked)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hope this helps,  Matthieu  On Aug 30, 2013, at 17:19 , Yago <iamxami@gmail.com wrote:   Hello all    I'm having problems with log configuration in my s4 apps.", "Have tried a lot of things in order to change the default behaviour of logback, but none of them worked.", "These were my attempts (some of them might look a bit strange or desperate):    - Put logback.xml in subprojects/s4-tools/build/install/s4-tools/lib/ directory as suggested in doc, and even in lib/ directory.", "- Edited all the logback.xml files in my s4 directory, changing level and/or pattern  - Edited all the default.s4.core.properties files, changing level  - Added both files to my application in /src/main/resources/ directory, so they are included in the built jar (checked).", "No result."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Have tried a lot of things in order to change the default behaviour of logback, but none of them worked.", "These were my attempts (some of them might look a bit strange or desperate):    - Put logback.xml in subprojects/s4-tools/build/install/s4-tools/lib/ directory as suggested in doc, and even in lib/ directory.", "- Edited all the logback.xml files in my s4 directory, changing level and/or pattern  - Edited all the default.s4.core.properties files, changing level  - Added both files to my application in /src/main/resources/ directory, so they are included in the built jar (checked).", "No result.", "- Tried to pass s4.logger_level=info as a parameter in node start."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These were my attempts (some of them might look a bit strange or desperate):    - Put logback.xml in subprojects/s4-tools/build/install/s4-tools/lib/ directory as suggested in doc, and even in lib/ directory.", "- Edited all the logback.xml files in my s4 directory, changing level and/or pattern  - Edited all the default.s4.core.properties files, changing level  - Added both files to my application in /src/main/resources/ directory, so they are included in the built jar (checked).", "No result.", "- Tried to pass s4.logger_level=info as a parameter in node start.", "A received an exception from ParametersInjectionModule: property already configured."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Edited all the logback.xml files in my s4 directory, changing level and/or pattern  - Edited all the default.s4.core.properties files, changing level  - Added both files to my application in /src/main/resources/ directory, so they are included in the built jar (checked).", "No result.", "- Tried to pass s4.logger_level=info as a parameter in node start.", "A received an exception from ParametersInjectionModule: property already configured.", "- Tried to pass that parameter in s4r deploy."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["No result.", "- Tried to pass s4.logger_level=info as a parameter in node start.", "A received an exception from ParametersInjectionModule: property already configured.", "- Tried to pass that parameter in s4r deploy.", "Nothing changed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Tried to pass s4.logger_level=info as a parameter in node start.", "A received an exception from ParametersInjectionModule: property already configured.", "- Tried to pass that parameter in s4r deploy.", "Nothing changed.", "My actual s4 version is 0.6.0 final, built from src (was 0.5.0 but updated myself because of this!)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Nothing changed.", "My actual s4 version is 0.6.0 final, built from src (was 0.5.0 but updated myself because of this!)", "Any ideas?", "maybe I'm going with log4j...", "Thanks + Regards     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Anyway, it requires a couple classes that handle all of the special method  binding events.", "http://svn.apache.org/viewvc/shale/sandbox/shale-clay-trinidad/src/main/java/org/apache/shale/clay/component/chain/trinidad/PropertyListenerCommand.java?view=markup   This class has a dependency with commons chains.", "It also requires some xml  configs.", "The clay configs are generated form the TLD's using a shale maven  plugin and we also have a commons chains config.", "I was thinking that instead of adding yet another dependencies in Trinidad  to provide native support like you have for Facelets, what would you think  about a maven architecture type that would generate a base project with the  exta bits?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["http://svn.apache.org/viewvc/shale/sandbox/shale-clay-trinidad/src/main/java/org/apache/shale/clay/component/chain/trinidad/PropertyListenerCommand.java?view=markup   This class has a dependency with commons chains.", "It also requires some xml  configs.", "The clay configs are generated form the TLD's using a shale maven  plugin and we also have a commons chains config.", "I was thinking that instead of adding yet another dependencies in Trinidad  to provide native support like you have for Facelets, what would you think  about a maven architecture type that would generate a base project with the  exta bits?", "So, you mean a new archetype?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It also requires some xml  configs.", "The clay configs are generated form the TLD's using a shale maven  plugin and we also have a commons chains config.", "I was thinking that instead of adding yet another dependencies in Trinidad  to provide native support like you have for Facelets, what would you think  about a maven architecture type that would generate a base project with the  exta bits?", "So, you mean a new archetype?", "Sounds excellent."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The clay configs are generated form the TLD's using a shale maven  plugin and we also have a commons chains config.", "I was thinking that instead of adding yet another dependencies in Trinidad  to provide native support like you have for Facelets, what would you think  about a maven architecture type that would generate a base project with the  exta bits?", "So, you mean a new archetype?", "Sounds excellent.", "-- Adam  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["To elaborate on my last message, the EXE specification says this:  All image files that import symbols, including virtually all executable (EXE) files, have an .idata section.", "A typical file layout for the import information follows:  Directory Table  Null Directory Entry  DLL1 Import Lookup Table  Null  DLL2 Import Lookup Table  Null  DLL3 Import Lookup Table  Null  Hint-Name Table  Note the use of the word \"typical.\"", "Apparently my EXE is not typical as it has 6 Import Lookup Tables.", "/Roger  -----Original Message----- From: Costello, Roger L. <costello@mitre.org  Sent: Sunday, March 3, 2019 3:17 PM To: users@daffodil.apache.org Subject: Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  Hi Steve,  The EXE specification is silent on the number of occurrences of the Import_Lookup_Table.", "All I know is that once we see the first Hint_Name_Table entry (a 2-byte address, followed by a null-terminated name, followed by an optional null) then we know that there are no more Import_Lookup_Tables."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A typical file layout for the import information follows:  Directory Table  Null Directory Entry  DLL1 Import Lookup Table  Null  DLL2 Import Lookup Table  Null  DLL3 Import Lookup Table  Null  Hint-Name Table  Note the use of the word \"typical.\"", "Apparently my EXE is not typical as it has 6 Import Lookup Tables.", "/Roger  -----Original Message----- From: Costello, Roger L. <costello@mitre.org  Sent: Sunday, March 3, 2019 3:17 PM To: users@daffodil.apache.org Subject: Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  Hi Steve,  The EXE specification is silent on the number of occurrences of the Import_Lookup_Table.", "All I know is that once we see the first Hint_Name_Table entry (a 2-byte address, followed by a null-terminated name, followed by an optional null) then we know that there are no more Import_Lookup_Tables.", "Is there a way to express (for Import_Lookup_Table):  \tThere are no more occurrences \tonce we encounter a 2-byte \taddress, followed by a null-terminated \tstring, followed by an optional \tnull."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger  -----Original Message----- From: Costello, Roger L. <costello@mitre.org  Sent: Sunday, March 3, 2019 3:17 PM To: users@daffodil.apache.org Subject: Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  Hi Steve,  The EXE specification is silent on the number of occurrences of the Import_Lookup_Table.", "All I know is that once we see the first Hint_Name_Table entry (a 2-byte address, followed by a null-terminated name, followed by an optional null) then we know that there are no more Import_Lookup_Tables.", "Is there a way to express (for Import_Lookup_Table):  \tThere are no more occurrences \tonce we encounter a 2-byte \taddress, followed by a null-terminated \tstring, followed by an optional \tnull.", "Is it possible to express that?", "/Roger  -----Original Message----- From: Steve Lawrence <slawrence@apache.org Sent: Sunday, March 3, 2019 11:25 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: [EXT] Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  I think we need more information, specifically how do you know what is the last Import_Lookup_Table?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["All I know is that once we see the first Hint_Name_Table entry (a 2-byte address, followed by a null-terminated name, followed by an optional null) then we know that there are no more Import_Lookup_Tables.", "Is there a way to express (for Import_Lookup_Table):  \tThere are no more occurrences \tonce we encounter a 2-byte \taddress, followed by a null-terminated \tstring, followed by an optional \tnull.", "Is it possible to express that?", "/Roger  -----Original Message----- From: Steve Lawrence <slawrence@apache.org Sent: Sunday, March 3, 2019 11:25 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: [EXT] Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  I think we need more information, specifically how do you know what is the last Import_Lookup_Table?", "Within an Import_Lookup_Table you say that the Lookup_Table_Entry ends with the last entry is all nulls, but that doesn't tell us where the last Import_Lookup_Table is."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there a way to express (for Import_Lookup_Table):  \tThere are no more occurrences \tonce we encounter a 2-byte \taddress, followed by a null-terminated \tstring, followed by an optional \tnull.", "Is it possible to express that?", "/Roger  -----Original Message----- From: Steve Lawrence <slawrence@apache.org Sent: Sunday, March 3, 2019 11:25 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: [EXT] Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  I think we need more information, specifically how do you know what is the last Import_Lookup_Table?", "Within an Import_Lookup_Table you say that the Lookup_Table_Entry ends with the last entry is all nulls, but that doesn't tell us where the last Import_Lookup_Table is.", "Is it when there's a special value of a Lookup_Table_Entry?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is it possible to express that?", "/Roger  -----Original Message----- From: Steve Lawrence <slawrence@apache.org Sent: Sunday, March 3, 2019 11:25 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: [EXT] Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  I think we need more information, specifically how do you know what is the last Import_Lookup_Table?", "Within an Import_Lookup_Table you say that the Lookup_Table_Entry ends with the last entry is all nulls, but that doesn't tell us where the last Import_Lookup_Table is.", "Is it when there's a special value of a Lookup_Table_Entry?", "Or maybe when the Import_Lookup_table has a no Lookup_Table_Entires and just has the terminating Null?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["/Roger  -----Original Message----- From: Steve Lawrence <slawrence@apache.org Sent: Sunday, March 3, 2019 11:25 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: [EXT] Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  I think we need more information, specifically how do you know what is the last Import_Lookup_Table?", "Within an Import_Lookup_Table you say that the Lookup_Table_Entry ends with the last entry is all nulls, but that doesn't tell us where the last Import_Lookup_Table is.", "Is it when there's a special value of a Lookup_Table_Entry?", "Or maybe when the Import_Lookup_table has a no Lookup_Table_Entires and just has the terminating Null?", "Something else?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Within an Import_Lookup_Table you say that the Lookup_Table_Entry ends with the last entry is all nulls, but that doesn't tell us where the last Import_Lookup_Table is.", "Is it when there's a special value of a Lookup_Table_Entry?", "Or maybe when the Import_Lookup_table has a no Lookup_Table_Entires and just has the terminating Null?", "Something else?", "You'll likely need to add a discriminator somewhere, but we'd need more information to know what that discriminator should be."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is it when there's a special value of a Lookup_Table_Entry?", "Or maybe when the Import_Lookup_table has a no Lookup_Table_Entires and just has the terminating Null?", "Something else?", "You'll likely need to add a discriminator somewhere, but we'd need more information to know what that discriminator should be.", "- Steve  On 3/3/19 7:55 AM, Costello, Roger L. wrote:  Hello DFDL community,    In the Windows EXE file format there is one or more   Import_Lookup_Tables followed by a Hint_Name_Table."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Or maybe when the Import_Lookup_table has a no Lookup_Table_Entires and just has the terminating Null?", "Something else?", "You'll likely need to add a discriminator somewhere, but we'd need more information to know what that discriminator should be.", "- Steve  On 3/3/19 7:55 AM, Costello, Roger L. wrote:  Hello DFDL community,    In the Windows EXE file format there is one or more   Import_Lookup_Tables followed by a Hint_Name_Table.", "I am struggling   with how to inform Daffodil, \"Hey Daffodil, the input is finished with   the Import_Lookup_Tables, now it's time to build the Hint_Name_Table.\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Something else?", "You'll likely need to add a discriminator somewhere, but we'd need more information to know what that discriminator should be.", "- Steve  On 3/3/19 7:55 AM, Costello, Roger L. wrote:  Hello DFDL community,    In the Windows EXE file format there is one or more   Import_Lookup_Tables followed by a Hint_Name_Table.", "I am struggling   with how to inform Daffodil, \"Hey Daffodil, the input is finished with   the Import_Lookup_Tables, now it's time to build the Hint_Name_Table.\"", "I am hoping you can show me how to inform Daffodil of this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You'll likely need to add a discriminator somewhere, but we'd need more information to know what that discriminator should be.", "- Steve  On 3/3/19 7:55 AM, Costello, Roger L. wrote:  Hello DFDL community,    In the Windows EXE file format there is one or more   Import_Lookup_Tables followed by a Hint_Name_Table.", "I am struggling   with how to inform Daffodil, \"Hey Daffodil, the input is finished with   the Import_Lookup_Tables, now it's time to build the Hint_Name_Table.\"", "I am hoping you can show me how to inform Daffodil of this.", "Each Import_Lookup_Table consists of one of more 32-bit fields,   terminated by a 32-bit field containing all nulls."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Steve  On 3/3/19 7:55 AM, Costello, Roger L. wrote:  Hello DFDL community,    In the Windows EXE file format there is one or more   Import_Lookup_Tables followed by a Hint_Name_Table.", "I am struggling   with how to inform Daffodil, \"Hey Daffodil, the input is finished with   the Import_Lookup_Tables, now it's time to build the Hint_Name_Table.\"", "I am hoping you can show me how to inform Daffodil of this.", "Each Import_Lookup_Table consists of one of more 32-bit fields,   terminated by a 32-bit field containing all nulls.", "The Hint_Name_Table consists of one of more entries; each entry   consists of a 2-byte address, followed by a null-terminated name,   followed by an optional null (to align to a 2-byte boundary)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am hoping you can show me how to inform Daffodil of this.", "Each Import_Lookup_Table consists of one of more 32-bit fields,   terminated by a 32-bit field containing all nulls.", "The Hint_Name_Table consists of one of more entries; each entry   consists of a 2-byte address, followed by a null-terminated name,   followed by an optional null (to align to a 2-byte boundary).", "Here is a graphic that illustrates the Import_Lookup_Tables followed   by the  Hint_Name_Table:    Here is the relevant portion of my DFDL schema:    <xs:elementname=\"idata_Section\"  <xs:complexType  <xs:sequence  <xs:elementref=\"Import_Lookup_Table\"                           maxOccurs=\"unbounded\"/   <xs:elementref=\"Hint_Name_Table\"/  </xs:sequence  </xs:complexType  </xs:element    <xs:elementname=\"Import_Lookup_Table\"  <xs:complexType  <xs:sequence  <xs:elementref=\"Lookup_Table_Entry\"                           maxOccurs=\"unbounded\"/   <xs:elementname=\"Null_Lookup_Table_Entry\"                           type=\"xs:hexBinary\"                           dfdl:lengthKind=\"explicit\"                           dfdl:length=\"4\"                           dfdl:lengthUnits=\"bytes\" <xs:annotation   <xs:appinfosource=\"http://www.ogf.org/dfdl/\"  <dfdl:assert                               { .", "eq xs:hexBinary(\"00000000\") }   </dfdl:assert </xs:appinfo </xs:annotation </xs:element   </xs:sequence </xs:complexType </xs:element    How do I inform Daffodil that the input has finished with the Import_Lookup_Tables?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Each Import_Lookup_Table consists of one of more 32-bit fields,   terminated by a 32-bit field containing all nulls.", "The Hint_Name_Table consists of one of more entries; each entry   consists of a 2-byte address, followed by a null-terminated name,   followed by an optional null (to align to a 2-byte boundary).", "Here is a graphic that illustrates the Import_Lookup_Tables followed   by the  Hint_Name_Table:    Here is the relevant portion of my DFDL schema:    <xs:elementname=\"idata_Section\"  <xs:complexType  <xs:sequence  <xs:elementref=\"Import_Lookup_Table\"                           maxOccurs=\"unbounded\"/   <xs:elementref=\"Hint_Name_Table\"/  </xs:sequence  </xs:complexType  </xs:element    <xs:elementname=\"Import_Lookup_Table\"  <xs:complexType  <xs:sequence  <xs:elementref=\"Lookup_Table_Entry\"                           maxOccurs=\"unbounded\"/   <xs:elementname=\"Null_Lookup_Table_Entry\"                           type=\"xs:hexBinary\"                           dfdl:lengthKind=\"explicit\"                           dfdl:length=\"4\"                           dfdl:lengthUnits=\"bytes\" <xs:annotation   <xs:appinfosource=\"http://www.ogf.org/dfdl/\"  <dfdl:assert                               { .", "eq xs:hexBinary(\"00000000\") }   </dfdl:assert </xs:appinfo </xs:annotation </xs:element   </xs:sequence </xs:complexType </xs:element    How do I inform Daffodil that the input has finished with the Import_Lookup_Tables?", "/Roger     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I wrote a document called 'Using XAP TreeView\" describing the TreeView component, how to use it and a bit about XAP functionaility in general.", "If there is interest I can post it as word or pdf.", "Ben     Regards        Ben Bloch  Director, Community Development  Nexaweb Technologies  1 Van de Graaff, Burlington MA 01803  o 781.345.5449, c 617.834.1769      "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["binarySeconds only works for xs:dateTime elements, which means that the resulting infoset will also have a day/month/year/hour/minute/second parts.", "There's no way around that.", "Note that the dfdl:calendarPattern does not describe the format of the infoset, but is used to describe the format of input calendar textual data.", "The infoset will always be YYYY-mm-dd hh:mm:ss.", "If you wanted only the year, you would need to parse the field as a full dateTime, and then use inputValueCalc to extract just the year."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Note that the dfdl:calendarPattern does not describe the format of the infoset, but is used to describe the format of input calendar textual data.", "The infoset will always be YYYY-mm-dd hh:mm:ss.", "If you wanted only the year, you would need to parse the field as a full dateTime, and then use inputValueCalc to extract just the year.", "For example:    <xs:element name=\"root\"     <xs:complexType       <xs:sequence         <xs:element name=\"dateTime\" type=\"xs:dateTime\"           dfdl:lengthKind=\"explicit\"           dfdl:lengthUnits=\"bytes\"           dfdl:length=\"4\"           dfdl:binaryCalendarRep=\"binarySeconds\"           dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /         <xs:element name=\"year\" type=\"xs:int\"           dfdl:inputValueCalc=\"{ fn:year-from-dateTime(../dateTime) }\" /       </xs:sequence     </xs:complexType   </xs:element    On 10/1/19 8:38 AM, Costello, Roger L. wrote:  Hello DFDL community,    My binary input file contains the number of seconds since epoch for the start of a year.", "For example, the number of seconds since epoch for the start of 1998 is: 883612800    I want the XML to just show the year (not month, day, and time)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The infoset will always be YYYY-mm-dd hh:mm:ss.", "If you wanted only the year, you would need to parse the field as a full dateTime, and then use inputValueCalc to extract just the year.", "For example:    <xs:element name=\"root\"     <xs:complexType       <xs:sequence         <xs:element name=\"dateTime\" type=\"xs:dateTime\"           dfdl:lengthKind=\"explicit\"           dfdl:lengthUnits=\"bytes\"           dfdl:length=\"4\"           dfdl:binaryCalendarRep=\"binarySeconds\"           dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /         <xs:element name=\"year\" type=\"xs:int\"           dfdl:inputValueCalc=\"{ fn:year-from-dateTime(../dateTime) }\" /       </xs:sequence     </xs:complexType   </xs:element    On 10/1/19 8:38 AM, Costello, Roger L. wrote:  Hello DFDL community,    My binary input file contains the number of seconds since epoch for the start of a year.", "For example, the number of seconds since epoch for the start of 1998 is: 883612800    I want the XML to just show the year (not month, day, and time).", "For example, I want the XML to show this:    <Date1998</Date    I thought this would do the job:    <xs:element name=\"Date\" type=\"xs:dateTime\"      dfdl:lengthKind=\"explicit\"       dfdl:length=\"4\"       dfdl:binaryCalendarRep=\"binarySeconds\"       dfdl:calendarPattern=\"yyyy\"       dfdl:calendarPatternKind=\"explicit\"      dfdl:lengthUnits=\"bytes\"       dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /    But apparently that's not correct."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you wanted only the year, you would need to parse the field as a full dateTime, and then use inputValueCalc to extract just the year.", "For example:    <xs:element name=\"root\"     <xs:complexType       <xs:sequence         <xs:element name=\"dateTime\" type=\"xs:dateTime\"           dfdl:lengthKind=\"explicit\"           dfdl:lengthUnits=\"bytes\"           dfdl:length=\"4\"           dfdl:binaryCalendarRep=\"binarySeconds\"           dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /         <xs:element name=\"year\" type=\"xs:int\"           dfdl:inputValueCalc=\"{ fn:year-from-dateTime(../dateTime) }\" /       </xs:sequence     </xs:complexType   </xs:element    On 10/1/19 8:38 AM, Costello, Roger L. wrote:  Hello DFDL community,    My binary input file contains the number of seconds since epoch for the start of a year.", "For example, the number of seconds since epoch for the start of 1998 is: 883612800    I want the XML to just show the year (not month, day, and time).", "For example, I want the XML to show this:    <Date1998</Date    I thought this would do the job:    <xs:element name=\"Date\" type=\"xs:dateTime\"      dfdl:lengthKind=\"explicit\"       dfdl:length=\"4\"       dfdl:binaryCalendarRep=\"binarySeconds\"       dfdl:calendarPattern=\"yyyy\"       dfdl:calendarPatternKind=\"explicit\"      dfdl:lengthUnits=\"bytes\"       dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /    But apparently that's not correct.", "What is the correct way to do it, please?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For example:    <xs:element name=\"root\"     <xs:complexType       <xs:sequence         <xs:element name=\"dateTime\" type=\"xs:dateTime\"           dfdl:lengthKind=\"explicit\"           dfdl:lengthUnits=\"bytes\"           dfdl:length=\"4\"           dfdl:binaryCalendarRep=\"binarySeconds\"           dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /         <xs:element name=\"year\" type=\"xs:int\"           dfdl:inputValueCalc=\"{ fn:year-from-dateTime(../dateTime) }\" /       </xs:sequence     </xs:complexType   </xs:element    On 10/1/19 8:38 AM, Costello, Roger L. wrote:  Hello DFDL community,    My binary input file contains the number of seconds since epoch for the start of a year.", "For example, the number of seconds since epoch for the start of 1998 is: 883612800    I want the XML to just show the year (not month, day, and time).", "For example, I want the XML to show this:    <Date1998</Date    I thought this would do the job:    <xs:element name=\"Date\" type=\"xs:dateTime\"      dfdl:lengthKind=\"explicit\"       dfdl:length=\"4\"       dfdl:binaryCalendarRep=\"binarySeconds\"       dfdl:calendarPattern=\"yyyy\"       dfdl:calendarPatternKind=\"explicit\"      dfdl:lengthUnits=\"bytes\"       dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /    But apparently that's not correct.", "What is the correct way to do it, please?", "/Roger      "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Brett  On 17/03/2011, at 10:09 AM, Eric Kolotyluk wrote:   So I uninstalled and reinstalled NPanday, but I have the same problem with  VS 2010 - so it seems to be 2010 related.", "The plugin seems to work for VS 2005, so I'll experiment with that.", "In the  end I really need this to work in VS 2010 as that is our development  environment.", "I don't have VS 2008.", "Cheers, Eric    On Mon, Mar 14, 2011 at 4:32 PM, Brett Porter <brett@apache.org wrote:    The problem is that the wizard doesn't install some things in the GAC yet,  making the redundant Maven command necessary."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The plugin seems to work for VS 2005, so I'll experiment with that.", "In the  end I really need this to work in VS 2010 as that is our development  environment.", "I don't have VS 2008.", "Cheers, Eric    On Mon, Mar 14, 2011 at 4:32 PM, Brett Porter <brett@apache.org wrote:    The problem is that the wizard doesn't install some things in the GAC yet,  making the redundant Maven command necessary.", "What failure did you get from  that?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't have VS 2008.", "Cheers, Eric    On Mon, Mar 14, 2011 at 4:32 PM, Brett Porter <brett@apache.org wrote:    The problem is that the wizard doesn't install some things in the GAC yet,  making the redundant Maven command necessary.", "What failure did you get from  that?", "On 15/03/2011, at 10:14 AM, Eric Kolotyluk wrote:    That's the funny thing, I have never installed NPanday before - this was  the first time.", "One thing that is confusing is the Install Wizard vs the mvn install."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers, Eric    On Mon, Mar 14, 2011 at 4:32 PM, Brett Porter <brett@apache.org wrote:    The problem is that the wizard doesn't install some things in the GAC yet,  making the redundant Maven command necessary.", "What failure did you get from  that?", "On 15/03/2011, at 10:14 AM, Eric Kolotyluk wrote:    That's the funny thing, I have never installed NPanday before - this was  the first time.", "One thing that is confusing is the Install Wizard vs the mvn install.", "I ran  the wizard first, but then I tried the mvn install as the instructions are  not all that clear."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What failure did you get from  that?", "On 15/03/2011, at 10:14 AM, Eric Kolotyluk wrote:    That's the funny thing, I have never installed NPanday before - this was  the first time.", "One thing that is confusing is the Install Wizard vs the mvn install.", "I ran  the wizard first, but then I tried the mvn install as the instructions are  not all that clear.", "The mvn install failed anyway, is that what could have  screwed things up?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 15/03/2011, at 10:14 AM, Eric Kolotyluk wrote:    That's the funny thing, I have never installed NPanday before - this was  the first time.", "One thing that is confusing is the Install Wizard vs the mvn install.", "I ran  the wizard first, but then I tried the mvn install as the instructions are  not all that clear.", "The mvn install failed anyway, is that what could have  screwed things up?", "Cheers, Eric    On Mon, Mar 14, 2011 at 2:33 PM, Brett Porter <brett@apache.org wrote:    It sounds like you had an old version installed beforehand."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One thing that is confusing is the Install Wizard vs the mvn install.", "I ran  the wizard first, but then I tried the mvn install as the instructions are  not all that clear.", "The mvn install failed anyway, is that what could have  screwed things up?", "Cheers, Eric    On Mon, Mar 14, 2011 at 2:33 PM, Brett Porter <brett@apache.org wrote:    It sounds like you had an old version installed beforehand.", "We should  handle this better, so feel free to file a bug."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["I ran  the wizard first, but then I tried the mvn install as the instructions are  not all that clear.", "The mvn install failed anyway, is that what could have  screwed things up?", "Cheers, Eric    On Mon, Mar 14, 2011 at 2:33 PM, Brett Porter <brett@apache.org wrote:    It sounds like you had an old version installed beforehand.", "We should  handle this better, so feel free to file a bug.", "However, to get it working I  suggest you follow the uninstall instructions on the web site (make sure  everything is removed, particularly the AddIn file in your documents  folder), make sure it does not appear in VS and then try installing again."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["The mvn install failed anyway, is that what could have  screwed things up?", "Cheers, Eric    On Mon, Mar 14, 2011 at 2:33 PM, Brett Porter <brett@apache.org wrote:    It sounds like you had an old version installed beforehand.", "We should  handle this better, so feel free to file a bug.", "However, to get it working I  suggest you follow the uninstall instructions on the web site (make sure  everything is removed, particularly the AddIn file in your documents  folder), make sure it does not appear in VS and then try installing again.", "On 15/03/2011, at 1:57 AM, Eric Kolotyluk wrote:    After running the NPanday installation wizard, when I start VS 2010 I  get the following message (see attached)    Exception adding NPanday to the tools m...  A command with that name already exists."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["We should  handle this better, so feel free to file a bug.", "However, to get it working I  suggest you follow the uninstall instructions on the web site (make sure  everything is removed, particularly the AddIn file in your documents  folder), make sure it does not appear in VS and then try installing again.", "On 15/03/2011, at 1:57 AM, Eric Kolotyluk wrote:    After running the NPanday installation wizard, when I start VS 2010 I  get the following message (see attached)    Exception adding NPanday to the tools m...  A command with that name already exists.", "NPanday shows up in the Tools menu, but it does nothing when I click it,  and then the menu item goes disabled.", "Does anyone know how to fix this?"], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, to get it working I  suggest you follow the uninstall instructions on the web site (make sure  everything is removed, particularly the AddIn file in your documents  folder), make sure it does not appear in VS and then try installing again.", "On 15/03/2011, at 1:57 AM, Eric Kolotyluk wrote:    After running the NPanday installation wizard, when I start VS 2010 I  get the following message (see attached)    Exception adding NPanday to the tools m...  A command with that name already exists.", "NPanday shows up in the Tools menu, but it does nothing when I click it,  and then the menu item goes disabled.", "Does anyone know how to fix this?", "Cheers, Eric        --  Brett Porter  brett@apache.org  http://brettporter.wordpress.com/  http://au.linkedin.com/in/brettporter              --  Brett Porter  brett@apache.org  http://brettporter.wordpress.com/  http://au.linkedin.com/in/brettporter            -- Brett Porter brett@apache.org http://brettporter.wordpress.com/ http://au.linkedin.com/in/brettporter      "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, the gradle wrapper is not included anymore in the source distribution from RC4 , but you can follow the instructions in the README file to install gradle.", "The approach is similar to other projects that rely on maven but do not include it in the release: you have to install maven in order to build the project.", "Note we haven't voted yet on RC4, and we also need to update the walkthrough for installing gradle.", "Regards,  Matthieu  On Apr 13, 2013, at 10:05 , Dingyu Yang wrote:   Hi, I download the src of S4-RC4.", "I find the gradlew file missing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The approach is similar to other projects that rely on maven but do not include it in the release: you have to install maven in order to build the project.", "Note we haven't voted yet on RC4, and we also need to update the walkthrough for installing gradle.", "Regards,  Matthieu  On Apr 13, 2013, at 10:05 , Dingyu Yang wrote:   Hi, I download the src of S4-RC4.", "I find the gradlew file missing.", "Is there other compile files?     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Do you intend to pull in metadata only in source about the number of records?", "It seems like it but you have mentioned that you pull data D (whole data?)", "in source.", "If so what is the work left for workunits?", "Yep I intend to pull the meta data in the Source as that would be required to create the partition information which will be passed to the WorkUnits."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It seems like it but you have mentioned that you pull data D (whole data?)", "in source.", "If so what is the work left for workunits?", "Yep I intend to pull the meta data in the Source as that would be required to create the partition information which will be passed to the WorkUnits.", "However there are some sources which does not produce the metadata for the extracted data like twitter search."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["in source.", "If so what is the work left for workunits?", "Yep I intend to pull the meta data in the Source as that would be required to create the partition information which will be passed to the WorkUnits.", "However there are some sources which does not produce the metadata for the extracted data like twitter search.", "Take a look at the use case of twitter data, the search API of the twitter does not give the meta data for the big search entries."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If so what is the work left for workunits?", "Yep I intend to pull the meta data in the Source as that would be required to create the partition information which will be passed to the WorkUnits.", "However there are some sources which does not produce the metadata for the extracted data like twitter search.", "Take a look at the use case of twitter data, the search API of the twitter does not give the meta data for the big search entries.", "https://dev.twitter.com/rest/public/search   -What you mean by keeping things in-memory between source / workunits."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yep I intend to pull the meta data in the Source as that would be required to create the partition information which will be passed to the WorkUnits.", "However there are some sources which does not produce the metadata for the extracted data like twitter search.", "Take a look at the use case of twitter data, the search API of the twitter does not give the meta data for the big search entries.", "https://dev.twitter.com/rest/public/search   -What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However there are some sources which does not produce the metadata for the extracted data like twitter search.", "Take a look at the use case of twitter data, the search API of the twitter does not give the meta data for the big search entries.", "https://dev.twitter.com/rest/public/search   -What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode.", "I have not made it clear for the yarn use cases, I intent to use Distributed memory ( Apache Ignite) for the same."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://dev.twitter.com/rest/public/search   -What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode.", "I have not made it clear for the yarn use cases, I intent to use Distributed memory ( Apache Ignite) for the same.", "I hope I am clear this time.", "On Fri, Sep 8, 2017 at 6:05 AM, Abhishek Tiwari <abti@apache.org wrote:   I am not super clear with your use-case:   - Do you intend to pull in metadata only in source about the number of  records?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have not made it clear for the yarn use cases, I intent to use Distributed memory ( Apache Ignite) for the same.", "I hope I am clear this time.", "On Fri, Sep 8, 2017 at 6:05 AM, Abhishek Tiwari <abti@apache.org wrote:   I am not super clear with your use-case:   - Do you intend to pull in metadata only in source about the number of  records?", "It seems like it but you have mentioned that you pull data D  (whole data?)", "in source."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Sep 8, 2017 at 6:05 AM, Abhishek Tiwari <abti@apache.org wrote:   I am not super clear with your use-case:   - Do you intend to pull in metadata only in source about the number of  records?", "It seems like it but you have mentioned that you pull data D  (whole data?)", "in source.", "If so what is the work left for workunits?", "- What you mean by keeping things in-memory between source / workunits."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It seems like it but you have mentioned that you pull data D  (whole data?)", "in source.", "If so what is the work left for workunits?", "- What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["in source.", "If so what is the work left for workunits?", "- What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode.", "Regards,  Abhishek   On Wed, Sep 6, 2017 at 5:20 AM, Vicky Kak <vicky.kak@gmail.com wrote:   Hey Guys,   I have checked in sample code demonstrating the pattern as explained  above."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If so what is the work left for workunits?", "- What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode.", "Regards,  Abhishek   On Wed, Sep 6, 2017 at 5:20 AM, Vicky Kak <vicky.kak@gmail.com wrote:   Hey Guys,   I have checked in sample code demonstrating the pattern as explained  above.", "https://github.com/dallaybatta/gobblin-examples   I am soon going to put the documentation about the same, please note that  it is just a quick hack to demonstrate the pattern as explained in the  email chain."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode.", "Regards,  Abhishek   On Wed, Sep 6, 2017 at 5:20 AM, Vicky Kak <vicky.kak@gmail.com wrote:   Hey Guys,   I have checked in sample code demonstrating the pattern as explained  above.", "https://github.com/dallaybatta/gobblin-examples   I am soon going to put the documentation about the same, please note that  it is just a quick hack to demonstrate the pattern as explained in the  email chain.", "Regards,  Vicky     On Tue, Sep 5, 2017 at 6:48 PM, Vicky Kak <vicky.kak@gmail.com wrote:   I am not able to see this email yet in the email archive here  https://lists.apache.org/list.html?user@gobblin.incubator.apache.org   Can anyone take a note of it and get it working?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards,  Abhishek   On Wed, Sep 6, 2017 at 5:20 AM, Vicky Kak <vicky.kak@gmail.com wrote:   Hey Guys,   I have checked in sample code demonstrating the pattern as explained  above.", "https://github.com/dallaybatta/gobblin-examples   I am soon going to put the documentation about the same, please note that  it is just a quick hack to demonstrate the pattern as explained in the  email chain.", "Regards,  Vicky     On Tue, Sep 5, 2017 at 6:48 PM, Vicky Kak <vicky.kak@gmail.com wrote:   I am not able to see this email yet in the email archive here  https://lists.apache.org/list.html?user@gobblin.incubator.apache.org   Can anyone take a note of it and get it working?", "Thanks,  Vicky     On Wed, Aug 30, 2017 at 4:08 PM, Vicky Kak <vicky.kak@gmail.com wrote:   Hi Guys,   We have got a use case where there is no meta data information about  the data to be processed in Gobblin.", "We need to read the whole data chunk  and then create a partition, I would be interested to know how this is  being addressed by others."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards,  Vicky     On Tue, Sep 5, 2017 at 6:48 PM, Vicky Kak <vicky.kak@gmail.com wrote:   I am not able to see this email yet in the email archive here  https://lists.apache.org/list.html?user@gobblin.incubator.apache.org   Can anyone take a note of it and get it working?", "Thanks,  Vicky     On Wed, Aug 30, 2017 at 4:08 PM, Vicky Kak <vicky.kak@gmail.com wrote:   Hi Guys,   We have got a use case where there is no meta data information about  the data to be processed in Gobblin.", "We need to read the whole data chunk  and then create a partition, I would be interested to know how this is  being addressed by others.", "Let me explain it with the sample generic data,  assume that we have got data D with N records in it.", "We do the following  1) In the Source implementation we pull all the data D using rest API."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We need to read the whole data chunk  and then create a partition, I would be interested to know how this is  being addressed by others.", "Let me explain it with the sample generic data,  assume that we have got data D with N records in it.", "We do the following  1) In the Source implementation we pull all the data D using rest API.", "We have got the N records in the Source implementation and we are creating  n(workunit number)*M( records to be processed by each workunit) = N.  2) We are passing the starting id to the workunit via the SourceState.", "3) Each WorkUnit makes an redundant REST call to fetch the sub set of  D, starting from the id that is passed from Source to it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let me explain it with the sample generic data,  assume that we have got data D with N records in it.", "We do the following  1) In the Source implementation we pull all the data D using rest API.", "We have got the N records in the Source implementation and we are creating  n(workunit number)*M( records to be processed by each workunit) = N.  2) We are passing the starting id to the workunit via the SourceState.", "3) Each WorkUnit makes an redundant REST call to fetch the sub set of  D, starting from the id that is passed from Source to it.", "So there are 1 REST call in Source and n REST calls to get the data,  total of n+1 calls are being made although the data can be fetched by a  single call in the Source."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We do the following  1) In the Source implementation we pull all the data D using rest API.", "We have got the N records in the Source implementation and we are creating  n(workunit number)*M( records to be processed by each workunit) = N.  2) We are passing the starting id to the workunit via the SourceState.", "3) Each WorkUnit makes an redundant REST call to fetch the sub set of  D, starting from the id that is passed from Source to it.", "So there are 1 REST call in Source and n REST calls to get the data,  total of n+1 calls are being made although the data can be fetched by a  single call in the Source.", "What I am thinking is to have the data D in the memory ( it should be  distributed memory for YARN case) and pass the reference of it to the  WorkUnits for processing, however would like to know how this is being  addressed by others."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We have got the N records in the Source implementation and we are creating  n(workunit number)*M( records to be processed by each workunit) = N.  2) We are passing the starting id to the workunit via the SourceState.", "3) Each WorkUnit makes an redundant REST call to fetch the sub set of  D, starting from the id that is passed from Source to it.", "So there are 1 REST call in Source and n REST calls to get the data,  total of n+1 calls are being made although the data can be fetched by a  single call in the Source.", "What I am thinking is to have the data D in the memory ( it should be  distributed memory for YARN case) and pass the reference of it to the  WorkUnits for processing, however would like to know how this is being  addressed by others.", "This can be one of the patterns of data to be  processed by the Gobblin."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So there are 1 REST call in Source and n REST calls to get the data,  total of n+1 calls are being made although the data can be fetched by a  single call in the Source.", "What I am thinking is to have the data D in the memory ( it should be  distributed memory for YARN case) and pass the reference of it to the  WorkUnits for processing, however would like to know how this is being  addressed by others.", "This can be one of the patterns of data to be  processed by the Gobblin.", "May be we can have a document explaining various data patterns, how to  partition them and use in the Gobblin.", "Thanks,  Vicky         "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Harold Lim wrote:  Hi Akara,    It's just that I've modified the Java implementation of olio to run with Glassfish, postgresql and HDFS.", "I'm not really interested in the performance of the database but rather the filesystem.", "At the very least, I'm just interested in the performance at the front end (response time of user browsing the web app, which is indirectly related to the performance of the file system).", "Ultimately, I'm interested in the throughput of each storage nodes of my HDFS.", "Also, when I run faban, I get exception reading /my.cnf, which is true since postgresql doesn't have my.cnf."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm not really interested in the performance of the database but rather the filesystem.", "At the very least, I'm just interested in the performance at the front end (response time of user browsing the web app, which is indirectly related to the performance of the file system).", "Ultimately, I'm interested in the throughput of each storage nodes of my HDFS.", "Also, when I run faban, I get exception reading /my.cnf, which is true since postgresql doesn't have my.cnf.", "Just ignore it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["At the very least, I'm just interested in the performance at the front end (response time of user browsing the web app, which is indirectly related to the performance of the file system).", "Ultimately, I'm interested in the throughput of each storage nodes of my HDFS.", "Also, when I run faban, I get exception reading /my.cnf, which is true since postgresql doesn't have my.cnf.", "Just ignore it.", "We would have to edit the Olio benchmark class and  rebuild the driver to get rid of it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also, when I run faban, I get exception reading /my.cnf, which is true since postgresql doesn't have my.cnf.", "Just ignore it.", "We would have to edit the Olio benchmark class and  rebuild the driver to get rid of it.", "Future versions will allow you to  use different services transparently, avoiding the message altogether.", "One question though: In the olio driver tab, is the host = the IP of the master?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Just ignore it.", "We would have to edit the Olio benchmark class and  rebuild the driver to get rid of it.", "Future versions will allow you to  use different services transparently, avoiding the message altogether.", "One question though: In the olio driver tab, is the host = the IP of the master?", "or the IP of the machine containing glassfish?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We would have to edit the Olio benchmark class and  rebuild the driver to get rid of it.", "Future versions will allow you to  use different services transparently, avoiding the message altogether.", "One question though: In the olio driver tab, is the host = the IP of the master?", "or the IP of the machine containing glassfish?", "My current set up is like this: 1 machine for master, 1 machine for postgresql, 1 machine for glassfish."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Future versions will allow you to  use different services transparently, avoiding the message altogether.", "One question though: In the olio driver tab, is the host = the IP of the master?", "or the IP of the machine containing glassfish?", "My current set up is like this: 1 machine for master, 1 machine for postgresql, 1 machine for glassfish.", "The host in this tab refers to the driver systems."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One question though: In the olio driver tab, is the host = the IP of the master?", "or the IP of the machine containing glassfish?", "My current set up is like this: 1 machine for master, 1 machine for postgresql, 1 machine for glassfish.", "The host in this tab refers to the driver systems.", "You can use ip  addresses or host names, separated by space."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["or the IP of the machine containing glassfish?", "My current set up is like this: 1 machine for master, 1 machine for postgresql, 1 machine for glassfish.", "The host in this tab refers to the driver systems.", "You can use ip  addresses or host names, separated by space.", "I set up the host to be the IP of glassfish."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The host in this tab refers to the driver systems.", "You can use ip  addresses or host names, separated by space.", "I set up the host to be the IP of glassfish.", "When I run the benchmark, I get failed:    HomePage\t0\t0\t0%\t26.15%\tFAILED  Login\t0\t0\t0%\t10.22%\tFAILED  TagSearch\t0\t0\t0%\t33.45%\tFAILED  EventDetail\t0\t0\t0%\t24.68%\tFAILED  PersonDetail\t0\t0\t0%\t2.61%\tFAILED  AddPerson\t0\t0\t0%\t.84%\tPASSED  AddEvent\t0\t0\t0%\t2.04%\tFAILED  Simply, none of the operations went through.", "I'm pretty sure that olio is working for me because I can manually go to the olio web app from my browser and browse/create user, events, etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I set up the host to be the IP of glassfish.", "When I run the benchmark, I get failed:    HomePage\t0\t0\t0%\t26.15%\tFAILED  Login\t0\t0\t0%\t10.22%\tFAILED  TagSearch\t0\t0\t0%\t33.45%\tFAILED  EventDetail\t0\t0\t0%\t24.68%\tFAILED  PersonDetail\t0\t0\t0%\t2.61%\tFAILED  AddPerson\t0\t0\t0%\t.84%\tPASSED  AddEvent\t0\t0\t0%\t2.04%\tFAILED  Simply, none of the operations went through.", "I'm pretty sure that olio is working for me because I can manually go to the olio web app from my browser and browse/create user, events, etc.", "Well, we need to know what's not working, then.", "Check your run log."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When I run the benchmark, I get failed:    HomePage\t0\t0\t0%\t26.15%\tFAILED  Login\t0\t0\t0%\t10.22%\tFAILED  TagSearch\t0\t0\t0%\t33.45%\tFAILED  EventDetail\t0\t0\t0%\t24.68%\tFAILED  PersonDetail\t0\t0\t0%\t2.61%\tFAILED  AddPerson\t0\t0\t0%\t.84%\tPASSED  AddEvent\t0\t0\t0%\t2.04%\tFAILED  Simply, none of the operations went through.", "I'm pretty sure that olio is working for me because I can manually go to the olio web app from my browser and browse/create user, events, etc.", "Well, we need to know what's not working, then.", "Check your run log.", "-Akara            --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM wrote:    From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:51 PM  I've to check whether it is exposed,  but you can certainly edit the   config file and mark the host enabled=false."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Check your run log.", "-Akara            --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM wrote:    From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:51 PM  I've to check whether it is exposed,  but you can certainly edit the   config file and mark the host enabled=false.", "However, I  don't suggest   that.", "Olio depends on the agent to reload the DB, etc.", "Can you please let me know why you don't want to run the  agent on the DB   server?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-Akara            --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM wrote:    From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:51 PM  I've to check whether it is exposed,  but you can certainly edit the   config file and mark the host enabled=false.", "However, I  don't suggest   that.", "Olio depends on the agent to reload the DB, etc.", "Can you please let me know why you don't want to run the  agent on the DB   server?", "As for the data store, I don't think the distributed stores  are well   integrated."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Can you please let me know why you don't want to run the  agent on the DB   server?", "As for the data store, I don't think the distributed stores  are well   integrated.", "Why don't you get successful runs with the  local store first   and we can then add support for those.", "-Akara   Harold Lim wrote:  Thanks.", "I have a few more questions:   Is there a way to only test the webapp?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As for the data store, I don't think the distributed stores  are well   integrated.", "Why don't you get successful runs with the  local store first   and we can then add support for those.", "-Akara   Harold Lim wrote:  Thanks.", "I have a few more questions:   Is there a way to only test the webapp?", "(i.e., I don't  want to run faban agent on my DB server)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why don't you get successful runs with the  local store first   and we can then add support for those.", "-Akara   Harold Lim wrote:  Thanks.", "I have a few more questions:   Is there a way to only test the webapp?", "(i.e., I don't  want to run faban agent on my DB server).", "When I put blank  for host name of db server when setting up my run in faban,  I get like a null pointer exception."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-Akara   Harold Lim wrote:  Thanks.", "I have a few more questions:   Is there a way to only test the webapp?", "(i.e., I don't  want to run faban agent on my DB server).", "When I put blank  for host name of db server when setting up my run in faban,  I get like a null pointer exception.", "Also, in the data storage server section of faban, how  do I set it up if I am not using local store?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have a few more questions:   Is there a way to only test the webapp?", "(i.e., I don't  want to run faban agent on my DB server).", "When I put blank  for host name of db server when setting up my run in faban,  I get like a null pointer exception.", "Also, in the data storage server section of faban, how  do I set it up if I am not using local store?", "I have set up  a distributed storage to host my file store (e.g., hdfs,  mogilefs)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(i.e., I don't  want to run faban agent on my DB server).", "When I put blank  for host name of db server when setting up my run in faban,  I get like a null pointer exception.", "Also, in the data storage server section of faban, how  do I set it up if I am not using local store?", "I have set up  a distributed storage to host my file store (e.g., hdfs,  mogilefs).", "Thanks,  Harold    --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  wrote:  From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:10 PM  The file faban.xml did not get  generated at deployment for some reason."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When I put blank  for host name of db server when setting up my run in faban,  I get like a null pointer exception.", "Also, in the data storage server section of faban, how  do I set it up if I am not using local store?", "I have set up  a distributed storage to host my file store (e.g., hdfs,  mogilefs).", "Thanks,  Harold    --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  wrote:  From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:10 PM  The file faban.xml did not get  generated at deployment for some reason.", "Just  remove the  olio deploy directory under $FABAN/benchmarks and  restart  the Faban harness."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also, in the data storage server section of faban, how  do I set it up if I am not using local store?", "I have set up  a distributed storage to host my file store (e.g., hdfs,  mogilefs).", "Thanks,  Harold    --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  wrote:  From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:10 PM  The file faban.xml did not get  generated at deployment for some reason.", "Just  remove the  olio deploy directory under $FABAN/benchmarks and  restart  the Faban harness.", "As Faban unjars the Olio  benchmark jar it  should generate the file."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have set up  a distributed storage to host my file store (e.g., hdfs,  mogilefs).", "Thanks,  Harold    --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  wrote:  From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:10 PM  The file faban.xml did not get  generated at deployment for some reason.", "Just  remove the  olio deploy directory under $FABAN/benchmarks and  restart  the Faban harness.", "As Faban unjars the Olio  benchmark jar it  should generate the file.", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Harold    --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  wrote:  From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:10 PM  The file faban.xml did not get  generated at deployment for some reason.", "Just  remove the  olio deploy directory under $FABAN/benchmarks and  restart  the Faban harness.", "As Faban unjars the Olio  benchmark jar it  should generate the file.", "Thanks.", "-Akara   Harold Lim wrote:  When I try to schedule a run in Faban: I get  the  following error in my log:  record       <date2009-08-14T15:05:34</date       <millis1250276734815</millis       <sequence65</sequence       <loggercom.sun.faban.harness.common.BenchmarkDescription</logger       <levelWARNING</level       <classcom.sun.faban.harness.common.BenchmarkDescription</class       <methodreadDescription</method      <thread11</thread      <messageError reading  benchmark descriptor for   /home/harold/Desktop/faban/benchmarks/olio</message      <exception          <messagejava.io.IOException:  Element &lt;metric&gt; empty or missing  in  /home/harold/Desktop/faban/benchmarks/olio/META-INF/benchmark.xml</message        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodreadDescription</method            <line276</line        </frame        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodgenerateMaps</method            <line181</line        </frame        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodcheckMaps</method            <line165</line        </frame        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodgetDescription</method            <line126</line        </frame        <frame             <classcom.sun.faban.harness.engine.RunDaemon</class             <methodfetchNextRun</method            <line160</line        </frame        <frame             <classcom.sun.faban.harness.engine.RunDaemon</class            <methodrun</method            <line272</line        </frame    Any ideas what's causing that?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Just  remove the  olio deploy directory under $FABAN/benchmarks and  restart  the Faban harness.", "As Faban unjars the Olio  benchmark jar it  should generate the file.", "Thanks.", "-Akara   Harold Lim wrote:  When I try to schedule a run in Faban: I get  the  following error in my log:  record       <date2009-08-14T15:05:34</date       <millis1250276734815</millis       <sequence65</sequence       <loggercom.sun.faban.harness.common.BenchmarkDescription</logger       <levelWARNING</level       <classcom.sun.faban.harness.common.BenchmarkDescription</class       <methodreadDescription</method      <thread11</thread      <messageError reading  benchmark descriptor for   /home/harold/Desktop/faban/benchmarks/olio</message      <exception          <messagejava.io.IOException:  Element &lt;metric&gt; empty or missing  in  /home/harold/Desktop/faban/benchmarks/olio/META-INF/benchmark.xml</message        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodreadDescription</method            <line276</line        </frame        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodgenerateMaps</method            <line181</line        </frame        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodcheckMaps</method            <line165</line        </frame        <frame             <classcom.sun.faban.harness.common.BenchmarkDescription</class             <methodgetDescription</method            <line126</line        </frame        <frame             <classcom.sun.faban.harness.engine.RunDaemon</class             <methodfetchNextRun</method            <line160</line        </frame        <frame             <classcom.sun.faban.harness.engine.RunDaemon</class            <methodrun</method            <line272</line        </frame    Any ideas what's causing that?", "Thanks,  Harold                        "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [" On 30/09/2011, at 4:21 AM, Eric Kolotyluk wrote:   OK, I was finally able to get this project to build :-)  Glad to hear it, and sorry it took so long :)     1.", "There seems to be a bug in the compile plugin where it is succeeding    when it should fail.", "2.", "Trying to update AssemblyInfo.cs file is problematic with respect to    source control.", "The plugin should not update this file, rather it    should update one in the target directory."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There seems to be a bug in the compile plugin where it is succeeding    when it should fail.", "2.", "Trying to update AssemblyInfo.cs file is problematic with respect to    source control.", "The plugin should not update this file, rather it    should update one in the target directory.", "If there really is some    reason to update files that are likely under source control, then    the compile plugin needs to interact with the defined SCM."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2.", "Trying to update AssemblyInfo.cs file is problematic with respect to    source control.", "The plugin should not update this file, rather it    should update one in the target directory.", "If there really is some    reason to update files that are likely under source control, then    the compile plugin needs to interact with the defined SCM.", "Can you make sure separate issues are created for these in JIRA?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The plugin should not update this file, rather it    should update one in the target directory.", "If there really is some    reason to update files that are likely under source control, then    the compile plugin needs to interact with the defined SCM.", "Can you make sure separate issues are created for these in JIRA?", "I wasn't able to find an existing report of them.", "- Brett  -- Brett Porter brett@apache.org http://brettporter.wordpress.com/ http://au.linkedin.com/in/brettporter      "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello DFDL community,  For initiators:    *   Some data formats identify the start of data by preceding the data with a symbol   *   DFDL calls that symbol an \"initiator\"      *   Other people call it a \"tag\" or a \"label\".", "For terminators:    *   Some data formats identify the end of data by following the data with a symbol   *   DFDL calls that symbol a \"terminator\"      *   Other people call it ???", "What do other people call terminators?", "/Roger  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Unfortunately  at the  moment NPanday only supports Visual Studios English Version, there  are already  issues created for German support as well as Italian and Spanish.", "We  do not have    enough contributors that know the language.", "We would gladly accept  patches for  this so that NPanday can work on other languages as well.", "thanks for your help!", "i thought it would be something like that, did not find any issues  though."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We  do not have    enough contributors that know the language.", "We would gladly accept  patches for  this so that NPanday can work on other languages as well.", "thanks for your help!", "i thought it would be something like that, did not find any issues  though.", "what would i have to do to add german language support myself?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We would gladly accept  patches for  this so that NPanday can work on other languages as well.", "thanks for your help!", "i thought it would be something like that, did not find any issues  though.", "what would i have to do to add german language support myself?", "You would need to build the source code and in \\dotnet\\assemblies\\NPanday.VisualStudio.Addin\\Connect.cs you need to modify launchNPandayBuildSystem function there is a loop that checks for the specific Control Caption in the running VS  i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["thanks for your help!", "i thought it would be something like that, did not find any issues  though.", "what would i have to do to add german language support myself?", "You would need to build the source code and in \\dotnet\\assemblies\\NPanday.VisualStudio.Addin\\Connect.cs you need to modify launchNPandayBuildSystem function there is a loop that checks for the specific Control Caption in the running VS  i.e.", "if (control.Caption.Equals(\"C&onfiguration Manager...\"))  you can include the german equivalent for this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["i thought it would be something like that, did not find any issues  though.", "what would i have to do to add german language support myself?", "You would need to build the source code and in \\dotnet\\assemblies\\NPanday.VisualStudio.Addin\\Connect.cs you need to modify launchNPandayBuildSystem function there is a loop that checks for the specific Control Caption in the running VS  i.e.", "if (control.Caption.Equals(\"C&onfiguration Manager...\"))  you can include the german equivalent for this.", "for the creating of the project what command did you use?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You would need to build the source code and in \\dotnet\\assemblies\\NPanday.VisualStudio.Addin\\Connect.cs you need to modify launchNPandayBuildSystem function there is a loop that checks for the specific Control Caption in the running VS  i.e.", "if (control.Caption.Equals(\"C&onfiguration Manager...\"))  you can include the german equivalent for this.", "for the creating of the project what command did you use?", "i used        mvn archetype:generate  -DarchetypeArtifactId=maven-archetype-dotnet-simple  -DarchetypeGroupId=npanday  -DarchetypeVersion=1.3-incubating      as stated in \"Creating a simple project\".", "i dont see anything wrong  with it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["i used        mvn archetype:generate  -DarchetypeArtifactId=maven-archetype-dotnet-simple  -DarchetypeGroupId=npanday  -DarchetypeVersion=1.3-incubating      as stated in \"Creating a simple project\".", "i dont see anything wrong  with it.", "also tested it on my linux machine by now and it worked like a  charm...  the machine where it doesnt work runs windows xp by the way.", "also i never used maven on windows before, so it could be that im  overlooking  something.", "NPanday was originally developed under the windows XP OS, so it should work fine."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["i dont see anything wrong  with it.", "also tested it on my linux machine by now and it worked like a  charm...  the machine where it doesnt work runs windows xp by the way.", "also i never used maven on windows before, so it could be that im  overlooking  something.", "NPanday was originally developed under the windows XP OS, so it should work fine.", "You can check out the docs here http://incubator.apache.org/npanday/docs/1.3-incubating/index.html    regards,chris  "], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["On 4/5/07, Gareth Paglinawan <gareth.paglinawan@gmail.com wrote:   hi,   Do you have any getting start guide in using Trinidad in eclipse and tomcat.", "I have a problem that when I start the application, it says the resource is  not available.", "HTTP404.", "Please send me the complete step how to develop one   page.", "I already developed using JDeveloper but I want to use eclipse and  Tomcat."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have a problem that when I start the application, it says the resource is  not available.", "HTTP404.", "Please send me the complete step how to develop one   page.", "I already developed using JDeveloper but I want to use eclipse and  Tomcat.", "I followed the steps in apache doc but I am not successful."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["HTTP404.", "Please send me the complete step how to develop one   page.", "I already developed using JDeveloper but I want to use eclipse and  Tomcat.", "I followed the steps in apache doc but I am not successful.", "Please send me step by step guide to do this.    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have a problem either with 1.1.7 and 1.0.14.", "First, I had to manually add to the adf-faces jar the af.taglib.xml file from the sources, because there was only yhe afh.taglib.xml one... strange thing.", "Anyway I use the latest sources from apache to build the adf-faces jars.", "Now, I have a test page that seems to work, <h:* <af:* <afh:* and <t:* tags get replaced in my output by the generated HTML, but my <f:* ones does not.", "I have <f:facelets in my output page."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["First, I had to manually add to the adf-faces jar the af.taglib.xml file from the sources, because there was only yhe afh.taglib.xml one... strange thing.", "Anyway I use the latest sources from apache to build the adf-faces jars.", "Now, I have a test page that seems to work, <h:* <af:* <afh:* and <t:* tags get replaced in my output by the generated HTML, but my <f:* ones does not.", "I have <f:facelets in my output page.", "What's wrong?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Anyway I use the latest sources from apache to build the adf-faces jars.", "Now, I have a test page that seems to work, <h:* <af:* <afh:* and <t:* tags get replaced in my output by the generated HTML, but my <f:* ones does not.", "I have <f:facelets in my output page.", "What's wrong?", "This is the libraries I set for my project:  adf-faces-api-11-m7-SNAPSHOT.jar adf-faces-impl-11-m7-SNAPSHOT.jar myfaces-api-1.1.4-SNAPSHOT.jar myfaces-impl-1.1.4-SNAPSHOT.jar tomahawk-1.1.4-SNAPSHOT.jar jsf-facelets-1.1.7.jar el-api.jar el-ri.jar commons-beanutils-1.7.0.jar commons-codec-1.3.jar commons-collections-3.1.jar commons-digester-1.6.jar commons-el-1.0.jar commons-fileupload-1.0.jar commons-lang-2.1.jar commons-logging-1.0.4.jar commons-validator-1.1.4.jar jakarta-oro-2.0.7.jar jstl-1.1.0.jar  In my facelets page, I declared the namespaces this way:  <html     xmlns=\"http://www.w3.org/1999/xhtml\"     xmlns:ui=\"http://java.sun.com/jsf/facelets\"     xmlns:f=\"http://java.sun.com/jsf\"     xmlns:h=\"http://java.sun.com/jsf/html\"     xmlns:t=\"http://myfaces.apache.org/tomahawk\"     xmlns:af=\"http://myfaces.apache.org/adf/faces\"     xmlns:afh=\"http://myfaces.apache.org/adf/faces/html\"  What's wrong?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Now, I have a test page that seems to work, <h:* <af:* <afh:* and <t:* tags get replaced in my output by the generated HTML, but my <f:* ones does not.", "I have <f:facelets in my output page.", "What's wrong?", "This is the libraries I set for my project:  adf-faces-api-11-m7-SNAPSHOT.jar adf-faces-impl-11-m7-SNAPSHOT.jar myfaces-api-1.1.4-SNAPSHOT.jar myfaces-impl-1.1.4-SNAPSHOT.jar tomahawk-1.1.4-SNAPSHOT.jar jsf-facelets-1.1.7.jar el-api.jar el-ri.jar commons-beanutils-1.7.0.jar commons-codec-1.3.jar commons-collections-3.1.jar commons-digester-1.6.jar commons-el-1.0.jar commons-fileupload-1.0.jar commons-lang-2.1.jar commons-logging-1.0.4.jar commons-validator-1.1.4.jar jakarta-oro-2.0.7.jar jstl-1.1.0.jar  In my facelets page, I declared the namespaces this way:  <html     xmlns=\"http://www.w3.org/1999/xhtml\"     xmlns:ui=\"http://java.sun.com/jsf/facelets\"     xmlns:f=\"http://java.sun.com/jsf\"     xmlns:h=\"http://java.sun.com/jsf/html\"     xmlns:t=\"http://myfaces.apache.org/tomahawk\"     xmlns:af=\"http://myfaces.apache.org/adf/faces\"     xmlns:afh=\"http://myfaces.apache.org/adf/faces/html\"  What's wrong?", "2006/5/31, Cosma Colanicchia <cosmacol@gmail.com:  2006/5/31, Frank Felix Debatin <ffd@gmx.net:    I'm was starting with the latest release, 1.1.7, but I'll    follow your hint and try 1.0.14 first."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have <f:facelets in my output page.", "What's wrong?", "This is the libraries I set for my project:  adf-faces-api-11-m7-SNAPSHOT.jar adf-faces-impl-11-m7-SNAPSHOT.jar myfaces-api-1.1.4-SNAPSHOT.jar myfaces-impl-1.1.4-SNAPSHOT.jar tomahawk-1.1.4-SNAPSHOT.jar jsf-facelets-1.1.7.jar el-api.jar el-ri.jar commons-beanutils-1.7.0.jar commons-codec-1.3.jar commons-collections-3.1.jar commons-digester-1.6.jar commons-el-1.0.jar commons-fileupload-1.0.jar commons-lang-2.1.jar commons-logging-1.0.4.jar commons-validator-1.1.4.jar jakarta-oro-2.0.7.jar jstl-1.1.0.jar  In my facelets page, I declared the namespaces this way:  <html     xmlns=\"http://www.w3.org/1999/xhtml\"     xmlns:ui=\"http://java.sun.com/jsf/facelets\"     xmlns:f=\"http://java.sun.com/jsf\"     xmlns:h=\"http://java.sun.com/jsf/html\"     xmlns:t=\"http://myfaces.apache.org/tomahawk\"     xmlns:af=\"http://myfaces.apache.org/adf/faces\"     xmlns:afh=\"http://myfaces.apache.org/adf/faces/html\"  What's wrong?", "2006/5/31, Cosma Colanicchia <cosmacol@gmail.com:  2006/5/31, Frank Felix Debatin <ffd@gmx.net:    I'm was starting with the latest release, 1.1.7, but I'll    follow your hint and try 1.0.14 first.", "I was actually hoping you start with 1.1.x to see if you   find the same problems ;-)   Ok, I promise that, if I can work out without problems 1.0.14, I'll  try switch to 1.1.7 :-)     So,  the only required    libraries are those for tomahawk and for the sandbox,   right?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is the libraries I set for my project:  adf-faces-api-11-m7-SNAPSHOT.jar adf-faces-impl-11-m7-SNAPSHOT.jar myfaces-api-1.1.4-SNAPSHOT.jar myfaces-impl-1.1.4-SNAPSHOT.jar tomahawk-1.1.4-SNAPSHOT.jar jsf-facelets-1.1.7.jar el-api.jar el-ri.jar commons-beanutils-1.7.0.jar commons-codec-1.3.jar commons-collections-3.1.jar commons-digester-1.6.jar commons-el-1.0.jar commons-fileupload-1.0.jar commons-lang-2.1.jar commons-logging-1.0.4.jar commons-validator-1.1.4.jar jakarta-oro-2.0.7.jar jstl-1.1.0.jar  In my facelets page, I declared the namespaces this way:  <html     xmlns=\"http://www.w3.org/1999/xhtml\"     xmlns:ui=\"http://java.sun.com/jsf/facelets\"     xmlns:f=\"http://java.sun.com/jsf\"     xmlns:h=\"http://java.sun.com/jsf/html\"     xmlns:t=\"http://myfaces.apache.org/tomahawk\"     xmlns:af=\"http://myfaces.apache.org/adf/faces\"     xmlns:afh=\"http://myfaces.apache.org/adf/faces/html\"  What's wrong?", "2006/5/31, Cosma Colanicchia <cosmacol@gmail.com:  2006/5/31, Frank Felix Debatin <ffd@gmx.net:    I'm was starting with the latest release, 1.1.7, but I'll    follow your hint and try 1.0.14 first.", "I was actually hoping you start with 1.1.x to see if you   find the same problems ;-)   Ok, I promise that, if I can work out without problems 1.0.14, I'll  try switch to 1.1.7 :-)     So,  the only required    libraries are those for tomahawk and for the sandbox,   right?", "The facelets page mentions a tomahawk taglib contribution.", "I'm just using the ADF component, so I can't tell."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2006/5/31, Cosma Colanicchia <cosmacol@gmail.com:  2006/5/31, Frank Felix Debatin <ffd@gmx.net:    I'm was starting with the latest release, 1.1.7, but I'll    follow your hint and try 1.0.14 first.", "I was actually hoping you start with 1.1.x to see if you   find the same problems ;-)   Ok, I promise that, if I can work out without problems 1.0.14, I'll  try switch to 1.1.7 :-)     So,  the only required    libraries are those for tomahawk and for the sandbox,   right?", "The facelets page mentions a tomahawk taglib contribution.", "I'm just using the ADF component, so I can't tell.", "I'll probably leave the tomahawk and sandbox in the future, it seems  that ADF Faces has a rather complete feature set..    Frank Felix       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi    What version of npanday are you using?", "The groupid were changed to apache.org.npanday.plugins can you also check your local repository if there is a plugin that would match this version if not we have release with a repository packaged you can use that.", "Hope that helps,    Joe   ----- \"Cihan\" <ctozan@yahoo.com wrote:    Hi   I'm trying to build a .net project with npanday.its giving this error.", "Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]   in my repository, maven-aspx-plugin jar and pom files are present and versions are correct.", "what is the reason for this error?"], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The groupid were changed to apache.org.npanday.plugins can you also check your local repository if there is a plugin that would match this version if not we have release with a repository packaged you can use that.", "Hope that helps,    Joe   ----- \"Cihan\" <ctozan@yahoo.com wrote:    Hi   I'm trying to build a .net project with npanday.its giving this error.", "Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]   in my repository, maven-aspx-plugin jar and pom files are present and versions are correct.", "what is the reason for this error?", "Thanks    my pom file:<?"], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]   in my repository, maven-aspx-plugin jar and pom files are present and versions are correct.", "what is the reason for this error?", "Thanks    my pom file:<?", "<  <  <  <  <  <  </  <  <  <  <  <  <  <  <  <  <  <  <  <  </  </  </  </  </xmlversion=\"1.0\"encoding=\"utf-8\"?projectxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"xmlns=\"http://maven.apache.org/POM/4.0.0\"parentartifactIdSolution1-parent</artifactIdgroupIdKFS.Solution1</groupIdversion1.0-SNAPSHOT</versionrelativePath../pom.xml</relativePathparentmodelVersion4.0.0</modelVersionartifactIdWebSite</artifactIdpackagingasp</packagingnameKFS.Solution1 : WebSite</namebuildsourceDirectory.</sourceDirectorypluginsplugingroupIdorg.apache.npanday.plugins</groupId artifactIdmaven-aspx-plugin</artifactIdextensionstrue</extensionsconfigurationframeworkVersion3.5</frameworkVersionconfigurationpluginpluginsbuildproject     full error text:------------------------------------------------------------------  Executing Maven  Pom File: D:\\My Documents\\Visual Studio 2008\\Projects\\Solution1\\pom.xml  Goal: compile  Arguments: compile  NPanday Command: C:\\Documents and Settings\\U053797.KFS\\Desktop\\apache-maven-3.0.3-bin\\apache-maven-3.0.3\\bin\\mvn.bat compile   ------------------------------------------------------------------   [INFO] Scanning for projects...  [WARNING]  [WARNING] Some problems were encountered while building the effective model for KFS.Solution1:WebSite:asp:1.0-SNAPSHOT  [WARNING] 'build.plugins.plugin.version' for org.apache.npanday.plugins:maven-aspx-plugin is missing.", "@ line 16, column 15  [WARNING]  [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["what is the reason for this error?", "Thanks    my pom file:<?", "<  <  <  <  <  <  </  <  <  <  <  <  <  <  <  <  <  <  <  <  </  </  </  </  </xmlversion=\"1.0\"encoding=\"utf-8\"?projectxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"xmlns=\"http://maven.apache.org/POM/4.0.0\"parentartifactIdSolution1-parent</artifactIdgroupIdKFS.Solution1</groupIdversion1.0-SNAPSHOT</versionrelativePath../pom.xml</relativePathparentmodelVersion4.0.0</modelVersionartifactIdWebSite</artifactIdpackagingasp</packagingnameKFS.Solution1 : WebSite</namebuildsourceDirectory.</sourceDirectorypluginsplugingroupIdorg.apache.npanday.plugins</groupId artifactIdmaven-aspx-plugin</artifactIdextensionstrue</extensionsconfigurationframeworkVersion3.5</frameworkVersionconfigurationpluginpluginsbuildproject     full error text:------------------------------------------------------------------  Executing Maven  Pom File: D:\\My Documents\\Visual Studio 2008\\Projects\\Solution1\\pom.xml  Goal: compile  Arguments: compile  NPanday Command: C:\\Documents and Settings\\U053797.KFS\\Desktop\\apache-maven-3.0.3-bin\\apache-maven-3.0.3\\bin\\mvn.bat compile   ------------------------------------------------------------------   [INFO] Scanning for projects...  [WARNING]  [WARNING] Some problems were encountered while building the effective model for KFS.Solution1:WebSite:asp:1.0-SNAPSHOT  [WARNING] 'build.plugins.plugin.version' for org.apache.npanday.plugins:maven-aspx-plugin is missing.", "@ line 16, column 15  [WARNING]  [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.", "[WARNING]  [WARNING] For this reason, future Maven versions might no longer support building such malformed projects."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["<  <  <  <  <  <  </  <  <  <  <  <  <  <  <  <  <  <  <  <  </  </  </  </  </xmlversion=\"1.0\"encoding=\"utf-8\"?projectxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"xmlns=\"http://maven.apache.org/POM/4.0.0\"parentartifactIdSolution1-parent</artifactIdgroupIdKFS.Solution1</groupIdversion1.0-SNAPSHOT</versionrelativePath../pom.xml</relativePathparentmodelVersion4.0.0</modelVersionartifactIdWebSite</artifactIdpackagingasp</packagingnameKFS.Solution1 : WebSite</namebuildsourceDirectory.</sourceDirectorypluginsplugingroupIdorg.apache.npanday.plugins</groupId artifactIdmaven-aspx-plugin</artifactIdextensionstrue</extensionsconfigurationframeworkVersion3.5</frameworkVersionconfigurationpluginpluginsbuildproject     full error text:------------------------------------------------------------------  Executing Maven  Pom File: D:\\My Documents\\Visual Studio 2008\\Projects\\Solution1\\pom.xml  Goal: compile  Arguments: compile  NPanday Command: C:\\Documents and Settings\\U053797.KFS\\Desktop\\apache-maven-3.0.3-bin\\apache-maven-3.0.3\\bin\\mvn.bat compile   ------------------------------------------------------------------   [INFO] Scanning for projects...  [WARNING]  [WARNING] Some problems were encountered while building the effective model for KFS.Solution1:WebSite:asp:1.0-SNAPSHOT  [WARNING] 'build.plugins.plugin.version' for org.apache.npanday.plugins:maven-aspx-plugin is missing.", "@ line 16, column 15  [WARNING]  [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.", "[WARNING]  [WARNING] For this reason, future Maven versions might no longer support building such malformed projects.", "[WARNING]  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Build Order:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent  [INFO] KFS.Solution1 : WebSite  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : Solution1-parent 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : WebSite 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent ..................", "SUCCESS [0.015s]  [INFO] KFS.Solution1 : WebSite ........................... FAILURE [0.187s]  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 7.652s  [INFO] Finished at: Fri Aug 05 09:55:22 EEST 2011  [INFO] Final Memory: 3M/15M  [INFO] ------------------------------------------------------------------------  [ERROR] Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]  [ERROR]  [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["@ line 16, column 15  [WARNING]  [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.", "[WARNING]  [WARNING] For this reason, future Maven versions might no longer support building such malformed projects.", "[WARNING]  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Build Order:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent  [INFO] KFS.Solution1 : WebSite  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : Solution1-parent 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : WebSite 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent ..................", "SUCCESS [0.015s]  [INFO] KFS.Solution1 : WebSite ........................... FAILURE [0.187s]  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 7.652s  [INFO] Finished at: Fri Aug 05 09:55:22 EEST 2011  [INFO] Final Memory: 3M/15M  [INFO] ------------------------------------------------------------------------  [ERROR] Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]  [ERROR]  [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.", "[ERROR] Re-run Maven using the -X switch to enable full debug logging."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[WARNING]  [WARNING] For this reason, future Maven versions might no longer support building such malformed projects.", "[WARNING]  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Build Order:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent  [INFO] KFS.Solution1 : WebSite  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : Solution1-parent 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : WebSite 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent ..................", "SUCCESS [0.015s]  [INFO] KFS.Solution1 : WebSite ........................... FAILURE [0.187s]  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 7.652s  [INFO] Finished at: Fri Aug 05 09:55:22 EEST 2011  [INFO] Final Memory: 3M/15M  [INFO] ------------------------------------------------------------------------  [ERROR] Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]  [ERROR]  [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.", "[ERROR] Re-run Maven using the -X switch to enable full debug logging.", "[ERROR]  [ERROR] For more information about the errors and possible solutions, please read the following articles:  [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginVersionResolutionException  NPanday Execution Failed!, with exit code: 1   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["sorry for hearing that.", "is there any workaround method for this?", "On Thu, Feb 7, 2013 at 11:11 AM, Aaron McCurry <amccurry@gmail.com wrote:  Unfortunately yes the minimum for all the features to work is 0.20.2 (with  appends).", "I have never tried to run it on anything less than 0.20.2.", "Sorry."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Feb 7, 2013 at 11:11 AM, Aaron McCurry <amccurry@gmail.com wrote:  Unfortunately yes the minimum for all the features to work is 0.20.2 (with  appends).", "I have never tried to run it on anything less than 0.20.2.", "Sorry.", "Aaron    On Wed, Feb 6, 2013 at 9:38 PM, Li Li <fancyerii@gmail.com wrote:   I am using hadoop 0.18. does blur need higher version hadoop?", "I can't upgrade it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have never tried to run it on anything less than 0.20.2.", "Sorry.", "Aaron    On Wed, Feb 6, 2013 at 9:38 PM, Li Li <fancyerii@gmail.com wrote:   I am using hadoop 0.18. does blur need higher version hadoop?", "I can't upgrade it.", "it's out of my control."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sorry.", "Aaron    On Wed, Feb 6, 2013 at 9:38 PM, Li Li <fancyerii@gmail.com wrote:   I am using hadoop 0.18. does blur need higher version hadoop?", "I can't upgrade it.", "it's out of my control.", "On Thu, Feb 7, 2013 at 10:14 AM, Aaron McCurry <amccurry@gmail.com wrote:   See comments below."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Aaron    On Wed, Feb 6, 2013 at 9:38 PM, Li Li <fancyerii@gmail.com wrote:   I am using hadoop 0.18. does blur need higher version hadoop?", "I can't upgrade it.", "it's out of my control.", "On Thu, Feb 7, 2013 at 10:14 AM, Aaron McCurry <amccurry@gmail.com wrote:   See comments below.", "On Tue, Feb 5, 2013 at 10:53 PM, Li Li <fancyerii@gmail.com wrote:     I followed all the steps of http://wiki.apache.org/blur/QuickStart   the only difference is the machine I use don't have git installed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can't upgrade it.", "it's out of my control.", "On Thu, Feb 7, 2013 at 10:14 AM, Aaron McCurry <amccurry@gmail.com wrote:   See comments below.", "On Tue, Feb 5, 2013 at 10:53 PM, Li Li <fancyerii@gmail.com wrote:     I followed all the steps of http://wiki.apache.org/blur/QuickStart   the only difference is the machine I use don't have git installed.", "so   I use another machine to clone by:     git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git   git checkout 0.2-dev     then I copy it to my build machine   all is fine before start-up   there are still errors in log   RROR 20130206_11:47:00:000_CST [main]   concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread   [Thread[main,5,main]]   java.lang.RuntimeException: Safemode data missing   [/blur/clusters/default/safemode]           at    org.apache.blur.manager.indexserver.DistributedIndexServer.waitInSafeModeIfNeeded(DistributedIndexServer.java:177)           at    org.apache.blur.manager.indexserver.DistributedIndexServer.init(DistributedIndexServer.java:135)           at    org.apache.blur.thrift.ThriftBlurServer.createServer(ThriftBlurServer.java:187)           at   org.apache.blur.thrift.ThriftBlurServer.main(ThriftBlurServer.java:88)       This could be a bug with an empty ZooKeeper (meaning maybe blur isn't   setting things up correctly)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["it's out of my control.", "On Thu, Feb 7, 2013 at 10:14 AM, Aaron McCurry <amccurry@gmail.com wrote:   See comments below.", "On Tue, Feb 5, 2013 at 10:53 PM, Li Li <fancyerii@gmail.com wrote:     I followed all the steps of http://wiki.apache.org/blur/QuickStart   the only difference is the machine I use don't have git installed.", "so   I use another machine to clone by:     git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git   git checkout 0.2-dev     then I copy it to my build machine   all is fine before start-up   there are still errors in log   RROR 20130206_11:47:00:000_CST [main]   concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread   [Thread[main,5,main]]   java.lang.RuntimeException: Safemode data missing   [/blur/clusters/default/safemode]           at    org.apache.blur.manager.indexserver.DistributedIndexServer.waitInSafeModeIfNeeded(DistributedIndexServer.java:177)           at    org.apache.blur.manager.indexserver.DistributedIndexServer.init(DistributedIndexServer.java:135)           at    org.apache.blur.thrift.ThriftBlurServer.createServer(ThriftBlurServer.java:187)           at   org.apache.blur.thrift.ThriftBlurServer.main(ThriftBlurServer.java:88)       This could be a bug with an empty ZooKeeper (meaning maybe blur isn't   setting things up correctly).", "I'm going to retest that blur sets up   ZooKeeper correctly for safe mode and report back."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Feb 7, 2013 at 10:14 AM, Aaron McCurry <amccurry@gmail.com wrote:   See comments below.", "On Tue, Feb 5, 2013 at 10:53 PM, Li Li <fancyerii@gmail.com wrote:     I followed all the steps of http://wiki.apache.org/blur/QuickStart   the only difference is the machine I use don't have git installed.", "so   I use another machine to clone by:     git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git   git checkout 0.2-dev     then I copy it to my build machine   all is fine before start-up   there are still errors in log   RROR 20130206_11:47:00:000_CST [main]   concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread   [Thread[main,5,main]]   java.lang.RuntimeException: Safemode data missing   [/blur/clusters/default/safemode]           at    org.apache.blur.manager.indexserver.DistributedIndexServer.waitInSafeModeIfNeeded(DistributedIndexServer.java:177)           at    org.apache.blur.manager.indexserver.DistributedIndexServer.init(DistributedIndexServer.java:135)           at    org.apache.blur.thrift.ThriftBlurServer.createServer(ThriftBlurServer.java:187)           at   org.apache.blur.thrift.ThriftBlurServer.main(ThriftBlurServer.java:88)       This could be a bug with an empty ZooKeeper (meaning maybe blur isn't   setting things up correctly).", "I'm going to retest that blur sets up   ZooKeeper correctly for safe mode and report back.", "when I run the client   $./apache-blur-0.2.0-SNAPSHOT/bin/blur shell localhost:40020   java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)   Exception in thread \"main\" java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)       This is a very strange error, it's almost like the wrong thrift library  is   in the classpath somewhere."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Feb 5, 2013 at 10:53 PM, Li Li <fancyerii@gmail.com wrote:     I followed all the steps of http://wiki.apache.org/blur/QuickStart   the only difference is the machine I use don't have git installed.", "so   I use another machine to clone by:     git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git   git checkout 0.2-dev     then I copy it to my build machine   all is fine before start-up   there are still errors in log   RROR 20130206_11:47:00:000_CST [main]   concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread   [Thread[main,5,main]]   java.lang.RuntimeException: Safemode data missing   [/blur/clusters/default/safemode]           at    org.apache.blur.manager.indexserver.DistributedIndexServer.waitInSafeModeIfNeeded(DistributedIndexServer.java:177)           at    org.apache.blur.manager.indexserver.DistributedIndexServer.init(DistributedIndexServer.java:135)           at    org.apache.blur.thrift.ThriftBlurServer.createServer(ThriftBlurServer.java:187)           at   org.apache.blur.thrift.ThriftBlurServer.main(ThriftBlurServer.java:88)       This could be a bug with an empty ZooKeeper (meaning maybe blur isn't   setting things up correctly).", "I'm going to retest that blur sets up   ZooKeeper correctly for safe mode and report back.", "when I run the client   $./apache-blur-0.2.0-SNAPSHOT/bin/blur shell localhost:40020   java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)   Exception in thread \"main\" java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)       This is a very strange error, it's almost like the wrong thrift library  is   in the classpath somewhere.", "What hadoop version are you using?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm going to retest that blur sets up   ZooKeeper correctly for safe mode and report back.", "when I run the client   $./apache-blur-0.2.0-SNAPSHOT/bin/blur shell localhost:40020   java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)   Exception in thread \"main\" java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)       This is a very strange error, it's almost like the wrong thrift library  is   in the classpath somewhere.", "What hadoop version are you using?", "Could   there be another version of thrift pulled in from another project  somehow?", "Maybe hbase?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["when I run the client   $./apache-blur-0.2.0-SNAPSHOT/bin/blur shell localhost:40020   java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)   Exception in thread \"main\" java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)       This is a very strange error, it's almost like the wrong thrift library  is   in the classpath somewhere.", "What hadoop version are you using?", "Could   there be another version of thrift pulled in from another project  somehow?", "Maybe hbase?", "I know that the latest version of hbase is still using  0.8.0   of thrift."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Maybe hbase?", "I know that the latest version of hbase is still using  0.8.0   of thrift.", "On Tue, Feb 5, 2013 at 11:53 PM, Aaron McCurry <amccurry@gmail.com  wrote:    This is an interesting error, because it's looks like there is a  mixture   of    old code the trunk (0.1.x) with an error because of Lucene 4.0.", "Try   using    and building with the 0.2-dev, should become the trunk soon.", "Follow  this    guide and let me know what issues you find."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["I know that the latest version of hbase is still using  0.8.0   of thrift.", "On Tue, Feb 5, 2013 at 11:53 PM, Aaron McCurry <amccurry@gmail.com  wrote:    This is an interesting error, because it's looks like there is a  mixture   of    old code the trunk (0.1.x) with an error because of Lucene 4.0.", "Try   using    and building with the 0.2-dev, should become the trunk soon.", "Follow  this    guide and let me know what issues you find.", "Thanks!"], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Feb 5, 2013 at 11:53 PM, Aaron McCurry <amccurry@gmail.com  wrote:    This is an interesting error, because it's looks like there is a  mixture   of    old code the trunk (0.1.x) with an error because of Lucene 4.0.", "Try   using    and building with the 0.2-dev, should become the trunk soon.", "Follow  this    guide and let me know what issues you find.", "Thanks!", "http://wiki.apache.org/blur/QuickStart       Aaron          On Tue, Feb 5, 2013 at 4:41 AM, Li Li <fancyerii@gmail.com wrote:       hi        I follow the instructions of the README.md."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Try   using    and building with the 0.2-dev, should become the trunk soon.", "Follow  this    guide and let me know what issues you find.", "Thanks!", "http://wiki.apache.org/blur/QuickStart       Aaron          On Tue, Feb 5, 2013 at 4:41 AM, Li Li <fancyerii@gmail.com wrote:       hi        I follow the instructions of the README.md.", "when I    ./bin/start-all.sh, there are errors in log files        1. blur-shard-server ****-0.log        ERROR 20130205_17:34:57:057_CST [main]    concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread    [Thread[main,5,main]]    java.lang.NoSuchFieldError: NOT_ANALYZED_NO_NORMS            at    org.apache.blur.utils.BlurConstants.<clinit(BlurConstants.java:105)            at       org.apache.blur.thrift.ThriftBlurShardServer.createServer(ThriftBlurShardServer.java:187)            at       org.apache.blur.thrift.ThriftBlurShardServer.main(ThriftBlurShardServer.java:92)        2. logs/blur-controller-server**-0.log        ERROR 20130205_17:35:00:000_CST [main]    concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread    [Thread[main,5,main]]    java.lang.NoSuchMethodError:          org.apache.thrift.transport.TNonblockingServerSocket.<init(Ljava/net/InetSocketAddress;)V            at   org.apache.blur.thrift.ThriftServer.start(ThriftServer.java:80)            at       org.apache.blur.thrift.ThriftBlurControllerServer.main(ThriftBlurControllerServer.java:72)        "], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  Newbie question...", "I have my own file format.", "The files are saved on HDFS.", "I would like HCatalog to facilitate to read those files by Hive.", "Something like:  Hive | HCatalog | MyFiles  Where should I start with?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have my own file format.", "The files are saved on HDFS.", "I would like HCatalog to facilitate to read those files by Hive.", "Something like:  Hive | HCatalog | MyFiles  Where should I start with?", "Is there any sample integration of other File formats which I can use a reference?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The files are saved on HDFS.", "I would like HCatalog to facilitate to read those files by Hive.", "Something like:  Hive | HCatalog | MyFiles  Where should I start with?", "Is there any sample integration of other File formats which I can use a reference?", "--  Cheers, *Subroto Sanyal*  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I wonder if there's a commandMenuItem you can use in your version (I think commandLink is intended for stand-alone hyperlinks).", "Regards, Matt  On 12/6/06, Matt Cooper <matt.faces@gmail.com wrote:   In Trinidad, the solution for this is to use navigationPane hint=\"tabs\"  with 2 commandNavigationItem children.", "CommandNavigationItem has both  action and destination attributes (so you'd want to use the \"action\"  attribute and set partialSubmit=\"true\").", "Regards,  Matt   On 12/6/06, Marcus Bond <marcus_bonds@hotmail.com wrote:     Hi, I'm using ADF menuTabs with goMenuItem tags to basically give me a 2   tab   pane.", "I don't want to redirect to another page i just wish to change   which   tab is visible when the user clicks one."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards, Matt  On 12/6/06, Matt Cooper <matt.faces@gmail.com wrote:   In Trinidad, the solution for this is to use navigationPane hint=\"tabs\"  with 2 commandNavigationItem children.", "CommandNavigationItem has both  action and destination attributes (so you'd want to use the \"action\"  attribute and set partialSubmit=\"true\").", "Regards,  Matt   On 12/6/06, Marcus Bond <marcus_bonds@hotmail.com wrote:     Hi, I'm using ADF menuTabs with goMenuItem tags to basically give me a 2   tab   pane.", "I don't want to redirect to another page i just wish to change   which   tab is visible when the user clicks one.", "For me this would involve a   user   clicking a tab which then runs a server script to change the visibility   of a   property in a view bean, one to false and the other to true (The   rendered   property of the pane tags comes from a view bean) then the page would   refresh."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["CommandNavigationItem has both  action and destination attributes (so you'd want to use the \"action\"  attribute and set partialSubmit=\"true\").", "Regards,  Matt   On 12/6/06, Marcus Bond <marcus_bonds@hotmail.com wrote:     Hi, I'm using ADF menuTabs with goMenuItem tags to basically give me a 2   tab   pane.", "I don't want to redirect to another page i just wish to change   which   tab is visible when the user clicks one.", "For me this would involve a   user   clicking a tab which then runs a server script to change the visibility   of a   property in a view bean, one to false and the other to true (The   rendered   property of the pane tags comes from a view bean) then the page would   refresh.", "I have tried adding commandLinks to the goMenuItem's with no success, I   have   also tried putting an EL expression in the destination attribute with   the   method that returns the destination (the same page) having code to   change   the visibility of each pane but that also doesn't work..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards,  Matt   On 12/6/06, Marcus Bond <marcus_bonds@hotmail.com wrote:     Hi, I'm using ADF menuTabs with goMenuItem tags to basically give me a 2   tab   pane.", "I don't want to redirect to another page i just wish to change   which   tab is visible when the user clicks one.", "For me this would involve a   user   clicking a tab which then runs a server script to change the visibility   of a   property in a view bean, one to false and the other to true (The   rendered   property of the pane tags comes from a view bean) then the page would   refresh.", "I have tried adding commandLinks to the goMenuItem's with no success, I   have   also tried putting an EL expression in the destination attribute with   the   method that returns the destination (the same page) having code to   change   the visibility of each pane but that also doesn't work...", "Anyone have a solution to this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't want to redirect to another page i just wish to change   which   tab is visible when the user clicks one.", "For me this would involve a   user   clicking a tab which then runs a server script to change the visibility   of a   property in a view bean, one to false and the other to true (The   rendered   property of the pane tags comes from a view bean) then the page would   refresh.", "I have tried adding commandLinks to the goMenuItem's with no success, I   have   also tried putting an EL expression in the destination attribute with   the   method that returns the destination (the same page) having code to   change   the visibility of each pane but that also doesn't work...", "Anyone have a solution to this?", "Thanks,   Marcus."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For me this would involve a   user   clicking a tab which then runs a server script to change the visibility   of a   property in a view bean, one to false and the other to true (The   rendered   property of the pane tags comes from a view bean) then the page would   refresh.", "I have tried adding commandLinks to the goMenuItem's with no success, I   have   also tried putting an EL expression in the destination attribute with   the   method that returns the destination (the same page) having code to   change   the visibility of each pane but that also doesn't work...", "Anyone have a solution to this?", "Thanks,   Marcus.", "_________________________________________________________________   Windows Live\u2122 Messenger has arrived."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have tried adding commandLinks to the goMenuItem's with no success, I   have   also tried putting an EL expression in the destination attribute with   the   method that returns the destination (the same page) having code to   change   the visibility of each pane but that also doesn't work...", "Anyone have a solution to this?", "Thanks,   Marcus.", "_________________________________________________________________   Windows Live\u2122 Messenger has arrived.", "Click here to download it for free!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Anyone have a solution to this?", "Thanks,   Marcus.", "_________________________________________________________________   Windows Live\u2122 Messenger has arrived.", "Click here to download it for free!", "http://imagine-msn.com/messenger/launch80/?locale=en-gb       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I believe this JIRA could be related:  https://issues.apache.org/jira/browse/PIG-2693   On Sun, May 13, 2012 at 8:41 PM, Travis Crawford <traviscrawford@gmail.comwrote:   Hey Aniket -   I allegedly fixed this in:   Automagically setting parallelism based on input file size does not  work with HCatalog  https://issues.apache.org/jira/browse/PIG-2573   HCatLoader should report its input size so pig can estimate the number  of reducers  https://issues.apache.org/jira/browse/HCATALOG-328   If its not working we should fix it.", "Can you provide more details?", "--travis    On Sun, May 13, 2012 at 7:24 PM, Aniket Mokashi <aniket486@gmail.com  wrote:   Hi,     I have observed that pig's reducer estimator does not work with  HCatalog.", "I   am wondering if there is a way to make it work?", "If not, should we have a   jira with pig to track this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Can you provide more details?", "--travis    On Sun, May 13, 2012 at 7:24 PM, Aniket Mokashi <aniket486@gmail.com  wrote:   Hi,     I have observed that pig's reducer estimator does not work with  HCatalog.", "I   am wondering if there is a way to make it work?", "If not, should we have a   jira with pig to track this?", "Thanks,   Aniket     --  *Note that I'm no longer using my Yahoo!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["--travis    On Sun, May 13, 2012 at 7:24 PM, Aniket Mokashi <aniket486@gmail.com  wrote:   Hi,     I have observed that pig's reducer estimator does not work with  HCatalog.", "I   am wondering if there is a way to make it work?", "If not, should we have a   jira with pig to track this?", "Thanks,   Aniket     --  *Note that I'm no longer using my Yahoo!", "email address."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I   am wondering if there is a way to make it work?", "If not, should we have a   jira with pig to track this?", "Thanks,   Aniket     --  *Note that I'm no longer using my Yahoo!", "email address.", "Please email me at billgraham@gmail.com going forward."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This might be:  http://issues.apache.org/jira/browse/ADFFACES-24 ... which desparately needs some attention.", "-- Adam   On 1/3/07, Daniel Hannum <dhannum@quovadx.com wrote:  Hi.", "I'm reposting this from last week because I'm afraid no one saw it  during the vacation week.", "This is still a very serious issue for me and  I'd welcome any feedback you can provide.", "I've added references at the  bottom."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Adam   On 1/3/07, Daniel Hannum <dhannum@quovadx.com wrote:  Hi.", "I'm reposting this from last week because I'm afraid no one saw it  during the vacation week.", "This is still a very serious issue for me and  I'd welcome any feedback you can provide.", "I've added references at the  bottom.", "--     I have a form with a required field, and a commandMenuItem that goes to  another page (but has immediate set to true)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm reposting this from last week because I'm afraid no one saw it  during the vacation week.", "This is still a very serious issue for me and  I'd welcome any feedback you can provide.", "I've added references at the  bottom.", "--     I have a form with a required field, and a commandMenuItem that goes to  another page (but has immediate set to true).", "As expected, I can click  the menu link and go to the other page, and the immediate attribute  bypasses validations."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is still a very serious issue for me and  I'd welcome any feedback you can provide.", "I've added references at the  bottom.", "--     I have a form with a required field, and a commandMenuItem that goes to  another page (but has immediate set to true).", "As expected, I can click  the menu link and go to the other page, and the immediate attribute  bypasses validations.", "However, if I add that page a control that has autoSubmit=\"true\", even  if the autoSubmit has nothing to do with the required field, now when I  click the menu link, it will still stop me with validation errors."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I've added references at the  bottom.", "--     I have a form with a required field, and a commandMenuItem that goes to  another page (but has immediate set to true).", "As expected, I can click  the menu link and go to the other page, and the immediate attribute  bypasses validations.", "However, if I add that page a control that has autoSubmit=\"true\", even  if the autoSubmit has nothing to do with the required field, now when I  click the menu link, it will still stop me with validation errors.", "If I  click the menu link again, it will go to the page as expected."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["--     I have a form with a required field, and a commandMenuItem that goes to  another page (but has immediate set to true).", "As expected, I can click  the menu link and go to the other page, and the immediate attribute  bypasses validations.", "However, if I add that page a control that has autoSubmit=\"true\", even  if the autoSubmit has nothing to do with the required field, now when I  click the menu link, it will still stop me with validation errors.", "If I  click the menu link again, it will go to the page as expected.", "I saw  reports on the Oracle forums about ADF issues in this vein [1, 2]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, if I add that page a control that has autoSubmit=\"true\", even  if the autoSubmit has nothing to do with the required field, now when I  click the menu link, it will still stop me with validation errors.", "If I  click the menu link again, it will go to the page as expected.", "I saw  reports on the Oracle forums about ADF issues in this vein [1, 2].", "I  don't know if they have been fixed in Trinidad.", "Seems like this has to  be a bug, though."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If I  click the menu link again, it will go to the page as expected.", "I saw  reports on the Oracle forums about ADF issues in this vein [1, 2].", "I  don't know if they have been fixed in Trinidad.", "Seems like this has to  be a bug, though.", "To recap:     1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Seems like this has to  be a bug, though.", "To recap:     1.", "Make a page with a required text field, an autoSubmit checkbox,  and an immediate link to another page.", "2.", "Leave the field blank and click the link."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["To recap:     1.", "Make a page with a required text field, an autoSubmit checkbox,  and an immediate link to another page.", "2.", "Leave the field blank and click the link.", "You go to the page fine."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Make a page with a required text field, an autoSubmit checkbox,  and an immediate link to another page.", "2.", "Leave the field blank and click the link.", "You go to the page fine.", "Validations are skipped by immediate=\"true\"   3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2.", "Leave the field blank and click the link.", "You go to the page fine.", "Validations are skipped by immediate=\"true\"   3.", "Go back."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You go to the page fine.", "Validations are skipped by immediate=\"true\"   3.", "Go back.", "Still leave the field blank, but this time click the  autoSubmit checkbox.", "Now click the link."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Validations are skipped by immediate=\"true\"   3.", "Go back.", "Still leave the field blank, but this time click the  autoSubmit checkbox.", "Now click the link.", "You'll get validation errors  (despite the immediate=\"true\"..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Still leave the field blank, but this time click the  autoSubmit checkbox.", "Now click the link.", "You'll get validation errors  (despite the immediate=\"true\"...", "bug?)", "4."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Now click the link.", "You'll get validation errors  (despite the immediate=\"true\"...", "bug?)", "4.", "Once you have the validation errors on screen, ignore them and  click the link a second time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You'll get validation errors  (despite the immediate=\"true\"...", "bug?)", "4.", "Once you have the validation errors on screen, ignore them and  click the link a second time.", "Now you go to the page as expected."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["bug?)", "4.", "Once you have the validation errors on screen, ignore them and  click the link a second time.", "Now you go to the page as expected.", "Not being an expert in the JSF lifecycle or in the implementation of  autoSubmit, can someone explain what's going on?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["4.", "Once you have the validation errors on screen, ignore them and  click the link a second time.", "Now you go to the page as expected.", "Not being an expert in the JSF lifecycle or in the implementation of  autoSubmit, can someone explain what's going on?", "I feel validations  should always be bypassed with immediate=\"true\", not just when not using  autoSubmit."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Once you have the validation errors on screen, ignore them and  click the link a second time.", "Now you go to the page as expected.", "Not being an expert in the JSF lifecycle or in the implementation of  autoSubmit, can someone explain what's going on?", "I feel validations  should always be bypassed with immediate=\"true\", not just when not using  autoSubmit.", "I love autoSubmit, but this makes for a bad user experience."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Now you go to the page as expected.", "Not being an expert in the JSF lifecycle or in the implementation of  autoSubmit, can someone explain what's going on?", "I feel validations  should always be bypassed with immediate=\"true\", not just when not using  autoSubmit.", "I love autoSubmit, but this makes for a bad user experience.", "Thanks   Dan     [1]  http://forums.oracle.com/forums/thread.jspa?messageID=1387162&#1387162   [2] http://www.orablogs.com/fnimphius/archives/001787.html (seems to be  a solution but it involves customizing the ADF lifecycle by extending  Oracle-specific classes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Not being an expert in the JSF lifecycle or in the implementation of  autoSubmit, can someone explain what's going on?", "I feel validations  should always be bypassed with immediate=\"true\", not just when not using  autoSubmit.", "I love autoSubmit, but this makes for a bad user experience.", "Thanks   Dan     [1]  http://forums.oracle.com/forums/thread.jspa?messageID=1387162&#1387162   [2] http://www.orablogs.com/fnimphius/archives/001787.html (seems to be  a solution but it involves customizing the ADF lifecycle by extending  Oracle-specific classes.", "How I would adapt this for the version I'm on,  I don't know)       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["i am doing the  application in single system.so i am not using zoo keeper.also i am not getting any errors.in xml i have 4 PE config,one dispatcher with 3 partitioner  listing and 3 partitioner config specifying the stream name and keys  "], "labels": ["0"]}
{"abstract_id": 0, "sentences": ["On Wed, Jul 24, 2019 at 12:13 AM Amith Prasanna <amith.prasanna@sentienz.com wrote:   Hi all,   I am working on scheduling a job which pulls records from kafka to hdfs.", "This works fine while running in standalone mode.", "But on trying in  map-reduce mode using gobblin-mapreduce.sh script I'm getting class and  method not found errors.", "I'm using hadoop-2.8.1 and gobblin-0.13.", "And also  somehow I came to know that mapreduce.sh triggers job only once."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This works fine while running in standalone mode.", "But on trying in  map-reduce mode using gobblin-mapreduce.sh script I'm getting class and  method not found errors.", "I'm using hadoop-2.8.1 and gobblin-0.13.", "And also  somehow I came to know that mapreduce.sh triggers job only once.", "Can anyone  please give details or changes to be done for scheduling the job in  mapreduce or yarn mode?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But on trying in  map-reduce mode using gobblin-mapreduce.sh script I'm getting class and  method not found errors.", "I'm using hadoop-2.8.1 and gobblin-0.13.", "And also  somehow I came to know that mapreduce.sh triggers job only once.", "Can anyone  please give details or changes to be done for scheduling the job in  mapreduce or yarn mode?", "This is how jobconf file looks like:   job.name=KafkatoHdfsJob1  job.group=GobblinKafka  job.description=Gobblin quick start job for Kafka  job.lock.enabled=false  kafka.brokers=<host:9092  job.schedule=0/20 * * * * ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm using hadoop-2.8.1 and gobblin-0.13.", "And also  somehow I came to know that mapreduce.sh triggers job only once.", "Can anyone  please give details or changes to be done for scheduling the job in  mapreduce or yarn mode?", "This is how jobconf file looks like:   job.name=KafkatoHdfsJob1  job.group=GobblinKafka  job.description=Gobblin quick start job for Kafka  job.lock.enabled=false  kafka.brokers=<host:9092  job.schedule=0/20 * * * * ?", "topic.whitelist=test   source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource  extract.namespace=org.apache.gobblin.extract.kafka  writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder  writer.file.path.type=tablename  writer.destination.type=HDFS  writer.output.format=json  simple.writer.delimiter=\\n  data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher  launcher.type=MAPREDUCE  mr.job.max.mappers=1  mr.include.task.counters=100  mr.job.root.dir=/tmp/gobblin/mr-job  metrics.reporting.file.enabled=true  metrics.log.dir=/data/temp/gobblin-kafka/metrics  metrics.reporting.file.suffix=txt  bootstrap.with.offset=earliest  fs.uri=hdfs://<host:8020/  writer.fs.uri=hdfs://<host:8020/  state.store.fs.uri=hdfs://<host:8020/  mr.job.root.dir=/data/temp/gobblin-kafka/working  writer.staging.dir=/data/temp/gobblin-kafka/writer-staging  writer.output.dir=/data/temp/gobblin-kafka/writer-output  state.store.dir=/data/temp/gobblin-kafka/state-store  task.data.root.dir=/data/temp/jobs/kafkaetl/gobblin/gobblin-kafka/task-data  data.publisher.final.dir=/data/temp/test    Regards,  Amith   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  How to set --master, --deploy-mode, --driver-class-path and --driver-java-options through Apache Livy?", "I want to set the master, spark deploy-mode, driver-class-path and driver-java-options for the Spark job when the job is triggered through Apache Livy without having to restart the Livy server when these settings change.", "How to do this since there are no direct options to do this in Livy?", "I understand these are used to set the parameters for Client mode.", "I want to set these when I spawn a job in client mode."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I want to set the master, spark deploy-mode, driver-class-path and driver-java-options for the Spark job when the job is triggered through Apache Livy without having to restart the Livy server when these settings change.", "How to do this since there are no direct options to do this in Livy?", "I understand these are used to set the parameters for Client mode.", "I want to set these when I spawn a job in client mode.", "Also when I try to set \"spark.master\" through REST through \"conf\" param, it is not being set."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How to do this since there are no direct options to do this in Livy?", "I understand these are used to set the parameters for Client mode.", "I want to set these when I spawn a job in client mode.", "Also when I try to set \"spark.master\" through REST through \"conf\" param, it is not being set.", "This is the JSON I'm sending to the Livy API."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I understand these are used to set the parameters for Client mode.", "I want to set these when I spawn a job in client mode.", "Also when I try to set \"spark.master\" through REST through \"conf\" param, it is not being set.", "This is the JSON I'm sending to the Livy API.", "{                 \"file\": \"/user/livy/spark-examples.jar\",                 \"conf\" : {                                 \"spark.master\": \"yarn\",                                 \"spark.submit.deployMode\": \"cluster\"                 },                 \"args\": [\"2\"],                 \"className\": \"org.apache.spark.examples.SparkPi\" }  Thanks and Regards, Sarthak  Privileged/Confidential Information may be contained in this message and is intended only for the use of the addressee."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also when I try to set \"spark.master\" through REST through \"conf\" param, it is not being set.", "This is the JSON I'm sending to the Livy API.", "{                 \"file\": \"/user/livy/spark-examples.jar\",                 \"conf\" : {                                 \"spark.master\": \"yarn\",                                 \"spark.submit.deployMode\": \"cluster\"                 },                 \"args\": [\"2\"],                 \"className\": \"org.apache.spark.examples.SparkPi\" }  Thanks and Regards, Sarthak  Privileged/Confidential Information may be contained in this message and is intended only for the use of the addressee.", "If you are not the addressee, or person responsible for delivering it to the addressee, you should not copy or deliver this to anyone else.", "If you receive this message by mistake, please delete the message from any/all computer/s and notify the sender immediately by reply Email."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is the JSON I'm sending to the Livy API.", "{                 \"file\": \"/user/livy/spark-examples.jar\",                 \"conf\" : {                                 \"spark.master\": \"yarn\",                                 \"spark.submit.deployMode\": \"cluster\"                 },                 \"args\": [\"2\"],                 \"className\": \"org.apache.spark.examples.SparkPi\" }  Thanks and Regards, Sarthak  Privileged/Confidential Information may be contained in this message and is intended only for the use of the addressee.", "If you are not the addressee, or person responsible for delivering it to the addressee, you should not copy or deliver this to anyone else.", "If you receive this message by mistake, please delete the message from any/all computer/s and notify the sender immediately by reply Email.", "We appreciate your assistance in preserving the confidentiality of our correspondence."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["{                 \"file\": \"/user/livy/spark-examples.jar\",                 \"conf\" : {                                 \"spark.master\": \"yarn\",                                 \"spark.submit.deployMode\": \"cluster\"                 },                 \"args\": [\"2\"],                 \"className\": \"org.apache.spark.examples.SparkPi\" }  Thanks and Regards, Sarthak  Privileged/Confidential Information may be contained in this message and is intended only for the use of the addressee.", "If you are not the addressee, or person responsible for delivering it to the addressee, you should not copy or deliver this to anyone else.", "If you receive this message by mistake, please delete the message from any/all computer/s and notify the sender immediately by reply Email.", "We appreciate your assistance in preserving the confidentiality of our correspondence.", "Any information in this message that does not relate to the official business of the organization shall be understood as neither given nor endorsed by it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you receive this message by mistake, please delete the message from any/all computer/s and notify the sender immediately by reply Email.", "We appreciate your assistance in preserving the confidentiality of our correspondence.", "Any information in this message that does not relate to the official business of the organization shall be understood as neither given nor endorsed by it.", "Please advise immediately if you or your employer does not consent to Internet Email for messages of this kind.", "Thank you."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We appreciate your assistance in preserving the confidentiality of our correspondence.", "Any information in this message that does not relate to the official business of the organization shall be understood as neither given nor endorsed by it.", "Please advise immediately if you or your employer does not consent to Internet Email for messages of this kind.", "Thank you.", "(HCID0411)  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello,  We are using gobblin to ingest data from a remote hdfc cluster to a local one.", "Both clusters are in same geolocation.", "How ever when we start I hating data we observe two behaviour.", "1 The JVM  heap usage is erratic.", "Near the beginning of the job start we see frequent GC on the driver jvm."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Both clusters are in same geolocation.", "How ever when we start I hating data we observe two behaviour.", "1 The JVM  heap usage is erratic.", "Near the beginning of the job start we see frequent GC on the driver jvm.", "The memory usage is also really high."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How ever when we start I hating data we observe two behaviour.", "1 The JVM  heap usage is erratic.", "Near the beginning of the job start we see frequent GC on the driver jvm.", "The memory usage is also really high.", "We have around 56000 files and the size is 1TB  2 The initial time spent by the job on the driver box is nearly 6times of the map job duration."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Near the beginning of the job start we see frequent GC on the driver jvm.", "The memory usage is also really high.", "We have around 56000 files and the size is 1TB  2 The initial time spent by the job on the driver box is nearly 6times of the map job duration.", "This mean we are spending almost 6times rounding up the files  compare to actual copy.", "We would like to know if there is a way we could avoid these two pitfalls while using distcp-NG."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This mean we are spending almost 6times rounding up the files  compare to actual copy.", "We would like to know if there is a way we could avoid these two pitfalls while using distcp-NG.", "We are ok if the data is inconsistent at the dataset level as long as individual files are copied correctly.", "Another thing, we see there are a few parameters which are used in the code but I could not find their reference in the docs.", "E.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We would like to know if there is a way we could avoid these two pitfalls while using distcp-NG.", "We are ok if the data is inconsistent at the dataset level as long as individual files are copied correctly.", "Another thing, we see there are a few parameters which are used in the code but I could not find their reference in the docs.", "E.g.", "gobblin.prioritization.maxCopy.copyEntities Gobblin.copy.max.concurrent.listing.services Gobblin.copy.binpacking.Maxworkunitperbin Gobblin.copy.binpacking.MaxsizePerBin Gobblin.copy.abortonsinglefailure   Is there a better way to understand these other than looking into the code ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Shouldn't be a problem.", "Did you resolve this ?", "Shanti  2010/5/29 Bruno Guimar\u00e3es Sousa <brgsousa@gmail.com   Is it a problem if faban master driver is a 32 bit system, and all nodes in  SUT are 64 bit systems?", "changed the master driver's arch to 32 and \"run log\" presented errors like:  18:39:45 kamet SEVERE*  exception<http://csgengenharia.homeftp.net:9980/LogReader?runId=OlioDriver.1E&exception=21  * UIDriverAgent[0].7: Error initializing driver object.", "Exception:  Message: java.lang.NumberFormatException: For input string: \"\"    regards,  --  Bruno Guimar\u00e3es Sousa  www.ifba.edu.br  PONTONET - DGTI - IFBA  Ci\u00eancia da Computa\u00e7\u00e3o UFBA  Registered Linux user #465914   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["did you read the trinidad wiki on seam ?", "On 3/7/07, Juan Giovanolli <juan.giovanolli@santexgroup.com wrote:   I found that the problem posted befote was for duplication of jars  Now, after correct that , I get this problem when I start my jboss 4.0.5  server:   [code]  18:36:26,343 INFO  [Server] Starting JBoss (MX MicroKernel)...  18:36:26,343 INFO  [Server] Release ID: JBoss [Zion] 4.0.5.GA (build:  CVSTag=Branch_4_0 date=200610162339)  18:36:26,359 INFO  [Server] Home Dir: C:\\jboss-4.0.5.GA  18:36:26,359 INFO  [Server] Home URL: file:/C:/jboss-4.0.5.GA/  18:36:26,359 INFO  [Server] Patch URL: null  18:36:26,359 INFO  [Server] Server Name: default  18:36:26,359 INFO  [Server] Server Home Dir:  C:\\jboss-4.0.5.GA\\server\\default  18:36:26,359 INFO  [Server] Server Home URL:  file:/C:/jboss-4.0.5.GA/server/default/  18:36:26,359 INFO  [Server] Server Log Dir:  C:\\jboss-4.0.5.GA\\server\\default\\log  18:36:26,359 INFO  [Server] Server Temp Dir:  C:\\jboss-4.0.5.GA\\server\\default\\tmp  18:36:26,359 INFO  [Server] Root Deployment Filename: jboss-service.xml  18:36:26,734 INFO  [ServerInfo] Java version: 1.5.0_06,Sun Microsystems Inc.  18:36:26,734 INFO  [ServerInfo] Java VM: Java HotSpot(TM) Client VM  1.5.0_06-b05,Sun Microsystems Inc.  18:36:26,734 INFO  [ServerInfo] OS-System: Windows XP 5.1,x86  18:36:30,359 INFO  [Server] Core system initialized  18:36:31,781 INFO  [Log4jService$URLWatchTimerTask] Configuring from URL:  resource:log4j.xml  18:36:33,140 INFO  [SocketServerInvoker] Invoker started for locator:  InvokerLocator [socket://192.168.250.29:3873/]  18:36:33,687 INFO  [AspectDeployer] Deployed AOP:  file:/C:/jboss-4.0.5.GA/server/default/deploy/ejb3-interceptors-aop.xml  18:36:35,234 INFO  [AspectDeployer] Deployed AOP:  file:/C:/jboss-4.0.5.GA/server/default/deploy/jboss-portal.sar/portal-aop.xm  l  18:36:40,734 INFO  [WebService] Using RMI server codebase:  http://Santex29:8083/  18:36:42,906 INFO  [Embedded] Catalina naming disabled  18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set  in the classpath.", "Will load the default rule set.", "18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set  in the classpath.", "Will load the default rule set."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 3/7/07, Juan Giovanolli <juan.giovanolli@santexgroup.com wrote:   I found that the problem posted befote was for duplication of jars  Now, after correct that , I get this problem when I start my jboss 4.0.5  server:   [code]  18:36:26,343 INFO  [Server] Starting JBoss (MX MicroKernel)...  18:36:26,343 INFO  [Server] Release ID: JBoss [Zion] 4.0.5.GA (build:  CVSTag=Branch_4_0 date=200610162339)  18:36:26,359 INFO  [Server] Home Dir: C:\\jboss-4.0.5.GA  18:36:26,359 INFO  [Server] Home URL: file:/C:/jboss-4.0.5.GA/  18:36:26,359 INFO  [Server] Patch URL: null  18:36:26,359 INFO  [Server] Server Name: default  18:36:26,359 INFO  [Server] Server Home Dir:  C:\\jboss-4.0.5.GA\\server\\default  18:36:26,359 INFO  [Server] Server Home URL:  file:/C:/jboss-4.0.5.GA/server/default/  18:36:26,359 INFO  [Server] Server Log Dir:  C:\\jboss-4.0.5.GA\\server\\default\\log  18:36:26,359 INFO  [Server] Server Temp Dir:  C:\\jboss-4.0.5.GA\\server\\default\\tmp  18:36:26,359 INFO  [Server] Root Deployment Filename: jboss-service.xml  18:36:26,734 INFO  [ServerInfo] Java version: 1.5.0_06,Sun Microsystems Inc.  18:36:26,734 INFO  [ServerInfo] Java VM: Java HotSpot(TM) Client VM  1.5.0_06-b05,Sun Microsystems Inc.  18:36:26,734 INFO  [ServerInfo] OS-System: Windows XP 5.1,x86  18:36:30,359 INFO  [Server] Core system initialized  18:36:31,781 INFO  [Log4jService$URLWatchTimerTask] Configuring from URL:  resource:log4j.xml  18:36:33,140 INFO  [SocketServerInvoker] Invoker started for locator:  InvokerLocator [socket://192.168.250.29:3873/]  18:36:33,687 INFO  [AspectDeployer] Deployed AOP:  file:/C:/jboss-4.0.5.GA/server/default/deploy/ejb3-interceptors-aop.xml  18:36:35,234 INFO  [AspectDeployer] Deployed AOP:  file:/C:/jboss-4.0.5.GA/server/default/deploy/jboss-portal.sar/portal-aop.xm  l  18:36:40,734 INFO  [WebService] Using RMI server codebase:  http://Santex29:8083/  18:36:42,906 INFO  [Embedded] Catalina naming disabled  18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set  in the classpath.", "Will load the default rule set.", "18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set  in the classpath.", "Will load the default rule set.", "18:36:43,375 INFO  [Http11BaseProtocol] Initializing Coyote HTTP/1.1 on  http-0.0.0.0-8080  18:36:43,375 INFO  [Catalina] Initialization processed in 391 ms  18:36:43,375 INFO  [StandardService] Starting service jboss.web  18:36:43,375 INFO  [StandardEngine] Starting Servlet Engine: Apache  Tomcat/5.5.20  18:36:43,421 INFO  [StandardHost] XML validation disabled  18:36:43,453 INFO  [Catalina] Server startup in 78 ms  18:36:43,593 INFO  [TomcatDeployer] deploy, ctxPath=/portal-cms,  warUrl=.../tmp/deploy/tmp51467portal-cms-exp.war/  18:36:44,000 INFO  [WebappLoader] Dual registration of jndi stream handler:  factory already defined  18:36:44,156 ERROR [[/portal-cms]] Error configuring application listener of  class org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl  java.lang.ClassNotFoundException:  org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1355)          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1201)          at  org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:  3711)          at  org.apache.catalina.core.StandardContext.start(StandardContext.java:4211)          at  org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:7  59)          at  org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)          at  org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.apache.catalina.core.StandardContext.init(StandardContext.java:5052)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeployInternal(TomcatDeployer  .java:297)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeploy(TomcatDeployer.java:10  3)          at  org.jboss.web.AbstractWebDeployer.start(AbstractWebDeployer.java:371)          at org.jboss.web.WebModule.startModule(WebModule.java:83)          at org.jboss.web.WebModule.startService(WebModule.java:61)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Will load the default rule set.", "18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set  in the classpath.", "Will load the default rule set.", "18:36:43,375 INFO  [Http11BaseProtocol] Initializing Coyote HTTP/1.1 on  http-0.0.0.0-8080  18:36:43,375 INFO  [Catalina] Initialization processed in 391 ms  18:36:43,375 INFO  [StandardService] Starting service jboss.web  18:36:43,375 INFO  [StandardEngine] Starting Servlet Engine: Apache  Tomcat/5.5.20  18:36:43,421 INFO  [StandardHost] XML validation disabled  18:36:43,453 INFO  [Catalina] Server startup in 78 ms  18:36:43,593 INFO  [TomcatDeployer] deploy, ctxPath=/portal-cms,  warUrl=.../tmp/deploy/tmp51467portal-cms-exp.war/  18:36:44,000 INFO  [WebappLoader] Dual registration of jndi stream handler:  factory already defined  18:36:44,156 ERROR [[/portal-cms]] Error configuring application listener of  class org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl  java.lang.ClassNotFoundException:  org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1355)          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1201)          at  org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:  3711)          at  org.apache.catalina.core.StandardContext.start(StandardContext.java:4211)          at  org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:7  59)          at  org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)          at  org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.apache.catalina.core.StandardContext.init(StandardContext.java:5052)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeployInternal(TomcatDeployer  .java:297)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeploy(TomcatDeployer.java:10  3)          at  org.jboss.web.AbstractWebDeployer.start(AbstractWebDeployer.java:371)          at org.jboss.web.WebModule.startModule(WebModule.java:83)          at org.jboss.web.WebModule.startService(WebModule.java:61)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy98.start(Unknown Source)          at  org.jboss.web.AbstractWebContainer.start(AbstractWebContainer.java:466)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.system.InterceptorServiceMBeanSupport.invokeNext(InterceptorServic  eMBeanSupport.java:238)          at  org.jboss.ws.integration.jboss.DeployerInterceptor.start(DeployerInterceptor  .java:92)          at  org.jboss.deployment.SubDeployerInterceptorSupport$XMBeanInterceptor.start(S  ubDeployerInterceptorSupport.java:188)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:95)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy99.start(Unknown Source)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at  org.jboss.deployment.MainDeployer.addDeployer(MainDeployer.java:368)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy89.addDeployer(Unknown Source)          at org.jboss.web.tomcat.tc5.Tomcat5.startService(Tomcat5.java:506)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["18:36:42,984 INFO  [ClusterRuleSetFactory] Unable to find a cluster rule set  in the classpath.", "Will load the default rule set.", "18:36:43,375 INFO  [Http11BaseProtocol] Initializing Coyote HTTP/1.1 on  http-0.0.0.0-8080  18:36:43,375 INFO  [Catalina] Initialization processed in 391 ms  18:36:43,375 INFO  [StandardService] Starting service jboss.web  18:36:43,375 INFO  [StandardEngine] Starting Servlet Engine: Apache  Tomcat/5.5.20  18:36:43,421 INFO  [StandardHost] XML validation disabled  18:36:43,453 INFO  [Catalina] Server startup in 78 ms  18:36:43,593 INFO  [TomcatDeployer] deploy, ctxPath=/portal-cms,  warUrl=.../tmp/deploy/tmp51467portal-cms-exp.war/  18:36:44,000 INFO  [WebappLoader] Dual registration of jndi stream handler:  factory already defined  18:36:44,156 ERROR [[/portal-cms]] Error configuring application listener of  class org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl  java.lang.ClassNotFoundException:  org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1355)          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1201)          at  org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:  3711)          at  org.apache.catalina.core.StandardContext.start(StandardContext.java:4211)          at  org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:7  59)          at  org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)          at  org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.apache.catalina.core.StandardContext.init(StandardContext.java:5052)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeployInternal(TomcatDeployer  .java:297)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeploy(TomcatDeployer.java:10  3)          at  org.jboss.web.AbstractWebDeployer.start(AbstractWebDeployer.java:371)          at org.jboss.web.WebModule.startModule(WebModule.java:83)          at org.jboss.web.WebModule.startService(WebModule.java:61)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy98.start(Unknown Source)          at  org.jboss.web.AbstractWebContainer.start(AbstractWebContainer.java:466)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.system.InterceptorServiceMBeanSupport.invokeNext(InterceptorServic  eMBeanSupport.java:238)          at  org.jboss.ws.integration.jboss.DeployerInterceptor.start(DeployerInterceptor  .java:92)          at  org.jboss.deployment.SubDeployerInterceptorSupport$XMBeanInterceptor.start(S  ubDeployerInterceptorSupport.java:188)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:95)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy99.start(Unknown Source)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at  org.jboss.deployment.MainDeployer.addDeployer(MainDeployer.java:368)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy89.addDeployer(Unknown Source)          at org.jboss.web.tomcat.tc5.Tomcat5.startService(Tomcat5.java:506)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.deployment.SubDeployerInterceptor.invokeNext(SubDeployerIntercepto  r.java:124)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:109)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy4.start(Unknown Source)          at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)          at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy6.deploy(Unknown Source)          at  org.jboss.deployment.scanner.URLDeploymentScanner.deploy(URLDeploymentScanne  r.java:421)          at  org.jboss.deployment.scanner.URLDeploymentScanner.scan(URLDeploymentScanner."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Will load the default rule set.", "18:36:43,375 INFO  [Http11BaseProtocol] Initializing Coyote HTTP/1.1 on  http-0.0.0.0-8080  18:36:43,375 INFO  [Catalina] Initialization processed in 391 ms  18:36:43,375 INFO  [StandardService] Starting service jboss.web  18:36:43,375 INFO  [StandardEngine] Starting Servlet Engine: Apache  Tomcat/5.5.20  18:36:43,421 INFO  [StandardHost] XML validation disabled  18:36:43,453 INFO  [Catalina] Server startup in 78 ms  18:36:43,593 INFO  [TomcatDeployer] deploy, ctxPath=/portal-cms,  warUrl=.../tmp/deploy/tmp51467portal-cms-exp.war/  18:36:44,000 INFO  [WebappLoader] Dual registration of jndi stream handler:  factory already defined  18:36:44,156 ERROR [[/portal-cms]] Error configuring application listener of  class org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl  java.lang.ClassNotFoundException:  org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1355)          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1201)          at  org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:  3711)          at  org.apache.catalina.core.StandardContext.start(StandardContext.java:4211)          at  org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:7  59)          at  org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)          at  org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.apache.catalina.core.StandardContext.init(StandardContext.java:5052)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeployInternal(TomcatDeployer  .java:297)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeploy(TomcatDeployer.java:10  3)          at  org.jboss.web.AbstractWebDeployer.start(AbstractWebDeployer.java:371)          at org.jboss.web.WebModule.startModule(WebModule.java:83)          at org.jboss.web.WebModule.startService(WebModule.java:61)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy98.start(Unknown Source)          at  org.jboss.web.AbstractWebContainer.start(AbstractWebContainer.java:466)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.system.InterceptorServiceMBeanSupport.invokeNext(InterceptorServic  eMBeanSupport.java:238)          at  org.jboss.ws.integration.jboss.DeployerInterceptor.start(DeployerInterceptor  .java:92)          at  org.jboss.deployment.SubDeployerInterceptorSupport$XMBeanInterceptor.start(S  ubDeployerInterceptorSupport.java:188)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:95)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy99.start(Unknown Source)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at  org.jboss.deployment.MainDeployer.addDeployer(MainDeployer.java:368)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy89.addDeployer(Unknown Source)          at org.jboss.web.tomcat.tc5.Tomcat5.startService(Tomcat5.java:506)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.deployment.SubDeployerInterceptor.invokeNext(SubDeployerIntercepto  r.java:124)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:109)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy4.start(Unknown Source)          at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)          at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy6.deploy(Unknown Source)          at  org.jboss.deployment.scanner.URLDeploymentScanner.deploy(URLDeploymentScanne  r.java:421)          at  org.jboss.deployment.scanner.URLDeploymentScanner.scan(URLDeploymentScanner.", "java:634)          at  org.jboss.deployment.scanner.AbstractDeploymentScanner$ScannerThread.doScan(  AbstractDeploymentScanner.java:263)          at  org.jboss.deployment.scanner.AbstractDeploymentScanner.startService(Abstract  DeploymentScanner.java:336)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy98.start(Unknown Source)          at  org.jboss.web.AbstractWebContainer.start(AbstractWebContainer.java:466)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.system.InterceptorServiceMBeanSupport.invokeNext(InterceptorServic  eMBeanSupport.java:238)          at  org.jboss.ws.integration.jboss.DeployerInterceptor.start(DeployerInterceptor  .java:92)          at  org.jboss.deployment.SubDeployerInterceptorSupport$XMBeanInterceptor.start(S  ubDeployerInterceptorSupport.java:188)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:95)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy99.start(Unknown Source)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at  org.jboss.deployment.MainDeployer.addDeployer(MainDeployer.java:368)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy89.addDeployer(Unknown Source)          at org.jboss.web.tomcat.tc5.Tomcat5.startService(Tomcat5.java:506)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.deployment.SubDeployerInterceptor.invokeNext(SubDeployerIntercepto  r.java:124)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:109)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy4.start(Unknown Source)          at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)          at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy6.deploy(Unknown Source)          at  org.jboss.deployment.scanner.URLDeploymentScanner.deploy(URLDeploymentScanne  r.java:421)          at  org.jboss.deployment.scanner.URLDeploymentScanner.scan(URLDeploymentScanner.", "java:634)          at  org.jboss.deployment.scanner.AbstractDeploymentScanner$ScannerThread.doScan(  AbstractDeploymentScanner.java:263)          at  org.jboss.deployment.scanner.AbstractDeploymentScanner.startService(Abstract  DeploymentScanner.java:336)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy4.start(Unknown Source)          at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:766)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy5.deploy(Unknown Source)          at org.jboss.system.server.ServerImpl.doStart(ServerImpl.java:482)          at org.jboss.system.server.ServerImpl.start(ServerImpl.java:362)          at org.jboss.Main.boot(Main.java:200)          at org.jboss.Main$1.run(Main.java:490)          at java.lang.Thread.run(Thread.java:595)  [/code]   should I copy Trinidad into default/lib path of jboss???", "Thanks in advance     Juan Giovanolli  System Developer  C\ufffdrdoba \ufffd Argentina      --  Matthias Wessendorf http://tinyurl.com/fmywh  further stuff: blog: http://jroller.com/page/mwessendorf mail: mwessendorf-at-gmail-dot-com "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi again and sorry for asking so frequently,  can you tell me, what would be the steps to provide trinidad-skinning  for a custom component?", "which files would I have to touch (xss stylesheets, SkinFactory, ...)?", "Is there a quick way to achieve this?", "thanks in advance!", "-clem  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello, I am attempting to mavenize a .NET project we have and am getting this build error when attempting to use the auto-generated pom file the Npanday VS plugin made:  NPANDAY-1005-0001: Error copying dependency VBIDE:VBIDE:com_reference:{0002E157-0000-0000-C000-000000000046}-5.3-0:5.3.0.0:compile File C:\\Users\\MyUser\\.m2\\repository\\VBIDE\\VBIDE\\5.3.0.0\\VBIDE-5.3.0.0-{0002E157-0000-0000-C000-000000000046}-5.3-0.dll does not exist Downloading: http://m2.myCompany.net/nexus/content/groups/public//VBIDE/VBIDE/5.3.0.0/VBIDE-5.3.0.0-{0002E157-0000-0000-C000-000000000046}-5.3-0.com_reference  Unable to find resource 'VBIDE:VBIDE:com_reference:{0002E157-0000-0000-C000-000000000046}-5.3-0:5.3.0.0' in repository central (http://central)  NPANDAY-181-121:  Problem in resolving assembly: VBIDE:VBIDE:com_reference:{0002E157-0000-0000-C000-000000000046}-5.3-0:5.3.0.0:compile, Message = Unable to download the artifact from any repository  This appears to be related to a dependency in the pom file: <dependency       <groupIdVBIDE</groupId       <artifactIdVBIDE</artifactId       <version5.3.0.0</version       <typecom_reference</type       <classifier{0002E157-0000-0000-C000-000000000046}-5.3-0</classifier     </dependency  So it seems that basically my company's repository does not have this VBIDE artifact, no surpise there I suppose, this is the first time anyone in our company has attempted to mavenize a .net project.", "However I am not able to find another repository where VBIDE exists (http://mvnrepository.com/ does not appear to know what VBIDE is)  It is strange that the pom is advising that I need it but I am able to compile the project in Visual Studio.", "--  *Kent Johnston** **Developer*  *Lixar I.T.", "Inc.** *T: 613.785.3805 F: 613.722.5297 kjohnston@lixar.com www.lixar.com  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When I was trying different deployment options on the drawing board, one particular config stood out when co-locating DNs & Shard-servers...  1.", "Run Data-Nodes & Shard-servers co-located\u2026 2.", "Make sure all such machines are placed in one rack [Say RackA]\u2026 3.", "Start Data-Nodes alone {without Shard-servers} in a separate RackB\u2026  With such a config in place, hadoop places the 1st write-copy locally in RackA.", "2nd & 3rd Copies will be placed in RackB."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Run Data-Nodes & Shard-servers co-located\u2026 2.", "Make sure all such machines are placed in one rack [Say RackA]\u2026 3.", "Start Data-Nodes alone {without Shard-servers} in a separate RackB\u2026  With such a config in place, hadoop places the 1st write-copy locally in RackA.", "2nd & 3rd Copies will be placed in RackB.", "RackA machines can use better disks like SSDs or SAS15k drives with lower storage capacity."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Make sure all such machines are placed in one rack [Say RackA]\u2026 3.", "Start Data-Nodes alone {without Shard-servers} in a separate RackB\u2026  With such a config in place, hadoop places the 1st write-copy locally in RackA.", "2nd & 3rd Copies will be placed in RackB.", "RackA machines can use better disks like SSDs or SAS15k drives with lower storage capacity.", "RackB machines will be commodity/JBOD machines with high storage capacity [like a 2*4TB 7.2k SATA etc\u2026]  Reads will go via shard-servers and most of the time access local-but-excellent disk sub-system  Writes will experience bit of latency because of 2 racks but write-load on shard-servers will be much-less {No 2nd & 3rd copy to be written}  Apologies if this is basic stuff for the community."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Start Data-Nodes alone {without Shard-servers} in a separate RackB\u2026  With such a config in place, hadoop places the 1st write-copy locally in RackA.", "2nd & 3rd Copies will be placed in RackB.", "RackA machines can use better disks like SSDs or SAS15k drives with lower storage capacity.", "RackB machines will be commodity/JBOD machines with high storage capacity [like a 2*4TB 7.2k SATA etc\u2026]  Reads will go via shard-servers and most of the time access local-but-excellent disk sub-system  Writes will experience bit of latency because of 2 racks but write-load on shard-servers will be much-less {No 2nd & 3rd copy to be written}  Apologies if this is basic stuff for the community.", "-- Ravi  On Mon, Mar 2, 2015 at 11:31 AM, Ravikumar Govindarajan < ravikumar.govindarajan@gmail.com wrote:   Many thanks Aaron\u2026   Ok that sounds about right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2nd & 3rd Copies will be placed in RackB.", "RackA machines can use better disks like SSDs or SAS15k drives with lower storage capacity.", "RackB machines will be commodity/JBOD machines with high storage capacity [like a 2*4TB 7.2k SATA etc\u2026]  Reads will go via shard-servers and most of the time access local-but-excellent disk sub-system  Writes will experience bit of latency because of 2 racks but write-load on shard-servers will be much-less {No 2nd & 3rd copy to be written}  Apologies if this is basic stuff for the community.", "-- Ravi  On Mon, Mar 2, 2015 at 11:31 AM, Ravikumar Govindarajan < ravikumar.govindarajan@gmail.com wrote:   Many thanks Aaron\u2026   Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["RackA machines can use better disks like SSDs or SAS15k drives with lower storage capacity.", "RackB machines will be commodity/JBOD machines with high storage capacity [like a 2*4TB 7.2k SATA etc\u2026]  Reads will go via shard-servers and most of the time access local-but-excellent disk sub-system  Writes will experience bit of latency because of 2 racks but write-load on shard-servers will be much-less {No 2nd & 3rd copy to be written}  Apologies if this is basic stuff for the community.", "-- Ravi  On Mon, Mar 2, 2015 at 11:31 AM, Ravikumar Govindarajan < ravikumar.govindarajan@gmail.com wrote:   Many thanks Aaron\u2026   Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc.", "Fields will be around 20-25.."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["RackB machines will be commodity/JBOD machines with high storage capacity [like a 2*4TB 7.2k SATA etc\u2026]  Reads will go via shard-servers and most of the time access local-but-excellent disk sub-system  Writes will experience bit of latency because of 2 racks but write-load on shard-servers will be much-less {No 2nd & 3rd copy to be written}  Apologies if this is basic stuff for the community.", "-- Ravi  On Mon, Mar 2, 2015 at 11:31 AM, Ravikumar Govindarajan < ravikumar.govindarajan@gmail.com wrote:   Many thanks Aaron\u2026   Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc.", "Fields will be around 20-25..", "But majority of searches happen on only 5-6  fields."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- Ravi  On Mon, Mar 2, 2015 at 11:31 AM, Ravikumar Govindarajan < ravikumar.govindarajan@gmail.com wrote:   Many thanks Aaron\u2026   Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc.", "Fields will be around 20-25..", "But majority of searches happen on only 5-6  fields.", "Term count ~= 2.5 million."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This will greatly depend on your data, how  many fields, how many terms, etc.", "Fields will be around 20-25..", "But majority of searches happen on only 5-6  fields.", "Term count ~= 2.5 million.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps    We are using latest version of 1.7."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Fields will be around 20-25..", "But majority of searches happen on only 5-6  fields.", "Term count ~= 2.5 million.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps    We are using latest version of 1.7.", "Actually we imported around 2TB of  data as dry-run with just 16GB heap without major GC issues using good old  CMS."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But majority of searches happen on only 5-6  fields.", "Term count ~= 2.5 million.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps    We are using latest version of 1.7.", "Actually we imported around 2TB of  data as dry-run with just 16GB heap without major GC issues using good old  CMS.", "There is no sorting/faceting during searches also."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Term count ~= 2.5 million.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps    We are using latest version of 1.7.", "Actually we imported around 2TB of  data as dry-run with just 16GB heap without major GC issues using good old  CMS.", "There is no sorting/faceting during searches also.", "My reluctance stems  from the fact that I am not quite familiar with G1 :)   I am favouring more for write-thru cache [at-least around 50Gb] rather  than read-cache because we have a lot of free RAM available & I feel read  cache is going to use very less RAM."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Actually we imported around 2TB of  data as dry-run with just 16GB heap without major GC issues using good old  CMS.", "There is no sorting/faceting during searches also.", "My reluctance stems  from the fact that I am not quite familiar with G1 :)   I am favouring more for write-thru cache [at-least around 50Gb] rather  than read-cache because we have a lot of free RAM available & I feel read  cache is going to use very less RAM.", "But I am not sure about this.", "Any  pointers will greatly help."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is no sorting/faceting during searches also.", "My reluctance stems  from the fact that I am not quite familiar with G1 :)   I am favouring more for write-thru cache [at-least around 50Gb] rather  than read-cache because we have a lot of free RAM available & I feel read  cache is going to use very less RAM.", "But I am not sure about this.", "Any  pointers will greatly help.", "I would also recommend that you increase the  block cache buffer and file buffer sizes from 8K to 64K    This is one issue we faced during dry-run."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But I am not sure about this.", "Any  pointers will greatly help.", "I would also recommend that you increase the  block cache buffer and file buffer sizes from 8K to 64K    This is one issue we faced during dry-run.", "Marking-up from 8K to 16K  solved the issue.", "I thought 32K must be a good fit for us."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would also recommend that you increase the  block cache buffer and file buffer sizes from 8K to 64K    This is one issue we faced during dry-run.", "Marking-up from 8K to 16K  solved the issue.", "I thought 32K must be a good fit for us.", "Will surely  explore this\u2026   Thanks again for helping out   On Sat, Feb 28, 2015 at 3:04 AM, Aaron McCurry <amccurry@gmail.com wrote:   On Fri, Feb 27, 2015 at 1:29 AM, Ravikumar Govindarajan <  ravikumar.govindarajan@gmail.com wrote:    Hi,     I need a general guidance on number of machines/shards required for our   first blur set-up     Some data as follows     1.", "Shard-Server Config : 128GB RAM, 16-core dual socket with   hyper-threading."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I thought 32K must be a good fit for us.", "Will surely  explore this\u2026   Thanks again for helping out   On Sat, Feb 28, 2015 at 3:04 AM, Aaron McCurry <amccurry@gmail.com wrote:   On Fri, Feb 27, 2015 at 1:29 AM, Ravikumar Govindarajan <  ravikumar.govindarajan@gmail.com wrote:    Hi,     I need a general guidance on number of machines/shards required for our   first blur set-up     Some data as follows     1.", "Shard-Server Config : 128GB RAM, 16-core dual socket with   hyper-threading.", "32 procs   2.", "Total dataset size: 10TB."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Will surely  explore this\u2026   Thanks again for helping out   On Sat, Feb 28, 2015 at 3:04 AM, Aaron McCurry <amccurry@gmail.com wrote:   On Fri, Feb 27, 2015 at 1:29 AM, Ravikumar Govindarajan <  ravikumar.govindarajan@gmail.com wrote:    Hi,     I need a general guidance on number of machines/shards required for our   first blur set-up     Some data as follows     1.", "Shard-Server Config : 128GB RAM, 16-core dual socket with   hyper-threading.", "32 procs   2.", "Total dataset size: 10TB.", "With rep-factor=3, total cluster-size=30TB."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Shard-Server Config : 128GB RAM, 16-core dual socket with   hyper-threading.", "32 procs   2.", "Total dataset size: 10TB.", "With rep-factor=3, total cluster-size=30TB.", "Pre-populated via       MR or Thrift...   3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Total dataset size: 10TB.", "With rep-factor=3, total cluster-size=30TB.", "Pre-populated via       MR or Thrift...   3.", "We receive very less queries per minute [600-900 queries].", "But the   response times for       every query must be <=150 ms     Initially we thought we can create 500 shards each of 20GB size with  around   20 machines."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["With rep-factor=3, total cluster-size=30TB.", "Pre-populated via       MR or Thrift...   3.", "We receive very less queries per minute [600-900 queries].", "But the   response times for       every query must be <=150 ms     Initially we thought we can create 500 shards each of 20GB size with  around   20 machines.", "Ok that sounds about right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Pre-populated via       MR or Thrift...   3.", "We receive very less queries per minute [600-900 queries].", "But the   response times for       every query must be <=150 ms     Initially we thought we can create 500 shards each of 20GB size with  around   20 machines.", "Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We receive very less queries per minute [600-900 queries].", "But the   response times for       every query must be <=150 ms     Initially we thought we can create 500 shards each of 20GB size with  around   20 machines.", "Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc.", "Can each shard-server machine with above specs handle 25 shards?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But the   response times for       every query must be <=150 ms     Initially we thought we can create 500 shards each of 20GB size with  around   20 machines.", "Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc.", "Can each shard-server machine with above specs handle 25 shards?", "Is  such a   configuration over-utilized/under-utilized?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Ok that sounds about right.", "This will greatly depend on your data, how  many fields, how many terms, etc.", "Can each shard-server machine with above specs handle 25 shards?", "Is  such a   configuration over-utilized/under-utilized?", "Again, it likely depends."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Can each shard-server machine with above specs handle 25 shards?", "Is  such a   configuration over-utilized/under-utilized?", "Again, it likely depends.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps.", "Whatever is leftover can  be  allocated to the block cache."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps.", "Whatever is leftover can  be  allocated to the block cache.", "I would also recommend that you increase  the  block cache buffer and file buffer sizes from 8K to 64K.", "This will also  decrease heap pressure for the number of entries in the block cache lru  map.", "How do folks run in production."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would also recommend that you increase  the  block cache buffer and file buffer sizes from 8K to 64K.", "This will also  decrease heap pressure for the number of entries in the block cache lru  map.", "How do folks run in production.", "Any numbers/pointers will be really  helpful     Generally large shard servers (like the ones you are suggesting) with a  few  controllers (6-12) are typical setups.", "If I can provide more details I will follow up."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This will also  decrease heap pressure for the number of entries in the block cache lru  map.", "How do folks run in production.", "Any numbers/pointers will be really  helpful     Generally large shard servers (like the ones you are suggesting) with a  few  controllers (6-12) are typical setups.", "If I can provide more details I will follow up.", "Aaron       --   Ravi       "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Bruno,  my comment is (hopefully) related to the looping problem earlier mentioned in your test, for further info please take a look into the issue, where I just added further infos see: https://issues.apache.org/jira/browse/ODFTOOLKIT-388?focusedCommentId=13969480&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13969480  PS: FOR EDITING ODF To easily edited the content.xml of a file I suggest to use JEdit, see www.jedit.org using the Archive plugin http://plugins.jedit.org/plugins/?Archive (after pressing open file, select an ODF and before pressing OPEN button, open the plugin dialog and select a file within the ODF zip).", "After that I use the http://plugins.jedit.org/plugins/?XML to indent the XML.", "(the latter can be mapped to a short-cut making work faster).", "The big advantage of this approach you may edit and save the embedded XML without unzip and zip all the time!", "Regards, Svante  Am 15.04.2014 14:08, schrieb Bruno Girin:  Hi Svante,   I'm happy to help with this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["After that I use the http://plugins.jedit.org/plugins/?XML to indent the XML.", "(the latter can be mapped to a short-cut making work faster).", "The big advantage of this approach you may edit and save the embedded XML without unzip and zip all the time!", "Regards, Svante  Am 15.04.2014 14:08, schrieb Bruno Girin:  Hi Svante,   I'm happy to help with this.", "However, I'm not sure how that relates to my  problem: I am just trying to read a spreadsheet's content by iterating over  tables, rows and cells in that spreadsheet not knowing how many of those  items I have when I open the file."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards, Svante  Am 15.04.2014 14:08, schrieb Bruno Girin:  Hi Svante,   I'm happy to help with this.", "However, I'm not sure how that relates to my  problem: I am just trying to read a spreadsheet's content by iterating over  tables, rows and cells in that spreadsheet not knowing how many of those  items I have when I open the file.", "From Nick's comment, it looks like my code is not iterating over the  spreadsheet properly so I would welcome any suggestion as to how I should  do that.", "In short, what I'm trying to do is this:   open spreadsheet  for table : spreadsheet {    for row : table {      for cell : row {        store the value in a completely separate structure      }    }  }    Cheers,   Bruno     On 15 April 2014 12:45, Svante Schubert <svante.schubert@gmail.com wrote:   I am still working on a patch for fixing the performance problem for  spreadsheets.", "But again this will come a little later this year with a  major contribution and major refactoring is required on my side before  submittable."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, I'm not sure how that relates to my  problem: I am just trying to read a spreadsheet's content by iterating over  tables, rows and cells in that spreadsheet not knowing how many of those  items I have when I open the file.", "From Nick's comment, it looks like my code is not iterating over the  spreadsheet properly so I would welcome any suggestion as to how I should  do that.", "In short, what I'm trying to do is this:   open spreadsheet  for table : spreadsheet {    for row : table {      for cell : row {        store the value in a completely separate structure      }    }  }    Cheers,   Bruno     On 15 April 2014 12:45, Svante Schubert <svante.schubert@gmail.com wrote:   I am still working on a patch for fixing the performance problem for  spreadsheets.", "But again this will come a little later this year with a  major contribution and major refactoring is required on my side before  submittable.", "Basically the idea for fix is to separate any functionality for altering  cells into two basic functions to avoid code redundancy:   One function will be selecting the range of cells to be altered (might  be a single cell, row, column, full table or sub-rectangle)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From Nick's comment, it looks like my code is not iterating over the  spreadsheet properly so I would welcome any suggestion as to how I should  do that.", "In short, what I'm trying to do is this:   open spreadsheet  for table : spreadsheet {    for row : table {      for cell : row {        store the value in a completely separate structure      }    }  }    Cheers,   Bruno     On 15 April 2014 12:45, Svante Schubert <svante.schubert@gmail.com wrote:   I am still working on a patch for fixing the performance problem for  spreadsheets.", "But again this will come a little later this year with a  major contribution and major refactoring is required on my side before  submittable.", "Basically the idea for fix is to separate any functionality for altering  cells into two basic functions to avoid code redundancy:   One function will be selecting the range of cells to be altered (might  be a single cell, row, column, full table or sub-rectangle).", "The  altering might be any arbitrary change to be applied on the range's  column/row/cells (e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In short, what I'm trying to do is this:   open spreadsheet  for table : spreadsheet {    for row : table {      for cell : row {        store the value in a completely separate structure      }    }  }    Cheers,   Bruno     On 15 April 2014 12:45, Svante Schubert <svante.schubert@gmail.com wrote:   I am still working on a patch for fixing the performance problem for  spreadsheets.", "But again this will come a little later this year with a  major contribution and major refactoring is required on my side before  submittable.", "Basically the idea for fix is to separate any functionality for altering  cells into two basic functions to avoid code redundancy:   One function will be selecting the range of cells to be altered (might  be a single cell, row, column, full table or sub-rectangle).", "The  altering might be any arbitrary change to be applied on the range's  column/row/cells (e.g.", "\"draw borders around the selected range\" or for  instance \"alter the styles from those cells containing content/format\")."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The  altering might be any arbitrary change to be applied on the range's  column/row/cells (e.g.", "\"draw borders around the selected range\" or for  instance \"alter the styles from those cells containing content/format\").", "This change will be covered by second function (or class with a certain  method/interface a only JDK 8 support Lambda functions, calling a  function with a function as parameter).", "By this split, I could reuse the selection part, which is quite  difficult with all the repeated/coverage of columns/rows and cells.", "Note: The second function will be called for every column/row/cells  within the given range."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This change will be covered by second function (or class with a certain  method/interface a only JDK 8 support Lambda functions, calling a  function with a function as parameter).", "By this split, I could reuse the selection part, which is quite  difficult with all the repeated/coverage of columns/rows and cells.", "Note: The second function will be called for every column/row/cells  within the given range.", "Seems to have quite a good performance in my  current tests..   Best regard,  Svante   Am 15.04.2014 13:22, schrieb Bruno Girin:  Hi Nick,    On 15 April 2014 11:49, Nicholas Evans <nick.evans@inology.nl wrote:   Dear Bruno,   I have tried out your test code and cannot reproduce the exception that  you  get.", "I don't seem to be able to reproduce it either when running it in the ODF  toolkit copy taken from SVN this morning but it hangs instead."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["By this split, I could reuse the selection part, which is quite  difficult with all the repeated/coverage of columns/rows and cells.", "Note: The second function will be called for every column/row/cells  within the given range.", "Seems to have quite a good performance in my  current tests..   Best regard,  Svante   Am 15.04.2014 13:22, schrieb Bruno Girin:  Hi Nick,    On 15 April 2014 11:49, Nicholas Evans <nick.evans@inology.nl wrote:   Dear Bruno,   I have tried out your test code and cannot reproduce the exception that  you  get.", "I don't seem to be able to reproduce it either when running it in the ODF  toolkit copy taken from SVN this morning but it hangs instead.", "This is  the  behaviour I was seeing when I wrote the first implementation using  v0.5-incubating from the Maven repositories; moving to v0.6-incubating  using the .jar directly seemed to fix the hanging problem but triggered  the  exception."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Note: The second function will be called for every column/row/cells  within the given range.", "Seems to have quite a good performance in my  current tests..   Best regard,  Svante   Am 15.04.2014 13:22, schrieb Bruno Girin:  Hi Nick,    On 15 April 2014 11:49, Nicholas Evans <nick.evans@inology.nl wrote:   Dear Bruno,   I have tried out your test code and cannot reproduce the exception that  you  get.", "I don't seem to be able to reproduce it either when running it in the ODF  toolkit copy taken from SVN this morning but it hangs instead.", "This is  the  behaviour I was seeing when I wrote the first implementation using  v0.5-incubating from the Maven repositories; moving to v0.6-incubating  using the .jar directly seemed to fix the hanging problem but triggered  the  exception.", "Obviously what might have happened is that by doing that I  introduced an interfering dependency."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Seems to have quite a good performance in my  current tests..   Best regard,  Svante   Am 15.04.2014 13:22, schrieb Bruno Girin:  Hi Nick,    On 15 April 2014 11:49, Nicholas Evans <nick.evans@inology.nl wrote:   Dear Bruno,   I have tried out your test code and cannot reproduce the exception that  you  get.", "I don't seem to be able to reproduce it either when running it in the ODF  toolkit copy taken from SVN this morning but it hangs instead.", "This is  the  behaviour I was seeing when I wrote the first implementation using  v0.5-incubating from the Maven repositories; moving to v0.6-incubating  using the .jar directly seemed to fix the hanging problem but triggered  the  exception.", "Obviously what might have happened is that by doing that I  introduced an interfering dependency.", "Is there any plan to make ODF Toolkit available in the Maven repositories  again so that client projects can just reference the Maven repo?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't seem to be able to reproduce it either when running it in the ODF  toolkit copy taken from SVN this morning but it hangs instead.", "This is  the  behaviour I was seeing when I wrote the first implementation using  v0.5-incubating from the Maven repositories; moving to v0.6-incubating  using the .jar directly seemed to fix the hanging problem but triggered  the  exception.", "Obviously what might have happened is that by doing that I  introduced an interfering dependency.", "Is there any plan to make ODF Toolkit available in the Maven repositories  again so that client projects can just reference the Maven repo?", "I can load the spreadsheet in without a problem, and can also query the  spreadsheet as expected."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can load the spreadsheet in without a problem, and can also query the  spreadsheet as expected.", "I couldn't get your test to pass because it seems to take a long time to  run.", "Methods like getRowCount() can return much higher values than you  expect (on your test code it returns 1048576 for me), and  getRowByIndex()  is a very slow method for large numbers of rows.", "Right, so what is the best way to iterate over all rows in a table and  all  cells in a row?", "This is a very simple spreadsheet with 1 table, 1 row  and 3  cells in the first row."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Right, so what is the best way to iterate over all rows in a table and  all  cells in a row?", "This is a very simple spreadsheet with 1 table, 1 row  and 3  cells in the first row.", "If getRowCount() and getRowByIndex() are unreliable and slow, should they  be deprecated or at least identified as not safe for general use?", "Are you running this code in a clean project without other dependencies  which might be interfering?", "If not perhaps you could try this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is a very simple spreadsheet with 1 table, 1 row  and 3  cells in the first row.", "If getRowCount() and getRowByIndex() are unreliable and slow, should they  be deprecated or at least identified as not safe for general use?", "Are you running this code in a clean project without other dependencies  which might be interfering?", "If not perhaps you could try this?", "This is very possible as my project also uses Apache POI so there may be  some dependencies that interfere."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If getRowCount() and getRowByIndex() are unreliable and slow, should they  be deprecated or at least identified as not safe for general use?", "Are you running this code in a clean project without other dependencies  which might be interfering?", "If not perhaps you could try this?", "This is very possible as my project also uses Apache POI so there may be  some dependencies that interfere.", "As explained above, I'm currently using  the 0.6 jar files direct but would prefer to just reference the project  in  a Maven repo to let Maven sort out dependencies."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["libffm does not have such feature.", "https://github.com/ycjuan/libffm  Sckit SGD [1] adjust y as follows: \"n_samples / (n_classes * np.bincount(y))\" [1] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html  I think this can easily be achieved using SQL.", "Thanks, Makoto  2019\u5e7410\u670831\u65e5(\u6728) 19:38 Shadi Mari <shadimari@gmail.com:   Hello  I am having extremely imbalanced dataset and trying to find support for class weights in hivemall ffm classifier in specific, however couldnt find any mention in the docs.", "Is this feature supported, otherwise i had to go with negative downsampling.", "Please advise   Thank you  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, I am using Olio java with glassfish 3.0 and configure my database my.cfg following the values with Olio java distribution.", "However, I encounter a problem of Broken pipe when the workloads are addevent or addPerson.", "I set the <threadstart to 100.", "This problem happens randomly (50%) when a benchmark test begins.", "And then I have to manually restart the glassfish server to solve the issue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, I encounter a problem of Broken pipe when the workloads are addevent or addPerson.", "I set the <threadstart to 100.", "This problem happens randomly (50%) when a benchmark test begins.", "And then I have to manually restart the glassfish server to solve the issue.", "Could anyone help on this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I set the <threadstart to 100.", "This problem happens randomly (50%) when a benchmark test begins.", "And then I have to manually restart the glassfish server to solve the issue.", "Could anyone help on this?", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Agreed, was just going to ask the same.", "On Wed, Jun 7, 2017 at 12:34 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Max,   Are you picking this up?", "I see some open PRs from you that are not too  active.", "It would be nice to have a release in 3-4 weeks that also targets  full compatibility with Apache so we can graduate to top level.", "Besides  summer break is getting close and after the summer 1.9.0 is scheduled."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Jun 7, 2017 at 12:34 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Max,   Are you picking this up?", "I see some open PRs from you that are not too  active.", "It would be nice to have a release in 3-4 weeks that also targets  full compatibility with Apache so we can graduate to top level.", "Besides  summer break is getting close and after the summer 1.9.0 is scheduled.", "Cheers  Bolke    On 18 May 2017, at 20:54, Bolke de Bruin <bdbruin@gmail.com wrote:     https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow <  https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow     (See higher up in the thread)     Please make sure to address some of the outstanding Apache issues (see  also quote below):     1."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Besides  summer break is getting close and after the summer 1.9.0 is scheduled.", "Cheers  Bolke    On 18 May 2017, at 20:54, Bolke de Bruin <bdbruin@gmail.com wrote:     https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow <  https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow     (See higher up in the thread)     Please make sure to address some of the outstanding Apache issues (see  also quote below):     1.", "Your name is still mentioned somewhere as author.", "A patch wasn\u2019t  cherry picked earlier for this   2.", "Copyrights 2016-2017 Apache, before Airbnb / you   3."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Cheers  Bolke    On 18 May 2017, at 20:54, Bolke de Bruin <bdbruin@gmail.com wrote:     https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow <  https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow     (See higher up in the thread)     Please make sure to address some of the outstanding Apache issues (see  also quote below):     1.", "Your name is still mentioned somewhere as author.", "A patch wasn\u2019t  cherry picked earlier for this   2.", "Copyrights 2016-2017 Apache, before Airbnb / you   3.", "License file formatting     Otherwise it won\u2019t pass the IPMC."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Your name is still mentioned somewhere as author.", "A patch wasn\u2019t  cherry picked earlier for this   2.", "Copyrights 2016-2017 Apache, before Airbnb / you   3.", "License file formatting     Otherwise it won\u2019t pass the IPMC.", "Quote from the IPMC:   ====     +1, however there's a few issues with the LICENSE file:     - Would be good to list out the locations of each file (or path to a  group   of files) (some have this, and others do not so its hard to follow)   - There's errant /* .. */ around each license declaration, which should  be   removed."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Copyrights 2016-2017 Apache, before Airbnb / you   3.", "License file formatting     Otherwise it won\u2019t pass the IPMC.", "Quote from the IPMC:   ====     +1, however there's a few issues with the LICENSE file:     - Would be good to list out the locations of each file (or path to a  group   of files) (some have this, and others do not so its hard to follow)   - There's errant /* .. */ around each license declaration, which should  be   removed.", "- Missing license bodies for FooTable v2, jQuery Clock Plugin,     Likewise, your NOTICE has copyright 2011-2017, however Airflow hasn't  been   incubating that long.", "If you like, you can give origination notices to  the   original creators here to specify the original copyright dates."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["- Missing license bodies for FooTable v2, jQuery Clock Plugin,     Likewise, your NOTICE has copyright 2011-2017, however Airflow hasn't  been   incubating that long.", "If you like, you can give origination notices to  the   original creators here to specify the original copyright dates.", "I would challenge the podling to see if there's a way to simplify their   LICENSE by instead using npm or some other javascript packaging tool to   build a distribution, rather than shipping the dependencies in the source   release, makes it much easier to use.", "As the podling matures, would be good to see information about the author   switch from an individual to a community (in setup.cfg, its already in   setup.py so may have been a miss)     It would be great to see a binary distribution in the next vote to see  how   that may work, its not clear how to build it from this.", "Likewise, don't   hesitate to clean up your old release artifacts, I downloaded the wrong   artifact at first."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["If you like, you can give origination notices to  the   original creators here to specify the original copyright dates.", "I would challenge the podling to see if there's a way to simplify their   LICENSE by instead using npm or some other javascript packaging tool to   build a distribution, rather than shipping the dependencies in the source   release, makes it much easier to use.", "As the podling matures, would be good to see information about the author   switch from an individual to a community (in setup.cfg, its already in   setup.py so may have been a miss)     It would be great to see a binary distribution in the next vote to see  how   that may work, its not clear how to build it from this.", "Likewise, don't   hesitate to clean up your old release artifacts, I downloaded the wrong   artifact at first.", "====     Bolke."], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["I would challenge the podling to see if there's a way to simplify their   LICENSE by instead using npm or some other javascript packaging tool to   build a distribution, rather than shipping the dependencies in the source   release, makes it much easier to use.", "As the podling matures, would be good to see information about the author   switch from an individual to a community (in setup.cfg, its already in   setup.py so may have been a miss)     It would be great to see a binary distribution in the next vote to see  how   that may work, its not clear how to build it from this.", "Likewise, don't   hesitate to clean up your old release artifacts, I downloaded the wrong   artifact at first.", "====     Bolke.", "On 18 May 2017, at 20:49, Maxime Beauchemin <maximebeauchemin@gmail.com  <mailto:maximebeauchemin@gmail.com wrote:     Chris & Bolke, do you have a TODO list / wiki detailing the step-by-step   process?"], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["As the podling matures, would be good to see information about the author   switch from an individual to a community (in setup.cfg, its already in   setup.py so may have been a miss)     It would be great to see a binary distribution in the next vote to see  how   that may work, its not clear how to build it from this.", "Likewise, don't   hesitate to clean up your old release artifacts, I downloaded the wrong   artifact at first.", "====     Bolke.", "On 18 May 2017, at 20:49, Maxime Beauchemin <maximebeauchemin@gmail.com  <mailto:maximebeauchemin@gmail.com wrote:     Chris & Bolke, do you have a TODO list / wiki detailing the step-by-step   process?", "Max     On Thu, May 18, 2017 at 11:46 AM, Maxime Beauchemin <   maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com wrote:     @Andrewm, we can only assume that the author of each commit in master  on   top of 1.8.1 wants their commits into 1.8.2."], "labels": ["1", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Likewise, don't   hesitate to clean up your old release artifacts, I downloaded the wrong   artifact at first.", "====     Bolke.", "On 18 May 2017, at 20:49, Maxime Beauchemin <maximebeauchemin@gmail.com  <mailto:maximebeauchemin@gmail.com wrote:     Chris & Bolke, do you have a TODO list / wiki detailing the step-by-step   process?", "Max     On Thu, May 18, 2017 at 11:46 AM, Maxime Beauchemin <   maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com wrote:     @Andrewm, we can only assume that the author of each commit in master  on   top of 1.8.1 wants their commits into 1.8.2.", "-------------------------     Ok cool, I'll take this on then, and I'm asking Arthur to see if he  wants   to help / oversee the process."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["====     Bolke.", "On 18 May 2017, at 20:49, Maxime Beauchemin <maximebeauchemin@gmail.com  <mailto:maximebeauchemin@gmail.com wrote:     Chris & Bolke, do you have a TODO list / wiki detailing the step-by-step   process?", "Max     On Thu, May 18, 2017 at 11:46 AM, Maxime Beauchemin <   maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com wrote:     @Andrewm, we can only assume that the author of each commit in master  on   top of 1.8.1 wants their commits into 1.8.2.", "-------------------------     Ok cool, I'll take this on then, and I'm asking Arthur to see if he  wants   to help / oversee the process.", "I'm planning to make 1.8.2 essentially same as 1.8.1 plus the set of   \"cherries\" that we use at Airbnb in production and every bugfix / minor   feature that looks benign to us."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 18 May 2017, at 20:49, Maxime Beauchemin <maximebeauchemin@gmail.com  <mailto:maximebeauchemin@gmail.com wrote:     Chris & Bolke, do you have a TODO list / wiki detailing the step-by-step   process?", "Max     On Thu, May 18, 2017 at 11:46 AM, Maxime Beauchemin <   maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com wrote:     @Andrewm, we can only assume that the author of each commit in master  on   top of 1.8.1 wants their commits into 1.8.2.", "-------------------------     Ok cool, I'll take this on then, and I'm asking Arthur to see if he  wants   to help / oversee the process.", "I'm planning to make 1.8.2 essentially same as 1.8.1 plus the set of   \"cherries\" that we use at Airbnb in production and every bugfix / minor   feature that looks benign to us.", "Given that, we're committing to try  out RC   along with everyone else."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max     On Thu, May 18, 2017 at 11:46 AM, Maxime Beauchemin <   maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com wrote:     @Andrewm, we can only assume that the author of each commit in master  on   top of 1.8.1 wants their commits into 1.8.2.", "-------------------------     Ok cool, I'll take this on then, and I'm asking Arthur to see if he  wants   to help / oversee the process.", "I'm planning to make 1.8.2 essentially same as 1.8.1 plus the set of   \"cherries\" that we use at Airbnb in production and every bugfix / minor   feature that looks benign to us.", "Given that, we're committing to try  out RC   along with everyone else.", "What cadence are we aiming at?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-------------------------     Ok cool, I'll take this on then, and I'm asking Arthur to see if he  wants   to help / oversee the process.", "I'm planning to make 1.8.2 essentially same as 1.8.1 plus the set of   \"cherries\" that we use at Airbnb in production and every bugfix / minor   feature that looks benign to us.", "Given that, we're committing to try  out RC   along with everyone else.", "What cadence are we aiming at?", "What should be the target date for the  RC?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Given that, we're committing to try  out RC   along with everyone else.", "What cadence are we aiming at?", "What should be the target date for the  RC?", "Max     On Thu, May 18, 2017 at 11:29 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Max,     Sounds reasonable.", "For the Release Manager it is really mostly a   management job."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["What cadence are we aiming at?", "What should be the target date for the  RC?", "Max     On Thu, May 18, 2017 at 11:29 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Max,     Sounds reasonable.", "For the Release Manager it is really mostly a   management job.", "Chasing, prioritising etc."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["Max     On Thu, May 18, 2017 at 11:29 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Max,     Sounds reasonable.", "For the Release Manager it is really mostly a   management job.", "Chasing, prioritising etc.", "While it is nice to have a  rm   also being able to run the RCs themselves I don\u2019t think it is an  absolute   requirement.", "Especially, as I think we should trust the community to  test   and then vote."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["For the Release Manager it is really mostly a   management job.", "Chasing, prioritising etc.", "While it is nice to have a  rm   also being able to run the RCs themselves I don\u2019t think it is an  absolute   requirement.", "Especially, as I think we should trust the community to  test   and then vote.", "As mentioned the 1.8.X release series should focus on bug fixes,   performance issue and minor feature updates (UI fixes, fixes to some   hooks/operators)."], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["Chasing, prioritising etc.", "While it is nice to have a  rm   also being able to run the RCs themselves I don\u2019t think it is an  absolute   requirement.", "Especially, as I think we should trust the community to  test   and then vote.", "As mentioned the 1.8.X release series should focus on bug fixes,   performance issue and minor feature updates (UI fixes, fixes to some   hooks/operators).", "1.9.X is for the larger changes."], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["While it is nice to have a  rm   also being able to run the RCs themselves I don\u2019t think it is an  absolute   requirement.", "Especially, as I think we should trust the community to  test   and then vote.", "As mentioned the 1.8.X release series should focus on bug fixes,   performance issue and minor feature updates (UI fixes, fixes to some   hooks/operators).", "1.9.X is for the larger changes.", "So indeed please  keep   1.8.2 simple!"], "labels": ["1", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Especially, as I think we should trust the community to  test   and then vote.", "As mentioned the 1.8.X release series should focus on bug fixes,   performance issue and minor feature updates (UI fixes, fixes to some   hooks/operators).", "1.9.X is for the larger changes.", "So indeed please  keep   1.8.2 simple!", "Fully understand that business priorities can take precedence."], "labels": ["1", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["1.9.X is for the larger changes.", "So indeed please  keep   1.8.2 simple!", "Fully understand that business priorities can take precedence.", "I (and  I   guess Chris as well) were just hoping that also some of the other   committers would chime in.", "Cheers   Bolke       On 18 May 2017, at 20:18, Maxime Beauchemin <  maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com   wrote:     Hey,     Sorry about the delay answering, I wanted to sync up with the Airflow   team   here at Airbnb before I replied here."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["So indeed please  keep   1.8.2 simple!", "Fully understand that business priorities can take precedence.", "I (and  I   guess Chris as well) were just hoping that also some of the other   committers would chime in.", "Cheers   Bolke       On 18 May 2017, at 20:18, Maxime Beauchemin <  maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com   wrote:     Hey,     Sorry about the delay answering, I wanted to sync up with the Airflow   team   here at Airbnb before I replied here.", "Quick note to say that the folks at Airbnb are putting a plan  together   as   to how we can move towards smooth releases with higher confidence in  the   future."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Fully understand that business priorities can take precedence.", "I (and  I   guess Chris as well) were just hoping that also some of the other   committers would chime in.", "Cheers   Bolke       On 18 May 2017, at 20:18, Maxime Beauchemin <  maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com   wrote:     Hey,     Sorry about the delay answering, I wanted to sync up with the Airflow   team   here at Airbnb before I replied here.", "Quick note to say that the folks at Airbnb are putting a plan  together   as   to how we can move towards smooth releases with higher confidence in  the   future.", "That plan involves improving the build/test process as well  as   our   staging infrastructure, possibly enabling progressive rollouts   internally."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Quick note to say that the folks at Airbnb are putting a plan  together   as   to how we can move towards smooth releases with higher confidence in  the   future.", "That plan involves improving the build/test process as well  as   our   staging infrastructure, possibly enabling progressive rollouts   internally.", "For context, the team that works on Airflow at Airbnb is \"Data  Platform\"   and is also on the hook for big chunks of non-Airflow-related   infrastructure work that hit us recently and accounts for more than  the   team's bandwidth at this time.", "Given that, the team doesn't want to   commit   the time/risk to deploy RCs in production in the short term.", "Clearly   Airflow is still a priority for the team, but on the short term we  have   critical things prioritized above that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That plan involves improving the build/test process as well  as   our   staging infrastructure, possibly enabling progressive rollouts   internally.", "For context, the team that works on Airflow at Airbnb is \"Data  Platform\"   and is also on the hook for big chunks of non-Airflow-related   infrastructure work that hit us recently and accounts for more than  the   team's bandwidth at this time.", "Given that, the team doesn't want to   commit   the time/risk to deploy RCs in production in the short term.", "Clearly   Airflow is still a priority for the team, but on the short term we  have   critical things prioritized above that.", "Part of the solution is for us to hire more engineers, and one of the   open   seats is a dedicated role on Airflow tackling things from feature   building   to release management."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For context, the team that works on Airflow at Airbnb is \"Data  Platform\"   and is also on the hook for big chunks of non-Airflow-related   infrastructure work that hit us recently and accounts for more than  the   team's bandwidth at this time.", "Given that, the team doesn't want to   commit   the time/risk to deploy RCs in production in the short term.", "Clearly   Airflow is still a priority for the team, but on the short term we  have   critical things prioritized above that.", "Part of the solution is for us to hire more engineers, and one of the   open   seats is a dedicated role on Airflow tackling things from feature   building   to release management.", "Hopefully we can widen our bandwidth shortly."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Given that, the team doesn't want to   commit   the time/risk to deploy RCs in production in the short term.", "Clearly   Airflow is still a priority for the team, but on the short term we  have   critical things prioritized above that.", "Part of the solution is for us to hire more engineers, and one of the   open   seats is a dedicated role on Airflow tackling things from feature   building   to release management.", "Hopefully we can widen our bandwidth shortly.", "In the meantime, I can commit the time to handle a release, but this   release won't hit production at Airbnb for a little while, which  makes   me   wonder whether it's worth committing the time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Clearly   Airflow is still a priority for the team, but on the short term we  have   critical things prioritized above that.", "Part of the solution is for us to hire more engineers, and one of the   open   seats is a dedicated role on Airflow tackling things from feature   building   to release management.", "Hopefully we can widen our bandwidth shortly.", "In the meantime, I can commit the time to handle a release, but this   release won't hit production at Airbnb for a little while, which  makes   me   wonder whether it's worth committing the time.", "Maybe there's a   Fedora/RHEL-type scenario here (using a cutting-edge community  edition   to   stabilize LTS releases), but we know it's not ideal for Airbnb and  for   the   community."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Part of the solution is for us to hire more engineers, and one of the   open   seats is a dedicated role on Airflow tackling things from feature   building   to release management.", "Hopefully we can widen our bandwidth shortly.", "In the meantime, I can commit the time to handle a release, but this   release won't hit production at Airbnb for a little while, which  makes   me   wonder whether it's worth committing the time.", "Maybe there's a   Fedora/RHEL-type scenario here (using a cutting-edge community  edition   to   stabilize LTS releases), but we know it's not ideal for Airbnb and  for   the   community.", "The end goal is clearly to have steady, high-confidence,   mostly   automated, regular releases and it feels like time is best spent   working in   that direction."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Hopefully we can widen our bandwidth shortly.", "In the meantime, I can commit the time to handle a release, but this   release won't hit production at Airbnb for a little while, which  makes   me   wonder whether it's worth committing the time.", "Maybe there's a   Fedora/RHEL-type scenario here (using a cutting-edge community  edition   to   stabilize LTS releases), but we know it's not ideal for Airbnb and  for   the   community.", "The end goal is clearly to have steady, high-confidence,   mostly   automated, regular releases and it feels like time is best spent   working in   that direction.", "Another option is to make [upcoming] 1.8.2 very simple, as 1.8.1 +  the   few   cherries we run in production already at Airbnb, holding the 50+  extra   commits in master for 1.8.3."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["In the meantime, I can commit the time to handle a release, but this   release won't hit production at Airbnb for a little while, which  makes   me   wonder whether it's worth committing the time.", "Maybe there's a   Fedora/RHEL-type scenario here (using a cutting-edge community  edition   to   stabilize LTS releases), but we know it's not ideal for Airbnb and  for   the   community.", "The end goal is clearly to have steady, high-confidence,   mostly   automated, regular releases and it feels like time is best spent   working in   that direction.", "Another option is to make [upcoming] 1.8.2 very simple, as 1.8.1 +  the   few   cherries we run in production already at Airbnb, holding the 50+  extra   commits in master for 1.8.3.", "This is marginally useful but helps  getting   the release mechanics oiled up."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Maybe there's a   Fedora/RHEL-type scenario here (using a cutting-edge community  edition   to   stabilize LTS releases), but we know it's not ideal for Airbnb and  for   the   community.", "The end goal is clearly to have steady, high-confidence,   mostly   automated, regular releases and it feels like time is best spent   working in   that direction.", "Another option is to make [upcoming] 1.8.2 very simple, as 1.8.1 +  the   few   cherries we run in production already at Airbnb, holding the 50+  extra   commits in master for 1.8.3.", "This is marginally useful but helps  getting   the release mechanics oiled up.", "I'm trying to be as transparent as I can here, and open to discuss  the   different ways we can move forward."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The end goal is clearly to have steady, high-confidence,   mostly   automated, regular releases and it feels like time is best spent   working in   that direction.", "Another option is to make [upcoming] 1.8.2 very simple, as 1.8.1 +  the   few   cherries we run in production already at Airbnb, holding the 50+  extra   commits in master for 1.8.3.", "This is marginally useful but helps  getting   the release mechanics oiled up.", "I'm trying to be as transparent as I can here, and open to discuss  the   different ways we can move forward.", "Max     On Sun, May 14, 2017 at 4:44 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Folks,     With 1.8.1 we have very much improved the reliability airflow,  which is   great as many new features entered 1.8.0 and the gap from 1.7.1 was   huge."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Another option is to make [upcoming] 1.8.2 very simple, as 1.8.1 +  the   few   cherries we run in production already at Airbnb, holding the 50+  extra   commits in master for 1.8.3.", "This is marginally useful but helps  getting   the release mechanics oiled up.", "I'm trying to be as transparent as I can here, and open to discuss  the   different ways we can move forward.", "Max     On Sun, May 14, 2017 at 4:44 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Folks,     With 1.8.1 we have very much improved the reliability airflow,  which is   great as many new features entered 1.8.0 and the gap from 1.7.1 was   huge.", "What is also great is that we are slowly but surely increasing the  test   coverage which mitigates some of the risk of regressions going   forward."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is marginally useful but helps  getting   the release mechanics oiled up.", "I'm trying to be as transparent as I can here, and open to discuss  the   different ways we can move forward.", "Max     On Sun, May 14, 2017 at 4:44 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Folks,     With 1.8.1 we have very much improved the reliability airflow,  which is   great as many new features entered 1.8.0 and the gap from 1.7.1 was   huge.", "What is also great is that we are slowly but surely increasing the  test   coverage which mitigates some of the risk of regressions going   forward.", "As   you know the 1.8.X releases will continue to focus on improved   reliability,   performance improvements and minor feature updates."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm trying to be as transparent as I can here, and open to discuss  the   different ways we can move forward.", "Max     On Sun, May 14, 2017 at 4:44 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Folks,     With 1.8.1 we have very much improved the reliability airflow,  which is   great as many new features entered 1.8.0 and the gap from 1.7.1 was   huge.", "What is also great is that we are slowly but surely increasing the  test   coverage which mitigates some of the risk of regressions going   forward.", "As   you know the 1.8.X releases will continue to focus on improved   reliability,   performance improvements and minor feature updates.", "The 1.9.X  release   cycle, which should start around September, will allow for larger   feature   updates."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max     On Sun, May 14, 2017 at 4:44 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Folks,     With 1.8.1 we have very much improved the reliability airflow,  which is   great as many new features entered 1.8.0 and the gap from 1.7.1 was   huge.", "What is also great is that we are slowly but surely increasing the  test   coverage which mitigates some of the risk of regressions going   forward.", "As   you know the 1.8.X releases will continue to focus on improved   reliability,   performance improvements and minor feature updates.", "The 1.9.X  release   cycle, which should start around September, will allow for larger   feature   updates.", "I expect 1.8.2 not to have too many PRs, so it will be a relatively   simple   release process:     1."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["What is also great is that we are slowly but surely increasing the  test   coverage which mitigates some of the risk of regressions going   forward.", "As   you know the 1.8.X releases will continue to focus on improved   reliability,   performance improvements and minor feature updates.", "The 1.9.X  release   cycle, which should start around September, will allow for larger   feature   updates.", "I expect 1.8.2 not to have too many PRs, so it will be a relatively   simple   release process:     1.", "Apply bug fixes   2."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["As   you know the 1.8.X releases will continue to focus on improved   reliability,   performance improvements and minor feature updates.", "The 1.9.X  release   cycle, which should start around September, will allow for larger   feature   updates.", "I expect 1.8.2 not to have too many PRs, so it will be a relatively   simple   release process:     1.", "Apply bug fixes   2.", "Add performance fixes   3."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["The 1.9.X  release   cycle, which should start around September, will allow for larger   feature   updates.", "I expect 1.8.2 not to have too many PRs, so it will be a relatively   simple   release process:     1.", "Apply bug fixes   2.", "Add performance fixes   3.", "Fix some outstanding Apache requirements (Author, Licensing etc)     The process of creating a distribution has been detailed by Chris  here:   https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow   <   https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow     Now we just need a volunteer (preferably from the committers) to be  the   Release Manager for 1.8.2 :-)."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["I expect 1.8.2 not to have too many PRs, so it will be a relatively   simple   release process:     1.", "Apply bug fixes   2.", "Add performance fixes   3.", "Fix some outstanding Apache requirements (Author, Licensing etc)     The process of creating a distribution has been detailed by Chris  here:   https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow   <   https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow     Now we just need a volunteer (preferably from the committers) to be  the   Release Manager for 1.8.2 :-).", "Who is willing to take this on and make history?"], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["@Bolke, yea, you're right, according to:  https://blogs.apache.org/infra/entry/apache_gains_additional_travis_ci   And can we setup long running services (ie.", "Hadoop, KDC, Presto, MySQL, etc) in the Jenkins environment  I don't think so.", "To my knowledge what Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs) than integration tests.", "Yea, you're right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hadoop, KDC, Presto, MySQL, etc) in the Jenkins environment  I don't think so.", "To my knowledge what Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs) than integration tests.", "Yea, you're right.", "I agree."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["To my knowledge what Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs) than integration tests.", "Yea, you're right.", "I agree.", "I did most integration testing in private environments when RCs were cut."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["the hadoop-qa runs) than integration tests.", "Yea, you're right.", "I agree.", "I did most integration testing in private environments when RCs were cut.", "Travis seems like it might be a good way to go, if we can make it work."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I agree.", "I did most integration testing in private environments when RCs were cut.", "Travis seems like it might be a good way to go, if we can make it work.", "I agree with Li Xuan that failures in Travis can be a pain to replicate--I just suffered through this on another open source project--but it is do-able.", "On Tue, Dec 20, 2016 at 12:13 PM, Li Xuan Ji <xuanji@gmail.com wrote:   Chris, is this the authoritative list of services apache provides to  projects?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I did most integration testing in private environments when RCs were cut.", "Travis seems like it might be a good way to go, if we can make it work.", "I agree with Li Xuan that failures in Travis can be a pain to replicate--I just suffered through this on another open source project--but it is do-able.", "On Tue, Dec 20, 2016 at 12:13 PM, Li Xuan Ji <xuanji@gmail.com wrote:   Chris, is this the authoritative list of services apache provides to  projects?", "https://www.apache.org/dev/services.html   The builds section / page links to https://ci.apache.org/ and lists the  following build services: buildbot, gump, jenkins (the link to continuum is  broken)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Dec 20, 2016 at 12:13 PM, Li Xuan Ji <xuanji@gmail.com wrote:   Chris, is this the authoritative list of services apache provides to  projects?", "https://www.apache.org/dev/services.html   The builds section / page links to https://ci.apache.org/ and lists the  following build services: buildbot, gump, jenkins (the link to continuum is  broken).", "From my brief reading, it seems they use a shared pool of machines  (for all projects) are designed for running unit/integration tests that can  be torn down after running, not long-running services.", "There's no references to Travis on those pages, there are some tickets  related to travis (  https://www.google.ca/search?sitesearch=*.apache.org&q=travis) but I think  they are referring to integration with the public travis-ci.org service  that we already use.", "The page also says that FreeBSD jails and Ubuntu VMs are available,  although I'm not sure what they are normally used for."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://www.apache.org/dev/services.html   The builds section / page links to https://ci.apache.org/ and lists the  following build services: buildbot, gump, jenkins (the link to continuum is  broken).", "From my brief reading, it seems they use a shared pool of machines  (for all projects) are designed for running unit/integration tests that can  be torn down after running, not long-running services.", "There's no references to Travis on those pages, there are some tickets  related to travis (  https://www.google.ca/search?sitesearch=*.apache.org&q=travis) but I think  they are referring to integration with the public travis-ci.org service  that we already use.", "The page also says that FreeBSD jails and Ubuntu VMs are available,  although I'm not sure what they are normally used for.", "One thing I don't really like about the \"official\" infrastructure is  access, eg we can't ssh into the travis-ci.org builds to debug errors, and  the travis build script we have is quite verbose, presumably for debugging  purposes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There's no references to Travis on those pages, there are some tickets  related to travis (  https://www.google.ca/search?sitesearch=*.apache.org&q=travis) but I think  they are referring to integration with the public travis-ci.org service  that we already use.", "The page also says that FreeBSD jails and Ubuntu VMs are available,  although I'm not sure what they are normally used for.", "One thing I don't really like about the \"official\" infrastructure is  access, eg we can't ssh into the travis-ci.org builds to debug errors, and  the travis build script we have is quite verbose, presumably for debugging  purposes.", "On 20 December 2016 at 13:31, Georg Walther <georg.walther@markovian.com  wrote:    Hi,       why would you want these services to be long-running?", "Why not write   integration tests against clean instances of each service or clean   instances with data fixtures on top?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The page also says that FreeBSD jails and Ubuntu VMs are available,  although I'm not sure what they are normally used for.", "One thing I don't really like about the \"official\" infrastructure is  access, eg we can't ssh into the travis-ci.org builds to debug errors, and  the travis build script we have is quite verbose, presumably for debugging  purposes.", "On 20 December 2016 at 13:31, Georg Walther <georg.walther@markovian.com  wrote:    Hi,       why would you want these services to be long-running?", "Why not write   integration tests against clean instances of each service or clean   instances with data fixtures on top?", "Travis offers spinning up a variety of services (   https://docs.travis-ci.com/user/database-setup/) so does wercker (   http://devcenter.wercker.com/docs/services)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One thing I don't really like about the \"official\" infrastructure is  access, eg we can't ssh into the travis-ci.org builds to debug errors, and  the travis build script we have is quite verbose, presumably for debugging  purposes.", "On 20 December 2016 at 13:31, Georg Walther <georg.walther@markovian.com  wrote:    Hi,       why would you want these services to be long-running?", "Why not write   integration tests against clean instances of each service or clean   instances with data fixtures on top?", "Travis offers spinning up a variety of services (   https://docs.travis-ci.com/user/database-setup/) so does wercker (   http://devcenter.wercker.com/docs/services).", "These services are docker containers run off of pre-build Docker images."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 20 December 2016 at 13:31, Georg Walther <georg.walther@markovian.com  wrote:    Hi,       why would you want these services to be long-running?", "Why not write   integration tests against clean instances of each service or clean   instances with data fixtures on top?", "Travis offers spinning up a variety of services (   https://docs.travis-ci.com/user/database-setup/) so does wercker (   http://devcenter.wercker.com/docs/services).", "These services are docker containers run off of pre-build Docker images.", "Best,     Georg       On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com  wrote:      Hi Chris,       Didn\u2019t they offer a dedicated Travis thing as well?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why not write   integration tests against clean instances of each service or clean   instances with data fixtures on top?", "Travis offers spinning up a variety of services (   https://docs.travis-ci.com/user/database-setup/) so does wercker (   http://devcenter.wercker.com/docs/services).", "These services are docker containers run off of pre-build Docker images.", "Best,     Georg       On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com  wrote:      Hi Chris,       Didn\u2019t they offer a dedicated Travis thing as well?", "And can we setup  long    running services (ie."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Travis offers spinning up a variety of services (   https://docs.travis-ci.com/user/database-setup/) so does wercker (   http://devcenter.wercker.com/docs/services).", "These services are docker containers run off of pre-build Docker images.", "Best,     Georg       On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com  wrote:      Hi Chris,       Didn\u2019t they offer a dedicated Travis thing as well?", "And can we setup  long    running services (ie.", "Hadoop, KDC, Presto, MySQL, etc) in the Jenkins    environment, because that would be sufficient I guess?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These services are docker containers run off of pre-build Docker images.", "Best,     Georg       On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com  wrote:      Hi Chris,       Didn\u2019t they offer a dedicated Travis thing as well?", "And can we setup  long    running services (ie.", "Hadoop, KDC, Presto, MySQL, etc) in the Jenkins    environment, because that would be sufficient I guess?", "To my knowledge   what    Apache offers is more suited for unit tests (ie."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Best,     Georg       On Tue, Dec 20, 2016 at 5:18 PM, Bolke de Bruin <bdbruin@gmail.com  wrote:      Hi Chris,       Didn\u2019t they offer a dedicated Travis thing as well?", "And can we setup  long    running services (ie.", "Hadoop, KDC, Presto, MySQL, etc) in the Jenkins    environment, because that would be sufficient I guess?", "To my knowledge   what    Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs)  than    integration tests."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And can we setup  long    running services (ie.", "Hadoop, KDC, Presto, MySQL, etc) in the Jenkins    environment, because that would be sufficient I guess?", "To my knowledge   what    Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs)  than    integration tests.", "What are your thoughts?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["To my knowledge   what    Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs)  than    integration tests.", "What are your thoughts?", "Bolke        Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <  criccomini@apache.org       het volgende geschreven:         Hey Bolke,         Thanks for stepping up.", "The \"traditional\" Apache infra is Jenkins  with   a     pool of machines that they provide."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["the hadoop-qa runs)  than    integration tests.", "What are your thoughts?", "Bolke        Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <  criccomini@apache.org       het volgende geschreven:         Hey Bolke,         Thanks for stepping up.", "The \"traditional\" Apache infra is Jenkins  with   a     pool of machines that they provide.", "That might or might not be    satisfactory     for us (it's certainly antiquated technology)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What are your thoughts?", "Bolke        Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <  criccomini@apache.org       het volgende geschreven:         Hey Bolke,         Thanks for stepping up.", "The \"traditional\" Apache infra is Jenkins  with   a     pool of machines that they provide.", "That might or might not be    satisfactory     for us (it's certainly antiquated technology).", "If we decide we don't   like     it, I think that's OK, as long as we move forward knowing that any   other     testing solution can disappear at any time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Bolke        Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <  criccomini@apache.org       het volgende geschreven:         Hey Bolke,         Thanks for stepping up.", "The \"traditional\" Apache infra is Jenkins  with   a     pool of machines that they provide.", "That might or might not be    satisfactory     for us (it's certainly antiquated technology).", "If we decide we don't   like     it, I think that's OK, as long as we move forward knowing that any   other     testing solution can disappear at any time.", "My 2c."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The \"traditional\" Apache infra is Jenkins  with   a     pool of machines that they provide.", "That might or might not be    satisfactory     for us (it's certainly antiquated technology).", "If we decide we don't   like     it, I think that's OK, as long as we move forward knowing that any   other     testing solution can disappear at any time.", "My 2c.", ":)         Cheers,     Chris         On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <     dan.davydov@airbnb.com.invalid wrote:         This is extremely generous of you!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That might or might not be    satisfactory     for us (it's certainly antiquated technology).", "If we decide we don't   like     it, I think that's OK, as long as we move forward knowing that any   other     testing solution can disappear at any time.", "My 2c.", ":)         Cheers,     Chris         On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <     dan.davydov@airbnb.com.invalid wrote:         This is extremely generous of you!", "I do agree with the approach of    trying     to get funding from Apache and having shared resources (e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If we decide we don't   like     it, I think that's OK, as long as we move forward knowing that any   other     testing solution can disappear at any time.", "My 2c.", ":)         Cheers,     Chris         On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <     dan.davydov@airbnb.com.invalid wrote:         This is extremely generous of you!", "I do agree with the approach of    trying     to get funding from Apache and having shared resources (e.g.", "so that   we     don't depend on any one company or individual for the uptime of the     integration environment, plus so we would have public cloud   integration     potentially)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My 2c.", ":)         Cheers,     Chris         On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <     dan.davydov@airbnb.com.invalid wrote:         This is extremely generous of you!", "I do agree with the approach of    trying     to get funding from Apache and having shared resources (e.g.", "so that   we     don't depend on any one company or individual for the uptime of the     integration environment, plus so we would have public cloud   integration     potentially).", "On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:         Hi,         I have been thinking about an integration test environment."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":)         Cheers,     Chris         On Wed, Dec 14, 2016 at 11:11 AM, Dan Davydov <     dan.davydov@airbnb.com.invalid wrote:         This is extremely generous of you!", "I do agree with the approach of    trying     to get funding from Apache and having shared resources (e.g.", "so that   we     don't depend on any one company or individual for the uptime of the     integration environment, plus so we would have public cloud   integration     potentially).", "On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:         Hi,         I have been thinking about an integration test environment.", "Aside   from     any     technical requirements we need a place to do it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I do agree with the approach of    trying     to get funding from Apache and having shared resources (e.g.", "so that   we     don't depend on any one company or individual for the uptime of the     integration environment, plus so we would have public cloud   integration     potentially).", "On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:         Hi,         I have been thinking about an integration test environment.", "Aside   from     any     technical requirements we need a place to do it.", "I am willing to   offer    a     place in Lab env I am running or to fund an environment in  AWS/GCloud    if     Apache cannot make these kind of resources available."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["so that   we     don't depend on any one company or individual for the uptime of the     integration environment, plus so we would have public cloud   integration     potentially).", "On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:         Hi,         I have been thinking about an integration test environment.", "Aside   from     any     technical requirements we need a place to do it.", "I am willing to   offer    a     place in Lab env I am running or to fund an environment in  AWS/GCloud    if     Apache cannot make these kind of resources available.", "If running in our Lab there is virtually no restriction what we  could    do,     however I will hand select people who have access to this   environment."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Dec 14, 2016 at 1:22 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:         Hi,         I have been thinking about an integration test environment.", "Aside   from     any     technical requirements we need a place to do it.", "I am willing to   offer    a     place in Lab env I am running or to fund an environment in  AWS/GCloud    if     Apache cannot make these kind of resources available.", "If running in our Lab there is virtually no restriction what we  could    do,     however I will hand select people who have access to this   environment.", "I     will also hold ultimate power to remove access from anyone."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am willing to   offer    a     place in Lab env I am running or to fund an environment in  AWS/GCloud    if     Apache cannot make these kind of resources available.", "If running in our Lab there is virtually no restriction what we  could    do,     however I will hand select people who have access to this   environment.", "I     will also hold ultimate power to remove access from anyone.", "I even    might     ask for a confirmation that you will behave when using our property     (don\u2019t     worry won\u2019t cover it with legal wording).", "This is a IAAS service so   we     need     to cover the things we need ourselves, but the upside is we can and   it    is     free."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If running in our Lab there is virtually no restriction what we  could    do,     however I will hand select people who have access to this   environment.", "I     will also hold ultimate power to remove access from anyone.", "I even    might     ask for a confirmation that you will behave when using our property     (don\u2019t     worry won\u2019t cover it with legal wording).", "This is a IAAS service so   we     need     to cover the things we need ourselves, but the upside is we can and   it    is     free.", "We could setup a Gitlab instance that mirrors from Apache a   kicks     off     runners to do testing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I even    might     ask for a confirmation that you will behave when using our property     (don\u2019t     worry won\u2019t cover it with legal wording).", "This is a IAAS service so   we     need     to cover the things we need ourselves, but the upside is we can and   it    is     free.", "We could setup a Gitlab instance that mirrors from Apache a   kicks     off     runners to do testing.", "Downside 1) it might not be entirely Apache    like.", "Sorry cant help that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Downside 1) it might not be entirely Apache    like.", "Sorry cant help that.", "2) there is no guaranteed up time 3) I might   need     to     remove it in the future e.g.", "when I change jobs for example :).", "4)  No     public cloud integration, it\u2019s a private stack after all."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2) there is no guaranteed up time 3) I might   need     to     remove it in the future e.g.", "when I change jobs for example :).", "4)  No     public cloud integration, it\u2019s a private stack after all.", "I can also fund on AWS/GCloud.", "Again, I probably want to have   ultimate     power on access to this environment - it\u2019s my company\u2019s money on  the    line     after all."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["when I change jobs for example :).", "4)  No     public cloud integration, it\u2019s a private stack after all.", "I can also fund on AWS/GCloud.", "Again, I probably want to have   ultimate     power on access to this environment - it\u2019s my company\u2019s money on  the    line     after all.", "Major downside to this is that it is dependent on and    limited     by     the budget I can make available."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["4)  No     public cloud integration, it\u2019s a private stack after all.", "I can also fund on AWS/GCloud.", "Again, I probably want to have   ultimate     power on access to this environment - it\u2019s my company\u2019s money on  the    line     after all.", "Major downside to this is that it is dependent on and    limited     by     the budget I can make available.", "Upside is that it is not company     property."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can also fund on AWS/GCloud.", "Again, I probably want to have   ultimate     power on access to this environment - it\u2019s my company\u2019s money on  the    line     after all.", "Major downside to this is that it is dependent on and    limited     by     the budget I can make available.", "Upside is that it is not company     property.", "Also I personally have less exposure to public cloud environments  due    to     company restrictions."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Again, I probably want to have   ultimate     power on access to this environment - it\u2019s my company\u2019s money on  the    line     after all.", "Major downside to this is that it is dependent on and    limited     by     the budget I can make available.", "Upside is that it is not company     property.", "Also I personally have less exposure to public cloud environments  due    to     company restrictions.", "Are there any other options?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Major downside to this is that it is dependent on and    limited     by     the budget I can make available.", "Upside is that it is not company     property.", "Also I personally have less exposure to public cloud environments  due    to     company restrictions.", "Are there any other options?", "Any thoughts?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Upside is that it is not company     property.", "Also I personally have less exposure to public cloud environments  due    to     company restrictions.", "Are there any other options?", "Any thoughts?", "Bolke                                         --  Im Xuan Ji!   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["IMHO, a DAG run without a start date is non-sensical but is not enforced  That said, our UI allows for the manual creation of DAG Runs without a start date as shown in the images below:      - https://www.dropbox.com/s/3sxcqh04eztpl7p/Screenshot%    202017-02-22%2016.00.40.png?dl=0    <https://www.dropbox.com/s/3sxcqh04eztpl7p/Screenshot%202017-02-22%2016.00.40.png?dl=0    - https://www.dropbox.com/s/4q6rr9dwghag1yy/Screenshot%    202017-02-22%2016.02.22.png?dl=0    <https://www.dropbox.com/s/4q6rr9dwghag1yy/Screenshot%202017-02-22%2016.02.22.png?dl=0   On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin < maximebeauchemin@gmail.com wrote:   Our database may have edge cases that could be associated with running any  previous version that may or may not have been part of an official release.", "Let's see if anyone else reports the issue.", "If no one does, one option is  to release 1.8.0 as is with a comment in the release notes, and have a  future official minor apache release 1.8.1 that would fix these minor  issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one release  cycle?", "Oh, and do you have a documented step by step process for releasing?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let's see if anyone else reports the issue.", "If no one does, one option is  to release 1.8.0 as is with a comment in the release notes, and have a  future official minor apache release 1.8.1 that would fix these minor  issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one release  cycle?", "Oh, and do you have a documented step by step process for releasing?", "I'd like to add the Pypi part to this doc and add committers that are  interested to have rights on the project on Pypi."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If no one does, one option is  to release 1.8.0 as is with a comment in the release notes, and have a  future official minor apache release 1.8.1 that would fix these minor  issues that are not deal breaker.", "@bolke, I'm curious, how long does it take you to go through one release  cycle?", "Oh, and do you have a documented step by step process for releasing?", "I'd like to add the Pypi part to this doc and add committers that are  interested to have rights on the project on Pypi.", "Max   On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:    So it is a database integrity issue?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'd like to add the Pypi part to this doc and add committers that are  interested to have rights on the project on Pypi.", "Max   On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:    So it is a database integrity issue?", "Afaik a start_date should always be   set for a DagRun (create_dagrun) does so  I didn't check the code though.", "Sent from my iPhone      On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.INVALID   wrote:       Should clarify this occurs when a dagrun does not have a start date,  not   a    dag (which makes it even less likely to happen).", "I don't think this is  a    blocker for releasing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Max   On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:    So it is a database integrity issue?", "Afaik a start_date should always be   set for a DagRun (create_dagrun) does so  I didn't check the code though.", "Sent from my iPhone      On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.INVALID   wrote:       Should clarify this occurs when a dagrun does not have a start date,  not   a    dag (which makes it even less likely to happen).", "I don't think this is  a    blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <dan.davydov@airbnb.com   wrote:       I rolled this out in our prod and the webservers failed to load due to    this commit:       [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag    7c94d81c390881643f94d5e3d7d6fb351a445b72       This fixed it:    -                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\" title=\"Start   Date:    {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span    +                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span       This is caused by assuming that all DAGs have start dates set, so a   broken    DAG will take down the whole UI."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sent from my iPhone      On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.INVALID   wrote:       Should clarify this occurs when a dagrun does not have a start date,  not   a    dag (which makes it even less likely to happen).", "I don't think this is  a    blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <dan.davydov@airbnb.com   wrote:       I rolled this out in our prod and the webservers failed to load due to    this commit:       [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag    7c94d81c390881643f94d5e3d7d6fb351a445b72       This fixed it:    -                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\" title=\"Start   Date:    {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span    +                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span       This is caused by assuming that all DAGs have start dates set, so a   broken    DAG will take down the whole UI.", "Not sure if we want to make this a   blocker    for the release or not, I'm guessing for most deployments this would   occur    pretty rarely.", "I'll submit a PR to fix it soon."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't think this is  a    blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <dan.davydov@airbnb.com   wrote:       I rolled this out in our prod and the webservers failed to load due to    this commit:       [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag    7c94d81c390881643f94d5e3d7d6fb351a445b72       This fixed it:    -                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\" title=\"Start   Date:    {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span    +                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span       This is caused by assuming that all DAGs have start dates set, so a   broken    DAG will take down the whole UI.", "Not sure if we want to make this a   blocker    for the release or not, I'm guessing for most deployments this would   occur    pretty rarely.", "I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <  criccomini@apache.org       wrote:       Ack that the vote has already passed, but belated +1 (binding)       On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:       IPMC Voting can be found here:       http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <    http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E       Kind regards,    Bolke       On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com  wrote:       Hello,       Apache Airflow (incubating) 1.8.0 (based on RC4) has been accepted."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <dan.davydov@airbnb.com   wrote:       I rolled this out in our prod and the webservers failed to load due to    this commit:       [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag    7c94d81c390881643f94d5e3d7d6fb351a445b72       This fixed it:    -                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\" title=\"Start   Date:    {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span    +                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span       This is caused by assuming that all DAGs have start dates set, so a   broken    DAG will take down the whole UI.", "Not sure if we want to make this a   blocker    for the release or not, I'm guessing for most deployments this would   occur    pretty rarely.", "I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <  criccomini@apache.org       wrote:       Ack that the vote has already passed, but belated +1 (binding)       On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:       IPMC Voting can be found here:       http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <    http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E       Kind regards,    Bolke       On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com  wrote:       Hello,       Apache Airflow (incubating) 1.8.0 (based on RC4) has been accepted.", "9 \u201c+1\u201d votes received:       - Maxime Beauchemin (binding)    - Arthur Wiedmer (binding)    - Dan Davydov (binding)    - Jeremiah Lowin (binding)    - Siddharth Anand (binding)    - Alex van Boxel (binding)    - Bolke de Bruin (binding)       - Jayesh Senjaliya (non-binding)    - Yi (non-binding)       Vote thread (start):    http://mail-archives.apache.org/mod_mbox/incubator-    airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-    6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["Not sure if we want to make this a   blocker    for the release or not, I'm guessing for most deployments this would   occur    pretty rarely.", "I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <  criccomini@apache.org       wrote:       Ack that the vote has already passed, but belated +1 (binding)       On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:       IPMC Voting can be found here:       http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <    http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E       Kind regards,    Bolke       On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com  wrote:       Hello,       Apache Airflow (incubating) 1.8.0 (based on RC4) has been accepted.", "9 \u201c+1\u201d votes received:       - Maxime Beauchemin (binding)    - Arthur Wiedmer (binding)    - Dan Davydov (binding)    - Jeremiah Lowin (binding)    - Siddharth Anand (binding)    - Alex van Boxel (binding)    - Bolke de Bruin (binding)       - Jayesh Senjaliya (non-binding)    - Yi (non-binding)       Vote thread (start):    http://mail-archives.apache.org/mod_mbox/incubator-    airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-    6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-    092E-48D2-AA0F-    15F44376A8FF@gmail.com%3E       Next steps:    1) will start the voting process at the IPMC mailinglist."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["I'll submit a PR to fix it soon.", "On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <  criccomini@apache.org       wrote:       Ack that the vote has already passed, but belated +1 (binding)       On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:       IPMC Voting can be found here:       http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <    http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E       Kind regards,    Bolke       On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com  wrote:       Hello,       Apache Airflow (incubating) 1.8.0 (based on RC4) has been accepted.", "9 \u201c+1\u201d votes received:       - Maxime Beauchemin (binding)    - Arthur Wiedmer (binding)    - Dan Davydov (binding)    - Jeremiah Lowin (binding)    - Siddharth Anand (binding)    - Alex van Boxel (binding)    - Bolke de Bruin (binding)       - Jayesh Senjaliya (non-binding)    - Yi (non-binding)       Vote thread (start):    http://mail-archives.apache.org/mod_mbox/incubator-    airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-    6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-    092E-48D2-AA0F-    15F44376A8FF@gmail.com%3E       Next steps:    1) will start the voting process at the IPMC mailinglist.", "I do  expect    some changes to be required mostly in documentation maybe a license   here    and there."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["On Tue, Feb 21, 2017 at 9:49 AM, Chris Riccomini <  criccomini@apache.org       wrote:       Ack that the vote has already passed, but belated +1 (binding)       On Tue, Feb 21, 2017 at 7:42 AM, Bolke de Bruin <bdbruin@gmail.com    wrote:       IPMC Voting can be found here:       http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3e <    http://mail-archives.apache.org/mod_mbox/incubator-general/    201702.mbox/%    3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%3E       Kind regards,    Bolke       On 21 Feb 2017, at 08:20, Bolke de Bruin <bdbruin@gmail.com  wrote:       Hello,       Apache Airflow (incubating) 1.8.0 (based on RC4) has been accepted.", "9 \u201c+1\u201d votes received:       - Maxime Beauchemin (binding)    - Arthur Wiedmer (binding)    - Dan Davydov (binding)    - Jeremiah Lowin (binding)    - Siddharth Anand (binding)    - Alex van Boxel (binding)    - Bolke de Bruin (binding)       - Jayesh Senjaliya (non-binding)    - Yi (non-binding)       Vote thread (start):    http://mail-archives.apache.org/mod_mbox/incubator-    airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-    6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-    092E-48D2-AA0F-    15F44376A8FF@gmail.com%3E       Next steps:    1) will start the voting process at the IPMC mailinglist.", "I do  expect    some changes to be required mostly in documentation maybe a license   here    and there.", "So, we might end up with changes to stable."], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["9 \u201c+1\u201d votes received:       - Maxime Beauchemin (binding)    - Arthur Wiedmer (binding)    - Dan Davydov (binding)    - Jeremiah Lowin (binding)    - Siddharth Anand (binding)    - Alex van Boxel (binding)    - Bolke de Bruin (binding)       - Jayesh Senjaliya (non-binding)    - Yi (non-binding)       Vote thread (start):    http://mail-archives.apache.org/mod_mbox/incubator-    airflow-dev/201702.mbox/%3cD360D9BE-C358-42A1-9188-    6C92C31A2F8B@gmail.com%3e <http://mail-archives.apache.", "org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-    092E-48D2-AA0F-    15F44376A8FF@gmail.com%3E       Next steps:    1) will start the voting process at the IPMC mailinglist.", "I do  expect    some changes to be required mostly in documentation maybe a license   here    and there.", "So, we might end up with changes to stable.", "As long as   these    are    not (significant) code changes I will not re-raise the vote."], "labels": ["1", "1", "1", "0", "1"]}
{"abstract_id": 0, "sentences": ["I do  expect    some changes to be required mostly in documentation maybe a license   here    and there.", "So, we might end up with changes to stable.", "As long as   these    are    not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and finalisation I  will    rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the tar  ball    needs to propagate to the mirrors."], "labels": ["1", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["So, we might end up with changes to stable.", "As long as   these    are    not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and finalisation I  will    rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the tar  ball    needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)"], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["3) I will upload it to the incubator release page, then the tar  ball    needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi.", "It seems we can    keep    the apache branding as lib cloud is doing this as well (    https://libcloud.apache.org/downloads.html#pypi-package <    https://libcloud.apache.org/downloads.html#pypi-package).", "Jippie!"], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi.", "It seems we can    keep    the apache branding as lib cloud is doing this as well (    https://libcloud.apache.org/downloads.html#pypi-package <    https://libcloud.apache.org/downloads.html#pypi-package).", "Jippie!", "Bolke                    "], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi  When I think about how to get help with Airflow, I feel we are missing something in the middle.", "Here's the order I usually go through:  1.", "Read the documentation and/or source code.", "2.", "Search gitter, possibly ask a question there."], "labels": ["0", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Here's the order I usually go through:  1.", "Read the documentation and/or source code.", "2.", "Search gitter, possibly ask a question there.", "3."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["2.", "Search gitter, possibly ask a question there.", "3.", "Search the dev mailing list, possibly ask a question there.", "4."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Search gitter, possibly ask a question there.", "3.", "Search the dev mailing list, possibly ask a question there.", "4.", "File a Jira ticket."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["3.", "Search the dev mailing list, possibly ask a question there.", "4.", "File a Jira ticket.", "Before the migration to Apache Incubator, we used a google group which was great for discussions spanning a long time, and for archiving knowledge."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Search the dev mailing list, possibly ask a question there.", "4.", "File a Jira ticket.", "Before the migration to Apache Incubator, we used a google group which was great for discussions spanning a long time, and for archiving knowledge.", "Gitter is really not good at this."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Before the migration to Apache Incubator, we used a google group which was great for discussions spanning a long time, and for archiving knowledge.", "Gitter is really not good at this.", "And the dev mailing list is fine but it's not a great UI, there's no search, etc.", "I think we need something between steps 2 and 3 that's focused on users.", "As an example, Pandas encourages posting questions on stackoverflow."], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["Gitter is really not good at this.", "And the dev mailing list is fine but it's not a great UI, there's no search, etc.", "I think we need something between steps 2 and 3 that's focused on users.", "As an example, Pandas encourages posting questions on stackoverflow.", "As a result there's a huge body of curated knowledge there."], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think we need something between steps 2 and 3 that's focused on users.", "As an example, Pandas encourages posting questions on stackoverflow.", "As a result there's a huge body of curated knowledge there.", "I'm not sure what the best solution is.", "I'm not even sure that others see this as a problem that needs to be addressed."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As an example, Pandas encourages posting questions on stackoverflow.", "As a result there's a huge body of curated knowledge there.", "I'm not sure what the best solution is.", "I'm not even sure that others see this as a problem that needs to be addressed.", "I'm curious what other people think."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As a result there's a huge body of curated knowledge there.", "I'm not sure what the best solution is.", "I'm not even sure that others see this as a problem that needs to be addressed.", "I'm curious what other people think.", "thanks, Dennis  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I disagree.", "Num_runs should NOT be used anymore and I would really like to know \u2018stuck\u2019 schedulers on release or on master, preferably with celery executor (LocalExecutor can sometimes look stuck but isn\u2019t).", "Restarting should only be required for clearing up database connections as we are not very good at that yet.", "- Bolke   Op 9 aug. 2016, om 20:30 heeft Lance Norskog <lance.norskog@gmail.com het volgende geschreven:    Yes, it is still current advice.", "My experience is that after running for (let's say) days, the app develops  memory corruption."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Num_runs should NOT be used anymore and I would really like to know \u2018stuck\u2019 schedulers on release or on master, preferably with celery executor (LocalExecutor can sometimes look stuck but isn\u2019t).", "Restarting should only be required for clearing up database connections as we are not very good at that yet.", "- Bolke   Op 9 aug. 2016, om 20:30 heeft Lance Norskog <lance.norskog@gmail.com het volgende geschreven:    Yes, it is still current advice.", "My experience is that after running for (let's say) days, the app develops  memory corruption.", "I've seen three different ways that memory corruption  shows up."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My experience is that after running for (let's say) days, the app develops  memory corruption.", "I've seen three different ways that memory corruption  shows up.", "The scheduler failure is just one of these three symptoms.", "The other two symptoms are  1) the main page of the UI shows a different list of running DAGs than is  what is really configured,  2) a task contains some configuration data that should be in a neighboring  task, and fails.", "Frankly, I would configure all 5 daemons to restart periodically, not just  the scheduler daemon."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I've seen three different ways that memory corruption  shows up.", "The scheduler failure is just one of these three symptoms.", "The other two symptoms are  1) the main page of the UI shows a different list of running DAGs than is  what is really configured,  2) a task contains some configuration data that should be in a neighboring  task, and fails.", "Frankly, I would configure all 5 daemons to restart periodically, not just  the scheduler daemon.", "On Tue, Aug 9, 2016 at 8:50 AM, Andrew Phillips <andrewp@apache.org wrote:    Hi all    I just wanted to check to what extent the advice in [1] and [2], namely to  restart the scheduler \"every once in a while\", is still considered accurate?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The scheduler failure is just one of these three symptoms.", "The other two symptoms are  1) the main page of the UI shows a different list of running DAGs than is  what is really configured,  2) a task contains some configuration data that should be in a neighboring  task, and fails.", "Frankly, I would configure all 5 daemons to restart periodically, not just  the scheduler daemon.", "On Tue, Aug 9, 2016 at 8:50 AM, Andrew Phillips <andrewp@apache.org wrote:    Hi all    I just wanted to check to what extent the advice in [1] and [2], namely to  restart the scheduler \"every once in a while\", is still considered accurate?", "\"Restart your scheduler process to get a clean environment every once in a  while."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The other two symptoms are  1) the main page of the UI shows a different list of running DAGs than is  what is really configured,  2) a task contains some configuration data that should be in a neighboring  task, and fails.", "Frankly, I would configure all 5 daemons to restart periodically, not just  the scheduler daemon.", "On Tue, Aug 9, 2016 at 8:50 AM, Andrew Phillips <andrewp@apache.org wrote:    Hi all    I just wanted to check to what extent the advice in [1] and [2], namely to  restart the scheduler \"every once in a while\", is still considered accurate?", "\"Restart your scheduler process to get a clean environment every once in a  while.", "Use --num_runs N scheduler CLI option to make it stop after N runs  and have some supervisor ensuring it is always running."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Frankly, I would configure all 5 daemons to restart periodically, not just  the scheduler daemon.", "On Tue, Aug 9, 2016 at 8:50 AM, Andrew Phillips <andrewp@apache.org wrote:    Hi all    I just wanted to check to what extent the advice in [1] and [2], namely to  restart the scheduler \"every once in a while\", is still considered accurate?", "\"Restart your scheduler process to get a clean environment every once in a  while.", "Use --num_runs N scheduler CLI option to make it stop after N runs  and have some supervisor ensuring it is always running.", "See issue 698\"    \"The scheduler should be restarted frequently    In our experience, a long running scheduler process, at least with the  CeleryExecutor, ends up not scheduling some tasks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\"Restart your scheduler process to get a clean environment every once in a  while.", "Use --num_runs N scheduler CLI option to make it stop after N runs  and have some supervisor ensuring it is always running.", "See issue 698\"    \"The scheduler should be restarted frequently    In our experience, a long running scheduler process, at least with the  CeleryExecutor, ends up not scheduling some tasks.", "We still don\u2019t know the  exact cause, unfortunately.", "Fortunately, airflow has a built-in workaround in the form of the \u2014  num_runs flag."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["See issue 698\"    \"The scheduler should be restarted frequently    In our experience, a long running scheduler process, at least with the  CeleryExecutor, ends up not scheduling some tasks.", "We still don\u2019t know the  exact cause, unfortunately.", "Fortunately, airflow has a built-in workaround in the form of the \u2014  num_runs flag.", "It specifies a number of iterations for the scheduler to run  of its loop before it quits.", "We\u2019re running it with 10 iterations, Airbnb  runs it with 5."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We still don\u2019t know the  exact cause, unfortunately.", "Fortunately, airflow has a built-in workaround in the form of the \u2014  num_runs flag.", "It specifies a number of iterations for the scheduler to run  of its loop before it quits.", "We\u2019re running it with 10 iterations, Airbnb  runs it with 5.", "Note that this will cause problems when using the  LocalExecutor.\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Fortunately, airflow has a built-in workaround in the form of the \u2014  num_runs flag.", "It specifies a number of iterations for the scheduler to run  of its loop before it quits.", "We\u2019re running it with 10 iterations, Airbnb  runs it with 5.", "Note that this will cause problems when using the  LocalExecutor.\"", "Both documents are pretty now, so I assume this is considered still  relevant."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It specifies a number of iterations for the scheduler to run  of its loop before it quits.", "We\u2019re running it with 10 iterations, Airbnb  runs it with 5.", "Note that this will cause problems when using the  LocalExecutor.\"", "Both documents are pretty now, so I assume this is considered still  relevant.", "Could you give some guidance on what kind of frequency is  recommended here, or is that very dependent on the particular installation?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We\u2019re running it with 10 iterations, Airbnb  runs it with 5.", "Note that this will cause problems when using the  LocalExecutor.\"", "Both documents are pretty now, so I assume this is considered still  relevant.", "Could you give some guidance on what kind of frequency is  recommended here, or is that very dependent on the particular installation?", "Also, which of the current JIRA issues (if any) is the new version of  \"issue 698\" as mentioned in the first quote?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Note that this will cause problems when using the  LocalExecutor.\"", "Both documents are pretty now, so I assume this is considered still  relevant.", "Could you give some guidance on what kind of frequency is  recommended here, or is that very dependent on the particular installation?", "Also, which of the current JIRA issues (if any) is the new version of  \"issue 698\" as mentioned in the first quote?", "There seem to be quite a few  issues relating to the scheduler getting stuck [3] - which one(s) should we  follow and/or add information to to best track progress on this topic?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Both documents are pretty now, so I assume this is considered still  relevant.", "Could you give some guidance on what kind of frequency is  recommended here, or is that very dependent on the particular installation?", "Also, which of the current JIRA issues (if any) is the new version of  \"issue 698\" as mentioned in the first quote?", "There seem to be quite a few  issues relating to the scheduler getting stuck [3] - which one(s) should we  follow and/or add information to to best track progress on this topic?", "Thanks!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["+1 on this.", "We are using Airflow as a cron replacement, and we have biweekly jobs and monthly jobs as well.", "It would be really useful to be able to configure it such that dags run on the start_date and the timestamps corresponds to it.", "On Tue, Jul 19, 2016 at 10:49 AM, Tyrone Hinderson <thinderson@reonomy.com wrote:   I'm aware that a DAG scheduled to start at time X with interval Y will  first run at time X + Y.", "The documentation describes this:   \"Note that if you run a DAG on a schedule_interval of one day, the run  stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We are using Airflow as a cron replacement, and we have biweekly jobs and monthly jobs as well.", "It would be really useful to be able to configure it such that dags run on the start_date and the timestamps corresponds to it.", "On Tue, Jul 19, 2016 at 10:49 AM, Tyrone Hinderson <thinderson@reonomy.com wrote:   I'm aware that a DAG scheduled to start at time X with interval Y will  first run at time X + Y.", "The documentation describes this:   \"Note that if you run a DAG on a schedule_interval of one day, the run  stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59.", "In other  words, the job instance is started once the period it covers has ended.\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It would be really useful to be able to configure it such that dags run on the start_date and the timestamps corresponds to it.", "On Tue, Jul 19, 2016 at 10:49 AM, Tyrone Hinderson <thinderson@reonomy.com wrote:   I'm aware that a DAG scheduled to start at time X with interval Y will  first run at time X + Y.", "The documentation describes this:   \"Note that if you run a DAG on a schedule_interval of one day, the run  stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59.", "In other  words, the job instance is started once the period it covers has ended.\"", "I'd like to know if this behavior is configurable?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Jul 19, 2016 at 10:49 AM, Tyrone Hinderson <thinderson@reonomy.com wrote:   I'm aware that a DAG scheduled to start at time X with interval Y will  first run at time X + Y.", "The documentation describes this:   \"Note that if you run a DAG on a schedule_interval of one day, the run  stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59.", "In other  words, the job instance is started once the period it covers has ended.\"", "I'd like to know if this behavior is configurable?", "There may be a  particular way of thinking about business processes that fits this pattern;  however, seeing last week's date on a weekly job that ran today confuses my  team, and I'd love to use a flag that makes  1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The documentation describes this:   \"Note that if you run a DAG on a schedule_interval of one day, the run  stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59.", "In other  words, the job instance is started once the period it covers has ended.\"", "I'd like to know if this behavior is configurable?", "There may be a  particular way of thinking about business processes that fits this pattern;  however, seeing last week's date on a weekly job that ran today confuses my  team, and I'd love to use a flag that makes  1.", "DAGs run on the start_date  2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In other  words, the job instance is started once the period it covers has ended.\"", "I'd like to know if this behavior is configurable?", "There may be a  particular way of thinking about business processes that fits this pattern;  however, seeing last week's date on a weekly job that ran today confuses my  team, and I'd love to use a flag that makes  1.", "DAGs run on the start_date  2.", "DagRun timestamps correspond with the intended actual run date/time.   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["mysql_hook uses MySQLdb.", "Just see if you are not hit by this issue:  http://stackoverflow.com/questions/6383310/python-mysqldb-library-not-loaded-libmysqlclient-18-dylib    On Mon, Jun 20, 2016 at 5:58 AM, Msr Msr <msrmaillist@gmail.com wrote:   Hi,   I have installed Airflow on Mac and trying to use  MySqlOperator.", "It is giving below error message.", "------------------   mms-MacBook-Pro:~ mm$ airflow initdb   [2016-06-19 17:21:06,794] {__init__.py:36} INFO - Using executor  SequentialExecutor   DB: sqlite:////Users/mm/airflow/airflow.db   [2016-06-19 17:21:07,345] {db.py:222} INFO - Creating tables   INFO  [alembic.runtime.migration] Context impl SQLiteImpl.", "INFO  [alembic.runtime.migration] Will assume non-transactional DDL."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Just see if you are not hit by this issue:  http://stackoverflow.com/questions/6383310/python-mysqldb-library-not-loaded-libmysqlclient-18-dylib    On Mon, Jun 20, 2016 at 5:58 AM, Msr Msr <msrmaillist@gmail.com wrote:   Hi,   I have installed Airflow on Mac and trying to use  MySqlOperator.", "It is giving below error message.", "------------------   mms-MacBook-Pro:~ mm$ airflow initdb   [2016-06-19 17:21:06,794] {__init__.py:36} INFO - Using executor  SequentialExecutor   DB: sqlite:////Users/mm/airflow/airflow.db   [2016-06-19 17:21:07,345] {db.py:222} INFO - Creating tables   INFO  [alembic.runtime.migration] Context impl SQLiteImpl.", "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.", "ERROR [airflow.models.DagBag] Failed to import:  /Users/mm/airflow/dags/S3test.py   Traceback (most recent call last):     File \"/Users/mm/anaconda/lib/python2.7/site-packages/airflow/models.py\",  line 247, in process_file       m = imp.load_source(mod_name, filepath)     File \"/Users/mm/airflow/dags/S3test.py\", line 4, in <module       from airflow.operators import MySqlOperator   ImportError: cannot import name MySqlOperator   -------------------    pip install airflow[mysql] always gives Requirements met    Could someone please suggest what could be the reason for this error and  how to resolve it    Thanks,   msr   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think I mentioned this on another similar thread and I think our use case might be somewhat similar.", "We have a daily ETL that loads data in to a database in one DAG, and then need to do a weekly rollup report every Tuesday that is in another DAG.", "The first DAG has a final TriggerDagRunOperator that decides if today is Tuesday or not, and if yes, triggers the weekly rollup DAG that operates on the data in the database at that moment - which, since it's Tuesday, is all the data we want.", "If that sounds like what you're trying to do, your first DAG might have a TriggerDagRunOperator that decides if today is the first of the month, and then triggers some other DAG.", "Laura  On Thu, Jun 30, 2016 at 12:09 PM, Jeremiah Lowin <jlowin@apache.org wrote:   Interesting -- this could be an extension of open enhancement AIRFLOW-100  https://issues.apache.org/jira/browse/AIRFLOW-100."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We have a daily ETL that loads data in to a database in one DAG, and then need to do a weekly rollup report every Tuesday that is in another DAG.", "The first DAG has a final TriggerDagRunOperator that decides if today is Tuesday or not, and if yes, triggers the weekly rollup DAG that operates on the data in the database at that moment - which, since it's Tuesday, is all the data we want.", "If that sounds like what you're trying to do, your first DAG might have a TriggerDagRunOperator that decides if today is the first of the month, and then triggers some other DAG.", "Laura  On Thu, Jun 30, 2016 at 12:09 PM, Jeremiah Lowin <jlowin@apache.org wrote:   Interesting -- this could be an extension of open enhancement AIRFLOW-100  https://issues.apache.org/jira/browse/AIRFLOW-100.", "Let me see if I can  restate this correctly:   - You have a daily ETL job  - You have a monthly reporting job, for arguments sake lets say it runs on  the last day of each month with an execution date equal to the last day of  the prior month (for example on 7/31/2016 the task with execution date  6/30/2016 will run)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The first DAG has a final TriggerDagRunOperator that decides if today is Tuesday or not, and if yes, triggers the weekly rollup DAG that operates on the data in the database at that moment - which, since it's Tuesday, is all the data we want.", "If that sounds like what you're trying to do, your first DAG might have a TriggerDagRunOperator that decides if today is the first of the month, and then triggers some other DAG.", "Laura  On Thu, Jun 30, 2016 at 12:09 PM, Jeremiah Lowin <jlowin@apache.org wrote:   Interesting -- this could be an extension of open enhancement AIRFLOW-100  https://issues.apache.org/jira/browse/AIRFLOW-100.", "Let me see if I can  restate this correctly:   - You have a daily ETL job  - You have a monthly reporting job, for arguments sake lets say it runs on  the last day of each month with an execution date equal to the last day of  the prior month (for example on 7/31/2016 the task with execution date  6/30/2016 will run).", "You want the monthly job with execution date 6/30/2016 to wait for (and  include) the daily ETLs through 7/31/2016."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If that sounds like what you're trying to do, your first DAG might have a TriggerDagRunOperator that decides if today is the first of the month, and then triggers some other DAG.", "Laura  On Thu, Jun 30, 2016 at 12:09 PM, Jeremiah Lowin <jlowin@apache.org wrote:   Interesting -- this could be an extension of open enhancement AIRFLOW-100  https://issues.apache.org/jira/browse/AIRFLOW-100.", "Let me see if I can  restate this correctly:   - You have a daily ETL job  - You have a monthly reporting job, for arguments sake lets say it runs on  the last day of each month with an execution date equal to the last day of  the prior month (for example on 7/31/2016 the task with execution date  6/30/2016 will run).", "You want the monthly job with execution date 6/30/2016 to wait for (and  include) the daily ETLs through 7/31/2016.", "In some months, that requires a  31 day delta, in others 30 (in others 28... and forget about leap years)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Laura  On Thu, Jun 30, 2016 at 12:09 PM, Jeremiah Lowin <jlowin@apache.org wrote:   Interesting -- this could be an extension of open enhancement AIRFLOW-100  https://issues.apache.org/jira/browse/AIRFLOW-100.", "Let me see if I can  restate this correctly:   - You have a daily ETL job  - You have a monthly reporting job, for arguments sake lets say it runs on  the last day of each month with an execution date equal to the last day of  the prior month (for example on 7/31/2016 the task with execution date  6/30/2016 will run).", "You want the monthly job with execution date 6/30/2016 to wait for (and  include) the daily ETLs through 7/31/2016.", "In some months, that requires a  31 day delta, in others 30 (in others 28... and forget about leap years).", "It sounds like the simplest solution (and the one proposed in A-100) is to  allow ExternalTaskSensor to accept not just a static delta, but potentially  a callable that accepts the current execution date and returns the desired  execution date for the sensed task."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let me see if I can  restate this correctly:   - You have a daily ETL job  - You have a monthly reporting job, for arguments sake lets say it runs on  the last day of each month with an execution date equal to the last day of  the prior month (for example on 7/31/2016 the task with execution date  6/30/2016 will run).", "You want the monthly job with execution date 6/30/2016 to wait for (and  include) the daily ETLs through 7/31/2016.", "In some months, that requires a  31 day delta, in others 30 (in others 28... and forget about leap years).", "It sounds like the simplest solution (and the one proposed in A-100) is to  allow ExternalTaskSensor to accept not just a static delta, but potentially  a callable that accepts the current execution date and returns the desired  execution date for the sensed task.", "In this case, it would take in  6/30/2016 and return 7/31/2016 as the last day of the following month."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You want the monthly job with execution date 6/30/2016 to wait for (and  include) the daily ETLs through 7/31/2016.", "In some months, that requires a  31 day delta, in others 30 (in others 28... and forget about leap years).", "It sounds like the simplest solution (and the one proposed in A-100) is to  allow ExternalTaskSensor to accept not just a static delta, but potentially  a callable that accepts the current execution date and returns the desired  execution date for the sensed task.", "In this case, it would take in  6/30/2016 and return 7/31/2016 as the last day of the following month.", "I  don't think any headway has been made on actually implementing the solution  but it should be straightforward -- I will try to get to it if I have some  time in the next few days."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In some months, that requires a  31 day delta, in others 30 (in others 28... and forget about leap years).", "It sounds like the simplest solution (and the one proposed in A-100) is to  allow ExternalTaskSensor to accept not just a static delta, but potentially  a callable that accepts the current execution date and returns the desired  execution date for the sensed task.", "In this case, it would take in  6/30/2016 and return 7/31/2016 as the last day of the following month.", "I  don't think any headway has been made on actually implementing the solution  but it should be straightforward -- I will try to get to it if I have some  time in the next few days.", "On Wed, Jun 29, 2016 at 11:25 AM Adrian Bridgett <adrian@opensignal.com  wrote:    I'm hitting a bit of an annoying problem and wondering about the best   course of action."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It sounds like the simplest solution (and the one proposed in A-100) is to  allow ExternalTaskSensor to accept not just a static delta, but potentially  a callable that accepts the current execution date and returns the desired  execution date for the sensed task.", "In this case, it would take in  6/30/2016 and return 7/31/2016 as the last day of the following month.", "I  don't think any headway has been made on actually implementing the solution  but it should be straightforward -- I will try to get to it if I have some  time in the next few days.", "On Wed, Jun 29, 2016 at 11:25 AM Adrian Bridgett <adrian@opensignal.com  wrote:    I'm hitting a bit of an annoying problem and wondering about the best   course of action.", "We have several dags:   - a daily ETL job   - several reporting jobs (daily, weekly or monthly) which use the data   from previous ETL jobs     I wish to have a dependency such that the reporting jobs depend upon the   last ETL job that the report uses."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In this case, it would take in  6/30/2016 and return 7/31/2016 as the last day of the following month.", "I  don't think any headway has been made on actually implementing the solution  but it should be straightforward -- I will try to get to it if I have some  time in the next few days.", "On Wed, Jun 29, 2016 at 11:25 AM Adrian Bridgett <adrian@opensignal.com  wrote:    I'm hitting a bit of an annoying problem and wondering about the best   course of action.", "We have several dags:   - a daily ETL job   - several reporting jobs (daily, weekly or monthly) which use the data   from previous ETL jobs     I wish to have a dependency such that the reporting jobs depend upon the   last ETL job that the report uses.", "We're happy to set depends_on_past   in the ETL job."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I  don't think any headway has been made on actually implementing the solution  but it should be straightforward -- I will try to get to it if I have some  time in the next few days.", "On Wed, Jun 29, 2016 at 11:25 AM Adrian Bridgett <adrian@opensignal.com  wrote:    I'm hitting a bit of an annoying problem and wondering about the best   course of action.", "We have several dags:   - a daily ETL job   - several reporting jobs (daily, weekly or monthly) which use the data   from previous ETL jobs     I wish to have a dependency such that the reporting jobs depend upon the   last ETL job that the report uses.", "We're happy to set depends_on_past   in the ETL job.", "Daily jobs are easy - ExternalTaskSensor, job done."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Jun 29, 2016 at 11:25 AM Adrian Bridgett <adrian@opensignal.com  wrote:    I'm hitting a bit of an annoying problem and wondering about the best   course of action.", "We have several dags:   - a daily ETL job   - several reporting jobs (daily, weekly or monthly) which use the data   from previous ETL jobs     I wish to have a dependency such that the reporting jobs depend upon the   last ETL job that the report uses.", "We're happy to set depends_on_past   in the ETL job.", "Daily jobs are easy - ExternalTaskSensor, job done.", "Weekly jobs are a little trickier - we need to work out the   execution_delta - normally +6 for us (we deliberately run a day late to   prioritise other jobs)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We're happy to set depends_on_past   in the ETL job.", "Daily jobs are easy - ExternalTaskSensor, job done.", "Weekly jobs are a little trickier - we need to work out the   execution_delta - normally +6 for us (we deliberately run a day late to   prioritise other jobs).", "Monthly jobs.... this is where I'm struggling - how to work out the   execution_delta.", "I guess the ideal would be an upgrade from timedelta   to dateutil.relativedelta?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Daily jobs are easy - ExternalTaskSensor, job done.", "Weekly jobs are a little trickier - we need to work out the   execution_delta - normally +6 for us (we deliberately run a day late to   prioritise other jobs).", "Monthly jobs.... this is where I'm struggling - how to work out the   execution_delta.", "I guess the ideal would be an upgrade from timedelta   to dateutil.relativedelta?", "tomorrow_ds and ds_add don't help either."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Weekly jobs are a little trickier - we need to work out the   execution_delta - normally +6 for us (we deliberately run a day late to   prioritise other jobs).", "Monthly jobs.... this is where I'm struggling - how to work out the   execution_delta.", "I guess the ideal would be an upgrade from timedelta   to dateutil.relativedelta?", "tomorrow_ds and ds_add don't help either.", "I must admit, ds being the time that's just gone has caused me no end of   brain befudledness, especially when trying to get the initial job right   (so much so that I wrote this up in our DAG README, posting here for   others):     When adding a new job, it's critical to ensure that you've set the   schedule correctly:   - frequency (monthly, weekly, daily)   - schedule_interval (\"0 0 2 * *\", \"0 0 * * 0\", \"0 0 * * *\")   - start_date (choose a day that matches schedule_interval at least one   interval ago)   -- e.g if today is Thursday 2016-06-09, go back in time to when the   schedule will trigger,       then work out what \"ds\" (execution date) would be (remembering   that's the lapsed date)   --- for a monthly job, last trigger=2016-06-02, ds=2016-05-02   --- for a weekly job, last trigger=2016-06-05, ds=2016-05-29   --- for a daily job, last trigger=2016-06-09, ds=2016-06-08     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The information could be different for different airflow runs, so I hope to use python code to initialize it whenever airflow starts.", "Where is the best place to put such code ?", "Is the class DagBag's __init__() a good candidate ?", "Please advise.", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is the class DagBag's __init__() a good candidate ?", "Please advise.", "Thanks.", "#############################################  class DagBag(LoggingMixin):     \"\"\"     A dagbag is a collection of dags, parsed out of a folder tree and has high     level configuration settings, like what database to use as a backend and     what executor to use to fire off tasks.", "This makes it easier to run     distinct environments for say production and development, tests, or for     different teams or security profiles."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please advise.", "Thanks.", "#############################################  class DagBag(LoggingMixin):     \"\"\"     A dagbag is a collection of dags, parsed out of a folder tree and has high     level configuration settings, like what database to use as a backend and     what executor to use to fire off tasks.", "This makes it easier to run     distinct environments for say production and development, tests, or for     different teams or security profiles.", "What would have been system level     settings are now dagbag level so that one system can run multiple,     independent settings sets."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks.", "#############################################  class DagBag(LoggingMixin):     \"\"\"     A dagbag is a collection of dags, parsed out of a folder tree and has high     level configuration settings, like what database to use as a backend and     what executor to use to fire off tasks.", "This makes it easier to run     distinct environments for say production and development, tests, or for     different teams or security profiles.", "What would have been system level     settings are now dagbag level so that one system can run multiple,     independent settings sets.", ":param dag_folder: the folder to scan to find DAGs     :type dag_folder: str     :param executor: the executor to use when executing task instances         in this DagBag     :param include_examples: whether to include the examples that ship         with airflow or not     :type include_examples: bool     :param sync_to_db: whether to sync the properties of the DAGs to         the metadata DB while finding them, typically should be done         by the scheduler job only     :type sync_to_db: bool     \"\"\"     def __init__(             self,             dag_folder=None,             executor=DEFAULT_EXECUTOR,             include_examples=configuration.getboolean('core', 'LOAD_EXAMPLES'),             sync_to_db=False):          dag_folder = dag_folder or DAGS_FOLDER         self.logger.info(\"Filling up the DagBag from {}\".format(dag_folder))         self.dag_folder = dag_folder         self.dags = {}         self.sync_to_db = sync_to_db         self.file_last_changed = {}         self.executor = executor         self.import_errors = {}         if include_examples:             example_dag_folder = os.path.join(  ...  #############################   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I meant the API -- will check the wiki now.", "Thanks!", "On Wed, Feb 8, 2017 at 8:33 AM Bolke de Bruin <bdbruin@gmail.com wrote:   On this proposal?", "No, not yet.", "Just popped in my mind yesterday."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks!", "On Wed, Feb 8, 2017 at 8:33 AM Bolke de Bruin <bdbruin@gmail.com wrote:   On this proposal?", "No, not yet.", "Just popped in my mind yesterday.", "API there  is a bit on the wiki."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["No, not yet.", "Just popped in my mind yesterday.", "API there  is a bit on the wiki.", "On 8 Feb 2017, at 14:31, Jeremiah Lowin <jlowin@apache.org wrote:     Makes a lot of sense.", "At the NY meetup there was considerable interest in   using the API (and quite a few hacks around exposing the CLI!)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Just popped in my mind yesterday.", "API there  is a bit on the wiki.", "On 8 Feb 2017, at 14:31, Jeremiah Lowin <jlowin@apache.org wrote:     Makes a lot of sense.", "At the NY meetup there was considerable interest in   using the API (and quite a few hacks around exposing the CLI!)", "-- is  there   more complete documentation anywhere?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["API there  is a bit on the wiki.", "On 8 Feb 2017, at 14:31, Jeremiah Lowin <jlowin@apache.org wrote:     Makes a lot of sense.", "At the NY meetup there was considerable interest in   using the API (and quite a few hacks around exposing the CLI!)", "-- is  there   more complete documentation anywhere?", "Thanks Bolke     On Wed, Feb 8, 2017 at 1:36 AM Bolke de Bruin <bdbruin@gmail.com wrote:     Hi All,     Now that we have an API in place."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 8 Feb 2017, at 14:31, Jeremiah Lowin <jlowin@apache.org wrote:     Makes a lot of sense.", "At the NY meetup there was considerable interest in   using the API (and quite a few hacks around exposing the CLI!)", "-- is  there   more complete documentation anywhere?", "Thanks Bolke     On Wed, Feb 8, 2017 at 1:36 AM Bolke de Bruin <bdbruin@gmail.com wrote:     Hi All,     Now that we have an API in place.", "I would like to propose a new state  for   tasks named \u201cWAITING_ON_CALLBACK\u201d."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["At the NY meetup there was considerable interest in   using the API (and quite a few hacks around exposing the CLI!)", "-- is  there   more complete documentation anywhere?", "Thanks Bolke     On Wed, Feb 8, 2017 at 1:36 AM Bolke de Bruin <bdbruin@gmail.com wrote:     Hi All,     Now that we have an API in place.", "I would like to propose a new state  for   tasks named \u201cWAITING_ON_CALLBACK\u201d.", "Currently, we have tasks that have a   kind of polling mechanism (ie."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-- is  there   more complete documentation anywhere?", "Thanks Bolke     On Wed, Feb 8, 2017 at 1:36 AM Bolke de Bruin <bdbruin@gmail.com wrote:     Hi All,     Now that we have an API in place.", "I would like to propose a new state  for   tasks named \u201cWAITING_ON_CALLBACK\u201d.", "Currently, we have tasks that have a   kind of polling mechanism (ie.", "Sensors) that wait for an action to  happen   and check if that action happened by regularly polling a particular   backend."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks Bolke     On Wed, Feb 8, 2017 at 1:36 AM Bolke de Bruin <bdbruin@gmail.com wrote:     Hi All,     Now that we have an API in place.", "I would like to propose a new state  for   tasks named \u201cWAITING_ON_CALLBACK\u201d.", "Currently, we have tasks that have a   kind of polling mechanism (ie.", "Sensors) that wait for an action to  happen   and check if that action happened by regularly polling a particular   backend.", "This will always use a slot from one of the workers and could   starve an airflow cluster for resources."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Currently, we have tasks that have a   kind of polling mechanism (ie.", "Sensors) that wait for an action to  happen   and check if that action happened by regularly polling a particular   backend.", "This will always use a slot from one of the workers and could   starve an airflow cluster for resources.", "What if a callback to Airflow   could happen that task to change its status by calling a callback  mechanism   without taking up a worker slot.", "A timeout could (should) be associated   with the required callback so that the task can fail if required."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sensors) that wait for an action to  happen   and check if that action happened by regularly polling a particular   backend.", "This will always use a slot from one of the workers and could   starve an airflow cluster for resources.", "What if a callback to Airflow   could happen that task to change its status by calling a callback  mechanism   without taking up a worker slot.", "A timeout could (should) be associated   with the required callback so that the task can fail if required.", "So a  bit   more visual:       Task X from DAG Z  does some work and sets \u201cWAITING_ON_CALLBACK\u201d - API   post to /dags/Z/dag_runs/20170101T00:00:00/tasks/X with payload \u201cset  status   to SUCCESS\u201d     DAG Z happily continues."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This will always use a slot from one of the workers and could   starve an airflow cluster for resources.", "What if a callback to Airflow   could happen that task to change its status by calling a callback  mechanism   without taking up a worker slot.", "A timeout could (should) be associated   with the required callback so that the task can fail if required.", "So a  bit   more visual:       Task X from DAG Z  does some work and sets \u201cWAITING_ON_CALLBACK\u201d - API   post to /dags/Z/dag_runs/20170101T00:00:00/tasks/X with payload \u201cset  status   to SUCCESS\u201d     DAG Z happily continues.", "Or     Task X from DAG Z sets \u201cWAITING_ON_CALLBACK\u201d with timeout of 300s -  time   passes - scheduler sets task to FAILED."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What if a callback to Airflow   could happen that task to change its status by calling a callback  mechanism   without taking up a worker slot.", "A timeout could (should) be associated   with the required callback so that the task can fail if required.", "So a  bit   more visual:       Task X from DAG Z  does some work and sets \u201cWAITING_ON_CALLBACK\u201d - API   post to /dags/Z/dag_runs/20170101T00:00:00/tasks/X with payload \u201cset  status   to SUCCESS\u201d     DAG Z happily continues.", "Or     Task X from DAG Z sets \u201cWAITING_ON_CALLBACK\u201d with timeout of 300s -  time   passes - scheduler sets task to FAILED.", "Any thoughts?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["correct  On Tue, Nov 15, 2016 at 1:11 AM <nux@sroff.com wrote:  Thanks siddharth for your answer.", "I'm going to look at the extra \"failure processing branch\" as you suggested.", "It is intended that a DAGRun be deemed successful in all cases except for   failure.", "So, skipped nodes F and G, would result in a Successful DagRun.", "A DAGRun is deemed succesful or failed based solely on the status of the last tasks right ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm going to look at the extra \"failure processing branch\" as you suggested.", "It is intended that a DAGRun be deemed successful in all cases except for   failure.", "So, skipped nodes F and G, would result in a Successful DagRun.", "A DAGRun is deemed succesful or failed based solely on the status of the last tasks right ?", "Airflow does not consider non-terminal failed tasks ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It is intended that a DAGRun be deemed successful in all cases except for   failure.", "So, skipped nodes F and G, would result in a Successful DagRun.", "A DAGRun is deemed succesful or failed based solely on the status of the last tasks right ?", "Airflow does not consider non-terminal failed tasks ?", "At moment, I want my privacy to be protected."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So, skipped nodes F and G, would result in a Successful DagRun.", "A DAGRun is deemed succesful or failed based solely on the status of the last tasks right ?", "Airflow does not consider non-terminal failed tasks ?", "At moment, I want my privacy to be protected.", "https://mytemp.email/  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I was curious what the thoughts are regarding shifting Airflow to a publish model from the poll model currently used by many of the sensors inheriting from BaseSensorOperator.", "It would prevent much of the polling that may take up quite a bit of resources.", "Thanks.", "Regards, David  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi All,   We have implemented a solution for allowing the exclusion of individual tasks during a DAG run.", "However, when writing unit tests for this, we are encountering an issue with MySQL, which I am hoping someone is able to help us with.", "For our solution, we have a new 'TaskExclusion' table in the meta-data.", "Our unit tests were run by Travis, not locally.", "The code block under test:   class TaskExclusion(Base):     \"\"\"  This class is used to define objects that can be used to specify not to  run a given task in a given dag on a variety of execution date conditions."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, when writing unit tests for this, we are encountering an issue with MySQL, which I am hoping someone is able to help us with.", "For our solution, we have a new 'TaskExclusion' table in the meta-data.", "Our unit tests were run by Travis, not locally.", "The code block under test:   class TaskExclusion(Base):     \"\"\"  This class is used to define objects that can be used to specify not to  run a given task in a given dag on a variety of execution date conditions.", "These objects will be stored in the backend database in the task_exclusion  table."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For our solution, we have a new 'TaskExclusion' table in the meta-data.", "Our unit tests were run by Travis, not locally.", "The code block under test:   class TaskExclusion(Base):     \"\"\"  This class is used to define objects that can be used to specify not to  run a given task in a given dag on a variety of execution date conditions.", "These objects will be stored in the backend database in the task_exclusion  table.", "Static methods are provided for the creation, removal and investigation of  these objects.  \"\"\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Our unit tests were run by Travis, not locally.", "The code block under test:   class TaskExclusion(Base):     \"\"\"  This class is used to define objects that can be used to specify not to  run a given task in a given dag on a variety of execution date conditions.", "These objects will be stored in the backend database in the task_exclusion  table.", "Static methods are provided for the creation, removal and investigation of  these objects.  \"\"\"", "__tablename__ = \"task_exclusion\"   id = Column(Integer(), primary_key=True)     dag_id = Column(String(ID_LEN), nullable=False)     task_id = Column(String(ID_LEN), nullable=False)     exclusion_type = Column(String(32), nullable=False)     exclusion_start_date = Column(DateTime, nullable=True)     exclusion_end_date = Column(DateTime, nullable=True)     created_by = Column(String(256), nullable=False)     created_on = Column(DateTime, nullable=False)      @classmethod  @provide_session  def set(             cls,             dag_id,             task_id,             exclusion_type,             exclusion_start_date,             exclusion_end_date,             created_by,             session=None):         \"\"\"  Add a task exclusion to prevent a task running under certain  circumstances."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The code block under test:   class TaskExclusion(Base):     \"\"\"  This class is used to define objects that can be used to specify not to  run a given task in a given dag on a variety of execution date conditions.", "These objects will be stored in the backend database in the task_exclusion  table.", "Static methods are provided for the creation, removal and investigation of  these objects.  \"\"\"", "__tablename__ = \"task_exclusion\"   id = Column(Integer(), primary_key=True)     dag_id = Column(String(ID_LEN), nullable=False)     task_id = Column(String(ID_LEN), nullable=False)     exclusion_type = Column(String(32), nullable=False)     exclusion_start_date = Column(DateTime, nullable=True)     exclusion_end_date = Column(DateTime, nullable=True)     created_by = Column(String(256), nullable=False)     created_on = Column(DateTime, nullable=False)      @classmethod  @provide_session  def set(             cls,             dag_id,             task_id,             exclusion_type,             exclusion_start_date,             exclusion_end_date,             created_by,             session=None):         \"\"\"  Add a task exclusion to prevent a task running under certain  circumstances.", ":param dag_id: The dag_id of the DAG containing the task to exclude  from execution."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These objects will be stored in the backend database in the task_exclusion  table.", "Static methods are provided for the creation, removal and investigation of  these objects.  \"\"\"", "__tablename__ = \"task_exclusion\"   id = Column(Integer(), primary_key=True)     dag_id = Column(String(ID_LEN), nullable=False)     task_id = Column(String(ID_LEN), nullable=False)     exclusion_type = Column(String(32), nullable=False)     exclusion_start_date = Column(DateTime, nullable=True)     exclusion_end_date = Column(DateTime, nullable=True)     created_by = Column(String(256), nullable=False)     created_on = Column(DateTime, nullable=False)      @classmethod  @provide_session  def set(             cls,             dag_id,             task_id,             exclusion_type,             exclusion_start_date,             exclusion_end_date,             created_by,             session=None):         \"\"\"  Add a task exclusion to prevent a task running under certain  circumstances.", ":param dag_id: The dag_id of the DAG containing the task to exclude  from execution.", ":param task_id: The task_id of the task to exclude from execution."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["__tablename__ = \"task_exclusion\"   id = Column(Integer(), primary_key=True)     dag_id = Column(String(ID_LEN), nullable=False)     task_id = Column(String(ID_LEN), nullable=False)     exclusion_type = Column(String(32), nullable=False)     exclusion_start_date = Column(DateTime, nullable=True)     exclusion_end_date = Column(DateTime, nullable=True)     created_by = Column(String(256), nullable=False)     created_on = Column(DateTime, nullable=False)      @classmethod  @provide_session  def set(             cls,             dag_id,             task_id,             exclusion_type,             exclusion_start_date,             exclusion_end_date,             created_by,             session=None):         \"\"\"  Add a task exclusion to prevent a task running under certain  circumstances.", ":param dag_id: The dag_id of the DAG containing the task to exclude  from execution.", ":param task_id: The task_id of the task to exclude from execution.", ":param exclusion_type: The type of circumstances to exclude the task  from execution under.", "See the TaskExclusionType class for more detail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":param dag_id: The dag_id of the DAG containing the task to exclude  from execution.", ":param task_id: The task_id of the task to exclude from execution.", ":param exclusion_type: The type of circumstances to exclude the task  from execution under.", "See the TaskExclusionType class for more detail.", ":param exclusion_start_date: The execution_date to start excluding on."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":param task_id: The task_id of the task to exclude from execution.", ":param exclusion_type: The type of circumstances to exclude the task  from execution under.", "See the TaskExclusionType class for more detail.", ":param exclusion_start_date: The execution_date to start excluding on.", "This will be ignored if the exclusion_type is INDEFINITE."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":param exclusion_type: The type of circumstances to exclude the task  from execution under.", "See the TaskExclusionType class for more detail.", ":param exclusion_start_date: The execution_date to start excluding on.", "This will be ignored if the exclusion_type is INDEFINITE.", ":param exclusion_end_date: The execution_date to stop excluding on."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["See the TaskExclusionType class for more detail.", ":param exclusion_start_date: The execution_date to start excluding on.", "This will be ignored if the exclusion_type is INDEFINITE.", ":param exclusion_end_date: The execution_date to stop excluding on.", "This will be ignored if the exclusion_type is INDEFINITE or  SINGLE_DATE."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":param exclusion_start_date: The execution_date to start excluding on.", "This will be ignored if the exclusion_type is INDEFINITE.", ":param exclusion_end_date: The execution_date to stop excluding on.", "This will be ignored if the exclusion_type is INDEFINITE or  SINGLE_DATE.", ":param created_by: Who is creating this exclusion."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":param exclusion_end_date: The execution_date to stop excluding on.", "This will be ignored if the exclusion_type is INDEFINITE or  SINGLE_DATE.", ":param created_by: Who is creating this exclusion.", "Stored with the  exclusion record for auditing/debugging purposes.", ":return: None.  \"\"\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This will be ignored if the exclusion_type is INDEFINITE or  SINGLE_DATE.", ":param created_by: Who is creating this exclusion.", "Stored with the  exclusion record for auditing/debugging purposes.", ":return: None.  \"\"\"", "session.expunge_all()          # Set up execution date range correctly  if exclusion_type == TaskExclusionType.SINGLE_DATE:             if exclusion_start_date:                 exclusion_end_date = exclusion_start_date             else:                 raise AirflowException(                     \"No exclusion_start_date \"  )         elif exclusion_type == TaskExclusionType.DATE_RANGE:             if exclusion_start_date  exclusion_end_date:                 raise AirflowException(                     \"The exclusion_start_date is after the exclusion_end_date\"  )         elif exclusion_type == TaskExclusionType.INDEFINITE:             exclusion_start_date = None  exclusion_end_date = None  else:             raise AirflowException(                 \"The exclusion_type, {}, is not recognised.\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":return: None.  \"\"\"", "session.expunge_all()          # Set up execution date range correctly  if exclusion_type == TaskExclusionType.SINGLE_DATE:             if exclusion_start_date:                 exclusion_end_date = exclusion_start_date             else:                 raise AirflowException(                     \"No exclusion_start_date \"  )         elif exclusion_type == TaskExclusionType.DATE_RANGE:             if exclusion_start_date  exclusion_end_date:                 raise AirflowException(                     \"The exclusion_start_date is after the exclusion_end_date\"  )         elif exclusion_type == TaskExclusionType.INDEFINITE:             exclusion_start_date = None  exclusion_end_date = None  else:             raise AirflowException(                 \"The exclusion_type, {}, is not recognised.\"", ".format(exclusion_type)             )          # remove any duplicate exclusions  session.query(cls).filter(             cls.dag_id == dag_id,             cls.task_id == task_id,             cls.exclusion_type == exclusion_type,             cls.exclusion_start_date == exclusion_start_date,             cls.exclusion_end_date == exclusion_end_date         ).delete()          # insert new exclusion  session.add(TaskExclusion(             dag_id=dag_id,             task_id=task_id,             exclusion_type=exclusion_type,             exclusion_start_date=exclusion_start_date,             exclusion_end_date=exclusion_end_date,             created_by=created_by,             created_on=datetime.now())         )          session.commit()   The unit test:  class TaskExclusionTest(unittest.TestCase):     def test_set_exclusion(self, session=None):          session = settings.Session()          session.expunge_all()          dag_id = 'test_task_exclude'  task_id = 'test_task_exclude'  exec_date = datetime.datetime.now()          TaskExclusion.set(dag_id=dag_id,                           task_id=task_id,                           exclusion_type=TaskExclusionType.SINGLE_DATE,                           exclusion_start_date=exec_date,                           exclusion_end_date=exec_date,                           created_by='airflow')           exclusion = session.query(TaskExclusion).filter(                         TaskExclusion.dag_id == dag_id,                         TaskExclusion.task_id == task_id,                         TaskExclusion.exclusion_type == TaskExclusionType.SINGLE_DATE,                         TaskExclusion.exclusion_start_date == exec_date,                         TaskExclusion.exclusion_end_date == exec_date).first()          self.assertTrue(exclusion)   The unit test passes for postgreSQL and SQLite but fails for MySQL.", "I have checked and the 'exclusion' variable contains a TaskExclusion object for postgreSQL and SQLite but is set to 'None' for MySQL.", "Any suggestions on what could be causing this would be much appreciated."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [".format(exclusion_type)             )          # remove any duplicate exclusions  session.query(cls).filter(             cls.dag_id == dag_id,             cls.task_id == task_id,             cls.exclusion_type == exclusion_type,             cls.exclusion_start_date == exclusion_start_date,             cls.exclusion_end_date == exclusion_end_date         ).delete()          # insert new exclusion  session.add(TaskExclusion(             dag_id=dag_id,             task_id=task_id,             exclusion_type=exclusion_type,             exclusion_start_date=exclusion_start_date,             exclusion_end_date=exclusion_end_date,             created_by=created_by,             created_on=datetime.now())         )          session.commit()   The unit test:  class TaskExclusionTest(unittest.TestCase):     def test_set_exclusion(self, session=None):          session = settings.Session()          session.expunge_all()          dag_id = 'test_task_exclude'  task_id = 'test_task_exclude'  exec_date = datetime.datetime.now()          TaskExclusion.set(dag_id=dag_id,                           task_id=task_id,                           exclusion_type=TaskExclusionType.SINGLE_DATE,                           exclusion_start_date=exec_date,                           exclusion_end_date=exec_date,                           created_by='airflow')           exclusion = session.query(TaskExclusion).filter(                         TaskExclusion.dag_id == dag_id,                         TaskExclusion.task_id == task_id,                         TaskExclusion.exclusion_type == TaskExclusionType.SINGLE_DATE,                         TaskExclusion.exclusion_start_date == exec_date,                         TaskExclusion.exclusion_end_date == exec_date).first()          self.assertTrue(exclusion)   The unit test passes for postgreSQL and SQLite but fails for MySQL.", "I have checked and the 'exclusion' variable contains a TaskExclusion object for postgreSQL and SQLite but is set to 'None' for MySQL.", "Any suggestions on what could be causing this would be much appreciated.", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________ From: siddharth anand <sanand@apache.org Sent: 16 November 2016 00:40 To: dev@airflow.incubator.apache.org Subject: Re: Skip task  If your requirement is to skip a portion of tasks in a DagRun based on some state encountered while executing that DagRun, that is what BranchPythonOperator or ShortCircruitOperator (optionally paired with a Trigger Rule specified on a downstream task) is made for.", "These operators take a custom Python callable as a argument."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have checked and the 'exclusion' variable contains a TaskExclusion object for postgreSQL and SQLite but is set to 'None' for MySQL.", "Any suggestions on what could be causing this would be much appreciated.", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________ From: siddharth anand <sanand@apache.org Sent: 16 November 2016 00:40 To: dev@airflow.incubator.apache.org Subject: Re: Skip task  If your requirement is to skip a portion of tasks in a DagRun based on some state encountered while executing that DagRun, that is what BranchPythonOperator or ShortCircruitOperator (optionally paired with a Trigger Rule specified on a downstream task) is made for.", "These operators take a custom Python callable as a argument.", "The callable can check for the existence of data or files that should have been generated by an external system or an upstream task in the same DAG."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Any suggestions on what could be causing this would be much appreciated.", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________ From: siddharth anand <sanand@apache.org Sent: 16 November 2016 00:40 To: dev@airflow.incubator.apache.org Subject: Re: Skip task  If your requirement is to skip a portion of tasks in a DagRun based on some state encountered while executing that DagRun, that is what BranchPythonOperator or ShortCircruitOperator (optionally paired with a Trigger Rule specified on a downstream task) is made for.", "These operators take a custom Python callable as a argument.", "The callable can check for the existence of data or files that should have been generated by an external system or an upstream task in the same DAG.", "The callables need to return a Boolean value in the case of the ShortCircruitOperator or a selected choice (i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________ From: siddharth anand <sanand@apache.org Sent: 16 November 2016 00:40 To: dev@airflow.incubator.apache.org Subject: Re: Skip task  If your requirement is to skip a portion of tasks in a DagRun based on some state encountered while executing that DagRun, that is what BranchPythonOperator or ShortCircruitOperator (optionally paired with a Trigger Rule specified on a downstream task) is made for.", "These operators take a custom Python callable as a argument.", "The callable can check for the existence of data or files that should have been generated by an external system or an upstream task in the same DAG.", "The callables need to return a Boolean value in the case of the ShortCircruitOperator or a selected choice (i.e.", "branch to take) as in the case of the BranchPythonOperator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These operators take a custom Python callable as a argument.", "The callable can check for the existence of data or files that should have been generated by an external system or an upstream task in the same DAG.", "The callables need to return a Boolean value in the case of the ShortCircruitOperator or a selected choice (i.e.", "branch to take) as in the case of the BranchPythonOperator.", "If you have 20 tasks that all depend on the presence of 20 different files, you would need 20 ShortCircruitOperator or BranchPythonOperator tasks each either sharing a common callable or each with its own callable."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The callable can check for the existence of data or files that should have been generated by an external system or an upstream task in the same DAG.", "The callables need to return a Boolean value in the case of the ShortCircruitOperator or a selected choice (i.e.", "branch to take) as in the case of the BranchPythonOperator.", "If you have 20 tasks that all depend on the presence of 20 different files, you would need 20 ShortCircruitOperator or BranchPythonOperator tasks each either sharing a common callable or each with its own callable.", "One could argue that these tasks are \"overhead\" because they just encompass some conditional or control logic and that DAGs should only contain workhorse tasks (i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you have 20 tasks that all depend on the presence of 20 different files, you would need 20 ShortCircruitOperator or BranchPythonOperator tasks each either sharing a common callable or each with its own callable.", "One could argue that these tasks are \"overhead\" because they just encompass some conditional or control logic and that DAGs should only contain workhorse tasks (i.e.", "tasks that do some  work).", "DAGs with workhorse-only tasks are more of a pure dataflow approach -- i.e.", "no control-logic operators."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["One could argue that these tasks are \"overhead\" because they just encompass some conditional or control logic and that DAGs should only contain workhorse tasks (i.e.", "tasks that do some  work).", "DAGs with workhorse-only tasks are more of a pure dataflow approach -- i.e.", "no control-logic operators.", "However, I don't see another option."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["tasks that do some  work).", "DAGs with workhorse-only tasks are more of a pure dataflow approach -- i.e.", "no control-logic operators.", "However, I don't see another option.", "In the current system, a callable registered with a ShortCircruitOperator would check for the presence of a file -- if the file were not available, then a series of downstream tasks would be skipped in that DAGRun, until a task with a Trigger_Rule=\"all_done\" were encountered, downstream of which, tasks would no longer be skipped for the DagRun."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["DAGs with workhorse-only tasks are more of a pure dataflow approach -- i.e.", "no control-logic operators.", "However, I don't see another option.", "In the current system, a callable registered with a ShortCircruitOperator would check for the presence of a file -- if the file were not available, then a series of downstream tasks would be skipped in that DAGRun, until a task with a Trigger_Rule=\"all_done\" were encountered, downstream of which, tasks would no longer be skipped for the DagRun.", "I hope this makes sense."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, I don't see another option.", "In the current system, a callable registered with a ShortCircruitOperator would check for the presence of a file -- if the file were not available, then a series of downstream tasks would be skipped in that DAGRun, until a task with a Trigger_Rule=\"all_done\" were encountered, downstream of which, tasks would no longer be skipped for the DagRun.", "I hope this makes sense.", "A long time ago, I proposed UI functionality to skip a series of DAG runs via the UI, because I knew that no data was available for that time range from an external system.", "It wanted to essentially specify a \"blackout\" period in terms of a time range that covered multiple DagRuns."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the current system, a callable registered with a ShortCircruitOperator would check for the presence of a file -- if the file were not available, then a series of downstream tasks would be skipped in that DAGRun, until a task with a Trigger_Rule=\"all_done\" were encountered, downstream of which, tasks would no longer be skipped for the DagRun.", "I hope this makes sense.", "A long time ago, I proposed UI functionality to skip a series of DAG runs via the UI, because I knew that no data was available for that time range from an external system.", "It wanted to essentially specify a \"blackout\" period in terms of a time range that covered multiple DagRuns.", "My intention was for backfills to skip those days."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I hope this makes sense.", "A long time ago, I proposed UI functionality to skip a series of DAG runs via the UI, because I knew that no data was available for that time range from an external system.", "It wanted to essentially specify a \"blackout\" period in terms of a time range that covered multiple DagRuns.", "My intention was for backfills to skip those days.", "It turns out that my company did not end up having such a requirement, so I dropped the feature request."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A long time ago, I proposed UI functionality to skip a series of DAG runs via the UI, because I knew that no data was available for that time range from an external system.", "It wanted to essentially specify a \"blackout\" period in terms of a time range that covered multiple DagRuns.", "My intention was for backfills to skip those days.", "It turns out that my company did not end up having such a requirement, so I dropped the feature request.", "If this is what you are asking for, then I am +1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It wanted to essentially specify a \"blackout\" period in terms of a time range that covered multiple DagRuns.", "My intention was for backfills to skip those days.", "It turns out that my company did not end up having such a requirement, so I dropped the feature request.", "If this is what you are asking for, then I am +1.", "Please implement it and submit a PR."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My intention was for backfills to skip those days.", "It turns out that my company did not end up having such a requirement, so I dropped the feature request.", "If this is what you are asking for, then I am +1.", "Please implement it and submit a PR.", "On Tue, Nov 15, 2016 at 2:50 AM, Maycock, Luke < luke.maycock@affiliate.oliverwyman.com wrote:   Thank you for taking the time to respond."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please implement it and submit a PR.", "On Tue, Nov 15, 2016 at 2:50 AM, Maycock, Luke < luke.maycock@affiliate.oliverwyman.com wrote:   Thank you for taking the time to respond.", "This is a great approach if you  know at the time of creating the DAG which tasks you expect to need to  skip.", "However, I don't think this is exactly the use case I have.", "For  example, I may be expecting a file to arrive in an FTP folder for loading  into a database but one day it doesn't arrive so I just want to skip that  task on that day."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Nov 15, 2016 at 2:50 AM, Maycock, Luke < luke.maycock@affiliate.oliverwyman.com wrote:   Thank you for taking the time to respond.", "This is a great approach if you  know at the time of creating the DAG which tasks you expect to need to  skip.", "However, I don't think this is exactly the use case I have.", "For  example, I may be expecting a file to arrive in an FTP folder for loading  into a database but one day it doesn't arrive so I just want to skip that  task on that day.", "Our workflows commonly have around 20 of these types of tasks in."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is a great approach if you  know at the time of creating the DAG which tasks you expect to need to  skip.", "However, I don't think this is exactly the use case I have.", "For  example, I may be expecting a file to arrive in an FTP folder for loading  into a database but one day it doesn't arrive so I just want to skip that  task on that day.", "Our workflows commonly have around 20 of these types of tasks in.", "I could  configure all of these tasks in the way you suggested in case I ever need  to skip one of them."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, I don't think this is exactly the use case I have.", "For  example, I may be expecting a file to arrive in an FTP folder for loading  into a database but one day it doesn't arrive so I just want to skip that  task on that day.", "Our workflows commonly have around 20 of these types of tasks in.", "I could  configure all of these tasks in the way you suggested in case I ever need  to skip one of them.", "However, I'd prefer not to have to set the tasks up  this way and instead have the ability just to skip a task on an ad-hoc  basis."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Our workflows commonly have around 20 of these types of tasks in.", "I could  configure all of these tasks in the way you suggested in case I ever need  to skip one of them.", "However, I'd prefer not to have to set the tasks up  this way and instead have the ability just to skip a task on an ad-hoc  basis.", "I could then also use this functionality to add the ability to run  from a certain point in a DAG or to a certain point in the DAG.", "Thanks,  Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I could then also use this functionality to add the ability to run  from a certain point in a DAG or to a certain point in the DAG.", "Thanks,  Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/     ________________________________  From: siddharth anand <sanand@apache.org  Sent: 14 November 2016 19:48  To: dev@airflow.incubator.apache.org  Subject: Re: Skip task   For cases like this, we (Agari) use the following approach :     1.", "Create a Variable in the UI of type boolean such as *enable_feature_x*    2.", "Use a ShortCircuitOperator (or BranchPythonOperator) to Skip    downstream processing based on the value of *enable_feature_x*    3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/     ________________________________  From: siddharth anand <sanand@apache.org  Sent: 14 November 2016 19:48  To: dev@airflow.incubator.apache.org  Subject: Re: Skip task   For cases like this, we (Agari) use the following approach :     1.", "Create a Variable in the UI of type boolean such as *enable_feature_x*    2.", "Use a ShortCircuitOperator (or BranchPythonOperator) to Skip    downstream processing based on the value of *enable_feature_x*    3.", "Assuming that you don't want to skip ALL downstream tasks, you can    use a trigger_rule of all_done to resume processing some portion of your    downstream DAG after skipping an upstream portion   In other words, there is already a means to achieve what you are asking for  today.", "You can change the value of via *enable_feature_x  *the UI."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Create a Variable in the UI of type boolean such as *enable_feature_x*    2.", "Use a ShortCircuitOperator (or BranchPythonOperator) to Skip    downstream processing based on the value of *enable_feature_x*    3.", "Assuming that you don't want to skip ALL downstream tasks, you can    use a trigger_rule of all_done to resume processing some portion of your    downstream DAG after skipping an upstream portion   In other words, there is already a means to achieve what you are asking for  today.", "You can change the value of via *enable_feature_x  *the UI.", "If you'd  like to enhance the UI to better capture this pattern, pls submit a PR."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Use a ShortCircuitOperator (or BranchPythonOperator) to Skip    downstream processing based on the value of *enable_feature_x*    3.", "Assuming that you don't want to skip ALL downstream tasks, you can    use a trigger_rule of all_done to resume processing some portion of your    downstream DAG after skipping an upstream portion   In other words, there is already a means to achieve what you are asking for  today.", "You can change the value of via *enable_feature_x  *the UI.", "If you'd  like to enhance the UI to better capture this pattern, pls submit a PR.", "-s   On Thu, Nov 10, 2016 at 1:20 PM, Maycock, Luke <  luke.maycock@affiliate.oliverwyman.com wrote:    Hi Gerard,       I see the new status as having a number of uses:      1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You can change the value of via *enable_feature_x  *the UI.", "If you'd  like to enhance the UI to better capture this pattern, pls submit a PR.", "-s   On Thu, Nov 10, 2016 at 1:20 PM, Maycock, Luke <  luke.maycock@affiliate.oliverwyman.com wrote:    Hi Gerard,       I see the new status as having a number of uses:      1.", "A user can manually set a task to skip in a DAG run via the UI.", "2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you'd  like to enhance the UI to better capture this pattern, pls submit a PR.", "-s   On Thu, Nov 10, 2016 at 1:20 PM, Maycock, Luke <  luke.maycock@affiliate.oliverwyman.com wrote:    Hi Gerard,       I see the new status as having a number of uses:      1.", "A user can manually set a task to skip in a DAG run via the UI.", "2.", "We can then make use of this new status to add the following   functionality to Airflow:       *   Run a DAG run up to a certain point and have the rest of the  tasks   have the new status."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2.", "We can then make use of this new status to add the following   functionality to Airflow:       *   Run a DAG run up to a certain point and have the rest of the  tasks   have the new status.", "*  Run a DAG run from a certain task to the end, setting all   pre-requisite tasks to have this new status.", "I am happy to be challenged on the above use cases if there are better   ways to achieve the same things.", "Cheers,   Luke Maycock   OLIVER WYMAN   luke.maycock@affiliate.oliverwyman.com<mailto:luke."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We can then make use of this new status to add the following   functionality to Airflow:       *   Run a DAG run up to a certain point and have the rest of the  tasks   have the new status.", "*  Run a DAG run from a certain task to the end, setting all   pre-requisite tasks to have this new status.", "I am happy to be challenged on the above use cases if there are better   ways to achieve the same things.", "Cheers,   Luke Maycock   OLIVER WYMAN   luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com   www.oliverwyman.com<http://www.oliverwyman.com/         ________________________________   From: Gerard Toonstra <gtoonstra@gmail.com   Sent: 09 November 2016 18:08   To: dev@airflow.incubator.apache.org   Subject: Re: Skip task     Hey Luke,     Who or what makes the decision to skip processing that task?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*  Run a DAG run from a certain task to the end, setting all   pre-requisite tasks to have this new status.", "I am happy to be challenged on the above use cases if there are better   ways to achieve the same things.", "Cheers,   Luke Maycock   OLIVER WYMAN   luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com   www.oliverwyman.com<http://www.oliverwyman.com/         ________________________________   From: Gerard Toonstra <gtoonstra@gmail.com   Sent: 09 November 2016 18:08   To: dev@airflow.incubator.apache.org   Subject: Re: Skip task     Hey Luke,     Who or what makes the decision to skip processing that task?", "Rgds,     Gerard     On Wed, Nov 9, 2016 at 2:39 PM, Maycock, Luke <   luke.maycock@affiliate.oliverwyman.com wrote:      Hi Gerard,          Thank you for your quick response."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am happy to be challenged on the above use cases if there are better   ways to achieve the same things.", "Cheers,   Luke Maycock   OLIVER WYMAN   luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com   www.oliverwyman.com<http://www.oliverwyman.com/         ________________________________   From: Gerard Toonstra <gtoonstra@gmail.com   Sent: 09 November 2016 18:08   To: dev@airflow.incubator.apache.org   Subject: Re: Skip task     Hey Luke,     Who or what makes the decision to skip processing that task?", "Rgds,     Gerard     On Wed, Nov 9, 2016 at 2:39 PM, Maycock, Luke <   luke.maycock@affiliate.oliverwyman.com wrote:      Hi Gerard,          Thank you for your quick response.", "I am not trying to implement this for a specific operator but rather    trying to add it as a feature for any task in any DAG."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers,   Luke Maycock   OLIVER WYMAN   luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com   www.oliverwyman.com<http://www.oliverwyman.com/         ________________________________   From: Gerard Toonstra <gtoonstra@gmail.com   Sent: 09 November 2016 18:08   To: dev@airflow.incubator.apache.org   Subject: Re: Skip task     Hey Luke,     Who or what makes the decision to skip processing that task?", "Rgds,     Gerard     On Wed, Nov 9, 2016 at 2:39 PM, Maycock, Luke <   luke.maycock@affiliate.oliverwyman.com wrote:      Hi Gerard,          Thank you for your quick response.", "I am not trying to implement this for a specific operator but rather    trying to add it as a feature for any task in any DAG.", "Given that the skipped states propagate where all directly upstream  tasks    are skipped, I don't think this is the state we want to use."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["maycock@affiliate.oliverwyman.com   www.oliverwyman.com<http://www.oliverwyman.com/         ________________________________   From: Gerard Toonstra <gtoonstra@gmail.com   Sent: 09 November 2016 18:08   To: dev@airflow.incubator.apache.org   Subject: Re: Skip task     Hey Luke,     Who or what makes the decision to skip processing that task?", "Rgds,     Gerard     On Wed, Nov 9, 2016 at 2:39 PM, Maycock, Luke <   luke.maycock@affiliate.oliverwyman.com wrote:      Hi Gerard,          Thank you for your quick response.", "I am not trying to implement this for a specific operator but rather    trying to add it as a feature for any task in any DAG.", "Given that the skipped states propagate where all directly upstream  tasks    are skipped, I don't think this is the state we want to use.", "For the    functionality I'm looking for, I think I'll need to introduce a new   status,    maybe 'disabled'."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Rgds,     Gerard     On Wed, Nov 9, 2016 at 2:39 PM, Maycock, Luke <   luke.maycock@affiliate.oliverwyman.com wrote:      Hi Gerard,          Thank you for your quick response.", "I am not trying to implement this for a specific operator but rather    trying to add it as a feature for any task in any DAG.", "Given that the skipped states propagate where all directly upstream  tasks    are skipped, I don't think this is the state we want to use.", "For the    functionality I'm looking for, I think I'll need to introduce a new   status,    maybe 'disabled'.", "Again, thanks for your response."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am not trying to implement this for a specific operator but rather    trying to add it as a feature for any task in any DAG.", "Given that the skipped states propagate where all directly upstream  tasks    are skipped, I don't think this is the state we want to use.", "For the    functionality I'm looking for, I think I'll need to introduce a new   status,    maybe 'disabled'.", "Again, thanks for your response.", "Cheers,    Luke Maycock    OLIVER WYMAN    luke.maycock@affiliate.oliverwyman.com<mailto:luke."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Given that the skipped states propagate where all directly upstream  tasks    are skipped, I don't think this is the state we want to use.", "For the    functionality I'm looking for, I think I'll need to introduce a new   status,    maybe 'disabled'.", "Again, thanks for your response.", "Cheers,    Luke Maycock    OLIVER WYMAN    luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com    www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________    From: Gerard Toonstra <gtoonstra@gmail.com    Sent: 08 November 2016 18:19    To: dev@airflow.incubator.apache.org    Subject: Re: Skip task       Also in 1.7.1.3, there's the ShortCircuitOperator, which can give you  an    example."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For the    functionality I'm looking for, I think I'll need to introduce a new   status,    maybe 'disabled'.", "Again, thanks for your response.", "Cheers,    Luke Maycock    OLIVER WYMAN    luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com    www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________    From: Gerard Toonstra <gtoonstra@gmail.com    Sent: 08 November 2016 18:19    To: dev@airflow.incubator.apache.org    Subject: Re: Skip task       Also in 1.7.1.3, there's the ShortCircuitOperator, which can give you  an    example.", "https://github.com/apache/incubator-airflow/blob/1.7.1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Again, thanks for your response.", "Cheers,    Luke Maycock    OLIVER WYMAN    luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com    www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________    From: Gerard Toonstra <gtoonstra@gmail.com    Sent: 08 November 2016 18:19    To: dev@airflow.incubator.apache.org    Subject: Re: Skip task       Also in 1.7.1.3, there's the ShortCircuitOperator, which can give you  an    example.", "https://github.com/apache/incubator-airflow/blob/1.7.1.", "3/airflow/operators/python_operator.py       You'd have to modify this to your needs, but the way it works is that  if    the condition evaluates to True, none of the    downstream tasks are actually executed, they'd be skipped."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers,    Luke Maycock    OLIVER WYMAN    luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com    www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________    From: Gerard Toonstra <gtoonstra@gmail.com    Sent: 08 November 2016 18:19    To: dev@airflow.incubator.apache.org    Subject: Re: Skip task       Also in 1.7.1.3, there's the ShortCircuitOperator, which can give you  an    example.", "https://github.com/apache/incubator-airflow/blob/1.7.1.", "3/airflow/operators/python_operator.py       You'd have to modify this to your needs, but the way it works is that  if    the condition evaluates to True, none of the    downstream tasks are actually executed, they'd be skipped.", "The reason  for    putting them into SKIPPED state is that    the DAG final result would still be SUCCESS and not failed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The reason  for    putting them into SKIPPED state is that    the DAG final result would still be SUCCESS and not failed.", "You could copy the operator from there and don't do the full \"for  loop\",    only pick the tasks immediately downstream    from this operator and skip that.", "Or... if you need to skip additional    tasks downstream, add a parameter \"num_tasks\"    that decide on a halting condition for the for loop.", "I believe that should work.", "I didn't try that here, but you can test  that    and see what it does for you."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You could copy the operator from there and don't do the full \"for  loop\",    only pick the tasks immediately downstream    from this operator and skip that.", "Or... if you need to skip additional    tasks downstream, add a parameter \"num_tasks\"    that decide on a halting condition for the for loop.", "I believe that should work.", "I didn't try that here, but you can test  that    and see what it does for you.", "If you want this as a UI capability... for example have a human  operator    decide on skipping this yes or not, then    maybe the best way forward would be some kind of highly custom plugin   with    its own view."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I believe that should work.", "I didn't try that here, but you can test  that    and see what it does for you.", "If you want this as a UI capability... for example have a human  operator    decide on skipping this yes or not, then    maybe the best way forward would be some kind of highly custom plugin   with    its own view.", "In the end, you'd basically    do the same action in the backend, whether the python cond evaluates to    True or the button is clicked.", "In the plugin case though, you'd have to keep the UI and the structure  of    the DAG in sync and aligned, otherwise    it'd become a mess.... Airflow wasn't really developed for  workflow/human    interaction, but in workflows where only    automated processes are involved."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the end, you'd basically    do the same action in the backend, whether the python cond evaluates to    True or the button is clicked.", "In the plugin case though, you'd have to keep the UI and the structure  of    the DAG in sync and aligned, otherwise    it'd become a mess.... Airflow wasn't really developed for  workflow/human    interaction, but in workflows where only    automated processes are involved.", "That doesn't mean that you can't do    anything like that, but it may be costly resource    wise to get this done.", "For example, on the basis of the BranchOperator,   you    could call an external API to verify if a decision    was taken on a case, then follow branch A or B if the decision is there   or    put the state back into UP_FOR_RETRY.", "At the moment though, there's no programmatic way to reschedule that  task    to some minutes or hours into the future before    it's looked at again, unless you really dive into airflow, scheduling    semantics (@once vs. other schedules) and how    the scheduler works."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the plugin case though, you'd have to keep the UI and the structure  of    the DAG in sync and aligned, otherwise    it'd become a mess.... Airflow wasn't really developed for  workflow/human    interaction, but in workflows where only    automated processes are involved.", "That doesn't mean that you can't do    anything like that, but it may be costly resource    wise to get this done.", "For example, on the basis of the BranchOperator,   you    could call an external API to verify if a decision    was taken on a case, then follow branch A or B if the decision is there   or    put the state back into UP_FOR_RETRY.", "At the moment though, there's no programmatic way to reschedule that  task    to some minutes or hours into the future before    it's looked at again, unless you really dive into airflow, scheduling    semantics (@once vs. other schedules) and how    the scheduler works.", "Rgds,       Gerard                On Tue, Nov 8, 2016 at 5:30 PM, Maycock, Luke <    luke.maycock@affiliate.oliverwyman.com wrote:        Hi All,             I am using Airflow 1.7.1.3 and have a particular requirement, which I     don't think is currently supported by Airflow but just wanted to  check   in     case I was missing something."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That doesn't mean that you can't do    anything like that, but it may be costly resource    wise to get this done.", "For example, on the basis of the BranchOperator,   you    could call an external API to verify if a decision    was taken on a case, then follow branch A or B if the decision is there   or    put the state back into UP_FOR_RETRY.", "At the moment though, there's no programmatic way to reschedule that  task    to some minutes or hours into the future before    it's looked at again, unless you really dive into airflow, scheduling    semantics (@once vs. other schedules) and how    the scheduler works.", "Rgds,       Gerard                On Tue, Nov 8, 2016 at 5:30 PM, Maycock, Luke <    luke.maycock@affiliate.oliverwyman.com wrote:        Hi All,             I am using Airflow 1.7.1.3 and have a particular requirement, which I     don't think is currently supported by Airflow but just wanted to  check   in     case I was missing something.", "I occasionally wish to skip a particular task in a given DAG run such    that     the task does not run for that DAG run."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For example, on the basis of the BranchOperator,   you    could call an external API to verify if a decision    was taken on a case, then follow branch A or B if the decision is there   or    put the state back into UP_FOR_RETRY.", "At the moment though, there's no programmatic way to reschedule that  task    to some minutes or hours into the future before    it's looked at again, unless you really dive into airflow, scheduling    semantics (@once vs. other schedules) and how    the scheduler works.", "Rgds,       Gerard                On Tue, Nov 8, 2016 at 5:30 PM, Maycock, Luke <    luke.maycock@affiliate.oliverwyman.com wrote:        Hi All,             I am using Airflow 1.7.1.3 and have a particular requirement, which I     don't think is currently supported by Airflow but just wanted to  check   in     case I was missing something.", "I occasionally wish to skip a particular task in a given DAG run such    that     the task does not run for that DAG run.", "Is this functionality  available    in     Airflow?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["At the moment though, there's no programmatic way to reschedule that  task    to some minutes or hours into the future before    it's looked at again, unless you really dive into airflow, scheduling    semantics (@once vs. other schedules) and how    the scheduler works.", "Rgds,       Gerard                On Tue, Nov 8, 2016 at 5:30 PM, Maycock, Luke <    luke.maycock@affiliate.oliverwyman.com wrote:        Hi All,             I am using Airflow 1.7.1.3 and have a particular requirement, which I     don't think is currently supported by Airflow but just wanted to  check   in     case I was missing something.", "I occasionally wish to skip a particular task in a given DAG run such    that     the task does not run for that DAG run.", "Is this functionality  available    in     Airflow?", "I am aware of the BranchPythonOperator (https://airflow.incubator."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is this functionality  available    in     Airflow?", "I am aware of the BranchPythonOperator (https://airflow.incubator.", "apache.org/concepts.html#branching) but I don't think believe this  is     exactly what I am looking for.", "I am thinking that a button in the UI alongside the 'Mark Success'  and     'Run' buttons would be appropriate.", "If the functionality does not exist, does anyone have any suggestions   on     ways to implement this?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am aware of the BranchPythonOperator (https://airflow.incubator.", "apache.org/concepts.html#branching) but I don't think believe this  is     exactly what I am looking for.", "I am thinking that a button in the UI alongside the 'Mark Success'  and     'Run' buttons would be appropriate.", "If the functionality does not exist, does anyone have any suggestions   on     ways to implement this?", "Cheers,     Luke Maycock     OLIVER WYMAN     luke.maycock@affiliate.oliverwyman.com<mailto:luke."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["apache.org/concepts.html#branching) but I don't think believe this  is     exactly what I am looking for.", "I am thinking that a button in the UI alongside the 'Mark Success'  and     'Run' buttons would be appropriate.", "If the functionality does not exist, does anyone have any suggestions   on     ways to implement this?", "Cheers,     Luke Maycock     OLIVER WYMAN     luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com     www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________     This e-mail and any attachments may be confidential or legally    privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If the functionality does not exist, does anyone have any suggestions   on     ways to implement this?", "Cheers,     Luke Maycock     OLIVER WYMAN     luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com     www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________     This e-mail and any attachments may be confidential or legally    privileged.", "If you received this message in error or are not the intended   recipient,     you should destroy the e-mail message and any attachments or copies,   and     you are prohibited from retaining, distributing, disclosing or using   any     information contained herein.", "Please inform us of the erroneous   delivery    by     return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["maycock@affiliate.oliverwyman.com     www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________     This e-mail and any attachments may be confidential or legally    privileged.", "If you received this message in error or are not the intended   recipient,     you should destroy the e-mail message and any attachments or copies,   and     you are prohibited from retaining, distributing, disclosing or using   any     information contained herein.", "Please inform us of the erroneous   delivery    by     return e-mail.", "Thank you for your cooperation.", "________________________________    This e-mail and any attachments may be confidential or legally   privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thank you for your cooperation.", "________________________________    This e-mail and any attachments may be confidential or legally   privileged.", "If you received this message in error or are not the intended  recipient,    you should destroy the e-mail message and any attachments or copies,  and    you are prohibited from retaining, distributing, disclosing or using  any    information contained herein.", "Please inform us of the erroneous  delivery   by    return e-mail.", "Thank you for your cooperation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["________________________________    This e-mail and any attachments may be confidential or legally   privileged.", "If you received this message in error or are not the intended  recipient,    you should destroy the e-mail message and any attachments or copies,  and    you are prohibited from retaining, distributing, disclosing or using  any    information contained herein.", "Please inform us of the erroneous  delivery   by    return e-mail.", "Thank you for your cooperation.", "________________________________   This e-mail and any attachments may be confidential or legally  privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please inform us of the erroneous  delivery   by    return e-mail.", "Thank you for your cooperation.", "________________________________   This e-mail and any attachments may be confidential or legally  privileged.", "If you received this message in error or are not the intended recipient,   you should destroy the e-mail message and any attachments or copies, and   you are prohibited from retaining, distributing, disclosing or using any   information contained herein.", "Please inform us of the erroneous delivery  by   return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thank you for your cooperation.", "________________________________   This e-mail and any attachments may be confidential or legally  privileged.", "If you received this message in error or are not the intended recipient,   you should destroy the e-mail message and any attachments or copies, and   you are prohibited from retaining, distributing, disclosing or using any   information contained herein.", "Please inform us of the erroneous delivery  by   return e-mail.", "Thank you for your cooperation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["________________________________   This e-mail and any attachments may be confidential or legally  privileged.", "If you received this message in error or are not the intended recipient,   you should destroy the e-mail message and any attachments or copies, and   you are prohibited from retaining, distributing, disclosing or using any   information contained herein.", "Please inform us of the erroneous delivery  by   return e-mail.", "Thank you for your cooperation.", "________________________________  This e-mail and any attachments may be confidential or legally privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you received this message in error or are not the intended recipient,   you should destroy the e-mail message and any attachments or copies, and   you are prohibited from retaining, distributing, disclosing or using any   information contained herein.", "Please inform us of the erroneous delivery  by   return e-mail.", "Thank you for your cooperation.", "________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient,  you should destroy the e-mail message and any attachments or copies, and  you are prohibited from retaining, distributing, disclosing or using any  information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please inform us of the erroneous delivery  by   return e-mail.", "Thank you for your cooperation.", "________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient,  you should destroy the e-mail message and any attachments or copies, and  you are prohibited from retaining, distributing, disclosing or using any  information contained herein.", "Please inform us of the erroneous delivery by  return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient,  you should destroy the e-mail message and any attachments or copies, and  you are prohibited from retaining, distributing, disclosing or using any  information contained herein.", "Please inform us of the erroneous delivery by  return e-mail.", "Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you received this message in error or are not the intended recipient,  you should destroy the e-mail message and any attachments or copies, and  you are prohibited from retaining, distributing, disclosing or using any  information contained herein.", "Please inform us of the erroneous delivery by  return e-mail.", "Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Nice, thanks for fixing this!", "Were you able to clear the cache?", "Seems like I can't terminate builds since I don't have admin rights on the underlying repo, so I was assuming I couldn't clear the cache as welll...  Max  On Tue, Aug 9, 2016 at 6:15 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   This is fixed.", "This was due to an issue with Travis\u2019 cache: it can corrupt  the cache and therefore  untarring fails.", "Now hive is re-downloaded in case this happens."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Were you able to clear the cache?", "Seems like I can't terminate builds since I don't have admin rights on the underlying repo, so I was assuming I couldn't clear the cache as welll...  Max  On Tue, Aug 9, 2016 at 6:15 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   This is fixed.", "This was due to an issue with Travis\u2019 cache: it can corrupt  the cache and therefore  untarring fails.", "Now hive is re-downloaded in case this happens.", "- Bolke    Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org het  volgende geschreven:     We have a few non-deterministic unit test failures that are affecting  many   -- but not all -- PRs."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Seems like I can't terminate builds since I don't have admin rights on the underlying repo, so I was assuming I couldn't clear the cache as welll...  Max  On Tue, Aug 9, 2016 at 6:15 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   This is fixed.", "This was due to an issue with Travis\u2019 cache: it can corrupt  the cache and therefore  untarring fails.", "Now hive is re-downloaded in case this happens.", "- Bolke    Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org het  volgende geschreven:     We have a few non-deterministic unit test failures that are affecting  many   -- but not all -- PRs.", "I believe they are being ignored as \"unrelated\"  but   they have the potential to mask real issues and should be addressed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This was due to an issue with Travis\u2019 cache: it can corrupt  the cache and therefore  untarring fails.", "Now hive is re-downloaded in case this happens.", "- Bolke    Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org het  volgende geschreven:     We have a few non-deterministic unit test failures that are affecting  many   -- but not all -- PRs.", "I believe they are being ignored as \"unrelated\"  but   they have the potential to mask real issues and should be addressed.", "Unfortunately they're out of my expertise so I'm going to list the ones   I've identified and hope someone smarter than me can see if they can  help!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Now hive is re-downloaded in case this happens.", "- Bolke    Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org het  volgende geschreven:     We have a few non-deterministic unit test failures that are affecting  many   -- but not all -- PRs.", "I believe they are being ignored as \"unrelated\"  but   they have the potential to mask real issues and should be addressed.", "Unfortunately they're out of my expertise so I'm going to list the ones   I've identified and hope someone smarter than me can see if they can  help!", "In particular, we have a number of simple PR's that should obviously have   no problems (typos, readme edits, etc.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Bolke    Op 4 aug. 2016, om 14:50 heeft Jeremiah Lowin <jlowin@apache.org het  volgende geschreven:     We have a few non-deterministic unit test failures that are affecting  many   -- but not all -- PRs.", "I believe they are being ignored as \"unrelated\"  but   they have the potential to mask real issues and should be addressed.", "Unfortunately they're out of my expertise so I'm going to list the ones   I've identified and hope someone smarter than me can see if they can  help!", "In particular, we have a number of simple PR's that should obviously have   no problems (typos, readme edits, etc.)", "that are nonetheless failing  tests,   causing frustration for all."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["that are nonetheless failing  tests,   causing frustration for all.", "Here is one from just this morning:   https://github.com/apache/incubator-airflow/pull/1705/files     Thanks in advance!", "1.", "Python 3 Mysql (this one is pretty common), due to not being able to   find \"beeline\" which I believe is related to Hive.", "This is the error:     ======================================================================     ERROR: test_mysql_to_hive_partition (tests.TransferTests)     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/  operators/operators.py\",   line 208, in test_mysql_to_hive_partition        t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 2350, in run        force=force,)      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 1388, in run        result = task_copy.execute(context=context)      File \"/home/travis/build/apache/incubator-airflow/airflow/  operators/mysql_to_hive.py\",   line 131, in execute        recreate=self.recreate)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 322, in load_file        self.run_cli(hql)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 212, in run_cli        cwd=tmp_dir)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 858, in  __init__        restore_signals, start_new_session)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 1456, in   _execute_child        raise child_exception_type(errno_num, err_msg)     nose.proxy.FileNotFoundError: [Errno 2] No such file or directory:  'beeline'       2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["1.", "Python 3 Mysql (this one is pretty common), due to not being able to   find \"beeline\" which I believe is related to Hive.", "This is the error:     ======================================================================     ERROR: test_mysql_to_hive_partition (tests.TransferTests)     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/  operators/operators.py\",   line 208, in test_mysql_to_hive_partition        t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 2350, in run        force=force,)      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 1388, in run        result = task_copy.execute(context=context)      File \"/home/travis/build/apache/incubator-airflow/airflow/  operators/mysql_to_hive.py\",   line 131, in execute        recreate=self.recreate)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 322, in load_file        self.run_cli(hql)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 212, in run_cli        cwd=tmp_dir)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 858, in  __init__        restore_signals, start_new_session)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 1456, in   _execute_child        raise child_exception_type(errno_num, err_msg)     nose.proxy.FileNotFoundError: [Errno 2] No such file or directory:  'beeline'       2.", "Python 3 Postgres (this one is really infrequent):     ======================================================================     FAIL: Test that ignore_first_depends_on_past doesn't affect results     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 349, in test_dagrun_deadlock_ignore_depends_on_past        run_kwargs=dict(ignore_first_depends_on_past=True))      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 221, in evaluate_dagrun        self.assertEqual(ti.state, expected_state)     nose.proxy.AssertionError: None != 'success'     3.", "Mysql (py2 and py3, infrequent)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is the error:     ======================================================================     ERROR: test_mysql_to_hive_partition (tests.TransferTests)     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/  operators/operators.py\",   line 208, in test_mysql_to_hive_partition        t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 2350, in run        force=force,)      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 1388, in run        result = task_copy.execute(context=context)      File \"/home/travis/build/apache/incubator-airflow/airflow/  operators/mysql_to_hive.py\",   line 131, in execute        recreate=self.recreate)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 322, in load_file        self.run_cli(hql)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 212, in run_cli        cwd=tmp_dir)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 858, in  __init__        restore_signals, start_new_session)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 1456, in   _execute_child        raise child_exception_type(errno_num, err_msg)     nose.proxy.FileNotFoundError: [Errno 2] No such file or directory:  'beeline'       2.", "Python 3 Postgres (this one is really infrequent):     ======================================================================     FAIL: Test that ignore_first_depends_on_past doesn't affect results     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 349, in test_dagrun_deadlock_ignore_depends_on_past        run_kwargs=dict(ignore_first_depends_on_past=True))      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 221, in evaluate_dagrun        self.assertEqual(ti.state, expected_state)     nose.proxy.AssertionError: None != 'success'     3.", "Mysql (py2 and py3, infrequent).", "This appears to happen when the   SLA code is called wiht mysql.", "Bizarrely, this doesn't appear to   actually raise an error in the test -- it just prints a logging error."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Python 3 Postgres (this one is really infrequent):     ======================================================================     FAIL: Test that ignore_first_depends_on_past doesn't affect results     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 349, in test_dagrun_deadlock_ignore_depends_on_past        run_kwargs=dict(ignore_first_depends_on_past=True))      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 221, in evaluate_dagrun        self.assertEqual(ti.state, expected_state)     nose.proxy.AssertionError: None != 'success'     3.", "Mysql (py2 and py3, infrequent).", "This appears to happen when the   SLA code is called wiht mysql.", "Bizarrely, this doesn't appear to   actually raise an error in the test -- it just prints a logging error.", "It must be trapped somewhere."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Mysql (py2 and py3, infrequent).", "This appears to happen when the   SLA code is called wiht mysql.", "Bizarrely, this doesn't appear to   actually raise an error in the test -- it just prints a logging error.", "It must be trapped somewhere.", "ERROR [airflow.jobs.SchedulerJob] Boolean value of this clause is not  defined     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/airflow/jobs.py\",   line 667, in _do_dags        self.manage_slas(dag)      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 53, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/airflow/jobs.py\",   line 301, in manage_slas        .all()      File \"/home/travis/build/apache/incubator-airflow/.tox/py34-  cdh-airflow_backend_mysql/lib/python3.4/site-packages/  sqlalchemy/sql/elements.py\",   line 2760, in __bool__        raise TypeError(\"Boolean value of this clause is not defined\")     TypeError: Boolean value of this clause is not defined    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We hosted several metopes here at Blue Apron.", "I will bring it up to our administrative team and give an update.", "Mid-january is probably a good target.", "- Joe  On Thu, Dec 15, 2016 at 5:18 PM, Luke Ptz <lukeptzcode@gmail.com wrote:   Cool to see the interest is there!", "I unfortunately can't offer a space for  a meetup, can anyone else?"], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I will bring it up to our administrative team and give an update.", "Mid-january is probably a good target.", "- Joe  On Thu, Dec 15, 2016 at 5:18 PM, Luke Ptz <lukeptzcode@gmail.com wrote:   Cool to see the interest is there!", "I unfortunately can't offer a space for  a meetup, can anyone else?", "If not could always be informal/meet in a public  setting   On Wed, Dec 14, 2016 at 7:08 PM, Andrew Phillips <andrewp@apache.org  wrote:    We at Blue Apron would be very interested."], "labels": ["1", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["- Joe  On Thu, Dec 15, 2016 at 5:18 PM, Luke Ptz <lukeptzcode@gmail.com wrote:   Cool to see the interest is there!", "I unfortunately can't offer a space for  a meetup, can anyone else?", "If not could always be informal/meet in a public  setting   On Wed, Dec 14, 2016 at 7:08 PM, Andrew Phillips <andrewp@apache.org  wrote:    We at Blue Apron would be very interested.", "Same here.", "ap       --  *Joe Napolitano *| Sr. Data Engineer www.blueapron.com | 5 Crosby Street, New York, NY 10013  "], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Max,  Sounds good.", "Couple of things:   * Can I suggest using the v1-8-test branch as the branch to be used for preparing the rc?", "If we hit RC then move it over to v1-8-stable?", "V1-8-test already had some fixes in that should land in 1.8.2 and the RC should be tagged in the stable branch.", "That also reduces to amount of merge conflicts probably as many have been merged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Couple of things:   * Can I suggest using the v1-8-test branch as the branch to be used for preparing the rc?", "If we hit RC then move it over to v1-8-stable?", "V1-8-test already had some fixes in that should land in 1.8.2 and the RC should be tagged in the stable branch.", "That also reduces to amount of merge conflicts probably as many have been merged.", "Where did you branch off from?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If we hit RC then move it over to v1-8-stable?", "V1-8-test already had some fixes in that should land in 1.8.2 and the RC should be tagged in the stable branch.", "That also reduces to amount of merge conflicts probably as many have been merged.", "Where did you branch off from?", "Anyways, see also the release management thing on the wiki."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["V1-8-test already had some fixes in that should land in 1.8.2 and the RC should be tagged in the stable branch.", "That also reduces to amount of merge conflicts probably as many have been merged.", "Where did you branch off from?", "Anyways, see also the release management thing on the wiki.", "Blocker(!)"], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["That also reduces to amount of merge conflicts probably as many have been merged.", "Where did you branch off from?", "Anyways, see also the release management thing on the wiki.", "Blocker(!)", "* In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed (https://issues.apache.org/jira/browse/AIRFLOW-1294 <https://issues.apache.org/jira/browse/AIRFLOW-1294)."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Where did you branch off from?", "Anyways, see also the release management thing on the wiki.", "Blocker(!)", "* In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed (https://issues.apache.org/jira/browse/AIRFLOW-1294 <https://issues.apache.org/jira/browse/AIRFLOW-1294).", "Setting itself to NONE should probably be \u201cCONCURRENCY_REACHED\u201d (new state)."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Anyways, see also the release management thing on the wiki.", "Blocker(!)", "* In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed (https://issues.apache.org/jira/browse/AIRFLOW-1294 <https://issues.apache.org/jira/browse/AIRFLOW-1294).", "Setting itself to NONE should probably be \u201cCONCURRENCY_REACHED\u201d (new state).", "I have marked it as a blocker as we had multiple people hitting the issue, but I need 1-2 days to get a patch."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Blocker(!)", "* In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed (https://issues.apache.org/jira/browse/AIRFLOW-1294 <https://issues.apache.org/jira/browse/AIRFLOW-1294).", "Setting itself to NONE should probably be \u201cCONCURRENCY_REACHED\u201d (new state).", "I have marked it as a blocker as we had multiple people hitting the issue, but I need 1-2 days to get a patch.", "Feel free to downgrade to critical if you like :)."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["* In the backfills we can loose tasks to execute due to a task setting its own state to NONE if concurrency limits are reached, this makes them fall outside of the scope the backfill is managing hence they will not be executed (https://issues.apache.org/jira/browse/AIRFLOW-1294 <https://issues.apache.org/jira/browse/AIRFLOW-1294).", "Setting itself to NONE should probably be \u201cCONCURRENCY_REACHED\u201d (new state).", "I have marked it as a blocker as we had multiple people hitting the issue, but I need 1-2 days to get a patch.", "Feel free to downgrade to critical if you like :).", "Cheers Bolke    On 8 Jun 2017, at 02:35, Maxime Beauchemin <maximebeauchemin@gmail.com wrote:    What a pleasant, mind numbing afternoon doing some release management    Notes:  * Added a warning that the package name has changed on Pypi  <https://pypi.python.org/pypi/airflow  * Removed references to my name here  <https://github.com/apache/incubator-airflow/pull/2352 and merged  * Addressed John D. Ament's concerns here  <https://github.com/apache/incubator-airflow/pull/2354, please review!"], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Setting itself to NONE should probably be \u201cCONCURRENCY_REACHED\u201d (new state).", "I have marked it as a blocker as we had multiple people hitting the issue, but I need 1-2 days to get a patch.", "Feel free to downgrade to critical if you like :).", "Cheers Bolke    On 8 Jun 2017, at 02:35, Maxime Beauchemin <maximebeauchemin@gmail.com wrote:    What a pleasant, mind numbing afternoon doing some release management    Notes:  * Added a warning that the package name has changed on Pypi  <https://pypi.python.org/pypi/airflow  * Removed references to my name here  <https://github.com/apache/incubator-airflow/pull/2352 and merged  * Addressed John D. Ament's concerns here  <https://github.com/apache/incubator-airflow/pull/2354, please review!", "* \"footable\" appears to have been removed, not a problem anymore  * that `airflow-jira is a god send!"], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have marked it as a blocker as we had multiple people hitting the issue, but I need 1-2 days to get a patch.", "Feel free to downgrade to critical if you like :).", "Cheers Bolke    On 8 Jun 2017, at 02:35, Maxime Beauchemin <maximebeauchemin@gmail.com wrote:    What a pleasant, mind numbing afternoon doing some release management    Notes:  * Added a warning that the package name has changed on Pypi  <https://pypi.python.org/pypi/airflow  * Removed references to my name here  <https://github.com/apache/incubator-airflow/pull/2352 and merged  * Addressed John D. Ament's concerns here  <https://github.com/apache/incubator-airflow/pull/2354, please review!", "* \"footable\" appears to have been removed, not a problem anymore  * that `airflow-jira is a god send!", "thanks Bolke."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers Bolke    On 8 Jun 2017, at 02:35, Maxime Beauchemin <maximebeauchemin@gmail.com wrote:    What a pleasant, mind numbing afternoon doing some release management    Notes:  * Added a warning that the package name has changed on Pypi  <https://pypi.python.org/pypi/airflow  * Removed references to my name here  <https://github.com/apache/incubator-airflow/pull/2352 and merged  * Addressed John D. Ament's concerns here  <https://github.com/apache/incubator-airflow/pull/2354, please review!", "* \"footable\" appears to have been removed, not a problem anymore  * that `airflow-jira is a god send!", "thanks Bolke.", "* reviewed list of Airbnb's production cherries and flagged those as `Fix  Version == 1.8.2`  * Started branch v1-8-2.rc1 and started picking cherries using  `airflow-jira compare 1.8.2`    I'll finish going through picking everything that targeted 1.8.2 that does  not create merge conflict.", "If there's anything flagged as \"blocker\" that generates merge conflict,  I'll go case by case about it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["* \"footable\" appears to have been removed, not a problem anymore  * that `airflow-jira is a god send!", "thanks Bolke.", "* reviewed list of Airbnb's production cherries and flagged those as `Fix  Version == 1.8.2`  * Started branch v1-8-2.rc1 and started picking cherries using  `airflow-jira compare 1.8.2`    I'll finish going through picking everything that targeted 1.8.2 that does  not create merge conflict.", "If there's anything flagged as \"blocker\" that generates merge conflict,  I'll go case by case about it.", "Soon after, I should be able to announce 1.8.2 RC1, hopefully sometime  tomorrow or Friday."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["thanks Bolke.", "* reviewed list of Airbnb's production cherries and flagged those as `Fix  Version == 1.8.2`  * Started branch v1-8-2.rc1 and started picking cherries using  `airflow-jira compare 1.8.2`    I'll finish going through picking everything that targeted 1.8.2 that does  not create merge conflict.", "If there's anything flagged as \"blocker\" that generates merge conflict,  I'll go case by case about it.", "Soon after, I should be able to announce 1.8.2 RC1, hopefully sometime  tomorrow or Friday.", "Let me know if there's anything else I'm missing that I should consider."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["* reviewed list of Airbnb's production cherries and flagged those as `Fix  Version == 1.8.2`  * Started branch v1-8-2.rc1 and started picking cherries using  `airflow-jira compare 1.8.2`    I'll finish going through picking everything that targeted 1.8.2 that does  not create merge conflict.", "If there's anything flagged as \"blocker\" that generates merge conflict,  I'll go case by case about it.", "Soon after, I should be able to announce 1.8.2 RC1, hopefully sometime  tomorrow or Friday.", "Let me know if there's anything else I'm missing that I should consider.", "Cheers!"], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["If there's anything flagged as \"blocker\" that generates merge conflict,  I'll go case by case about it.", "Soon after, I should be able to announce 1.8.2 RC1, hopefully sometime  tomorrow or Friday.", "Let me know if there's anything else I'm missing that I should consider.", "Cheers!", "Max   "], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hey I just published an article about the \"Data Engineer\" role in modern organizations and thought it could be of interest to this community.", "https://medium.com/@maximebeauchemin/the-rise-of-the-data-engineer-91be18f1e603#.5rkm4htnf  Max  "], "labels": ["0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  I need some advice in solving a problem with local variables in DAG.", "I have a DAG < schedule intervel 30 mins .", "It has 3 tasks.", "t1 runs a python program on remote EC2.", "t2 waits for S3 file availability at particular location."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It has 3 tasks.", "t1 runs a python program on remote EC2.", "t2 waits for S3 file availability at particular location.", "This S3 file created by t1.", "Once the S3 file is available, t3 runs and process the file on S3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["t1 runs a python program on remote EC2.", "t2 waits for S3 file availability at particular location.", "This S3 file created by t1.", "Once the S3 file is available, t3 runs and process the file on S3.", "I have date-time as part of my S3 file location."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["t2 waits for S3 file availability at particular location.", "This S3 file created by t1.", "Once the S3 file is available, t3 runs and process the file on S3.", "I have date-time as part of my S3 file location.", "dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')  bucket_key2 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/sucess\"  t1 runs more than 1 hour so second instance of DAG  is already started and it changes the variable dttm2 value so job1 task # t2 is trying to locate the file at different location."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This S3 file created by t1.", "Once the S3 file is available, t3 runs and process the file on S3.", "I have date-time as part of my S3 file location.", "dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')  bucket_key2 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/sucess\"  t1 runs more than 1 hour so second instance of DAG  is already started and it changes the variable dttm2 value so job1 task # t2 is trying to locate the file at different location.", "To overcome this I am planning to use parameter {{execution_date}} instead of getting dttm2 value as shown above."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')  bucket_key2 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/sucess\"  t1 runs more than 1 hour so second instance of DAG  is already started and it changes the variable dttm2 value so job1 task # t2 is trying to locate the file at different location.", "To overcome this I am planning to use parameter {{execution_date}} instead of getting dttm2 value as shown above.", "In situations like these, is there any better approach to keep same value in a variable through out the particular run of DAG?", "Or use XCOM feature to push and pull the values across the tasks with different keys for each run?", "part of my dag is given below:  #  dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')   NL = \"\"\"           cd /home/ubuntu/Scripts/ ; python2 a11.py {{params.get(\"dttm2\")}} ; \"\"\"  t1 = BashOperator(     task_id='E_Ns_A',     bash_command=NL,     params={'dttm2':dttm2},     retries=3,     dag=dag)  bucket_key2 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/sucess\"  def detect_s3(name, dag=dag, upstream=t1):   task = S3KeySensor(     task_id = name,     bucket_key=bucket_key2,     s3_conn_id='s3conn',     dag=dag,     wildcard_match=True)   task.set_upstream(upstream)   return task  # Spark Module to clasiify data  bucket_key3 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/\" sparkcmd = \"\"\"            cd /home/ubuntu/SC; /home/ubuntu/anaconda3/bin/python  NbRunner.py;            aws s3 cp /home/ubuntu/NC.txt {{params.get(\"bkey\")}} --region us-west-1 \"\"\"  t3 = BashOperator(     task_id='CNs',     bash_command=sparkcmd,     params={\"bkey\":bucket_key3},     retries=1,     dag=dag)  t2 = detect_s3('t2')  t3.set_upstream(t2)   Thanks, MSR  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Just created https://issues.apache.org/jira/browse/AIRFLOW-1273, https://issues.apache.org/jira/browse/AIRFLOW-1272, and https://issues.apache.org/jira/browse/AIRFLOW-1271.", "We'll follow up with pull requests in the coming weeks.", "On Thu, Jun 1, 2017 at 10:44 AM Peter Dolan <peterdolan@google.com wrote:   Hi Bolke,   Great!", "And yes, we will be able to maintain these operators after the  contributions.", "We further would always insist on strong test coverage as  well."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We'll follow up with pull requests in the coming weeks.", "On Thu, Jun 1, 2017 at 10:44 AM Peter Dolan <peterdolan@google.com wrote:   Hi Bolke,   Great!", "And yes, we will be able to maintain these operators after the  contributions.", "We further would always insist on strong test coverage as  well.", "I'll open some JIRA tickets to track their implementation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Thu, Jun 1, 2017 at 10:44 AM Peter Dolan <peterdolan@google.com wrote:   Hi Bolke,   Great!", "And yes, we will be able to maintain these operators after the  contributions.", "We further would always insist on strong test coverage as  well.", "I'll open some JIRA tickets to track their implementation.", "Thanks,  Peter   On Thu, Jun 1, 2017 at 6:05 AM Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Peter,   That sounds great!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And yes, we will be able to maintain these operators after the  contributions.", "We further would always insist on strong test coverage as  well.", "I'll open some JIRA tickets to track their implementation.", "Thanks,  Peter   On Thu, Jun 1, 2017 at 6:05 AM Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Peter,   That sounds great!", "I think the main criteria for this is will you  maintain the code afterwards?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We further would always insist on strong test coverage as  well.", "I'll open some JIRA tickets to track their implementation.", "Thanks,  Peter   On Thu, Jun 1, 2017 at 6:05 AM Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Peter,   That sounds great!", "I think the main criteria for this is will you  maintain the code afterwards?", "The contrib section is slowly but steadily  growing and with operators/hooks we are particularly dependent on the  community as not all (or even none in some case) of the committers use  these themselves."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'll open some JIRA tickets to track their implementation.", "Thanks,  Peter   On Thu, Jun 1, 2017 at 6:05 AM Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Peter,   That sounds great!", "I think the main criteria for this is will you  maintain the code afterwards?", "The contrib section is slowly but steadily  growing and with operators/hooks we are particularly dependent on the  community as not all (or even none in some case) of the committers use  these themselves.", "In any case test coverage is required, but that is a given I think."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Peter   On Thu, Jun 1, 2017 at 6:05 AM Bolke de Bruin <bdbruin@gmail.com wrote:   Hi Peter,   That sounds great!", "I think the main criteria for this is will you  maintain the code afterwards?", "The contrib section is slowly but steadily  growing and with operators/hooks we are particularly dependent on the  community as not all (or even none in some case) of the committers use  these themselves.", "In any case test coverage is required, but that is a given I think.", "Kind regards,  Bolke    On 31 May 2017, at 21:10, Peter Dolan <peterdolan@google.com.INVALID  wrote:     Hello developers,     I work with Google Cloud ML, and my team and I are interested in  contributing a set of Operators to support working with the Cloud ML  platform."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think the main criteria for this is will you  maintain the code afterwards?", "The contrib section is slowly but steadily  growing and with operators/hooks we are particularly dependent on the  community as not all (or even none in some case) of the committers use  these themselves.", "In any case test coverage is required, but that is a given I think.", "Kind regards,  Bolke    On 31 May 2017, at 21:10, Peter Dolan <peterdolan@google.com.INVALID  wrote:     Hello developers,     I work with Google Cloud ML, and my team and I are interested in  contributing a set of Operators to support working with the Cloud ML  platform.", "The platform supports using the TensorFlow deep neural network  framework as a managed system."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In any case test coverage is required, but that is a given I think.", "Kind regards,  Bolke    On 31 May 2017, at 21:10, Peter Dolan <peterdolan@google.com.INVALID  wrote:     Hello developers,     I work with Google Cloud ML, and my team and I are interested in  contributing a set of Operators to support working with the Cloud ML  platform.", "The platform supports using the TensorFlow deep neural network  framework as a managed system.", "In particular, we would like to contribute    * CloudMLTrainingOperator, which would launch and monitor a Cloud ML  Training Job (  https://cloud.google.com/ml-engine/docs/how-tos/training-jobs <  https://cloud.google.com/ml-engine/docs/how-tos/training-jobs),    * CloudMLBatchPredictionOperator, which would launch and monitor a  Cloud ML Batch Prediction Job (  https://cloud.google.com/ml-engine/docs/how-tos/batch-predict <  https://cloud.google.com/ml-engine/docs/how-tos/batch-predict), and    * CloudMLVersionOperator, which can create, update, and delete  TensorFlow model versions (  https://cloud.google.com/ml-engine/docs/how-tos/managing-models-jobs <  https://cloud.google.com/ml-engine/docs/how-tos/managing-models-jobs)     I'm eager to hear if the Airflow project is open to these  contributions, and if any changes are suggested.", "We have working prototype  versions of all of them."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Kind regards,  Bolke    On 31 May 2017, at 21:10, Peter Dolan <peterdolan@google.com.INVALID  wrote:     Hello developers,     I work with Google Cloud ML, and my team and I are interested in  contributing a set of Operators to support working with the Cloud ML  platform.", "The platform supports using the TensorFlow deep neural network  framework as a managed system.", "In particular, we would like to contribute    * CloudMLTrainingOperator, which would launch and monitor a Cloud ML  Training Job (  https://cloud.google.com/ml-engine/docs/how-tos/training-jobs <  https://cloud.google.com/ml-engine/docs/how-tos/training-jobs),    * CloudMLBatchPredictionOperator, which would launch and monitor a  Cloud ML Batch Prediction Job (  https://cloud.google.com/ml-engine/docs/how-tos/batch-predict <  https://cloud.google.com/ml-engine/docs/how-tos/batch-predict), and    * CloudMLVersionOperator, which can create, update, and delete  TensorFlow model versions (  https://cloud.google.com/ml-engine/docs/how-tos/managing-models-jobs <  https://cloud.google.com/ml-engine/docs/how-tos/managing-models-jobs)     I'm eager to hear if the Airflow project is open to these  contributions, and if any changes are suggested.", "We have working prototype  versions of all of them.", "Thanks in advance,   Peter    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do the workers themselves need to be on Windows, or do you just need work to happen on a Windows machine?", "If the later, consider using the SSH Execute Operator?", "-R  On Tue, May 16, 2017 at 1:43 AM Niranda Perera <niranda.17@cse.mrt.ac.lk wrote:   To clarify the the question more,   I want to run workers on windows, as I have some apps in my workflow which  require windows.", "But I can keep my master instance in linux.", "I hope this issue of gunicorn not compatible with windows [1] would not be  a blocker to run the worker?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-R  On Tue, May 16, 2017 at 1:43 AM Niranda Perera <niranda.17@cse.mrt.ac.lk wrote:   To clarify the the question more,   I want to run workers on windows, as I have some apps in my workflow which  require windows.", "But I can keep my master instance in linux.", "I hope this issue of gunicorn not compatible with windows [1] would not be  a blocker to run the worker?", "I also want to monitor the status of the windows worker from the airflow  master and I hope not having gunicorn, would not break that functionality?", "[1]   http://stackoverflow.com/questions/32378494/how-to-run-airflow-on-windows/32378495   Best  Nira   Best regards   Niranda Perera  Research Assistant  Dept of CSE, University of Moratuwa  niranda.17@cse.mrt.ac.lk  +94 71 554 8430 <+94%2071%20554%208430  https://lk.linkedin.com/in/niranda   On Tue, May 16, 2017 at 12:00 PM, Niranda Perera <niranda.17@cse.mrt.ac.lk    wrote:    Hi devs,     Does airflow work on the windows environment?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi, I'm attempting to create a custom hook (for mongodb) as a plugin but it's not appearing in the list of connections in the UI.", "Steps: 1.", "Ran Airflow via docker (https://github.com/puckel/docker-airflow) 2.", "Copied the example from https://pythonhosted.org/airflow/plugins.html 3.", "Copied the postgres hook, renamed a few fields and ended up with https://gist.github.com/shin-nien/2d011972d8631c92675d1de58b000168 (it won't work for mongo yet but I wanted to test that it would be available)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Steps: 1.", "Ran Airflow via docker (https://github.com/puckel/docker-airflow) 2.", "Copied the example from https://pythonhosted.org/airflow/plugins.html 3.", "Copied the postgres hook, renamed a few fields and ended up with https://gist.github.com/shin-nien/2d011972d8631c92675d1de58b000168 (it won't work for mongo yet but I wanted to test that it would be available).", "4."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Copied the example from https://pythonhosted.org/airflow/plugins.html 3.", "Copied the postgres hook, renamed a few fields and ended up with https://gist.github.com/shin-nien/2d011972d8631c92675d1de58b000168 (it won't work for mongo yet but I wanted to test that it would be available).", "4.", "Restarted the container and observed pyc in plugins dir was created.", "5."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["4.", "Restarted the container and observed pyc in plugins dir was created.", "5.", "Went to UI and noticed the test menus 6.", "Went to UI/Connections but couldn't find my connector/hook  Any tips?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Restarted the container and observed pyc in plugins dir was created.", "5.", "Went to UI and noticed the test menus 6.", "Went to UI/Connections but couldn't find my connector/hook  Any tips?", "Thanks  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Airflow team,   I am using airflow with celery (2 nodes; i.e., two AWS instances) My dag looks like below (the python dag name is task_ABC.py).", "Note in the dag python file, I setup \"max_active_runs=1\"              /--------- TaskB1 ----------- TaskC1---------\\ TaskA ----------- TaskB2  ---------- TaskC2---------- TaskD            \\---------- TaskB3  ----------- TaskC3--------/  So, After TaskA; it runs TaskB1, TaskB2 and TaskB3 simultaneously.", "TaskB1, B2 and B3 are running same shell-script (TaskB.sh) with different input arguments.", "It drops \"Another instance is running, skipping\" warning for TaskB1 and TaskB3 (as the log below).", "It did not drop same warning in TaskB2, I think it's because TaskB2 is running in different celery node (I have two celery nodes)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Note in the dag python file, I setup \"max_active_runs=1\"              /--------- TaskB1 ----------- TaskC1---------\\ TaskA ----------- TaskB2  ---------- TaskC2---------- TaskD            \\---------- TaskB3  ----------- TaskC3--------/  So, After TaskA; it runs TaskB1, TaskB2 and TaskB3 simultaneously.", "TaskB1, B2 and B3 are running same shell-script (TaskB.sh) with different input arguments.", "It drops \"Another instance is running, skipping\" warning for TaskB1 and TaskB3 (as the log below).", "It did not drop same warning in TaskB2, I think it's because TaskB2 is running in different celery node (I have two celery nodes).", "If I manually make TaskB1 as successful, TaskB3 can proceed  The following is the log."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["TaskB1, B2 and B3 are running same shell-script (TaskB.sh) with different input arguments.", "It drops \"Another instance is running, skipping\" warning for TaskB1 and TaskB3 (as the log below).", "It did not drop same warning in TaskB2, I think it's because TaskB2 is running in different celery node (I have two celery nodes).", "If I manually make TaskB1 as successful, TaskB3 can proceed  The following is the log.", "Any idea to handle this ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It drops \"Another instance is running, skipping\" warning for TaskB1 and TaskB3 (as the log below).", "It did not drop same warning in TaskB2, I think it's because TaskB2 is running in different celery node (I have two celery nodes).", "If I manually make TaskB1 as successful, TaskB3 can proceed  The following is the log.", "Any idea to handle this ?", "Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It did not drop same warning in TaskB2, I think it's because TaskB2 is running in different celery node (I have two celery nodes).", "If I manually make TaskB1 as successful, TaskB3 can proceed  The following is the log.", "Any idea to handle this ?", "Thanks.", "-Jason  ========= Log of TaskB1 ============  [2017-05-20 23:09:47,270] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-20 23:09:49,017] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-20 23:09:49,165] {models.py:1196} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 2 --------------------------------------------------------------------------------  [2017-05-20 23:09:49,182] {models.py:1219} INFO - Executing <Task(PythonOperator): TaskB1 on 2017-05-20 03:40:00 [2017-05-20 23:09:49,214] {task_ABC.py:185} INFO - /mycode/process/gfs0p25/TaskB.sh 2017052012 rain [2017-05-21 00:09:56,054] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-21 00:09:59,759] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-21 00:10:00,008] {models.py:1146} WARNING - Another instance is running, skipping."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If I manually make TaskB1 as successful, TaskB3 can proceed  The following is the log.", "Any idea to handle this ?", "Thanks.", "-Jason  ========= Log of TaskB1 ============  [2017-05-20 23:09:47,270] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-20 23:09:49,017] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-20 23:09:49,165] {models.py:1196} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 2 --------------------------------------------------------------------------------  [2017-05-20 23:09:49,182] {models.py:1219} INFO - Executing <Task(PythonOperator): TaskB1 on 2017-05-20 03:40:00 [2017-05-20 23:09:49,214] {task_ABC.py:185} INFO - /mycode/process/gfs0p25/TaskB.sh 2017052012 rain [2017-05-21 00:09:56,054] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-21 00:09:59,759] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-21 00:10:00,008] {models.py:1146} WARNING - Another instance is running, skipping.", "========= Log of TaskB3 ============  [2017-05-20 23:09:44,660] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-20 23:09:46,047] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-20 23:09:46,205] {models.py:1196} INFO - -------------------------------------------------------------------------------- Starting attempt 1 of 2 --------------------------------------------------------------------------------  [2017-05-20 23:09:46,224] {models.py:1219} INFO - Executing <Task(PythonOperator): TaskB3 on 2017-05-20 03:40:00 [2017-05-20 23:09:46,257] {best_weather-BLEND-v1-1-0.py:245} INFO - /mycode/process/gfs0p25/TaskB.sh 2017052012 snow [2017-05-21 00:09:48,029] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-21 00:09:49,080] {models.py:154} INFO - Filling up the DagBag from /code/task_ABC.py [2017-05-21 00:09:49,156] {models.py:1146} WARNING - Another instance is running, skipping.  "], "labels": ["0", "0", "0", "0", "0"]}
