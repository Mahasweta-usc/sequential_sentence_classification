{"abstract_id": 0, "sentences": ["For adding something like this, there is a lighter weight way.", "Only  committers need to sign an ICLA, and software grants are only needed  separately for large blobs of pre-existing code.", "When we're in Apache, you can contribute by putting it on the Jena JIRA  as a patch file.", "This implies the necessary legal permission for ASF to use the patch  just by the act of doing it (the Jira installation has a note about this  when you create the entry, I think - we're all a bit new to Apache  processes).", "Some committer then handles it - I think (we new to this ...) by making  sure the project group is OK with it and then applying the patch."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Only  committers need to sign an ICLA, and software grants are only needed  separately for large blobs of pre-existing code.", "When we're in Apache, you can contribute by putting it on the Jena JIRA  as a patch file.", "This implies the necessary legal permission for ASF to use the patch  just by the act of doing it (the Jira installation has a note about this  when you create the entry, I think - we're all a bit new to Apache  processes).", "Some committer then handles it - I think (we new to this ...) by making  sure the project group is OK with it and then applying the patch.", "Or  the other way round."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Or  the other way round.", "The point is an email goes to the public  development mailing list for everyone, inc committers, to see.", "For  added examples, it's a bit of a no-op, but it means it's \"the project\"  dciding, not one person in isolation.", "It's no work for the contributor  - they just upload patch to JIRA.", "We'd be swamped by (electronic)  paperwork otherwise."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["For  added examples, it's a bit of a no-op, but it means it's \"the project\"  dciding, not one person in isolation.", "It's no work for the contributor  - they just upload patch to JIRA.", "We'd be swamped by (electronic)  paperwork otherwise.", "The full details are: http://www.apache.org/foundation/getinvolved.html   Thank you so much for all the hard work on Jena, and I hope I can  improve it (even if just a little).", "The more, the merrier."], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["Install that.", "If you didn't have ipython installed, when you start the CM or NM, you  would not have an active prompt, just log messages.", "If you restart  now, you should get an ipython prompt.", "Can you show the output of  data.baseDataObject.hosts now?", "Richard    On Fri, Apr 29, 2011 at 12:10 AM, hari narayanan <hari.zlatan@gmail.com  wrote:   No, but we installed it just now ... we dont have thrift installed ....   should we get that also ??"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Can you show the output of  data.baseDataObject.hosts now?", "Richard    On Fri, Apr 29, 2011 at 12:10 AM, hari narayanan <hari.zlatan@gmail.com  wrote:   No, but we installed it just now ... we dont have thrift installed ....   should we get that also ??", "And we always start CM as u said ....         On Thu, Apr 28, 2011 at 4:49 PM, Richard Gass <richardgass@gmail.com   wrote:     Do you have ipython installed?", "On Thu, Apr 28, 2011 at 11:38 PM, hari narayanan   <hari.zlatan@gmail.com   wrote:    We dont get output for data.baseDataObject.hosts in CM .... but, we    get    the    laptop name in tashiclient.", "getHosts with state Normal...       On Thu, Apr 28, 2011 at 4:32 PM, Richard Gass <richardgass@gmail.com    wrote:       Send me the following information."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["on the CM    \"data.baseDataObject.hosts\" or from the tashi client \"tashi    gethosts\"          On Thu, Apr 28, 2011 at 11:22 PM, hari narayanan    <hari.zlatan@gmail.com    wrote:     Hi,         we are able to start the vm image using xen command... but, it     doesnt     work     when we try with tashi ...", "Still get the same error \"no     hostid-none\"     ...     We     tried to change the CM and NM config files to make sure that     hostid     value is     equal to our actual laptop's hostname \"akshay\" ... and also ,we     are     able     to     ping the address and hostname of the laptop ... what could be the     prob?", "On Tue, Apr 26, 2011 at 10:16 PM, Akshay Sheth     <aks.sheth88@gmail.com     wrote:         Hey Micheal,         I was planning to use Sqlite3 but when I write the insert queries     that     the     tables hosts and networks dont exist.", "How do I fix this?", "Also     when I     put     data.baseDataObject.getHosts on CM I dont get any output."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Still get the same error \"no     hostid-none\"     ...     We     tried to change the CM and NM config files to make sure that     hostid     value is     equal to our actual laptop's hostname \"akshay\" ... and also ,we     are     able     to     ping the address and hostname of the laptop ... what could be the     prob?", "On Tue, Apr 26, 2011 at 10:16 PM, Akshay Sheth     <aks.sheth88@gmail.com     wrote:         Hey Micheal,         I was planning to use Sqlite3 but when I write the insert queries     that     the     tables hosts and networks dont exist.", "How do I fix this?", "Also     when I     put     data.baseDataObject.getHosts on CM I dont get any output.", "Also     eventually I     get no hostId for the VM."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From: Adrian Gonzalez <adr_gonzalez@yahoo.fr    Thank you very much everyone for your help on this topic.", "I'll have a look at Facelets (and perhaps Clay, but latter since it's less   used).", "I really have the impression that everyone (or quite everyone) using JSF is   using Facelets, so....     Just another question (last, I swear it !)", ":   Do you, JSF app developpers (and not component developers), use an IDE with drag   and drop capabilities (RAD kind of development) for facelets or no ?", "Which one ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I really have the impression that everyone (or quite everyone) using JSF is   using Facelets, so....     Just another question (last, I swear it !)", ":   Do you, JSF app developpers (and not component developers), use an IDE with drag   and drop capabilities (RAD kind of development) for facelets or no ?", "Which one ?", "I'm just using IBM RAD 6, and it's really difficult (ibm proprietary components,   JSF 1.0, JSP...)     You might also ask this one on the shale users list.", "Ryan Wynn works with Websphere (IBM Consulting) hangs out there [1]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm just using IBM RAD 6, and it's really difficult (ibm proprietary components,   JSF 1.0, JSP...)     You might also ask this one on the shale users list.", "Ryan Wynn works with Websphere (IBM Consulting) hangs out there [1].", "Although, a Shale Clay enthusiast - something like 20 porlets using Shale Clay.", "[1] https://issues.apache.org/struts/browse/SHALE-402    Gary   ----- Message d'origine ----   De : Gary VanMatre   \ufffd : adffaces-user@incubator.apache.org   Envoy\ufffd le : Jeudi, 1 F\ufffdvrier 2007, 16h49mn 50s   Objet : Re: RE : AW: templating mecanism ?", "From: Adrian Gonzalez       Thank you very much    But I really need to use JSP for rendering (not    facelets) - my IDE doesn't support JSF design with    facelets syntax."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Although, a Shale Clay enthusiast - something like 20 porlets using Shale Clay.", "[1] https://issues.apache.org/struts/browse/SHALE-402    Gary   ----- Message d'origine ----   De : Gary VanMatre   \ufffd : adffaces-user@incubator.apache.org   Envoy\ufffd le : Jeudi, 1 F\ufffdvrier 2007, 16h49mn 50s   Objet : Re: RE : AW: templating mecanism ?", "From: Adrian Gonzalez       Thank you very much    But I really need to use JSP for rendering (not    facelets) - my IDE doesn't support JSF design with    facelets syntax.", "Another less popular alternative would be Shale Clay [1].", "It can be used with JSP and has a number of options."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["[1] http://shale.apache.org/shale-clay/index.html        Do you know a templating solution for JSP engine ?", "--- D\ufffdring Markus a    \ufffdcrit :       Gary          Hello,     Trinidad comes with a buildin Facelets library and     Facelets has a very powerfull templating mechanism.", "Just have a look at it and test if it's what you     need.", "Greetings     Markus              -----Urspr\ufffdngliche Nachricht-----      Von: Adrian Gonzalez     [mailto:adr_gonzalez@yahoo.fr]      Gesendet: Donnerstag, 1.", "Februar 2007 14:32      An: adffaces-user@incubator.apache.org      Betreff: templating mecanism ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Moreover I don't want to use sitemesh engine for      composition (cause of the parsing performance     impact).", "The region tags enable composition but not     templating      I think.", "Thanks                                           __________________________________________________________________________      _      D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des     r\ufffdponses \ufffd toutes vos questions      !", "Profitez des connaissances, des opinions et des     exp\ufffdriences des      internautes sur Yahoo!", "Questions/R\ufffdponses      http://fr.answers.yahoo.com                             ___________________________________________________________________________    D\ufffdcouvrez une nouvelle fa\ufffdon d'obtenir des r\ufffdponses \ufffd toutes vos questions !"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For batches, stopping a batch means killing the Spark application, so all bets are off as to what happens there.", "On Wed, Jan 24, 2018 at 1:08 PM, Alex Bozarth <ajbozart@us.ibm.com wrote:   You are correct that you are using the term Job incorrectly (at least  according to how Spark/Livy uses it).", "Each spark-submit is a a single Spark  Application and can include many jobs (which are broken down themselves  into stages and tasks).", "In Livy using sessions would be like using  spark-shell rather than spark-submit, you probably want to use batches  instead (which utilize spark-submit), then you would use that delete  command as mentioned earlier.", "As for the result being listed as FAILED and  not CANCELLED, that is as intended."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In Livy using sessions would be like using  spark-shell rather than spark-submit, you probably want to use batches  instead (which utilize spark-submit), then you would use that delete  command as mentioned earlier.", "As for the result being listed as FAILED and  not CANCELLED, that is as intended.", "When a Livy Session is stopped  (deleted) is sends a command to all the running jobs (in your case each of  you apps only have one \"Job\") to set as failed.", "@Marcelo you wrote the code that does this, do you remember why you had  Jobs killed instead of cancelled when a Livy session is stopped?", "Otherwise  we may be able to open a JIRA and change this, but I am unsure of any  potential consequences."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["@Marcelo you wrote the code that does this, do you remember why you had  Jobs killed instead of cancelled when a Livy session is stopped?", "Otherwise  we may be able to open a JIRA and change this, but I am unsure of any  potential consequences.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth* <https://github.com/ajbozarth    505 Howard Street  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  San Francisco, CA 94105  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  United States  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g     [image: Inactive hide details for kant kodali ---01/23/2018 11:44:26  PM---I tried POST to sessions/{session id}/jobs/{job id}/cancel a]kant  kodali ---01/23/2018 11:44:26 PM---I tried POST to sessions/{session  id}/jobs/{job id}/cancel and that doesn't seem to cancel either.", "From: kant kodali <kanth909@gmail.com  To: user@livy.incubator.apache.org  Date: 01/23/2018 11:44 PM   Subject: Re: How to cancel the running streaming job using livy?", "------------------------------     I tried  POST to sessions/{session id}/jobs/{job id}/cancel and that  doesn't seem to cancel either."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Otherwise  we may be able to open a JIRA and change this, but I am unsure of any  potential consequences.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth* <https://github.com/ajbozarth    505 Howard Street  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  San Francisco, CA 94105  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g  United States  <https://maps.google.com/?q=505+Howard+Street+San+Francisco,+CA+94105+United+States&entry=gmail&source=g     [image: Inactive hide details for kant kodali ---01/23/2018 11:44:26  PM---I tried POST to sessions/{session id}/jobs/{job id}/cancel a]kant  kodali ---01/23/2018 11:44:26 PM---I tried POST to sessions/{session  id}/jobs/{job id}/cancel and that doesn't seem to cancel either.", "From: kant kodali <kanth909@gmail.com  To: user@livy.incubator.apache.org  Date: 01/23/2018 11:44 PM   Subject: Re: How to cancel the running streaming job using livy?", "------------------------------     I tried  POST to sessions/{session id}/jobs/{job id}/cancel and that  doesn't seem to cancel either.", "I think first of all the word \"job\" is used  in so many context that it might be misleading."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["say I do that following   spark-submit hello1.jar // streaming job1 (runs forever)  spark-submit hello2.jar //streaming job2 (runs forever)   The number of jobs I spawned is two and now I want to be able to cancel  one of them..These jobs reads data from kafka and will be split into stages  and task now sometimes these tasks are also called jobs according to SPARK  UI for some reason.", "And looks like live may be is cancelling those with the  above end point.", "It would be great help if someone could try from their end and see if they  are able to cancel the jobs?", "Thanks!", "On Fri, Jan 19, 2018 at 4:03 PM, Alex Bozarth <*ajbozart@us.ibm.com*  <ajbozart@us.ibm.com wrote:      Ah, that's why I couldn't find cancel in JobHandle, but it was     implemented in all it's implementations, which all implement it as would be     expected."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Jan 19, 2018 at 4:03 PM, Alex Bozarth <*ajbozart@us.ibm.com*  <ajbozart@us.ibm.com wrote:      Ah, that's why I couldn't find cancel in JobHandle, but it was     implemented in all it's implementations, which all implement it as would be     expected.", "*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=fDK7aF_qwcx3-sCSfUCbzeju-yaB8rqcutS_AuW_BRs&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=         [image: Inactive hide details for Marcelo Vanzin ---01/19/2018     03:55:43 PM---A JobHandle (which you get by submitting a Job) is a Futur]Marcelo     Vanzin ---01/19/2018 03:55:43 PM---A JobHandle (which you get by submitting     a Job) is a Future, and Futures have a \"cancel()\" method.", "From: Marcelo Vanzin <*vanzin@cloudera.com* <vanzin@cloudera.com     To: *user@livy.incubator.apache.org* <user@livy.incubator.apache.org     Date: 01/19/2018 03:55 PM      Subject: Re: How to cancel the running streaming job using livy?", "------------------------------        A JobHandle (which you get by submitting a Job) is a Future, and     Futures have a \"cancel()\" method.", "I don't remember the details about how \"cancel()\" is implemented in     Livy, though."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*Alex Bozarth*  Software Engineer  Spark Technology Center  ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=fDK7aF_qwcx3-sCSfUCbzeju-yaB8rqcutS_AuW_BRs&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=Io6A_oOejKvX7wP9hqKWr0NXa729OGgy1e-qdIwelfI&s=GCO_bHHbb3d10NSMTDbyhfJqnEzkvlFZJoH4oND7x2w&e=         [image: Inactive hide details for Marcelo Vanzin ---01/19/2018     03:55:43 PM---A JobHandle (which you get by submitting a Job) is a Futur]Marcelo     Vanzin ---01/19/2018 03:55:43 PM---A JobHandle (which you get by submitting     a Job) is a Future, and Futures have a \"cancel()\" method.", "From: Marcelo Vanzin <*vanzin@cloudera.com* <vanzin@cloudera.com     To: *user@livy.incubator.apache.org* <user@livy.incubator.apache.org     Date: 01/19/2018 03:55 PM      Subject: Re: How to cancel the running streaming job using livy?", "------------------------------        A JobHandle (which you get by submitting a Job) is a Future, and     Futures have a \"cancel()\" method.", "I don't remember the details about how \"cancel()\" is implemented in     Livy, though.", "On Fri, Jan 19, 2018 at 3:52 PM, Alex Bozarth <*ajbozart@us.ibm.com*     <ajbozart@us.ibm.com wrote:        Ok so I looked into this a bit more."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't remember the details about how \"cancel()\" is implemented in     Livy, though.", "On Fri, Jan 19, 2018 at 3:52 PM, Alex Bozarth <*ajbozart@us.ibm.com*     <ajbozart@us.ibm.com wrote:        Ok so I looked into this a bit more.", "I misunderstood you a bit           before, the delete call is for ending livy sessions using the rest API, not           jobs and not via the Java API.", "As for the Job state that makes sense, if           you end the session the session kills all currently running jobs.", "What you           want to to send cancel requests to the jobs the session is running."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From my           research I found that there is a way to do this via the REST API, but it           isn't documented for some reason.", "Doing a POST to /{session id}/jobs/{job           id}/cancel will cancel a job.", "As for the Java API, the feature isn't part           of the Java interface, but most implementations of it add it, such as the           Scala API which ScalaJobHandle class on sumbit which has a cancel function.", "I'm not sure how you're submitting you jobs, but there should be a cancel           function available to you somewhere depending on the client you're using.", "From this discussion I've realized our current documentation is even more           lacking that I had thought."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["------------------------------              Also just tried the below and got the state.", "It ended up in           \"FAILED\" stated when I expected it to be in \"CANCELLED\" state.", "Also from           the docs it is not clear if it kills the session or the job?", "if it kills           the session I can't spawn any other Job.", "Sorry cancelling job had been a           bit confusing for me."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It ended up in           \"FAILED\" stated when I expected it to be in \"CANCELLED\" state.", "Also from           the docs it is not clear if it kills the session or the job?", "if it kills           the session I can't spawn any other Job.", "Sorry cancelling job had been a           bit confusing for me.", "DELETE /sessions/0              On Thu, Jan 18, 2018 at 5:55 PM, kant kodali <           *kanth909@gmail.com* <kanth909@gmail.com wrote:              oh this raises couple questions."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also from           the docs it is not clear if it kills the session or the job?", "if it kills           the session I can't spawn any other Job.", "Sorry cancelling job had been a           bit confusing for me.", "DELETE /sessions/0              On Thu, Jan 18, 2018 at 5:55 PM, kant kodali <           *kanth909@gmail.com* <kanth909@gmail.com wrote:              oh this raises couple questions.", "1) Is there a programmatic way to cancel a job?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["1) Is there a programmatic way to cancel a job?", "2) is  there any programmatic way to get session id?", "If not, how do I get a sessionId when I spawn multiple jobs or multiple                       sessions?", "On Thu, Jan 18, 2018 at 5:39 PM, Alex Bozarth <                       *ajbozart@us.ibm.com* <ajbozart@us.ibm.com wrote:                       You make a DELETE call as detailed here:                       *http://livy.apache.org/docs/latest/rest-api.html#response*                       <https://urldefense.proofpoint.com/v2/url?u=http-3A__livy.apache.org_docs_latest_rest-2Dapi.html-23response&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=eAcZY6sAN_mkDv5Ves9UtZaotVvvUc3BBdkCEV_CqVg&e=                       *Alex Bozarth*                       Software Engineer                       Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=EV7HPze6ToE8xgFtDOw9zE2b3sGYWSW1rB-7ZhiJRok&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=                           [image: Inactive hide details for kant kodali                       ---01/18/2018 05:34:07 PM---Hi All, I was able to submit a streaming job to                       livy however]kant kodali ---01/18/2018 05:34:07                       PM---Hi All, I was able to submit a streaming job to livy however I wasn't                       able to find                        From: kant kodali <*kanth909@gmail.com*                       <kanth909@gmail.com                       To: *user@livy.incubator.apache.org*                       <user@livy.incubator.apache.org                       Date: 01/18/2018 05:34 PM                       Subject: How to cancel the running streaming job                       using livy?", "------------------------------                          Hi All,                        I was able to submit a streaming job to livy however                       I wasn't able to find any way to cancel the running the job?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If not, how do I get a sessionId when I spawn multiple jobs or multiple                       sessions?", "On Thu, Jan 18, 2018 at 5:39 PM, Alex Bozarth <                       *ajbozart@us.ibm.com* <ajbozart@us.ibm.com wrote:                       You make a DELETE call as detailed here:                       *http://livy.apache.org/docs/latest/rest-api.html#response*                       <https://urldefense.proofpoint.com/v2/url?u=http-3A__livy.apache.org_docs_latest_rest-2Dapi.html-23response&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=eAcZY6sAN_mkDv5Ves9UtZaotVvvUc3BBdkCEV_CqVg&e=                       *Alex Bozarth*                       Software Engineer                       Spark Technology Center   ------------------------------  *E-mail:* *ajbozart@us.ibm.com* <ajbozart@us.ibm.com  *GitHub: **github.com/ajbozarth*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ajbozarth&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=EV7HPze6ToE8xgFtDOw9zE2b3sGYWSW1rB-7ZhiJRok&e=    *505 Howard Street*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *San Francisco, CA 94105*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=  *United States*  <https://urldefense.proofpoint.com/v2/url?u=https-3A__maps.google.com_-3Fq-3D505-2BHoward-2BStreet-2BSan-2BFrancisco-2C-2BCA-2B94105-2BUnited-2BStates-26entry-3Dgmail-26source-3Dg&d=DwMFaQ&c=jf_iaSHvJObTbx-siA1ZOg&r=S1_S7Dymu4ZL6g7L21O78VQZ53vEnAyZ-cx37DPYDyo&m=nI9x8SjWSOnoLQr05P15W5ofGJayNWwF3InumEtLhVo&s=uy43iGDrczqx4GGhTSYqjjIeyjGpxPQ0611WcWeaB_s&e=                           [image: Inactive hide details for kant kodali                       ---01/18/2018 05:34:07 PM---Hi All, I was able to submit a streaming job to                       livy however]kant kodali ---01/18/2018 05:34:07                       PM---Hi All, I was able to submit a streaming job to livy however I wasn't                       able to find                        From: kant kodali <*kanth909@gmail.com*                       <kanth909@gmail.com                       To: *user@livy.incubator.apache.org*                       <user@livy.incubator.apache.org                       Date: 01/18/2018 05:34 PM                       Subject: How to cancel the running streaming job                       using livy?", "------------------------------                          Hi All,                        I was able to submit a streaming job to livy however                       I wasn't able to find any way to cancel the running the job?", "Please let me                       know.", "Thanks!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Furthermore, I couldn't find any examples with trinidad and my migrated JSF examples didn't work (\"illegal component hierarchy detected, expected UIXCommand but found another type of component instead.\").", "I reckon, it might be a problem with nested forms, however, this would be another entry...", "In my opinion, this is more than one reason do not move to facelets.", "Are there any other options?", "Did somebody get Tiles working with trinidad?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This has just been checked into the repository.", "Check it out.", "The setup  instructions are in docs/java_setup.html.", "Shanti  Harold Lim wrote:  Hi All,   When will the Java Implementation of Olio will be released?", "Also, will there be possibly implementations that use HBase in the future?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(Could be JSON too, but lack of a schema language for JSON makes it far less desirable IMHO.)", "The exceptions here are if speed/space concerns make the overhead of XML too high.", "There is an environmental argument against using XML.", "Consider all the wasted CPU cycles in the world dealing with XML's verbose and redundant structure.", "Given that computers use lots of energy, the \"Carbon Footprint\" of XML on global scale is something to think about."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But ignoring all that, there are cases where use of an expensive data format like XML just won't allow you to achieve the goal of your software.", "The two cases I know of where something like XML is unacceptable and one might prefer a dense binary data format are cutting-edge supercomputing applications - where every bit counts in space/speed if the application is going to work at all, and also ultra-low-power computing, where every bit counts, because the cost of just data compress/decompress consumes too much battery power.", "But even then, a standard binary format like EXI (binary XML - same infoset as XML, just denser binary representation) may be preferable to an ad-hoc file format with DFDL schema.", "Lastly another use case I've found for DFDL is what I call \"CSV-like\" data files.", "These arise when human beings will be editing data files by hand."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["These arise when human beings will be editing data files by hand.", "I have a lot of experience of \"CSV\" data files that aren't at all well behaved as true CSV data files are supposed to be.", "Given a spreadsheet program like MS-Excel, people will create a spreadsheet document with all sorts of headers and sections on a sheet.", "Then they'll export that sheet as \"CSV\" and claim the file is CSV data.", "These sorts of \"CSV-like\" files are often full of inconsistencies."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have a lot of experience of \"CSV\" data files that aren't at all well behaved as true CSV data files are supposed to be.", "Given a spreadsheet program like MS-Excel, people will create a spreadsheet document with all sorts of headers and sections on a sheet.", "Then they'll export that sheet as \"CSV\" and claim the file is CSV data.", "These sorts of \"CSV-like\" files are often full of inconsistencies.", "Empty cells are sometimes empty string, sometimes all-whitespace strings, sometimes  various markers like \"--\" or \"N/A\" or \"none\"  A DFDL schema can be written which handles all these human inconsistency factors, skipping section headers, standardizing \"--\", \"N/A\", etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What is the role of binary?", "Hello DFDL community,  DFDL gives us tremendous agility - we can quickly and easily transform binary to XML and XML to binary.", "Binary, with its conciseness, is beautiful for moving data.", "XML, with its vast tool suite, is beautiful for processing data.", "What do you see as the role of XML?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello DFDL community,  DFDL gives us tremendous agility - we can quickly and easily transform binary to XML and XML to binary.", "Binary, with its conciseness, is beautiful for moving data.", "XML, with its vast tool suite, is beautiful for processing data.", "What do you see as the role of XML?", "The role of binary?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Tison,  In Ozone the proto files are compiled using the maven plugin.", "Please find usage in the following file  https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/common/pom.xml#L178.", "Thanks,  Mukul   On 30/11/19 4:13 pm, tison wrote:  Hello here again,   I'm not sure how \"Ozone uses the shaded ByteString version\".", "Could you   please show me the way for doing so?", "Best,  tison."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please find usage in the following file  https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/common/pom.xml#L178.", "Thanks,  Mukul   On 30/11/19 4:13 pm, tison wrote:  Hello here again,   I'm not sure how \"Ozone uses the shaded ByteString version\".", "Could you   please show me the way for doing so?", "Best,  tison.", "tison <wander4096@gmail.com <mailto:wander4096@gmail.com   \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db \u4e0a\u534811:20\u5199\u9053\uff1a       Thanks for your information Mukul!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have one more question.", "Even if I want to use the shaded      ByteString version when      compile with protoc it requires protobuf-java deps.", "How can I      configure protoc use      the shaded version to generate codes?", "Best,      tison.", "Mukul Kumar Singh <mksingh.apache@gmail.com      <mailto:mksingh.apache@gmail.com \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db      \u4e0a\u534811:15\u5199\u9053\uff1a           Hi Tison,           Thanks for the interest in Ratis."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How can I      configure protoc use      the shaded version to generate codes?", "Best,      tison.", "Mukul Kumar Singh <mksingh.apache@gmail.com      <mailto:mksingh.apache@gmail.com \u4e8e2019\u5e7411\u670814\u65e5\u5468\u56db      \u4e0a\u534811:15\u5199\u9053\uff1a           Hi Tison,           Thanks for the interest in Ratis.", "There are 2 options as you          have already noticed.", "a) Ozone which is a consumer of Ratis, we have used the shaded          ByteString version in the Ozone codebase."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://maven.apache.org/POM/4.0.0\"     <modelVersion4.0.0</modelVersion     <groupIdTestGroup</groupId     <artifactIdTestArtifactName</artifactId     <packagingdotnet-executable</packaging     <nameExecutable-pom</name     <version1.0-SNAPSHOT</version     <build      <plugins        <plugin          <groupIdorg.codehaus.mojo</groupId          <artifactIdbuild-helper-maven-plugin</artifactId          <version1.5</version          <executions            <execution              <idattach-artifacts</id              <phasepackage</phase              <goals                <goalattach-artifact</goal              </goals              <configuration                <artifacts                  <artifact                    <filepathtobin/bin-name.exe.config</file                    <typedotnet-executable-config</type                  </artifact                </artifacts              </configuration            </execution          </executions        </plugin      </plugins    </build  </project      Am 12.10.10 01:12, schrieb Sergio Rupena:    I am trying to create a pom which allows me to bundle my 'app.config'  file together with my application.", "The documentation (see  http://www.npanday.org/docs/1.2/guide/maven/project-types.html  <http://www.npanday.org/docs/1.2/guide/maven/project-types.html  )  suggests that this should be possible using the dotnet-executable-config  packaging type.", "Using the normal maven-compile plugin this should be doable using the  following pom:     <?xml version=\"1.0\" encoding=\"utf-8\"?", "<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"  xmlns=\"http://maven.apache.org/POM/4.0.0\"      <modelVersion4.0.0</modelVersion      <groupIdTestGroup</groupId      <artifactIdTestArtifactName.Config</artifactId      <packagingdotnet-executable-config</packaging      <nameconfiguration file pom</name      <version1.0-SNAPSHOT</version   </project     But this results in an error:     [INFO] Scanning for projects...   [INFO]  ------------------------------------------------------------------------   [INFO] Building configuration file pom   [INFO]    task-segment: [install]   [INFO]  ------------------------------------------------------------------------   [INFO]  ------------------------------------------------------------------------   [ERROR] BUILD ERROR   [INFO]  ------------------------------------------------------------------------   [INFO] Cannot find lifecycle mapping for packaging:  'dotnet-executable-config'.", "Component descriptor cannot be found in the component repository:  org.apache.maven.lifecycle.mapping.LifecycleMappingdotnet-executable-con  fig."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So 5000.00 will be output as 5000  If I want to retain the digits to the right of the decimal point, then I should declare the price element with the type xs:string, is that correct?", "/Roger   -----Original Message----- From: Steve Lawrence <slawrence@apache.org  Sent: Monday, December 3, 2018 9:36 AM To: users@daffodil.apache.org; Costello, Roger L. <costello@mitre.org Subject: Re: How to retain the digits to the right of the decimal point?", "The pattern defines the format of the data.", "It does not define the format of the infoset.", "I believe the spec is ambiguous or silent about how various data fields should be output to the infoset."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I know we've had this issue with date/time fields recently.", "In the case of decimal numbers, Daffodil creates an infoset output with the minimum number of digits necessary to display it with the same precision.", "So 5000.00 will be output as 5000, but 5000.99 will be output with the extra decimal precision.", "- Steve  On 12/3/18 9:30 AM, Costello, Roger L. wrote:  Hi Mike,      * Use 0 instead of # for the rightmost two.", "I tried that:    <xs:elementname=\"price\"type=\"xs:decimal\"       dfdl:textStandardDecimalSeparator=\".\""], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So 5000.00 will be output as 5000, but 5000.99 will be output with the extra decimal precision.", "- Steve  On 12/3/18 9:30 AM, Costello, Roger L. wrote:  Hi Mike,      * Use 0 instead of # for the rightmost two.", "I tried that:    <xs:elementname=\"price\"type=\"xs:decimal\"       dfdl:textStandardDecimalSeparator=\".\"", "dfdl:textNumberPattern=\"####.00\"/    It gave the same result (the .00 is removed):    5000.00 -- parse -- 5000    Thoughts?", "/Roger    *From:* Mike Beckerle <mbeckerle@tresys.com  *Sent:* Monday, December 3, 2018 9:21 AM  *To:* users@daffodil.apache.org  *Subject:* Re: How to retain the digits to the right of the decimal point?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In a pattern, a zero denotes   any digit.", "A # denotes an optional digit.", "-------- Original message --------    From: \"Costello, Roger L.\" <costello@mitre.org   <mailto:costello@mitre.org    Date: 12/3/18 8:46 AM (GMT-05:00)    To: users@daffodil.apache.org <mailto:users@daffodil.apache.org    Subject: How to retain the digits to the right of the decimal point?", "Hello DFDL community,    My input contains decimal values such as: 2999.99 and 5000.00    When I parse my input, the .00 gets removed, e.g.,    5000.00 -- parse -- 5000    But the .99 is not removed, e.g.,    2999.99 -- parse -- 2999.99    I want to retain the two digits to the right of the decimal point,   even if they are 00    How to retain the digits?", "I thought this would do the job:    dfdl:textNumberPattern=\"####.##\"/    However, that has no effect."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A # denotes an optional digit.", "-------- Original message --------    From: \"Costello, Roger L.\" <costello@mitre.org   <mailto:costello@mitre.org    Date: 12/3/18 8:46 AM (GMT-05:00)    To: users@daffodil.apache.org <mailto:users@daffodil.apache.org    Subject: How to retain the digits to the right of the decimal point?", "Hello DFDL community,    My input contains decimal values such as: 2999.99 and 5000.00    When I parse my input, the .00 gets removed, e.g.,    5000.00 -- parse -- 5000    But the .99 is not removed, e.g.,    2999.99 -- parse -- 2999.99    I want to retain the two digits to the right of the decimal point,   even if they are 00    How to retain the digits?", "I thought this would do the job:    dfdl:textNumberPattern=\"####.##\"/    However, that has no effect.", "What's the right way to do it?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-------- Original message --------    From: \"Costello, Roger L.\" <costello@mitre.org   <mailto:costello@mitre.org    Date: 12/3/18 8:46 AM (GMT-05:00)    To: users@daffodil.apache.org <mailto:users@daffodil.apache.org    Subject: How to retain the digits to the right of the decimal point?", "Hello DFDL community,    My input contains decimal values such as: 2999.99 and 5000.00    When I parse my input, the .00 gets removed, e.g.,    5000.00 -- parse -- 5000    But the .99 is not removed, e.g.,    2999.99 -- parse -- 2999.99    I want to retain the two digits to the right of the decimal point,   even if they are 00    How to retain the digits?", "I thought this would do the job:    dfdl:textNumberPattern=\"####.##\"/    However, that has no effect.", "What's the right way to do it?", "/Roger     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-Akara  -------- Original Message -------- Subject: \tRe: Parsing Olio runtimeStats Date: \tWed, 05 May 2010 14:17:35 -0700 From: \tAkara Sucharitakul <akara.sucharitakul@oracle.com Reply-To: \takara.sucharitakul@oracle.com Organization: \tOracle To: \tolio-user@incubator.apache.org CC: \takara.sucharitakul <Akara.Sucharitakul@sun.com, Shanti Subramanyam  <shanti.subramanyam@gmail.com References:  <s2s89c38a6f1005051212z7cb84f71o589fa41c7ca8704d@mail.gmail.com    Let me try to address this below:  On 05/05/10 12:12, Vasileios Kontorinis wrote:  Akara and Shanti hi,      I am parsing the runtimeStats from the driver.log file and I run   into this interesting issue.", "In the output for _steady state_ I get \"-\" for the response time when   there are no successful operations since the last time runtimeStats   were printed.", "This can happen for two reasons:  1) The interval for printing the runtimeStats is small  (in my case   5secs) and some operations, especially the ones that take long and   have small frequency in matrix (add event, add user) just never happened.", "Yes, if the number of successful operations in that period is 0, most  calculations will be a divide by 0.", "That's why it is printing a '-'."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the output for _steady state_ I get \"-\" for the response time when   there are no successful operations since the last time runtimeStats   were printed.", "This can happen for two reasons:  1) The interval for printing the runtimeStats is small  (in my case   5secs) and some operations, especially the ones that take long and   have small frequency in matrix (add event, add user) just never happened.", "Yes, if the number of successful operations in that period is 0, most  calculations will be a divide by 0.", "That's why it is printing a '-'.", "2) There are a bunch of requests going on and none of them is   successful."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This can happen for two reasons:  1) The interval for printing the runtimeStats is small  (in my case   5secs) and some operations, especially the ones that take long and   have small frequency in matrix (add event, add user) just never happened.", "Yes, if the number of successful operations in that period is 0, most  calculations will be a divide by 0.", "That's why it is printing a '-'.", "2) There are a bunch of requests going on and none of them is   successful.", "This can happen when there is no sufficient memory in the   machine."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yes, if the number of successful operations in that period is 0, most  calculations will be a divide by 0.", "That's why it is printing a '-'.", "2) There are a bunch of requests going on and none of them is   successful.", "This can happen when there is no sufficient memory in the   machine.", "In that case there is lots of swapping, the cpu goes to 100%   utilization and all the operations time-out."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In that case there is lots of swapping, the cpu goes to 100%   utilization and all the operations time-out.", "Is there any small change I can do to distinguish between the two   cases?", "Maybe printing a small \"t\" when there are many requests timing   out, so that I can distinguish between the two cases?", "Any ideas are   welcome.", "There is the error count in the runtime stats that tell you about error  cases."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Any ideas are   welcome.", "There is the error count in the runtime stats that tell you about error  cases.", "This would also include timeouts.", "But the problem is not that  simple.", "The real problem is not the operations timing out but rather the  operations waiting that would time out."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is it easy to also log the   changes of active threads in the drive.log file ??", "Please look into log configuration in $FABAN/config/logging.properties.", "You should be able to make certain loggers log to a particular file.", "I  don't have the detail off  the  top of my head.", "-Akara   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Server: 2?", "Mem: 4G?", "Node: 4   Running cost: Cpu ?%, mem ?%  I will refer to this  and compare it, thanks first.", "From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 3:47 PM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Also I noticed the cpu is very high almost 100%, but mem<10%, whether it still says too many IO operation or other causes?", "/Sky  From: Sky Zhao [mailto:sky.zhao@ericsson.com] Sent: Wednesday, June 26, 2013 10:09 AM To: 's4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Thanks Matthieu very careful suggestions, your direction is right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(unless the timestamp you set is somehow repeated, which sounds peculiar).", "I would suggest to modify the keyfinder in a more suitable way.", "Probably by removing the timestamp from the key.", "In the twitter example for instance, the key is the topic of the tweet, and we aggregate counts by topic.", "Hope this helps,  Matthieu  On Jun 25, 2013, at 14:39 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  So I only guess, The serialization/deserlization costs much time, and occupy some limited memory, Once the serialization/deserlization upp bound is max, it will occupy much memory, the events starts to be blocked, so more memory in JVM and less (de)serlization, the performance could be more events for general."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Performance issues in stream processing can be related to I/O or GC.", "But the app design can have a dramatic impact as well.", "Have a look at your CPU usage as well.", "You might want to profile the app and adapter to identify the culprit in your application.", "Another path to explore is related to the content of events."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Another path to explore is related to the content of events.", "Serialization/deserialization may be costly for complex objects.", "What kind of data structure are you keeping in the events?", "On Jun 25, 2013, at 09:56 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  Also, can you teach me how to find or trace the block place, I am still confused why and where is block?", "What are you referring to?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Serialization/deserialization may be costly for complex objects.", "What kind of data structure are you keeping in the events?", "On Jun 25, 2013, at 09:56 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  Also, can you teach me how to find or trace the block place, I am still confused why and where is block?", "What are you referring to?", "How many nodes in twitter example for up to 200,000 events/s?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How many nodes in twitter example for up to 200,000 events/s?", "You'll never get to that number with that application, since the twitter sprinkler feed is ~ 1% of the total feed, and the reported peak rate of the total feed is a few tens of thousands of tweets / s  But if you were to read from a dump, I'd say a few nodes for the adapter and a few nodes for the app.", "Matthieu    From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com/] Sent: Tuesday, June 25, 2013 10:25 AM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Hi Matthieu, I tried to test again after modifying some configuration codes, see below, no any PE logic, just send events(only spend 38s for adapter sending events) don't know where is blocked?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 12:25 AM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Hi,  I would suggest to:  1/ check how much you can generate when creating events read from the file - without event sending to a remote stream.", "This gives you the upper bound for a single adapter (producer)  It costs 38s for only file-read from adapters   2/ check how much you can consume in the app cluster."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You'll never get to that number with that application, since the twitter sprinkler feed is ~ 1% of the total feed, and the reported peak rate of the total feed is a few tens of thousands of tweets / s  But if you were to read from a dump, I'd say a few nodes for the adapter and a few nodes for the app.", "Matthieu    From: Sky Zhao [mailto:sky.zhao@ericsson.com<http://ericsson.com/] Sent: Tuesday, June 25, 2013 10:25 AM To: 's4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org' Subject: RE: About 200,000 events/s  Hi Matthieu, I tried to test again after modifying some configuration codes, see below, no any PE logic, just send events(only spend 38s for adapter sending events) don't know where is blocked?", "From: Matthieu Morel [mailto:mmorel@apache.org] Sent: Tuesday, June 25, 2013 12:25 AM To: s4-user@incubator.apache.org<mailto:s4-user@incubator.apache.org Subject: Re: About 200,000 events/s  Hi,  I would suggest to:  1/ check how much you can generate when creating events read from the file - without event sending to a remote stream.", "This gives you the upper bound for a single adapter (producer)  It costs 38s for only file-read from adapters   2/ check how much you can consume in the app cluster.", "By default the remote senders are blocking, i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I removed all PE logic, just emit functions, very strange, it still cost 600s, seems somewhere blocking   3/ use more adapter processes.", "In the benchmarks subprojects, one can configure the number of injection processes, and you might need more than one I tried, seems improve a bit, but not obivouse, 2500 events/s maximum.", "4/ make sure the tuning parameters you are setting are appropriate.", "For instance, I am not sure using 100 threads for serializing events is a good setting (see my notes about context switching in a previous mail).", "Already changed to 10 threads  Also note that 200k msg/s/stream/node corresponds to the average rate in one minute _once the cluster has reached stable mode_."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For instance, I am not sure using 100 threads for serializing events is a good setting (see my notes about context switching in a previous mail).", "Already changed to 10 threads  Also note that 200k msg/s/stream/node corresponds to the average rate in one minute _once the cluster has reached stable mode_.", "Indeed JVMs typically perform better after a while, due to various kinds of dynamic optimizations.", "Do make sure you experiments are long enough.", "Here is metrics report, already run 620s(NO pe logic,) List event-emitted@seacluster1@partition-0.csv<mailto:event-emitted@seacluster1@partition-0.csv file  # time,count,1 min rate,mean rate,5 min rate,15 min rate 20,30482,728.4264567247864,1532.1851387286329,577.7027401449616,550.7086872323928 40,68256,1003.4693068790475,1710.158498360041,651.1838653093574,576.4097533046603 60,126665,1526.423951717675,2114.1420782852447,792.5372602881871,626.2019978354255 80,159222,1631.4770294401224,1992.409242530607,865.8704071266087,654.9705320161032 100,206876,1821.551542703833,2070.5009200329328,958.5809893144597,691.1999843184435 120,245115,1852.9758861695257,2044.0217493195803,1021.5173199955651,718.5310141264627 140,286057,1892.8858688327914,2044.444972093294,1084.2498192357743,746.5688788373484 160,324662,1952.4301928970413,2030.146074235012,1151.037578221836,776.8090098793148 180,371829,1959.4350067303067,2066.6134765239876,1202.907690109737,802.6284106918696 200,433557,2283.734039729985,2168.043356631868,1325.4143582100112,853.1681426416412 220,464815,2165.047817425384,2113.0019773033414,1362.4434184467839,876.2949286924105 240,504620,2065.9291371459913,2102.751240490619,1390.829435765405,896.605433400642 260,558158,2192.29985605397,2146.9020962821564,1462.6363950595012,931.9095985316526 280,595910,2199.1021924478428,2128.3677435991726,1513.937239488627,961.2098181907241 300,627181,1994.0377201896988,2090.694355349902,1511.0411290249551,972.3477079289204 320,664342,1959.8933962067783,2076.1406486970463,1534.4815810411815,992.1780521538831 340,698717,1912.6664972668054,2055.1073335596802,1551.413320562825,1009.8797759279687 360,738468,1940.3861933767728,2051.3430577192726,1579.897070517331,1031.4220907479778 380,769673,1808.904535049888,2025.4290025114635,1577.8601886321867,1045.7495364549434 400,807406,1834.34177336605,2018.480153382513,1597.9097843354657,1064.241627735365 420,851009,1918.7250831019285,2026.1694030316874,1634.835168023956,1088.691609058512 440,884427,1854.4469403637247,2010.0049048826818,1637.4121396194184,1101.510206100955 460,927835,1951.4786440268247,2016.9731382075943,1672.0801089577474,1125.0229080682857 480,975961,2076.6338294064494,2033.1853468961958,1719.2462212252974,1153.158261927912 500,1018777,2094.823218886582,2037.4852753404898,1746.4288709310954,1174.8624939692247 520,1063733,2137.998709779158,2045.5028969721004,1778.8271760046641,1198.4702839108422 540,1105407,2121.91619308999,2046.9043296729049,1798.2960826750962,1217.8576817035882 560,1148338,2126.4127152635606,2050.4509561002797,1820.616007231527,1238.2444234074262 580,1189999,2113.61602051879,2051.570290715584,1837.4446097375073,1256.7786337375683 600,1219584,1937.6445710562814,2031.6753069957329,1814.671731059335,1261.7477397513687 620,1237632,1632.4506998235333,1995.2499874637524,1755.4487350018405,1253.8445924435982    Seems very strange value for my example, far from 200,000 events/s/node/stream   Regards,  Matthieu   On Jun 24, 2013, at 11:19 , Sky Zhao <sky.zhao@ericsson.com<mailto:sky.zhao@ericsson.com wrote:  I try to use Adapter to send s4 events."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Dec 31, 2013 at 12:24 AM, Naresh Yadav <nyadav.ait@gmail.com wrote:  Hi tim,   list of tags is not small, can be really big so i cannot use negate the  tags approach...Other approach you said is using subfields how to do that  in blur...My thought on this was to introduce new column Tags which will  store sorted all tags of that row...So for cases where i need exact match  then will query on *Tags* column and case where i need partial match of  tags then will use Tag column..", "Your approach is largely the same as the subcolumn approach.", "Subcolumns would just allow you to do it without storing the original value multiple times.", "I actually haven't used them, but I reckon it'd look something like:  ColumnDefinition tags = new ColumnDefinition(\"fam\",\"tag\",null, true, \"text\",null); ColumnDefinition tagsExact = new ColumnDefinition(\"fam\",\"tag\",\"exact\",\"false,\"string\",null);  and querying: A) fam.tag:Tag1  B) fam.tag.exact:Tag1  Thanks, --tim  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sure sounds like bugs.", "Can you send the whole schema please?", "________________________________ From: Costello, Roger L. <costello@mitre.org Sent: Monday, April 13, 2020 11:11 AM To: users@daffodil.apache.org <users@daffodil.apache.org Subject: Two bugs in Daffodil?", "Hi Folks,  I have a binary data format.", "One field is a 1-byte name representing a person's age."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There is the exact same problem with initiator.", "I put dfdl:initiator=\"\" on the element declaration and Daffodil says there is no initiator.", "When I put it on dfdl:format, Daffodil doesn't give an error.", "I believe this is aother bug.", "Do you agree?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We probably should  also add an optional skinnable icon, which'd bring this back to parity  with the old ADF Faces panelTip.", "Cheers,  Adam    On 2/11/07, Steve Vangasse <steve@boardshop.co.uk wrote:   Hello,   I've been looking through the Trinidad code for a way to skin the   panelTip component.", "So far the best I have found are OraTipText   OraTipLabel which only skin the text and the label, not the   surrounding panel.", "I've resorted to using inlineStyle but I would much    prefer to keep the styling in the skin file.", "Does anyone know how this    can be done?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Benj,  RequestContext.getCurrentInstance().addPartialTarget(component);  -----Original Message----- From: Benj Fayle [mailto:bfayle@maketechnologies.com] Sent: Monday, April 02, 2007 2:16 PM To: adffaces-user@incubator.apache.org Subject: RE: PPR Issue   In ADF you could do force partial page rendering from your backing bean programmatically use:  AdfFacesContext.getCurrentInstance().addPartialTarget(getContentPanel()) ;  In this case getContentPanel() was a control reference in the backing bean that had a binding from the page.", "I'm not sure what the equivalent Trinidad class is to AdfFacesContext.", "Benj  -----Original Message----- From: Chris Gibbons [mailto:cgibbons@solutionstream.com]  Sent: Friday, March 30, 2007 3:33 PM To: adffaces-user@incubator.apache.org Subject: PPR Issue  Hi,        I have a page that has two text boxes and a string of text.", "The text string needs to reflect the values from the inputText boxes.", "I have autoSubmit=true, and have partialListeners on my outputText set to id's of the inputText boxes, and an update does trigger."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello Matthias  Thanks for your quick answer.", "The demo looks very exciting.", "But is/will be the ADF Faces Rich Client also under Apache 2.0 license and what about the different ADF Faces downloads offered by Oracle (with JDeveloper or stand alone).", "I have the impression that they are _not_ Apache 2.0 licensed.", "But that means there are at least two different streams of development which have the same origin but was separated in last summer and are now independent from each other."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The demo looks very exciting.", "But is/will be the ADF Faces Rich Client also under Apache 2.0 license and what about the different ADF Faces downloads offered by Oracle (with JDeveloper or stand alone).", "I have the impression that they are _not_ Apache 2.0 licensed.", "But that means there are at least two different streams of development which have the same origin but was separated in last summer and are now independent from each other.", "Am I right (I hope not!)?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have the impression that they are _not_ Apache 2.0 licensed.", "But that means there are at least two different streams of development which have the same origin but was separated in last summer and are now independent from each other.", "Am I right (I hope not!)?", "Michael    -----Urspr\u00fcngliche Nachricht----- Von: mwessendorf@gmail.com [mailto:mwessendorf@gmail.com] Im Auftrag von Matthias Wessendorf Gesendet: Donnerstag, 15.", "M\u00e4rz 2007 10:27 An: adffaces-user@incubator.apache.org Betreff: Re: Relation between Trinidad and ADF Faces  Hello Michael,  there is a ADF Faces Rich Client, currently under development, see here: http://www.oracle.com/technology/products/jdev/viewlets/1013/richclient_viewlet_swf.html  That stuff uses Trinidad features (like framework or build feature)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["M\u00e4rz 2007 10:27 An: adffaces-user@incubator.apache.org Betreff: Re: Relation between Trinidad and ADF Faces  Hello Michael,  there is a ADF Faces Rich Client, currently under development, see here: http://www.oracle.com/technology/products/jdev/viewlets/1013/richclient_viewlet_swf.html  That stuff uses Trinidad features (like framework or build feature).", "Trinidad is Apache 2.0 license based, so you can even fork it and sell the *enhancements* as closed source, no problems with that!", "The good on Apache 2.0 license is, it is damn liberal ;)   Does that help ?", "-M  On 3/15/07, Michael Trompertz <Michael.Trompertz@feltengmbh.de wrote:  Hello   Can anybody eplain the Relation between Trinidad and ADF Faces.", "What I found out is that Oracle donated ADF Faces to Apache in summer   2006."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1   *  Now when i am querying table testable why it is returning 2 results (One row from test table) even though it has only one record?", "*blur (default) disable test* *blur (default) remove test* *blur (default) query testable **   *2 results found in [43.491442 ms].", "Row [2] Record [2] Column [2] Data (bytes) [16]* *result# rowid recordid      * *0       r2             f2.c2* *0             rid2     f2   * *1       r1             f1.c1* *1             rid1     v1  *  Here even after removing test table, it still returns 2 results , one from test and one from testable tables.", "What is wrong here??", "Why is this residual data keep coming?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The mail lists strip out all attachments.", "You could copy  the text from the terminal or find a public image share.", "Thanks!", "Aaron   On Thursday, August 7, 2014, Ameya Aware <ameya.aware@gmail.com wrote:    Can i attach it here in mail?", "Will that work?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You can get the  latest 0.9.4 src tarball using:  git clone https://git-wip-us.apache.org/repos/asf/incubator-mrql.git  mrql-0.9.4  The EndpointWriter are Spark errors during Spark shutdown; you may simply  ignore them.", "Best regards,  Leonidas     On 01/07/2015 06:53 AM, Etienne Dumoulin wrote:     Hi MRQL users,    I am using mrql 0.9.2 and spark 1.0.2.", "I have a little issue with the spark mode.", "When I try to execute a small test job that is successful in MapReduce  mode I get an error.", "RMAT and pagerank examples throw a lot of warnings but I get the result at  the end."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["RMAT and pagerank examples throw a lot of warnings but I get the result at  the end.", "[hadoop@namenode ~]$  /home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/RMAT.mrql 100 1000  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: ( int, int, int, int ) - ( int, int )  Query type: !bag(( int, int ))  Physical plan:  MapReduce:     input: Generator  Run time: 6.165 secs  15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode2:40321]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:40321] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:40321  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 10:39:10 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:43173] <- [akka.tcp://sparkExecutor@datanode3:39739]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:39739] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:39739  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "/home/hadoop/mrql-0.9.2-incubating-src/bin/mrql.spark  -dist -nodes 1  mrql-0.9.2-incubating-src/queries/pagerank.mrql  Apache MRQL version 0.9.2 (compiled distributed Spark mode using 1 tasks)  Query type: long  Physical plan:  Aggregate:     input: MapAggregateReduce:               input: Source (binary): \"tmp/graph.bin\"  Run time: 6.352 secs  Query type: string  Result:  \"*** number of nodes: 76\"  Run time: 0.055 secs  Query type: !list(< node: int, rank: double )  Physical plan:  MapReduce:     input: Repeat (x_55):               init: MapReduce:                        input: Source (binary): \"tmp/graph.bin\"               step: MapReduce:                        input: x_55  Repeat #1: 67 true results  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Lost TID 6 (task 5.0:0)  15/01/07 12:32:08 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  Repeat #2: 61 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 9 (task 9.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 10 (task 9.0:0)  Repeat #3: 41 true results  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 13 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Loss was due to  java.lang.Error  java.lang.Error: Cannot up-coerce the numerical value null      at org.apache.mrql.SystemFunctions.error(SystemFunctions.java:38)      at org.apache.mrql.SystemFunctions.coerce(SystemFunctions.java:359)      at org.apache.mrql.MRQL_Lambda_15.eval(UserFunctions_6.java from  JavaSourceFromString:32)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:49)      at org.apache.mrql.MapReduceAlgebra$1.hasNext(MapReduceAlgebra.java:50)      at  scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)      at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)      at scala.collection.Iterator$class.foreach(Iterator.scala:727)      at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)      at  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)      at  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)      at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)      at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)      at org.apache.spark.scheduler.Task.run(Task.scala:51)      at  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)      at  java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)      at  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)      at java.lang.Thread.run(Thread.java:662)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 14 (task 14.0:0)  15/01/07 12:32:09 WARN scheduler.TaskSetManager: Lost TID 15 (task 14.0:0)  Repeat #4: 6 true results  Repeat #5: 0 true results  Result:  [ < node: 0, rank: 0.07872101046749681 , < node: 6, rank:  0.05065660974066882 , < node: 3, rank: 0.0459596116701186 , < node: 25,  rank: 0.04376273127207268 , < node: 12, rank: 0.04188453600736429 , <  node: 1, rank: 0.04040564312366409 , < node: 50, rank:  0.033875663833967604 , < node: 31, rank: 0.023745510555659932 , < node:  28, rank: 0.022415986790041594 , < node: 9, rank: 0.021938982955425616 ,  < node: 75, rank: 0.021586163633336066 , < node: 18, rank:  0.02116068257012926 , < node: 15, rank: 0.02026443639558245 , < node: 62,  rank: 0.017767555008348448 , < node: 53, rank: 0.017410210069182946 , <  node: 7, rank: 0.017195867587074802 , < node: 56, rank:  0.01699938245303271 , < node: 10, rank: 0.01654553114240747 , < node: 26,  rank: 0.01650909559014266 , < node: 37, rank: 0.016466833143024252 , ... ]  Run time: 5.379 secs  15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode2:32794]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode2:32794] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode2:32794  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]", "15/01/07 12:32:11 ERROR remote.EndpointWriter: AssociationError  [akka.tcp://spark@namenode:50514] <- [akka.tcp://sparkExecutor@datanode3:35978]:  Error [Shut down address: akka.tcp://sparkExecutor@datanode3:35978] [  akka.remote.ShutDownAssociation: Shut down address:  akka.tcp://sparkExecutor@datanode3:35978  Caused by: akka.remote.transport.Transport$InvalidAssociationException:  The remote system terminated the association because it is shutting down.  ]"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I guess I could just use the   table    header facet but it is not quite what I am wanting.", "I am placing a    selectOneChoice box on the table to select the number of rows visble.", "I    would like it inline with the range navigation stuff.", "Thanks,    -Brian       On 10/12/06, Adam Winer <awiner@gmail.com wrote:         Brian,         No, you're not missing anything.", "It's missing, and needs to be     added;  it looks like it just didn't make it in the open-source drop."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If so, how did it get into tempParams?", "-- Adam   On 2/15/07, Adam Winer <awiner@gmail.com wrote:  What is tempParams['source'] here?", "A DOM node, etc.?", "-- Adam    On 2/15/07, Christopher Cudennec <SmutjeJim@gmx.net wrote:   Hi...", "I'm getting closer to integrating trinidad :)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["please use the MyFaces lists.", "On 5/24/07, Nigel Magnay <nigel.magnay@gmail.com wrote:  Hi there   I'm successfully using the tr:tree control with a custom TreeModel behind it.", "Some of the documentation implies that I ought to be able to do some  kind of AJAX things with this tree - e.g when a node is expanded, just  re-render that part of the tree rather than the whole page.", "Unfortunately I can't seem to find out how to do this - is it possible ?", "--  Matthias Wessendorf  further stuff: blog: http://matthiaswessendorf.wordpress.com/ mail: matzew-at-apache-dot-org  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["2.", "ARIA is a straightforward implementation of a TOSCA parser and orchestrator, currently an Apache incubator project.", "3.", "Cloudify is a mature and feature-rich cloud orchestrator (Apache-licensed, currently at version 4.1) that uses a TOSCA-inspired language, but not real TOSCA.", "Because ARIA is still quite new and missing supporting plugins for various technologies (Docker, Openstack, AWS, Puppet, Chef, Juju, etc.)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["3.", "Cloudify is a mature and feature-rich cloud orchestrator (Apache-licensed, currently at version 4.1) that uses a TOSCA-inspired language, but not real TOSCA.", "Because ARIA is still quite new and missing supporting plugins for various technologies (Docker, Openstack, AWS, Puppet, Chef, Juju, etc.)", "we have created an adapter layer that help us use Cloudify plugins in ARIA.", "We consider this a temporary measure and intend to re-implement all these plugins natively for ARIA, as extensions in the repository."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cloudify is a mature and feature-rich cloud orchestrator (Apache-licensed, currently at version 4.1) that uses a TOSCA-inspired language, but not real TOSCA.", "Because ARIA is still quite new and missing supporting plugins for various technologies (Docker, Openstack, AWS, Puppet, Chef, Juju, etc.)", "we have created an adapter layer that help us use Cloudify plugins in ARIA.", "We consider this a temporary measure and intend to re-implement all these plugins natively for ARIA, as extensions in the repository.", "We're still building up our documentation on the wiki, as well as our list of examples."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["we have created an adapter layer that help us use Cloudify plugins in ARIA.", "We consider this a temporary measure and intend to re-implement all these plugins natively for ARIA, as extensions in the repository.", "We're still building up our documentation on the wiki, as well as our list of examples.", "We currently have a Hello World for Openstack, but not one for Docker yet.", "Have you tried the Openstack example?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have couple of days worth of data and when i run \"show partitions\" it provides the correct daa.", "d=20111215 d=20111216 d=20111217 d=20111218 d=20111219 d=20111220 d=20111221 d=20111222 d=20111223 d=20111224 d=20111225 d=20120415  However, when I run PIG with \"filter a by d == '20120415'\", it ends up scanning all data.", "Is this a known bug/enhancement in HCatalog?.", "Ideally, shouldn't it scan only the d=20120415 directory?", "Any pointers would be of great help."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That means the list T_lineitem  are shared for every PE.", "I don't know what is the problem.", "Thank you!", "Dingyu Yang   2012/10/3 Shailendra Mishra <shailendrah@gmail.com   I am assuming you are planning on doing windowed joins - all you need  to do is keep save the window state and on each insert to the state  data strucuture check if the window has expired.", "If the window does  expire then compute the join and output it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't know what is the problem.", "Thank you!", "Dingyu Yang   2012/10/3 Shailendra Mishra <shailendrah@gmail.com   I am assuming you are planning on doing windowed joins - all you need  to do is keep save the window state and on each insert to the state  data strucuture check if the window has expired.", "If the window does  expire then compute the join and output it.", "Now this logic works only  for tumbling windows for sliding windows you have to work harder,  however the logic is kinda similar."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Now this logic works only  for tumbling windows for sliding windows you have to work harder,  however the logic is kinda similar.", "- Shailendra    On Wed, Oct 3, 2012 at 1:24 AM, \u6768\u5b9a\u88d5 <yangdingyu@gmail.com wrote:   I have read the paper of S4: Distributed Stream Computing Platform.", "There is a example of Joining: Click-through rate.", "Two data tables  RawServe   and RawClick are joined according 'serve' column.", "The data with same serve in two tables are sent to the same PE."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The data with same serve in two tables are sent to the same PE.", "The data  are   streaming to the PE and joined.", "I have a question :   While there are new tuples sent to the PE, PE has no previous data and   previous data in the PE are discarded after streaming processing.", "As I know,the data is not stored in the PE.", "So how can I join the data in the PE?     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hello  On Fri, Feb 6, 2009 at 8:10 AM, Akara Sucharitakul <Akara.Sucharitakul@sun.com wrote:  Hi William,   What is the number of attendees added during the steady state?", "That would be 64888 * 0.0948 = 6151  EventDetail (success count)\t64888 % EventDetail views where attendee added\t9.48  Complete report is attached!", "In the summary report, under miscellaneous stats, there's the \"% EventDetail  views where attendee added\" field.", "Use this percentage point and calculate  the added attendees from the success count of EventDetail (first table -  \"Operation Mix.\")", "Thanks,  -Akara   William Voorsluys wrote:   Hello,   Here are some details about a 1 hour run that shows when the bursts  happen."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thus, the graph attached belongs to another  run, which was identical but different tools were running.", "System 'william2' is the web server (Ubuntu server 8.04 64-bit, 2  CPUs, 3GB RAM).", "System 'mysql1' runs only the database (Debian Etch 32-bit, 4 CPUS, 1GB  RAM).", "Both the filestore and the database files are on NFS.", "Cheers,   William     On Wed, Feb 4, 2009 at 4:45 AM, Shanti Subramanyam  <Shanti.Subramanyam@sun.com wrote:   I have noticed some bursts at 15-20 mins, but I think these are specific  to  our deployment (nfs issues) and we're working on tuning those - we  haven't  identified any Olio issue that will cause this behavior (at least not yet  !)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers,   William     On Wed, Feb 4, 2009 at 4:45 AM, Shanti Subramanyam  <Shanti.Subramanyam@sun.com wrote:   I have noticed some bursts at 15-20 mins, but I think these are specific  to  our deployment (nfs issues) and we're working on tuning those - we  haven't  identified any Olio issue that will cause this behavior (at least not yet  !)", "I assume you're running with a local filestore ?", "I'd be happy to take a  look  at one of your run outputs (assuming you collect vmstat, iostat etc.)", "Shanti   William Voorsluys wrote:   Hello,   What is the minimum time I should run Olio's PHP workload to be sure  my system really supports a certain amount of concurrent users?", "I've  been running for 30 minutes on steady state and all metrics pass, but  extending the run to 1 hour make things fail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there a documentation that describes workload characteristics, such  as when bursts take place?", "Thanks,   William       ------------------------------------------------------------------------         System parameters on server william2   #  # /etc/sysctl.conf - Configuration file for setting system variables  # See sysctl.conf (5) for information.", "#   #kernel.domainname = example.com   # the following stops low-level messages on console  kernel.printk = 4 4 1 7   # enable /proc/$pid/maps privacy so that memory relocations are not  # visible to other users.", "(Added in kernel 2.6.22.)", "kernel.maps_protect = 1   # Increase inotify availability  fs.inotify.max_user_watches = 524288   # protect bottom 64k of memory from mmap to prevent NULL-dereference  # attacks against potential future kernel security vulnerabilities."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  which OS and which version of Java are you using?", "do you have a trace?", "If not, can you try adding the -debug or option to  the gradle command?", "That may help diagnose the issue.", "Thanks,  Matthieu  On 8/14/12 11:22 AM, Qun Huang wrote:  Dear all,   I'm a new user of S4."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would recomend to  follow the provided suggestion for GSON, which is : \"Register an  InstanceCreator with Gson for this type to fix this problem\".", "Note that the missing no-arg constructor is also an issue when you  serialize with kryo 1, so you'd have to modify the provided serializer  to include custom serialization for the instance class.", "Please refer to  the kryo documentation for this.", "Regards,  Matthieu.", "On 9/4/12 9:19 AM, Raghavendar TS wrote:  Hi  In S4 0.3.0 I am trying to dispatch object of Instance.class from one PE  to another.But it is not working.ie <http://working.ie PE2 is not  receiving events from PE1 if the event consist of object of  Instance.class.What may be the problem?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Regards,  Matthieu.", "On 9/4/12 9:19 AM, Raghavendar TS wrote:  Hi  In S4 0.3.0 I am trying to dispatch object of Instance.class from one PE  to another.But it is not working.ie <http://working.ie PE2 is not  receiving events from PE1 if the event consist of object of  Instance.class.What may be the problem?", "Also I tried using GSON to serialize objects of Instance.class to string  and vice versa.It is throwing the exception during deserialization   Exception in thread \"main\" java.lang.RuntimeException: No-args  constructor for interface weka.core.Instance does not exist.", "Register an  InstanceCreator with Gson for this type to fix this problem.", "at  com.google.gson.MappedObjectConstructor.constructWithNoArgConstructor(MappedObjectConstructor.java:64)       at  com.google.gson.MappedObjectConstructor.construct(MappedObjectConstructor.java:53)       at  com.google.gson.JsonObjectDeserializationVisitor.constructTarget(JsonObjectDeserializationVisitor.java:41)       at  com.google.gson.JsonDeserializationVisitor.getTarget(JsonDeserializationVisitor.java:56)       at com.google.gson.ObjectNavigator.accept(ObjectNavigator.java:101)       at  com.google.gson.JsonDeserializationContextDefault.fromJsonObject(JsonDeserializationContextDefault.java:73)       at  com.google.gson.JsonDeserializationContextDefault.deserialize(JsonDeserializationContextDefault.java:51)       at com.google.gson.Gson.fromJson(Gson.java:495)       at com.google.gson.Gson.fromJson(Gson.java:444)       at com.google.gson.Gson.fromJson(Gson.java:396)       at com.google.gson.Gson.fromJson(Gson.java:372)       at gson.main(gson.java:62)   What is the problem."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [" I've been using the unsubscribe email, an it doesn't seem to work.", "Is there another option?", "-----Original Message----- From:\tmwessendorf@gmail.com on behalf of Matthias Wessendorf Sent:\tFri 8/25/2006 2:09 AM To:\tadffaces-user@incubator.apache.org Cc:\t Subject:\tRe: unsubscribe  that won't work...  use the unsubscribe email address  On 8/25/06, Hall, Peter <Peter.Hall@philotech.de wrote:  unsubscribe      --  Matthias Wessendorf  further stuff: blog: http://jroller.com/page/mwessendorf mail: mwessendorf-at-gmail-dot-com     "], "labels": ["0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Each character must be a digit character.", "Here is a sample input:  \t99999  My DFDL schema uses the xs:unsignedInt datatype to constrain the string to digits.", "It uses dfdl:length=5 to constrain the length of the string.", "I am trying to be very precise in my wording.", "Is the above wording correct?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yes.", "Is the name of your emulator really /usr/local/bin/qemu-system-x86_64?", "Yes.", "Also, do you know what prints those messages about leaking file  descriptors?", "Probably not important, but I'd like to clean that up."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["but for  every change i make to the css, i have to stop tomcat, clear the cache  folder and then start tomcat again.", "if i try to delete the cached css,  the file is locked and can't be deleted.", "Changes to the css are not  recognized immediately.", "i have played around with some web.xml  config-settings, but i haven't had success yet.", "this way it takes a lot of time to create a whole new style, is'nt there  a possibility to see immediately results after changes to the css an a  page reload?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["i have played around with some web.xml  config-settings, but i haven't had success yet.", "this way it takes a lot of time to create a whole new style, is'nt there  a possibility to see immediately results after changes to the css an a  page reload?", "does sombody have a hint for me?", "i would greatly appreciate some help.", "thanks in advance  felix gonschorek  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Inside Taverna server directory or in the java client?", "Please explain more   Thanks    -----Original Message----- From: alaninmcr [mailto:alaninmcr@googlemail.com]  Sent: Thursday, March 26, 2015 9:03 PM To: users@taverna.incubator.apache.org Subject: Re: A few questions before choosing Taverna for our project  On 26/03/2015 19:45, Ahmad Aburomman wrote:  Dear Stian,   I'm working on workflow and I ran it perfectly (tomcat6 and RESTful),   I got the output without provenance bundle  Before you started the run, did you set generate-provenance to true?", "Also, you need to make sure you are not sending the outputs to a Baclava file.", "See http://dev.mygrid.org.uk/wiki/display/tav250/REST+API#RESTAPI-Resource:/runs/{id}/generate-provenance   I don't know how to configure Taverna server to enable provenance   data, can you guide me please?", "It is not part of the server configuration."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Dear all,  Sorry for my previous mail, it had more details but my editor just get rid  of them.", "Any way it sent the important part which is the code.", "I still have the problem and actually the problem is simple.", "I need to  merge a set of text documents into one text document.", "To regenerate my  problem, I modified the demo7  available at http://incubator.apache.org/odftoolkit/simple/demo/demo7.html  This demo is generating a set of documents and I modified it to also merge  these documents into one document."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I still have the problem and actually the problem is simple.", "I need to  merge a set of text documents into one text document.", "To regenerate my  problem, I modified the demo7  available at http://incubator.apache.org/odftoolkit/simple/demo/demo7.html  This demo is generating a set of documents and I modified it to also merge  these documents into one document.", "I used two methods to do so:  One by using  org.odftoolkit.simple.TextDocument as following:          public static void generateAllOffersLetterTextDocument() throws  Exception {                 TextDocument templateDocument = TextDocument.loadDocument( fRootDirectory + \"/\"+ \"OfferTemplate.odt\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   TextDocument offerDoc = null;                 for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           offerDoc = TextDocument.loadDocument(fOutDirectory  + \"/\" + name+\"'s offer letter.odt\");                         templateDocument.insertDocument(offerDoc, \"/\" +  name);                 }                   templateDocument.save(fOutDirectory +  \"/AllOffersTextDoc.odt\");         }  And the other way is by extending  org.odftoolkit.odfdom.doc.OdfTextDocument by a new class MyTextDocument to  do exactly the same thing as follows:          public static void generateAllOffersODFDocument () throws  Exception {                 MyTextDocument allDocTextDoc = MyTextDocument."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["To regenerate my  problem, I modified the demo7  available at http://incubator.apache.org/odftoolkit/simple/demo/demo7.html  This demo is generating a set of documents and I modified it to also merge  these documents into one document.", "I used two methods to do so:  One by using  org.odftoolkit.simple.TextDocument as following:          public static void generateAllOffersLetterTextDocument() throws  Exception {                 TextDocument templateDocument = TextDocument.loadDocument( fRootDirectory + \"/\"+ \"OfferTemplate.odt\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   TextDocument offerDoc = null;                 for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           offerDoc = TextDocument.loadDocument(fOutDirectory  + \"/\" + name+\"'s offer letter.odt\");                         templateDocument.insertDocument(offerDoc, \"/\" +  name);                 }                   templateDocument.save(fOutDirectory +  \"/AllOffersTextDoc.odt\");         }  And the other way is by extending  org.odftoolkit.odfdom.doc.OdfTextDocument by a new class MyTextDocument to  do exactly the same thing as follows:          public static void generateAllOffersODFDocument () throws  Exception {                 MyTextDocument allDocTextDoc = MyTextDocument.", "newTextWorkProduct(fRootDirectory + \"/\"+ \"OfferTemplate.ott\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           allDocTextDoc.includeFile(fOutDirectory + \"/\" +  name+\"'s offer letter.odt\", \"/\" + name);                 }                    allDocTextDoc.save (fOutDirectory + \"/AllOffersODFDoc.odt\" );         }  While the includeFile is as follows:          public void includeFile (String filePath, String documentPath)  throws Exception{                 OdfPackageDocument newDocument;                   try {                         newDocument = OdfPackageDocument.loadDocument (filePath);                         insertDocument(newDocument, documentPath);                 } catch (Exception exception) {                         throw new Exception (\"Failed to include the file:  \" + filePath, exception);                 }         }  The results were exactly the same, the generated files didn't show the  agregated documents, only the first contents of the document, however the  conetns of the documents are inserted in their coresponding folders in the  zip file."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I used two methods to do so:  One by using  org.odftoolkit.simple.TextDocument as following:          public static void generateAllOffersLetterTextDocument() throws  Exception {                 TextDocument templateDocument = TextDocument.loadDocument( fRootDirectory + \"/\"+ \"OfferTemplate.odt\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   TextDocument offerDoc = null;                 for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           offerDoc = TextDocument.loadDocument(fOutDirectory  + \"/\" + name+\"'s offer letter.odt\");                         templateDocument.insertDocument(offerDoc, \"/\" +  name);                 }                   templateDocument.save(fOutDirectory +  \"/AllOffersTextDoc.odt\");         }  And the other way is by extending  org.odftoolkit.odfdom.doc.OdfTextDocument by a new class MyTextDocument to  do exactly the same thing as follows:          public static void generateAllOffersODFDocument () throws  Exception {                 MyTextDocument allDocTextDoc = MyTextDocument.", "newTextWorkProduct(fRootDirectory + \"/\"+ \"OfferTemplate.ott\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           allDocTextDoc.includeFile(fOutDirectory + \"/\" +  name+\"'s offer letter.odt\", \"/\" + name);                 }                    allDocTextDoc.save (fOutDirectory + \"/AllOffersODFDoc.odt\" );         }  While the includeFile is as follows:          public void includeFile (String filePath, String documentPath)  throws Exception{                 OdfPackageDocument newDocument;                   try {                         newDocument = OdfPackageDocument.loadDocument (filePath);                         insertDocument(newDocument, documentPath);                 } catch (Exception exception) {                         throw new Exception (\"Failed to include the file:  \" + filePath, exception);                 }         }  The results were exactly the same, the generated files didn't show the  agregated documents, only the first contents of the document, however the  conetns of the documents are inserted in their coresponding folders in the  zip file.", "In the attached zip file in the previous message, I included the source  code (FieldsDemo.java and MyTextDocument.java) as well as the source files  and the generated files."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["newTextWorkProduct(fRootDirectory + \"/\"+ \"OfferTemplate.ott\");                   SpreadsheetDocument dataDocument = SpreadsheetDocument.", "loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           allDocTextDoc.includeFile(fOutDirectory + \"/\" +  name+\"'s offer letter.odt\", \"/\" + name);                 }                    allDocTextDoc.save (fOutDirectory + \"/AllOffersODFDoc.odt\" );         }  While the includeFile is as follows:          public void includeFile (String filePath, String documentPath)  throws Exception{                 OdfPackageDocument newDocument;                   try {                         newDocument = OdfPackageDocument.loadDocument (filePath);                         insertDocument(newDocument, documentPath);                 } catch (Exception exception) {                         throw new Exception (\"Failed to include the file:  \" + filePath, exception);                 }         }  The results were exactly the same, the generated files didn't show the  agregated documents, only the first contents of the document, however the  conetns of the documents are inserted in their coresponding folders in the  zip file.", "In the attached zip file in the previous message, I included the source  code (FieldsDemo.java and MyTextDocument.java) as well as the source files  and the generated files.", "Sorry again for the previous messgae.", "I am  stucked here and I can not move forward, and I am looking for help."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["loadDocument(fRootDirectory + \"/\"+ \"Candidates.ods\");                 Table table = dataDocument.getTableByName(\"Sheet1\");                 int rowCount = table.getRowCount();                   for(int i = 1; i<rowCount; i++){                         Row row = table.getRowByIndex(i);                         String name =  row.getCellByIndex(0).getDisplayText();                           allDocTextDoc.includeFile(fOutDirectory + \"/\" +  name+\"'s offer letter.odt\", \"/\" + name);                 }                    allDocTextDoc.save (fOutDirectory + \"/AllOffersODFDoc.odt\" );         }  While the includeFile is as follows:          public void includeFile (String filePath, String documentPath)  throws Exception{                 OdfPackageDocument newDocument;                   try {                         newDocument = OdfPackageDocument.loadDocument (filePath);                         insertDocument(newDocument, documentPath);                 } catch (Exception exception) {                         throw new Exception (\"Failed to include the file:  \" + filePath, exception);                 }         }  The results were exactly the same, the generated files didn't show the  agregated documents, only the first contents of the document, however the  conetns of the documents are inserted in their coresponding folders in the  zip file.", "In the attached zip file in the previous message, I included the source  code (FieldsDemo.java and MyTextDocument.java) as well as the source files  and the generated files.", "Sorry again for the previous messgae.", "I am  stucked here and I can not move forward, and I am looking for help.", "Thanks & Best Regards / \u0627\u0644\u0633\u0644\u0627\u0645 \u0639\u0644\u064a\u0643\u0645   Ahmed Ibrahim MBA ,Senior IT Architect, SWG Cairo Lab Services BPM Client Solution Manager, MEA  IT Architect Egypt Profession Lead Mobile: +20 100 1615 506    From:   Ahmed I Ibrahim/Egypt/IBM@IBMEG To:     odf-users@incubator.apache.org,  Date:   04/15/2014 10:18 AM Subject:        Re: How to insert a text document into another text  document?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the attached zip file in the previous message, I included the source  code (FieldsDemo.java and MyTextDocument.java) as well as the source files  and the generated files.", "Sorry again for the previous messgae.", "I am  stucked here and I can not move forward, and I am looking for help.", "Thanks & Best Regards / \u0627\u0644\u0633\u0644\u0627\u0645 \u0639\u0644\u064a\u0643\u0645   Ahmed Ibrahim MBA ,Senior IT Architect, SWG Cairo Lab Services BPM Client Solution Manager, MEA  IT Architect Egypt Profession Lead Mobile: +20 100 1615 506    From:   Ahmed I Ibrahim/Egypt/IBM@IBMEG To:     odf-users@incubator.apache.org,  Date:   04/15/2014 10:18 AM Subject:        Re: How to insert a text document into another text  document?", "[attachment \"ODF_FileMerge.zip\" deleted by Ahmed I Ibrahim/Egypt/IBM]   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Let me know if you think we need to publish older versions of the docs.", "Max  On Wed, Mar 8, 2017 at 2:16 PM, Maxime Beauchemin < maximebeauchemin@gmail.com wrote:   Thanks for taking that on.", "It's mostly an effort in mocking all the libs  that cannot be built in the readthedocs environment.", "This may require a  specific `requirements.txt`-`readthedocs-reqs.txt` for that purpose with  only the core dependencies needed to parse the modules.", "Max   On Wed, Mar 8, 2017 at 1:40 PM, Daniel Huang <dxhuang@gmail.com wrote:   Just filed https://issues.apache.org/jira/browse/AIRFLOW-956 to setup  readthedocs."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It's mostly an effort in mocking all the libs  that cannot be built in the readthedocs environment.", "This may require a  specific `requirements.txt`-`readthedocs-reqs.txt` for that purpose with  only the core dependencies needed to parse the modules.", "Max   On Wed, Mar 8, 2017 at 1:40 PM, Daniel Huang <dxhuang@gmail.com wrote:   Just filed https://issues.apache.org/jira/browse/AIRFLOW-956 to setup  readthedocs.", "I'll give it a shot.", "On Wed, Mar 8, 2017 at 12:29 PM, Jake Gysland <jgysland@optoro.com  wrote:    I'm glad there's new documentation available for 1.8.0, but replacing  the   documentation for the latest stable release with the documentation for a   future release candidate can lead to experiences that are ...  frustrating,   as I've discovered this afternoon while trying to help a colleague QA a  DAG   and finding documentation for CLI flags that don't exist."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Max   On Wed, Mar 8, 2017 at 1:40 PM, Daniel Huang <dxhuang@gmail.com wrote:   Just filed https://issues.apache.org/jira/browse/AIRFLOW-956 to setup  readthedocs.", "I'll give it a shot.", "On Wed, Mar 8, 2017 at 12:29 PM, Jake Gysland <jgysland@optoro.com  wrote:    I'm glad there's new documentation available for 1.8.0, but replacing  the   documentation for the latest stable release with the documentation for a   future release candidate can lead to experiences that are ...  frustrating,   as I've discovered this afternoon while trying to help a colleague QA a  DAG   and finding documentation for CLI flags that don't exist.", "Can we at least get a version number (or... a commit hash?)", "into the   documentation somewhere that's reasonably visible?"], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks again.", "Michael                     ________________________________     From: Dan Davydov <dan.davydov@airbnb.com.INVALID     Sent: Friday, March 3, 2017 8:44 PM     To: dev@airflow.incubator.apache.org     Subject: Re: Airflow running different with different user id ?", "Within a couple of weeks.", "On Fri, Mar 3, 2017 at 12:34 PM, Michael Gong <gonwg@hotmail.com   wrote:          When approximately will it be released?", "Sent from my PP\u2022KING\u2122 smartphone           On Mar 3, 2017 1:42 PM, Dan Davydov <dan.davydov@airbnb.com  .INVALID      wrote:      Yes it is starting on 1.8.0 which will be released soon, you can  look    in      the documentation/grep for \"run_as\"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Within a couple of weeks.", "On Fri, Mar 3, 2017 at 12:34 PM, Michael Gong <gonwg@hotmail.com   wrote:          When approximately will it be released?", "Sent from my PP\u2022KING\u2122 smartphone           On Mar 3, 2017 1:42 PM, Dan Davydov <dan.davydov@airbnb.com  .INVALID      wrote:      Yes it is starting on 1.8.0 which will be released soon, you can  look    in      the documentation/grep for \"run_as\".", "On Mar 3, 2017 8:50 AM, \"Michael Gong\" <gonwg@hotmail.com wrote:            Hi,                   Suppose I have 1 airflow instance running 2 different DAGs, is  it      possible       to specify the 2 DAGs running under 2 different ids ?", "Any advises are welcomed."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Gunnar,  No, the tests simulates a version of fey.", "It starts its own actor system.", "The fey-test-actor.jar is included in fey-core.", "It is only a performer test used to make sure fey handles its performers in the right way.", "For now I would proceed removing the tests from assembly so you can generate the executable and proceed with the steps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The fey-test-actor.jar is included in fey-core.", "It is only a performer test used to make sure fey handles its performers in the right way.", "For now I would proceed removing the tests from assembly so you can generate the executable and proceed with the steps.", "I will have to do some investigation and see if I figure out why the tests breaks for you.", "Regards  Barbara Gomes +1 (650) 713-6092   On Feb 19, 2017, at 11:41 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi Barbara,    The same five tests fail with this change."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For now I would proceed removing the tests from assembly so you can generate the executable and proceed with the steps.", "I will have to do some investigation and see if I figure out why the tests breaks for you.", "Regards  Barbara Gomes +1 (650) 713-6092   On Feb 19, 2017, at 11:41 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:    Hi Barbara,    The same five tests fail with this change.", "Doesn't the iota-fey-core.jar file have to be created before running tests?", "[gunnar@localhost incubator-iota]$ find fey-core/target/scala-2.11 | grep jar  fey-core/target/scala-2.11/test-classes/fey-test-actor.jar    Thanks,    Gunnar      On Sun, Feb 19, 2017 at 11:21 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    I have noticed that when assembly runs the test, in some cases, some of the tests fails."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Doesn't the iota-fey-core.jar file have to be created before running tests?", "[gunnar@localhost incubator-iota]$ find fey-core/target/scala-2.11 | grep jar  fey-core/target/scala-2.11/test-classes/fey-test-actor.jar    Thanks,    Gunnar      On Sun, Feb 19, 2017 at 11:21 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    I have noticed that when assembly runs the test, in some cases, some of the tests fails.", "I will need sometime to investigate why it does not work 100% when assembly calls it.", "For now, you can run the tests separately:     sbt   projetc fey-core   test    and for now add the following line to the fey project in  project/Build.scala    test in assembly := {}    it should look like:  <Screen Shot 2017-02-19 at 10.18.47 AM.png    Please, let me know if that works for you    Regards    On Sun, Feb 19, 2017 at 9:17 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  HI Gunnar,     Thanks for the info.", "I will try to reproduce the issue on CentOS."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you re-run the tests, do you still get the SAME failed tests or it fails but in different tests?", "Regards    On Sun, Feb 19, 2017 at 7:52 AM, Gunnar Tapper <tapper.gunnar@gmail.com wrote:  Hi Barbara:    I'm on an x86 box with:    [gunnar@localhost ~]$ lsb_release -a  LSB Version:    :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-ia32:printing-4.0-noarch  Distributor ID: CentOS  Description:    CentOS release 6.8 (Final)  Release:        6.8  Codename:       Final    It may be too weak for what's needed here.", "I can spin up a AWS micro instance once I know what OS to install.", "Thanks,    Gunnar    On Sun, Feb 19, 2017 at 8:45 AM, Barbara Malta Gomes <barbaramaltagomes@gmail.com wrote:  Hi Gunnar,    Which Operational System are you using?", "sbt picks the order of the tests execution and it might be the problem on your machine."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I  *think* Max had a partial PR on that, but don't know the current state.", "Re: (2), agree.", "Should just do a bulk PR for it.", "Cheers,  Chris    On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com wrote:    Hi,    I am wondering what needs to be done to get to an Apache release?", "I think  now 1.7.1.3 is out we should be focused on getting one out as we are kind  of half way the incubation process."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["Should just do a bulk PR for it.", "Cheers,  Chris    On Tue, Jun 14, 2016 at 8:41 AM, Bolke de Bruin <bdbruin@gmail.com wrote:    Hi,    I am wondering what needs to be done to get to an Apache release?", "I think  now 1.7.1.3 is out we should be focused on getting one out as we are kind  of half way the incubation process.", "What comes to my mind is:    1.", "Replace highcharts by D3 (WIP:  https://github.com/apache/incubator-airflow/pull/1469)  2."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["What comes to my mind is:    1.", "Replace highcharts by D3 (WIP:  https://github.com/apache/incubator-airflow/pull/1469)  2.", "Add license headers everywhere (TM) (Sucks, as it will break many PRs  -  but lets do it quickly)  3.", "Have a review by Apache    Anything I am missing?", "- Bolke     "], "labels": ["1", "1", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["@bolke, is the release ready for an RC?", "If so, I can ship to our environments and vote accordingly.", "On Mon, Jul 11, 2016 at 3:28 PM, Chris Riccomini <criccomini@apache.org wrote:  If we need to update this branch I prefer very much to have commits that  are also applied against master.", "Absolutely +1 to this.", "Everything must go through master first."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Mon, Jul 11, 2016 at 3:28 PM, Chris Riccomini <criccomini@apache.org wrote:  If we need to update this branch I prefer very much to have commits that  are also applied against master.", "Absolutely +1 to this.", "Everything must go through master first.", "On Mon, Jul 11, 2016 at 7:06 AM, Bolke de Bruin <bdbruin@gmail.com wrote:   Ok.", "I created a \u201cbranch-1.7.2-apache\u201d branch based on airbnb_rb1.7.1_4 to  which I added cherry-picked commits to establish Apache compliancy (I  think!)."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If we need to update this branch I prefer very much to have commits  that are also applied against master.", "- Bolke     Op 11 jul.", "2016, om 15:16 heeft Bolke de Bruin <bdbruin@gmail.com het   volgende geschreven:     Ok.", "I am seeing a \u201cairbnb_rb1.7.1_4\u201d branch, I am assuming this is the   branch to start from (?).", "I will work on applying the commits that are required to become   compliant."], "labels": ["0", "0", "0", "0", "1"]}
{"abstract_id": 0, "sentences": ["2016, om 15:16 heeft Bolke de Bruin <bdbruin@gmail.com het   volgende geschreven:     Ok.", "I am seeing a \u201cairbnb_rb1.7.1_4\u201d branch, I am assuming this is the   branch to start from (?).", "I will work on applying the commits that are required to become   compliant.", "- Bolke       Op 11 jul.", "2016, om 05:22 heeft Maxime Beauchemin   <maximebeauchemin@gmail.com het volgende geschreven:     +1     On Fri, Jul 8, 2016 at 5:16 PM, Dan Davydov   <dan.davydov@airbnb.com.invalid   wrote:     +1 the minimal apache cherrypick release makes sense to me."], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["I will work on applying the commits that are required to become   compliant.", "- Bolke       Op 11 jul.", "2016, om 05:22 heeft Maxime Beauchemin   <maximebeauchemin@gmail.com het volgende geschreven:     +1     On Fri, Jul 8, 2016 at 5:16 PM, Dan Davydov   <dan.davydov@airbnb.com.invalid   wrote:     +1 the minimal apache cherrypick release makes sense to me.", "On Fri, Jul 8, 2016 at 1:14 PM, Chris Riccomini   <criccomini@apache.org   wrote:     Hey Bolke,     A fast release with the 1.7.1.3 + cherry picks listed above sounds   like   the   way to go.", "Then, a second release in sept where we just cut from   master."], "labels": ["1", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["My suggestion is to   start   the process and see if we get questions about this that require us   the   change our point of view.", "If we do an earlier release I would like to aim for July 19, but   that   might be a bit short notice.", "If needed I can put myself up as   release   manager till the 21st.", "If we do 1.7.1.3 + cherry picks I would say     * Licenses   * Notices   * Disclaimer   * Highcharts - d3     Makes sense?", "Anything missing here?"], "labels": ["1", "0", "1", "1", "0"]}
{"abstract_id": 0, "sentences": ["[1] https://github.com/paramiko/paramiko/blob/master/LICENSE     On Wed, Jul 6, 2016 at 10:16 PM, Chris Nauroth <   cnauroth@hortonworks.com     wrote:     Here are more details on Apache release requirements:     http://www.apache.org/dev/release-publishing.html       http://www.apache.org/dev/release       To summarize, it's much more focused on compliance with licensing,   signing   and Apache infrastructure requirements.", "That's the kind of   scrutiny   that   a release candidate will get from the Incubator PMC rather than   deep   testing for verification of new features or bug fixes.", "For that reason, I think it makes sense for a podling's first   Apache   release to focus on nothing but those ASF policy requirements.", "It's   completely normal for a podling's early release candidates to have   a   few   false starts that get voted down, because the policies are complex   the   first time around.", "Some projects have found it helpful to write a   \"How   to   Release\" web page during the first release, so that they have   step-by-step   notes to follow during subsequent releases."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["For that reason, I think it makes sense for a podling's first   Apache   release to focus on nothing but those ASF policy requirements.", "It's   completely normal for a podling's early release candidates to have   a   few   false starts that get voted down, because the policies are complex   the   first time around.", "Some projects have found it helpful to write a   \"How   to   Release\" web page during the first release, so that they have   step-by-step   notes to follow during subsequent releases.", "Focusing on \"latest   stable\"   with a few additional patches sounds like a great plan to me,   because   it   decouples the challenges of your first ASF release from other   software   development pressures, such as pressure from a user base to ship a   new   feature quickly.", "Regarding the LGPL question, in general, the answer is that we are   prohibited from redistributing any LGPL component, but it's   acceptable   to   have a soft dependency on an LGPL component, such that a user   could   deploy   the LGPL component separately to enable additional optional   features."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["I would want to aim earlier, but due to holidays I guess it   might   be   smarter to schedule it a bit after?", "So would I, personally.", "I'd be OK with starting RCs now, to be   frank.", "What   do others think?", "Should we vote on the above?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So would I, personally.", "I'd be OK with starting RCs now, to be   frank.", "What   do others think?", "Should we vote on the above?", "No need, IMO."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We just need to be moving in   that   direction.", "Cheers,   Chris     [1] https://issues.apache.org/jira/browse/LEGAL     On Wed, Jul 6, 2016 at 1:24 PM, Bolke de Bruin <   bdbruin@gmail.com   wrote:     Hi,     As I don\u00b9t think there are any show stoppers to have an Apache   release   should we aim for a release 1st week of September?", "I would   want   to   aim   earlier, but due to holidays I guess it might be smarter to   schedule   it   a   bit after?", "- RC last week of August giving about two weeks to have it run   in   production in our environments   - Guess voting needs to happen at the IPMC   - Release (Champagne!)", "Earlier there was some discussion about psycopg2 / postgres   interoperability (psycopg2 being LGPL)."], "labels": ["1", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["That's as good a place as any.", "You are also welcome to tweet your slides out as well (and mention @ApacheAirflow) so we can retweet via the Apache Airflow twitter account.", "-s  On Thu, Nov 17, 2016 at 2:04 PM, Rob Froetscher <rfroetscher@lumoslabs.com wrote:   Thanks, I've added the slides to the links page.", "I didn't see a specific  page for that meetup for slides in general.", "On Thu, Nov 17, 2016 at 11:39 AM, siddharth anand <sanand@apache.org  wrote:    Rob,   Wiki Access granted."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-s  On Thu, Nov 17, 2016 at 2:04 PM, Rob Froetscher <rfroetscher@lumoslabs.com wrote:   Thanks, I've added the slides to the links page.", "I didn't see a specific  page for that meetup for slides in general.", "On Thu, Nov 17, 2016 at 11:39 AM, siddharth anand <sanand@apache.org  wrote:    Rob,   Wiki Access granted.", "-s     On Thu, Nov 17, 2016 at 10:49 AM, Rob Froetscher <   rfroetscher@lumoslabs.com   wrote:      Here are our slides:    https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYd    Ohq_np90VlbVRc/edit?usp=sharing       I don't think I have permissions to edit the wiki       On Thu, Nov 17, 2016 at 10:41 AM, Chris Riccomini <  criccomini@apache.org       wrote:        We'll be posting the video recording shortly.", "IT is working on it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-s     On Thu, Nov 17, 2016 at 10:49 AM, Rob Froetscher <   rfroetscher@lumoslabs.com   wrote:      Here are our slides:    https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYd    Ohq_np90VlbVRc/edit?usp=sharing       I don't think I have permissions to edit the wiki       On Thu, Nov 17, 2016 at 10:41 AM, Chris Riccomini <  criccomini@apache.org       wrote:        We'll be posting the video recording shortly.", "IT is working on it.", ":)         Will post link on the meetup and mailing list.", "On Thu, Nov 17, 2016 at 10:12 AM, Siddharth Anand     <sanand@agari.com.invalid wrote:      Chris and WePayEng,      Thanks for hosting another great Airflow meet-up.", "Can all of the speakers post their slides online and add links to   those      talks in response to this email (and also on our Wiki)?"], "labels": ["0", "0", "1", "0", "1"]}
{"abstract_id": 0, "sentences": ["IT is working on it.", ":)         Will post link on the meetup and mailing list.", "On Thu, Nov 17, 2016 at 10:12 AM, Siddharth Anand     <sanand@agari.com.invalid wrote:      Chris and WePayEng,      Thanks for hosting another great Airflow meet-up.", "Can all of the speakers post their slides online and add links to   those      talks in response to this email (and also on our Wiki)?", "-s            "], "labels": ["0", "1", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["LatestOnly allows you to bypass missed runs and just do it once for most recent instance.", "Another difference, depends on past is tricky if you use BranchOperator because some branches may not run one day and run another - it will really mess up your logic.", "On Mon, Mar 20, 2017 at 12:45 PM, Ruslan Dautkhanov <dautkhanov@gmail.com wrote:   Thanks Boris.", "It does make sense.", "Although how it's different from depends_on_past task-level parameter?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Jan 20, 2017 at 8:20 AM, Jeremiah Lowin <jlowin@apache.org wrote:   Hi Laura,   The error is raised if an unused argument is passed to BaseOperator --  basically if there is anything in either args or kwargs.", "The original issue  was that in a number of cases arguments were misspelled or misused by  Operator subclasses and instead of raising an error, they were just passed  up the inheritance chain and finally (silently) absorbed by BaseOperator,  so there was no warning.", "I think a workaround should be straightforward -- when you call  super().__init__ for the BaseOperator, just pass arguments explicitly  rather than with args/kwargs, or (alternatively), pop arguments out of  kwargs when you use them ahead of calling that __init__.", "On Thu, Jan 19, 2017 at 10:23 AM Laura Lorenz <llorenz@industrydive.com  wrote:    Hi!", "Is there a way to determine the rationale behind deprecation  warnings?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there a way to determine the rationale behind deprecation  warnings?", "In particular I'm interested in the following:       /Users/llorenz/Envs/fileflow/lib/python2.7/site-packages/  airflow/models.py:1719:    PendingDeprecationWarning: Invalid arguments were passed to    DivePythonOperator.", "Support for passing such arguments will be dropped  in    Airflow 2.0.", "Invalid arguments were:       *args: ()       **kwargs: {'data_dependencies': {'something': 'write_a_file'}}         category=PendingDeprecationWarning        Our home grown plugin fileflow depends on this capability so I'd like to   get more information about how it will be changing to see if I can   anticipate a workaround to support airflow 2.0.", "Thanks!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In particular I'm interested in the following:       /Users/llorenz/Envs/fileflow/lib/python2.7/site-packages/  airflow/models.py:1719:    PendingDeprecationWarning: Invalid arguments were passed to    DivePythonOperator.", "Support for passing such arguments will be dropped  in    Airflow 2.0.", "Invalid arguments were:       *args: ()       **kwargs: {'data_dependencies': {'something': 'write_a_file'}}         category=PendingDeprecationWarning        Our home grown plugin fileflow depends on this capability so I'd like to   get more information about how it will be changing to see if I can   anticipate a workaround to support airflow 2.0.", "Thanks!", "Laura     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Fri, Apr 29, 2016 at 1:36 PM, Siddharth Anand < siddharthanand@yahoo.com.invalid wrote:   Jacob,What's the process?", "-s       On Friday, April 29, 2016 1:26 PM, Jakob Homan <jghoman@gmail.com  wrote:     Here are the examples from last month:  https://wiki.apache.org/incubator/April2016   Here's where to edit it for May: https://wiki.apache.org/incubator/May2016   Don't feel pressure to fill in a lot; the project is just starting,  which is a fine response.", "-jg    On 29 April 2016 at 13:23, Siddharth Anand  <siddharthanand@yahoo.com.invalid wrote:   I can if someone wants to send me pointers.", "-s        On Friday, April 29, 2016 1:18 PM, Chris Riccomini <  criccomini@apache.org wrote:        So, who wants to write the board report?", ":)            "], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The ShortCircuitOperator will need to specify a trigger rule of *all_done *to ensure it's always called.", "Then, specify a python callable that checks the status of upstream tasks for success.", "If all direct upstream tasks are successful, skip the last task.", "If not, then execute the last task which would carry out the failure notification that you desire.", "*Future* Implement an on-failure callback."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If not, then execute the last task which would carry out the failure notification that you desire.", "*Future* Implement an on-failure callback.", "Have a look at how dag.sla_miss_callback is implemented!", "-s  On Mon, Oct 31, 2016 at 3:24 PM, Casey Ching <casey@eazeup.com wrote:   Hello,   I\u2019d like the ability to send notifications when a DAG fails.", "Basically the  same as on_failure_callback that is available for operators but for a DAG  instead."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["----------------------------        Minor edits to pom.xml's to improve operation  ---------------------------------------------                   Key: ISIS-110                  URL: https://issues.apache.org/jira/browse/ISIS-110              Project: Isis           Issue Type: Sub-task             Reporter: Kevin Meyer             Priority: Trivial               Labels: maven              Fix For: 0.2.0-incubating    Minor changes to Isis pom.xml files to improve behaviour.", "Overall behaviour is unaffected by these changes, e.g.", "moving a dependency version definition higher up the build path, etc.", "-- This message is automatically generated by JIRA.", "If you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa For more information on JIRA, see: http://www.atlassian.com/software/jira            "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I wrote the EMR tooling  <https://github.com/apache/incubator-airflow/pull/1630 (hook, operators,  sensors) for Airflow so hopefully this could be a good video documentation  of how to use it.", "Tim wrote many of our Spark jobs and has more context on  how/why we use Airflow in this way for it.", "Let me know if you think this would be good.", "Happy to put together a short  abstract.", "Thanks,   Rob   On Thu, Oct 20, 2016 at 8:43 AM, Chris Riccomini <criccomini@apache.org  wrote:   Hey all,   We have one more slot to fill for the next meetup at WePay on November 16:     http://www.meetup.com/Bay-Area-Apache-Airflow-Incubating-Meetup/events/  234778571/   The slot is for 15m + 5m Q&A, so it's not too much of a commitment."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have several DAGs for which the *last* task is not moving from queued to  running.", "These DAGs used to run fine some time ago, but then we had issues with  rabbitmq cluster we use, and after resetting it up, the problem emerged.", "I'm pretty sure the queue is working fine, since all the tasks except the  very last one are queued automatically and run fine.", "For the sake of testing, I added a copy of the last task to the DAG, and  interestingly, the task that used to be the last and did not run, now  started to run normally, but the new last task is stuck.", "I checked logs at the DEBUG level and I could see that scheduler queues the  tasks, but those tasks don't show up in the Celery/Flower dashboard in the  corresponding queue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For the sake of testing, I added a copy of the last task to the DAG, and  interestingly, the task that used to be the last and did not run, now  started to run normally, but the new last task is stuck.", "I checked logs at the DEBUG level and I could see that scheduler queues the  tasks, but those tasks don't show up in the Celery/Flower dashboard in the  corresponding queue.", "When I run the task that is stuck from the webserver interface, they show  up in the queue in Flower dashboard and run successfully.", "So, overall, it seems that the issue is present with the scheduler but not  with webserver, and that this issue is only related to the very last task  in the DAG.", "I'm really stuck now, I would welcome any suggestions / ideas on what can  be done."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I checked logs at the DEBUG level and I could see that scheduler queues the  tasks, but those tasks don't show up in the Celery/Flower dashboard in the  corresponding queue.", "When I run the task that is stuck from the webserver interface, they show  up in the queue in Flower dashboard and run successfully.", "So, overall, it seems that the issue is present with the scheduler but not  with webserver, and that this issue is only related to the very last task  in the DAG.", "I'm really stuck now, I would welcome any suggestions / ideas on what can  be done.", "Thank you in advance!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["When was the issue  introduced?", "On the the issue Alex mentioned, we don\u2019t see that and I cannot  really align the description of the issue with the PR yet, ie.", "I need  clarification.", "Obviously, I\u2019m not very happy if we indeed need to retract the release as  we are ~12 hours away from closing of the vote at the IPMC mailinglist  (strangely enough no one has voted yet).", "However, if it is that serious  that it cannot wait for 1.8.1 then we need to do it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On the the issue Alex mentioned, we don\u2019t see that and I cannot  really align the description of the issue with the PR yet, ie.", "I need  clarification.", "Obviously, I\u2019m not very happy if we indeed need to retract the release as  we are ~12 hours away from closing of the vote at the IPMC mailinglist  (strangely enough no one has voted yet).", "However, if it is that serious  that it cannot wait for 1.8.1 then we need to do it.", "I would define  \u201cserious\u201d as many people are going to be affected by it and they will not  have a workaround available to them (ie."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["a task was added to a Dag in  a later stage).", "On 23 Feb 2017, at 20:15, Dan Davydov <dan.davydov@airbnb.com.INVALID  wrote:     Some more issues found by our users in addition to the one Alex reported   and the UI issue when a dagrun doesn't have a start date:   1.", "If a task fails it fails the whole dagrun immediately fails, this is a   very large change to how control flow works as the rest of the tasks in  the   DAG are not run (even e.g.", "leaf tasks).", "The same is true of the skipped   status (if a leaf task is skipped then the root task for the DAG will get   skipped and none of the other tasks in the DAG will run)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["leaf tasks).", "The same is true of the skipped   status (if a leaf task is skipped then the root task for the DAG will get   skipped and none of the other tasks in the DAG will run).", "2.", "The black squares in the UI for tasks that aren't ready to run yet are   confusing and make it hard for users to see which tasks haven't run yet   (lower contrast).", "We should never initialize tasks in the DB that do not   have a state (or at the least these should be white)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We should never initialize tasks in the DB that do not   have a state (or at the least these should be white).", "3.", "The Dagrun has a get_task_instance method that will fail if a dagrun   doesn't have a copy of a task instance created which we have seen happen   for some DAGs.", "This prevents those tasks from getting scheduled.", "I already patched 3 (and have a PR in flight for open source), and am   working on a patch for 1 internally."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks again for the mountain of work that went into packaging this     release.", "Max         On Wed, Feb 22, 2017 at 2:44 PM, Bolke de Bruin <bdbruin@gmail.com   wrote:         I thought you volunteered to baby sit 1.8.1 Chris ;-)?", "Sent from my iPhone         On 22 Feb 2017, at 23:31, Chris Riccomini <criccomini@apache.org     wrote:         I'm +1 for doing a 1.8.1 fast follow-on         On Wed, Feb 22, 2017 at 2:26 PM, Maxime Beauchemin <     maximebeauchemin@gmail.com wrote:         Our database may have edge cases that could be associated with   running     any     previous version that may or may not have been part of an official     release.", "Let's see if anyone else reports the issue.", "If no one does, one   option     is     to release 1.8.0 as is with a comment in the release notes, and   have a     future official minor apache release 1.8.1 that would fix these   minor     issues that are not deal breaker."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'd like to add the Pypi part to this doc and add committers that   are     interested to have rights on the project on Pypi.", "Max         On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com       wrote:         So it is a database integrity issue?", "Afaik a start_date should   always     be     set for a DagRun (create_dagrun) does so I didn't check the code     though.", "Sent from my iPhone         On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.", "INVALID     wrote:         Should clarify this occurs when a dagrun does not have a start   date,     not     a     dag (which makes it even less likely to happen)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["3) I will upload it to the incubator release page, then the   tar     ball     needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi.", "It seems   we     can     keep     the apache branding as lib cloud is doing this as well (     https://libcloud.apache.org/downloads.html#pypi-package <     https://libcloud.apache.org/downloads.html#pypi-package).", "Jippie!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["      [ https://issues.apache.org/jira/browse/JSPWIKI-329?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]  Florian Holeczek updated JSPWIKI-329: -------------------------------------          Fix Version/s:     (was: 2.7.x)     Affects Version/s: 2.7.x   wrong version history grouping on page info tab  -----------------------------------------------                   Key: JSPWIKI-329                  URL: https://issues.apache.org/jira/browse/JSPWIKI-329              Project: JSPWiki           Issue Type: Bug           Components: Default template     Affects Versions: 2.6.3, 2.7.x             Reporter: Florian Holeczek             Priority: Minor          Attachments: JSPWIKI-329.png    The version history is grouped by 20 entries per page.", "However, the grouping should follow the sorting order.", "At the moment, it starts at version-1, which results e.g.", "in only displayed version on page 1 if there are 20 versions available (see attached screenshot).", "--  This message is automatically generated by JIRA."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["     [ https://issues.apache.org/jira/browse/PARQUET-124?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14193313#comment-14193313 ]   Chris Albright edited comment on PARQUET-124 at 11/1/14 6:35 PM: -----------------------------------------------------------------  [~rdblue], I was not able to find a way compare to the path prefixes without using String#startsWith.", "It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI.", "If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If its important, I've got some other ideas, but I'm not sure its within scope of this issue.", "I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis.", "was (Author: chrisalbright): [~rdblue], I was not able to find a way to the path prefixes without converting using String#startsWith.", "It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis.", "was (Author: chrisalbright): [~rdblue], I was not able to find a way to the path prefixes without converting using String#startsWith.", "It does seem very unlikely that there would ever exist a situation where the footers come from a different filesystem than the root, since the footers are read out of the root to begin with :)   I thought of comparing the scheme as well, but I was not able to make that work since the scheme was missing from the root path URI.", "If its important, I've got some other ideas, but I'm not sure its within scope of this issue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'll go ahead and create a new one and work off of that.", "Pull request is updated, and passing in Travis.", "parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException  --------------------------------------------------------------------------------------------                   Key: PARQUET-124                  URL: https://issues.apache.org/jira/browse/PARQUET-124              Project: Parquet           Issue Type: Bug           Components: parquet-mr     Affects Versions: parquet-mr_1.6.0, 1.6.0rc2             Reporter: Chris Albright             Priority: Minor          Attachments: PARQUET-124-test    I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example), and in the process of updating the library versions, am getting the warning below.", "The version of Parquet-Hadoop in the original example is 1.0.0.", "I am using 1.6.0rc3  The ParquetFileWriter.mergeFooters(Path, List<Footer) method is performing a check to ensure the footers are all for files in the output directory."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The version of Parquet-Hadoop in the original example is 1.0.0.", "I am using 1.6.0rc3  The ParquetFileWriter.mergeFooters(Path, List<Footer) method is performing a check to ensure the footers are all for files in the output directory.", "The output directory is supplied by ParquetFileWriter.writeMetadataFile; in 1.0.0, the output path was converted to a fully qualified output path before the call to mergeFooters, but in 1.6.0rc[2,3] that conversion happens after the call to mergeFooters.", "Because of this, the check within merge footers is failing (the URI for the footers starts with file:, but not the URI for the root path does not)  Here is the warning message and stacktrace.", "Oct 30, 2014 9:11:31 PM WARNING: parquet.hadoop.ParquetOutputCommitter: could not write summary file for /tmp/1414728690018-0/output  parquet.io.ParquetEncodingException: file:/tmp/1414728690018-0/output/part-r-00000.parquet invalid: all the files must be contained in the root /tmp/1414728690018-0/output  \tat parquet.hadoop.ParquetFileWriter.mergeFooters(ParquetFileWriter.java:422)  \tat parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:398)  \tat parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:50)  \tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:936)  \tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:832)  \tat com.zenfractal.SparkParquetExample$.main(SparkParquetExample.scala:72)  \tat com.zenfractal.SparkParquetExample.main(SparkParquetExample.scala)  \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  \tat java.lang.reflect.Method.invoke(Method.java:606)  \tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)    -- This message was sent by Atlassian JIRA (v6.3.4#6332)  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/   ________________________________ From: Chris Riccomini <criccomini@apache.org Sent: 29 September 2016 18:14:02 To: dev@airflow.incubator.apache.org Subject: Re: Airflow Releases  Hey Luke,   Is there anything we can do to help get the next release in place?", "One thing that would definitely help is running master somewhere in your environment, and reporting any issues that see.", "Over the next few weeks, AirBNB and a few other folks will be doing the same in an effort to harden the 1.8 release.", "Cheers, Chris  On Thu, Sep 29, 2016 at 3:08 AM, Maycock, Luke <luke.maycock@affiliate.oliverwyman.com wrote:  Airflow Developers,    We were looking at writing a workflow framework in Python when we found Airflow.", "We have carried out some proof of concept work for using Airflow and wish to continue using it as it comes with lots of great features out-of-the-box."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, it appears that the latest release does not have the aforementioned fixes.", "Do you know when the next release of Airflow is expected?", "Is there anything we can do to help get the next release in place?", "Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Do you know when the next release of Airflow is expected?", "Is there anything we can do to help get the next release in place?", "Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail.", "Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We need a way to reproduce this.", "Please  add   your backend db (e.g.", "sqlite or something else) and an example of your  code   -- the one you are trying to clear.", "Then assign the bug to me (r39132).", "This may be related to the recursive clear of subdags, which was merged  in   https://github.com/apache/incubator-airflow/pull/1478     I mention in that PR the need to implement the CLI as well and this might   be a result of a half-implemented feature (i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't know if this has been completely fixed.", "You can rely on them to separate your task runs *most* of the time, but not *all* of the time- so don't write code that depends on exclusive operation.", "Lance  On Thu, Aug 11, 2016 at 1:15 PM, Kurt Muehlner <kmuehlner@connexity.com wrote:   I\u2019m not aware of a concurrency limit at task granularity, however, one  available option is the \u2018max_active_runs\u2019 parameter in the DAG class.", "max_active_runs (int) \u2013 maximum number of active DAG runs, beyond this  number of DAG runs in a running state, the scheduler won\u2019t create new  active DAG runs   I\u2019ve used the \u2018pool size of 1\u2019 option you mention as a very simple way to  ensure two DAGs run in serial.", "Kurt   On 8/11/16, 6:57 AM, \"\u05d4\u05d9\u05dc\u05d4 \u05d5\u05d9\u05d6\u05df\" <hilaviz@gmail.com wrote:       should I use pool of size 1?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Keys used to sign the release are available at  http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.1/KEYS?revision=1145809&view=markup.", "Please download, test, and try it out:       http://people.apache.org/~hashutosh/hcatalog-0.1.0-incubating-candidate-1/       The release, md5 signature, gpg signature, and rat report can all  be found at the above address.", "Should we release this?", "Vote closes on Thursday, July 15th.", "Thanks,    Ashutosh   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I read the article.", "It provides a nice example of trying to figure out the structure of a file that was poorly documented.", "Assertion: that example has nothing to do with DFDL.", "DFDL says nothing about how to figure out the structure of files.", "What DFDL does do is it tells you, once you have figured out the structure, then here's a way to describe the structure."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Dealing with them is day-to-day work for many engineers.", "This is a link to an article I wrote a long time ago (at least 7 years ago) about DFDL, and solving the \"data archaeology\" problem.", "It gives an example of an absolutely mundane day-to-day data problem that was typical of what I and many many other people were constantly solving for customers.", "It was some COBOL-ish data records that the customer couldn't figure out how to export from one system to get into some new system.", "They didn't have good configuration management, so the exact software that created the data wasn't clear."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why parse and unparse data formats?", "Hi Folks,    I want to elaborate on my question, as I feel it is a very important question.", "Does your average software engineer need to parse data formats?", "I can't think of any time, when implementing some software application, that I have needed to write a parser for a data format.", "The parsing is already done under the hood by some library."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For example, when I write code in XSLT I don't ever have to parse the input because there is a parser under the hood that does the parsing for me.", "I can understand why cybersecurity people want to parse data formats - they need to inspect the parsed data for malicious stuff.", "But as Mike noted, that's a tiny niche.", "Is DFDL relevant only to that tiny niche community?", "Hopefully it is relevant to other communities."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I can understand why cybersecurity people want to parse data formats - they need to inspect the parsed data for malicious stuff.", "But as Mike noted, that's a tiny niche.", "Is DFDL relevant only to that tiny niche community?", "Hopefully it is relevant to other communities.", "What communities?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is DFDL relevant only to that tiny niche community?", "Hopefully it is relevant to other communities.", "What communities?", "Who needs to parse?", "Who needs DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hopefully it is relevant to other communities.", "What communities?", "Who needs to parse?", "Who needs DFDL?", "/Roger      From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Monday, October 21, 2019 5:36 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Re: Why use DFDL?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Who needs to parse?", "Who needs DFDL?", "/Roger      From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Monday, October 21, 2019 5:36 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Re: Why use DFDL?", "Why parse and unparse data formats?", "Hi Mike,    Thank you for your great feedback."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I think that I have everything (or almost everything) you said already in my tutorial.", "I am trying to address a different concern: is parsing and unparsing applicable to the everyday software engineer?", "Where is parsing and unparsing needed?", "Why do I (the everyday software engineer) need parsing and unparsing, i.e., why do I need DFDL?", "As you point out, the cybercommunity needs to do parsing and unparsing, but the cybercommunity is a very small niche."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As you point out, the cybercommunity needs to do parsing and unparsing, but the cybercommunity is a very small niche.", "Are there other niches that need to do parsing and unparsing?", "Ideally most or all software engineers need to do parsing and unparsing; then DFDL would/should be in high demand.", "Again, I think the answer to these questions is supremely important.", "If I can't tell people why DFDL is important to them, well, then it's gonna be tough getting them interested in learning DFDL."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Ideally most or all software engineers need to do parsing and unparsing; then DFDL would/should be in high demand.", "Again, I think the answer to these questions is supremely important.", "If I can't tell people why DFDL is important to them, well, then it's gonna be tough getting them interested in learning DFDL.", "/Roger    From: Beckerle, Mike <mbeckerle@tresys.com<mailto:mbeckerle@tresys.com Sent: Monday, October 21, 2019 3:11 PM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: [EXT] Re: Why use DFDL?", "Why parse and unparse data formats?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Your last sentence on the slide wants to end with \"data\" not \"tool\".", "Your phrase \"Why parse and unparse data formats?\"", "gives me some concern.", "I mean do you have a choice?", "So I'm assuming you have to use the data, so the issue is why use DFDL vs. other ways of using the data:    The reasons to use DFDL are:    *   it is an emerging open standard."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Your phrase \"Why parse and unparse data formats?\"", "gives me some concern.", "I mean do you have a choice?", "So I'm assuming you have to use the data, so the issue is why use DFDL vs. other ways of using the data:    The reasons to use DFDL are:    *   it is an emerging open standard.", "In the long run standards give users power over vendors, reduce costs, increase skills leverage, etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["So I'm assuming you have to use the data, so the issue is why use DFDL vs. other ways of using the data:    The reasons to use DFDL are:    *   it is an emerging open standard.", "In the long run standards give users power over vendors, reduce costs, increase skills leverage, etc.", "*   it is comprehensive - can handle everything from military messaging formats to COBOL, binary and text and mixtures thereof.", "There are a few things it cannot describe as yet (TIFF for example), but it will evolve to cover those as well.", "*   it has superior unparsing capability to any existing data format description system - this is one area where the DFDL standard advances the state-of-the-art."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   it is comprehensive - can handle everything from military messaging formats to COBOL, binary and text and mixtures thereof.", "There are a few things it cannot describe as yet (TIFF for example), but it will evolve to cover those as well.", "*   it has superior unparsing capability to any existing data format description system - this is one area where the DFDL standard advances the state-of-the-art.", "*   there are multiple implementations including open-source and commercial.", "Some additional related points:    *   why use DFDL to parse/unparse  to/from an alternative standard textual form such as JSON or XML ?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   it has superior unparsing capability to any existing data format description system - this is one area where the DFDL standard advances the state-of-the-art.", "*   there are multiple implementations including open-source and commercial.", "Some additional related points:    *   why use DFDL to parse/unparse  to/from an alternative standard textual form such as JSON or XML ?", "*   Note that DFDL doesn't per-se require this.", "It is one common way to use the Daffodil implementation      *   Note that not all DFDL implementations even support this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It is one common way to use the Daffodil implementation      *   Note that not all DFDL implementations even support this.", "E.g., the ESA's DFDL4Space tool doesn't convert to/from JSON or XML.", "IBM DFDL can be used to convert data to XML, but when used in the most common ways, it takes data directly to/from the native internal data format used by that particular data-handling product/system.", "No intermediate step of XML/JSON is used.", "So I'd say the above point is really about Daffodil, not DFDL generally."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The above is about skills leverage and tools leverage.", "JSON support is built into all javascript based platforms such as web browsers, NODE.js, etc.", "XML has standard tools available from vendors.", "DFDL is another standard that adds capability to people with JSON/XML skills and/or tools already.", "Use of a textual format as an intermediate form has significant QA benefits for most systems."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The problem with this is it is typically procedural, not declarative.", "*   DFDL is also not turing complete.", "It is far easier to show correctness of a schema than a program.", "Point (a) above is just the standards vs. non-standards argument.", "At this point, I am digressing into lots of points made in this slideshare deck in slides 3, 4, 5:  https://www.slideshare.net/mbeckerle/tresys-dfdl-data-format-description-language-daffodil-open-source-public-overview-100432615    Another point is about the benefits and power of standards."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["*   DFDL is also not turing complete.", "It is far easier to show correctness of a schema than a program.", "Point (a) above is just the standards vs. non-standards argument.", "At this point, I am digressing into lots of points made in this slideshare deck in slides 3, 4, 5:  https://www.slideshare.net/mbeckerle/tresys-dfdl-data-format-description-language-daffodil-open-source-public-overview-100432615    Another point is about the benefits and power of standards.", "DFDL is simply better than existing ad-hoc data format description languages in that it is far more comprehensive than most commercial and other open-source systems, and it is an emerging open standard, with multiple implementations with a good deal of demonstrated interoperability: https://cwiki.apache.org/confluence/display/DAFFODIL/Daffodil+Compatibility+with+IBM+DFDL    DFDL is still quite new, and I would expect some users to choose other things until Daffodil gets out of Apache Incubator status, and the DFDL standard is fully ratified by Open Grid Forum, and is proposed as a standard by a larger/more-recognized body."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It is far easier to show correctness of a schema than a program.", "Point (a) above is just the standards vs. non-standards argument.", "At this point, I am digressing into lots of points made in this slideshare deck in slides 3, 4, 5:  https://www.slideshare.net/mbeckerle/tresys-dfdl-data-format-description-language-daffodil-open-source-public-overview-100432615    Another point is about the benefits and power of standards.", "DFDL is simply better than existing ad-hoc data format description languages in that it is far more comprehensive than most commercial and other open-source systems, and it is an emerging open standard, with multiple implementations with a good deal of demonstrated interoperability: https://cwiki.apache.org/confluence/display/DAFFODIL/Daffodil+Compatibility+with+IBM+DFDL    DFDL is still quite new, and I would expect some users to choose other things until Daffodil gets out of Apache Incubator status, and the DFDL standard is fully ratified by Open Grid Forum, and is proposed as a standard by a larger/more-recognized body.", "The fact that IBM has DFDL in multiple products now is a strong statement of support going forward."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["At this point, I am digressing into lots of points made in this slideshare deck in slides 3, 4, 5:  https://www.slideshare.net/mbeckerle/tresys-dfdl-data-format-description-language-daffodil-open-source-public-overview-100432615    Another point is about the benefits and power of standards.", "DFDL is simply better than existing ad-hoc data format description languages in that it is far more comprehensive than most commercial and other open-source systems, and it is an emerging open standard, with multiple implementations with a good deal of demonstrated interoperability: https://cwiki.apache.org/confluence/display/DAFFODIL/Daffodil+Compatibility+with+IBM+DFDL    DFDL is still quite new, and I would expect some users to choose other things until Daffodil gets out of Apache Incubator status, and the DFDL standard is fully ratified by Open Grid Forum, and is proposed as a standard by a larger/more-recognized body.", "The fact that IBM has DFDL in multiple products now is a strong statement of support going forward.", "________________________________  From: Costello, Roger L. <costello@mitre.org<mailto:costello@mitre.org Sent: Sunday, October 20, 2019 8:08 AM To: users@daffodil.apache.org<mailto:users@daffodil.apache.org <users@daffodil.apache.org<mailto:users@daffodil.apache.org Subject: Why use DFDL?", "Why parse and unparse data formats?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Why parse and unparse data formats?", "Hi Folks,    It occurred to me that in my tutorial I have explained what DFDL is and how to use DFDL, but I never explained why DFDL should be used.", "Below is a slide that takes a stab at why.", "I am sure there are other reasons.", "Would you provide other reasons, please?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["And actually, dfdl:textNumberPadKind isn't a thing, it should be dfdl:textPadKind.", "But if textPadKind=\"none\" then no padding will be added.", "But another option is that dfdl:fillByte is being used to fill those extra bytes.", "Perhaps dfdl:fillByte=\"f\"?", "- Steve  On 6/25/19 2:28 PM, Costello, Roger L. wrote:  Hi Steve,     I would guess that you have:    textNumberPadKind=\"padChar\",   textNumberPadCharacter=\"f\", and   textNumberJustification=\"left\"    Actually, I looked at my defaults.dfdl.xsd file and it doesn't mention any of those properties."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, with the pattern \"#,###\", the value \"100\" will unparse to   \"100\"--no comma is needed and it will not zero pad.", "But your test1   element is defined as having a length of 5 and the the unparsed value   has a length of 3.", "In this case, Daffodil uses textNumberPadKind and   related properties (textNumberPadCharacter, textNumberJustification,  etc.)", "to pad the unparsed value up to the needed 5 characters.", "So I would guess that you have textNumberPadKind=\"padChar\", textNumberPadCharacter=\"f\", and textNumberJustification=\"left\"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I don't think I would say to never use the '#' character.", "There are certainly going to be times where you don't want extra padding characters, like in some delimited formats where numbers do not have explicit lengths.", "- Steve    On 6/25/19 1:32 PM, Costello, Roger L. wrote:  Hello DFDL community,   My input file has this:   0,100   0,100   My DFDL schema is this:   <xs:elementname=\"input\"  <xs:complexType  <xs:sequencedfdl:separator=\"%NL;\"dfdl:separatorPosition=\"infix\"  <xs:elementname=\"test1\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"#,###\"/   <xs:elementname=\"test2\"type=\"xs:unsignedInt\"                   dfdl:length=\"5\"dfdl:lengthKind=\"explicit\"                   dfdl:textNumberCheckPolicy=\"strict\"                   dfdl:textNumberPattern=\"0,000\"/ </xs:sequence   </xs:complexType </xs:element   The output of parsing is this:   <input  <test1100</test1  <test2100</test2  </input   The output of unparsing is this:   100ff  0,100   Huh?", "Why am I getting 100ff?", "I think the lesson learned is never use the pound (#) symbol in   dfdl:textNumberPattern."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Kim,  Thanks.", "That worked.", "-Harold  --- On Thu, 10/22/09, Kim LiChong <Kim.Lichong@Sun.COM wrote:   From: Kim LiChong <Kim.Lichong@Sun.COM  Subject: Re: cannot Find Symbol: com.sun.faban.driver.transport.hc3.ApacheHC3Transport  To: olio-user@incubator.apache.org  Date: Thursday, October 22, 2009, 1:10 AM  Hi Harold,    Recent checkins for the workload driver for Java now depend  on a newer   version of Faban which is now available.", "Can you please get Faban version 1.0RC2 from the faban  website   <http://faban.sunsource.net/index.html#Downloads and  use this as your   FABAN_HOME when compiling?", "thanks,    Kim   Hi All,     When I try to compile the workload driver of Olio, I'm  getting the following errors."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["How can I specify a group id when using Kafka Source, now I just set \u201c  group.id=XXX\u201d in UserConfig.", "Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* \u8212\u7426<shuqi@eefung.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:19  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Manu,    Gearpump: 0.7.6_2.11   Kafka: 0.8.2.1_2.10.", "Thanks.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u8212\u7426  \u5730\u5740\uff1a\u957f\u6c99\u5e02\u5cb3\u9e93\u533a\u6587\u8f69\u8def27\u53f7\u9e93\u8c37\u4f01\u4e1a\u5e7f\u573aA4\u680b1\u5355\u51436F  \u7f51\u5740\uff1ahttp://www.eefung.com  \u5fae\u535a\uff1ahttp://weibo.com/eefung  \u90ae\u7f16\uff1a410013  \u7535\u8bdd\uff1a400-677-0986  \u4f20\u771f\uff1a0731-88519609    \u539f\u59cb\u90ae\u4ef6  *\u53d1\u4ef6\u4eba:* Manu Zhang<owenzhang1990@gmail.com  *\u6536\u4ef6\u4eba:* user<user@gearpump.incubator.apache.org  *\u53d1\u9001\u65f6\u95f4:* 2016\u5e745\u67083\u65e5(\u5468\u4e8c)\u200714:13  *\u4e3b\u9898:* Re: Questions About Kafka Source   Hi Qi,   Your code looks right."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Which gearpump version and kafka version have you  used ?", "On Tue, May 3, 2016 at 1:37 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi Manu,    Thanks for your help.", "I used the kafka-console-consumer with the same zks and topic, and it can  consume messages.", "There is still lots of messages in that topic.", "Belowing is the function I used to get Kafka Soruce, could you please  help to check if it is ok, thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yes, both zookeeper servers and kafka brokers configs are comma-separated  list strings.", "One way to check whether your configurations is correct it to  consume from the topic using kafka-console-consumer.", "This also makes sure  the topic has data to consume.", "Hope this helps.", "Thanks,  Manu   On Tue, May 3, 2016 at 12:07 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi,    I constructed a DAG as show blowing, \u201ckafka source\u201dconsumes messages  from kafka topic \u201cwebs\u201d, its metrics shows  that it consumes lots of  messages, but actually there is no messages handled and I also can\u2019t find  active group under topic \u201cwebs\u201d, the log is ok too."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This also makes sure  the topic has data to consume.", "Hope this helps.", "Thanks,  Manu   On Tue, May 3, 2016 at 12:07 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi,    I constructed a DAG as show blowing, \u201ckafka source\u201dconsumes messages  from kafka topic \u201cwebs\u201d, its metrics shows  that it consumes lots of  messages, but actually there is no messages handled and I also can\u2019t find  active group under topic \u201cwebs\u201d, the log is ok too.", "I just wonder the properties of kafka for zks and brokers, if there is a  list of zookeeper servers, should I use comma to separate?", "just like below:    zks=zk1:3181,zk2:3181,zk3:3181   brokers=kfk1:9096,kfk2:9096,kfk3:9096    Thanks for your help."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hope this helps.", "Thanks,  Manu   On Tue, May 3, 2016 at 12:07 PM \u8212\u7426 <shuqi@eefung.com wrote:   Hi,    I constructed a DAG as show blowing, \u201ckafka source\u201dconsumes messages  from kafka topic \u201cwebs\u201d, its metrics shows  that it consumes lots of  messages, but actually there is no messages handled and I also can\u2019t find  active group under topic \u201cwebs\u201d, the log is ok too.", "I just wonder the properties of kafka for zks and brokers, if there is a  list of zookeeper servers, should I use comma to separate?", "just like below:    zks=zk1:3181,zk2:3181,zk3:3181   brokers=kfk1:9096,kfk2:9096,kfk3:9096    Thanks for your help.", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  Qi Shu    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Also how to run other examples described with s4.", "I am really interested in knowing about s4.", "I would really suggest that you follow the manual.", "You probably forgot  to checkout tag 0.3.", "See: http://docs.s4.io/tutorials/getting_started.html  Matthieu  "], "labels": ["0", "0", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Further investigation is required but I need to focus on the first  Apache release for this month.", "GA of FFM will be v0.5.1 release scheduled on Dec.   Makoto   2017-10-18 1:36 GMT+09:00 Makoto Yui <myui@apache.org:  Thanks.", "I'll test FFM with it tomorrow.", "Makoto   2017-10-18 1:19 GMT+09:00 Shadi Mari <shadimari@gmail.com:  Attached us a sample of 500 examples from my training set represented as  vector of features.", "Regards,    On Tue, Oct 17, 2017 at 7:08 PM, Makoto Yui <myui@apache.org wrote:   I need to reproduce your test."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks               On Tue, Oct 17, 2017 at 6:51 PM, Makoto Yui <myui@apache.org wrote:     The empirical loss (cumulative logloss) is too large.", "The simple test in FieldAwareFactorizationMachineUDTFTest shows that   empirical loss is decreasing properly but it seems optimization is not   working correctly in your case.", "Could you show me the training hyperparameters?", "Makoto     2017-10-17 19:01 GMT+09:00 Shadi Mari <shadimari@gmail.com:    Hello,       I am trying to understand the results produced by FFM on each    iteration    during the training of Criteo 2014 dataset.", "Basically, I have 10 mappers running concurrently (each has ~4.5M    records),    and follows is an output by one of the mappers:       -----------------------------       fm.FactorizationMachineUDTF|: Wrote 4479491 records to a temporary    file    for    iterative training: hivemall_fm392724107368114556.sgmt (2.02 GiB)    Iteration #2 [curLosses=1.5967339372694769E10,    prevLosses=4.182558816480771E10, changeRate=0.6182399322209704,    #trainingExamples=4479491]       -----------------------------       Looking at the source code, FFM implementation uses LogLess    performance    metric when classification is specified, however the curLossess    counter    is    very high 1.5967339372694769E10          What does this mean?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A received an exception from ParametersInjectionModule: property already configured.", "- Tried to pass that parameter in s4r deploy.", "Nothing changed.", "My actual s4 version is 0.6.0 final, built from src (was 0.5.0 but updated myself because of this!)", "Any ideas?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Tried to pass that parameter in s4r deploy.", "Nothing changed.", "My actual s4 version is 0.6.0 final, built from src (was 0.5.0 but updated myself because of this!)", "Any ideas?", "maybe I'm going with log4j..."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On 12/13/06, Gary VanMatre <gvanmatre@apache.org wrote:  I've been experimenting with using Shale Clay for template composition using  Trinidad components.", "Anyway, it requires a couple classes that handle all of the special method  binding events.", "http://svn.apache.org/viewvc/shale/sandbox/shale-clay-trinidad/src/main/java/org/apache/shale/clay/component/chain/trinidad/PropertyListenerCommand.java?view=markup   This class has a dependency with commons chains.", "It also requires some xml  configs.", "The clay configs are generated form the TLD's using a shale maven  plugin and we also have a commons chains config."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Apparently my EXE is not typical as it has 6 Import Lookup Tables.", "/Roger  -----Original Message----- From: Costello, Roger L. <costello@mitre.org  Sent: Sunday, March 3, 2019 3:17 PM To: users@daffodil.apache.org Subject: Re: Need help informing Daffodil that we're finished with this field and it's time to build the next field  Hi Steve,  The EXE specification is silent on the number of occurrences of the Import_Lookup_Table.", "All I know is that once we see the first Hint_Name_Table entry (a 2-byte address, followed by a null-terminated name, followed by an optional null) then we know that there are no more Import_Lookup_Tables.", "Is there a way to express (for Import_Lookup_Table):  \tThere are no more occurrences \tonce we encounter a 2-byte \taddress, followed by a null-terminated \tstring, followed by an optional \tnull.", "Is it possible to express that?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am struggling   with how to inform Daffodil, \"Hey Daffodil, the input is finished with   the Import_Lookup_Tables, now it's time to build the Hint_Name_Table.\"", "I am hoping you can show me how to inform Daffodil of this.", "Each Import_Lookup_Table consists of one of more 32-bit fields,   terminated by a 32-bit field containing all nulls.", "The Hint_Name_Table consists of one of more entries; each entry   consists of a 2-byte address, followed by a null-terminated name,   followed by an optional null (to align to a 2-byte boundary).", "Here is a graphic that illustrates the Import_Lookup_Tables followed   by the  Hint_Name_Table:    Here is the relevant portion of my DFDL schema:    <xs:elementname=\"idata_Section\"  <xs:complexType  <xs:sequence  <xs:elementref=\"Import_Lookup_Table\"                           maxOccurs=\"unbounded\"/   <xs:elementref=\"Hint_Name_Table\"/  </xs:sequence  </xs:complexType  </xs:element    <xs:elementname=\"Import_Lookup_Table\"  <xs:complexType  <xs:sequence  <xs:elementref=\"Lookup_Table_Entry\"                           maxOccurs=\"unbounded\"/   <xs:elementname=\"Null_Lookup_Table_Entry\"                           type=\"xs:hexBinary\"                           dfdl:lengthKind=\"explicit\"                           dfdl:length=\"4\"                           dfdl:lengthUnits=\"bytes\" <xs:annotation   <xs:appinfosource=\"http://www.ogf.org/dfdl/\"  <dfdl:assert                               { ."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["There's no way around that.", "Note that the dfdl:calendarPattern does not describe the format of the infoset, but is used to describe the format of input calendar textual data.", "The infoset will always be YYYY-mm-dd hh:mm:ss.", "If you wanted only the year, you would need to parse the field as a full dateTime, and then use inputValueCalc to extract just the year.", "For example:    <xs:element name=\"root\"     <xs:complexType       <xs:sequence         <xs:element name=\"dateTime\" type=\"xs:dateTime\"           dfdl:lengthKind=\"explicit\"           dfdl:lengthUnits=\"bytes\"           dfdl:length=\"4\"           dfdl:binaryCalendarRep=\"binarySeconds\"           dfdl:binaryCalendarEpoch=\"1970-01-01T00:00:00\" /         <xs:element name=\"year\" type=\"xs:int\"           dfdl:inputValueCalc=\"{ fn:year-from-dateTime(../dateTime) }\" /       </xs:sequence     </xs:complexType   </xs:element    On 10/1/19 8:38 AM, Costello, Roger L. wrote:  Hello DFDL community,    My binary input file contains the number of seconds since epoch for the start of a year."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That's only a recent addition, and I've only got 2005 & 2008 myself to test on.", "Perhaps someone else here can offer you some more advice, or work with you to debug the issue.", "- Brett  On 17/03/2011, at 10:09 AM, Eric Kolotyluk wrote:   So I uninstalled and reinstalled NPanday, but I have the same problem with  VS 2010 - so it seems to be 2010 related.", "The plugin seems to work for VS 2005, so I'll experiment with that.", "In the  end I really need this to work in VS 2010 as that is our development  environment."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Perhaps someone else here can offer you some more advice, or work with you to debug the issue.", "- Brett  On 17/03/2011, at 10:09 AM, Eric Kolotyluk wrote:   So I uninstalled and reinstalled NPanday, but I have the same problem with  VS 2010 - so it seems to be 2010 related.", "The plugin seems to work for VS 2005, so I'll experiment with that.", "In the  end I really need this to work in VS 2010 as that is our development  environment.", "I don't have VS 2008."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In the  end I really need this to work in VS 2010 as that is our development  environment.", "I don't have VS 2008.", "Cheers, Eric    On Mon, Mar 14, 2011 at 4:32 PM, Brett Porter <brett@apache.org wrote:    The problem is that the wizard doesn't install some things in the GAC yet,  making the redundant Maven command necessary.", "What failure did you get from  that?", "On 15/03/2011, at 10:14 AM, Eric Kolotyluk wrote:    That's the funny thing, I have never installed NPanday before - this was  the first time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers, Eric    On Mon, Mar 14, 2011 at 2:33 PM, Brett Porter <brett@apache.org wrote:    It sounds like you had an old version installed beforehand.", "We should  handle this better, so feel free to file a bug.", "However, to get it working I  suggest you follow the uninstall instructions on the web site (make sure  everything is removed, particularly the AddIn file in your documents  folder), make sure it does not appear in VS and then try installing again.", "On 15/03/2011, at 1:57 AM, Eric Kolotyluk wrote:    After running the NPanday installation wizard, when I start VS 2010 I  get the following message (see attached)    Exception adding NPanday to the tools m...  A command with that name already exists.", "NPanday shows up in the Tools menu, but it does nothing when I click it,  and then the menu item goes disabled."], "labels": ["0", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Take a look at the use case of twitter data, the search API of the twitter does not give the meta data for the big search entries.", "https://dev.twitter.com/rest/public/search   -What you mean by keeping things in-memory between source / workunits.", "That wont be possible for something like Yarn mode.", "I have not made it clear for the yarn use cases, I intent to use Distributed memory ( Apache Ignite) for the same.", "I hope I am clear this time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That wont be possible for something like Yarn mode.", "I have not made it clear for the yarn use cases, I intent to use Distributed memory ( Apache Ignite) for the same.", "I hope I am clear this time.", "On Fri, Sep 8, 2017 at 6:05 AM, Abhishek Tiwari <abti@apache.org wrote:   I am not super clear with your use-case:   - Do you intend to pull in metadata only in source about the number of  records?", "It seems like it but you have mentioned that you pull data D  (whole data?)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I hope I am clear this time.", "On Fri, Sep 8, 2017 at 6:05 AM, Abhishek Tiwari <abti@apache.org wrote:   I am not super clear with your use-case:   - Do you intend to pull in metadata only in source about the number of  records?", "It seems like it but you have mentioned that you pull data D  (whole data?)", "in source.", "If so what is the work left for workunits?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["That wont be possible for something like Yarn mode.", "Regards,  Abhishek   On Wed, Sep 6, 2017 at 5:20 AM, Vicky Kak <vicky.kak@gmail.com wrote:   Hey Guys,   I have checked in sample code demonstrating the pattern as explained  above.", "https://github.com/dallaybatta/gobblin-examples   I am soon going to put the documentation about the same, please note that  it is just a quick hack to demonstrate the pattern as explained in the  email chain.", "Regards,  Vicky     On Tue, Sep 5, 2017 at 6:48 PM, Vicky Kak <vicky.kak@gmail.com wrote:   I am not able to see this email yet in the email archive here  https://lists.apache.org/list.html?user@gobblin.incubator.apache.org   Can anyone take a note of it and get it working?", "Thanks,  Vicky     On Wed, Aug 30, 2017 at 4:08 PM, Vicky Kak <vicky.kak@gmail.com wrote:   Hi Guys,   We have got a use case where there is no meta data information about  the data to be processed in Gobblin."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://github.com/dallaybatta/gobblin-examples   I am soon going to put the documentation about the same, please note that  it is just a quick hack to demonstrate the pattern as explained in the  email chain.", "Regards,  Vicky     On Tue, Sep 5, 2017 at 6:48 PM, Vicky Kak <vicky.kak@gmail.com wrote:   I am not able to see this email yet in the email archive here  https://lists.apache.org/list.html?user@gobblin.incubator.apache.org   Can anyone take a note of it and get it working?", "Thanks,  Vicky     On Wed, Aug 30, 2017 at 4:08 PM, Vicky Kak <vicky.kak@gmail.com wrote:   Hi Guys,   We have got a use case where there is no meta data information about  the data to be processed in Gobblin.", "We need to read the whole data chunk  and then create a partition, I would be interested to know how this is  being addressed by others.", "Let me explain it with the sample generic data,  assume that we have got data D with N records in it."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Vicky     On Wed, Aug 30, 2017 at 4:08 PM, Vicky Kak <vicky.kak@gmail.com wrote:   Hi Guys,   We have got a use case where there is no meta data information about  the data to be processed in Gobblin.", "We need to read the whole data chunk  and then create a partition, I would be interested to know how this is  being addressed by others.", "Let me explain it with the sample generic data,  assume that we have got data D with N records in it.", "We do the following  1) In the Source implementation we pull all the data D using rest API.", "We have got the N records in the Source implementation and we are creating  n(workunit number)*M( records to be processed by each workunit) = N.  2) We are passing the starting id to the workunit via the SourceState."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["3) Each WorkUnit makes an redundant REST call to fetch the sub set of  D, starting from the id that is passed from Source to it.", "So there are 1 REST call in Source and n REST calls to get the data,  total of n+1 calls are being made although the data can be fetched by a  single call in the Source.", "What I am thinking is to have the data D in the memory ( it should be  distributed memory for YARN case) and pass the reference of it to the  WorkUnits for processing, however would like to know how this is being  addressed by others.", "This can be one of the patterns of data to be  processed by the Gobblin.", "May be we can have a document explaining various data patterns, how to  partition them and use in the Gobblin."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Tim,  I tried subcolumn but it does not seems to be supported...Here is my code :  http://pastebin.com/YQYetczs  Query \"data.tag.exact:Pune\";      DO NOT RETURN RESULTS Query \"data.tag:Pune\";               RETURN RESULTS  Also i checked with schema command on blur shell, it do not shows subcolumn exact...", "Please have a look at my sample code and suggest changes to make it work..", "Thanks Naresh   On Thu, Jan 2, 2014 at 5:04 PM, Tim Williams <williamstw@gmail.com wrote:   On Tue, Dec 31, 2013 at 12:24 AM, Naresh Yadav <nyadav.ait@gmail.com  wrote:   Hi tim,     list of tags is not small, can be really big so i cannot use negate the   tags approach...Other approach you said is using subfields how to do that   in blur...My thought on this was to introduce new column Tags which will   store sorted all tags of that row...So for cases where i need exact match   then will query on *Tags* column and case where i need partial match of   tags then will use Tag column..", "Your approach is largely the same as the subcolumn approach.", "Subcolumns would just allow you to do it without storing the original  value multiple times."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please have a look at my sample code and suggest changes to make it work..", "Thanks Naresh   On Thu, Jan 2, 2014 at 5:04 PM, Tim Williams <williamstw@gmail.com wrote:   On Tue, Dec 31, 2013 at 12:24 AM, Naresh Yadav <nyadav.ait@gmail.com  wrote:   Hi tim,     list of tags is not small, can be really big so i cannot use negate the   tags approach...Other approach you said is using subfields how to do that   in blur...My thought on this was to introduce new column Tags which will   store sorted all tags of that row...So for cases where i need exact match   then will query on *Tags* column and case where i need partial match of   tags then will use Tag column..", "Your approach is largely the same as the subcolumn approach.", "Subcolumns would just allow you to do it without storing the original  value multiple times.", "I actually haven't used them, but I reckon it'd  look something like:   ColumnDefinition tags = new ColumnDefinition(\"fam\",\"tag\",null, true,  \"text\",null);  ColumnDefinition tagsExact = new  ColumnDefinition(\"fam\",\"tag\",\"exact\",\"false,\"string\",null);   and querying:  A) fam.tag:Tag1   B) fam.tag.exact:Tag1   Thanks,  --tim   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Ultimately, I'm interested in the throughput of each storage nodes of my HDFS.", "Also, when I run faban, I get exception reading /my.cnf, which is true since postgresql doesn't have my.cnf.", "Just ignore it.", "We would have to edit the Olio benchmark class and  rebuild the driver to get rid of it.", "Future versions will allow you to  use different services transparently, avoiding the message altogether."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My current set up is like this: 1 machine for master, 1 machine for postgresql, 1 machine for glassfish.", "The host in this tab refers to the driver systems.", "You can use ip  addresses or host names, separated by space.", "I set up the host to be the IP of glassfish.", "When I run the benchmark, I get failed:    HomePage\t0\t0\t0%\t26.15%\tFAILED  Login\t0\t0\t0%\t10.22%\tFAILED  TagSearch\t0\t0\t0%\t33.45%\tFAILED  EventDetail\t0\t0\t0%\t24.68%\tFAILED  PersonDetail\t0\t0\t0%\t2.61%\tFAILED  AddPerson\t0\t0\t0%\t.84%\tPASSED  AddEvent\t0\t0\t0%\t2.04%\tFAILED  Simply, none of the operations went through."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["You can use ip  addresses or host names, separated by space.", "I set up the host to be the IP of glassfish.", "When I run the benchmark, I get failed:    HomePage\t0\t0\t0%\t26.15%\tFAILED  Login\t0\t0\t0%\t10.22%\tFAILED  TagSearch\t0\t0\t0%\t33.45%\tFAILED  EventDetail\t0\t0\t0%\t24.68%\tFAILED  PersonDetail\t0\t0\t0%\t2.61%\tFAILED  AddPerson\t0\t0\t0%\t.84%\tPASSED  AddEvent\t0\t0\t0%\t2.04%\tFAILED  Simply, none of the operations went through.", "I'm pretty sure that olio is working for me because I can manually go to the olio web app from my browser and browse/create user, events, etc.", "Well, we need to know what's not working, then."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I'm pretty sure that olio is working for me because I can manually go to the olio web app from my browser and browse/create user, events, etc.", "Well, we need to know what's not working, then.", "Check your run log.", "-Akara            --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM wrote:    From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:51 PM  I've to check whether it is exposed,  but you can certainly edit the   config file and mark the host enabled=false.", "However, I  don't suggest   that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Well, we need to know what's not working, then.", "Check your run log.", "-Akara            --- On Fri, 8/14/09, Akara Sucharitakul <Akara.Sucharitakul@Sun.COM wrote:    From: Akara Sucharitakul <Akara.Sucharitakul@Sun.COM  Subject: Re: Olio + Faban  To: olio-user@incubator.apache.org  Date: Friday, August 14, 2009, 4:51 PM  I've to check whether it is exposed,  but you can certainly edit the   config file and mark the host enabled=false.", "However, I  don't suggest   that.", "Olio depends on the agent to reload the DB, etc."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, I  don't suggest   that.", "Olio depends on the agent to reload the DB, etc.", "Can you please let me know why you don't want to run the  agent on the DB   server?", "As for the data store, I don't think the distributed stores  are well   integrated.", "Why don't you get successful runs with the  local store first   and we can then add support for those."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Olio depends on the agent to reload the DB, etc.", "Can you please let me know why you don't want to run the  agent on the DB   server?", "As for the data store, I don't think the distributed stores  are well   integrated.", "Why don't you get successful runs with the  local store first   and we can then add support for those.", "-Akara   Harold Lim wrote:  Thanks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Trying to update AssemblyInfo.cs file is problematic with respect to    source control.", "The plugin should not update this file, rather it    should update one in the target directory.", "If there really is some    reason to update files that are likely under source control, then    the compile plugin needs to interact with the defined SCM.", "Can you make sure separate issues are created for these in JIRA?", "I wasn't able to find an existing report of them."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [" ----- \"Christoph Karner\" <lechris48@yahoo.de wrote:    --   Joe Ocaba   ----- \"Christoph Karner\" de wrote:      hello!", "im experimenting with npanday 1.3-incubating and ran into a few    problems:     * the visual studio add-in starts succesfully, yet there are no    npanday entries    in the solution/project context menu    (add-in installed with installer)     * when i try to create a project from command line, maven doesnt  seem    to find    the archetype        as it fails complaining that there is no pom.xml in the  directory    (the    repository is in my .m2 though)       my configuration    maven 3    visual studio 2010 (german)     thanks for taking the time in testing out 1.3 Chris.", "Unfortunately  at the  moment NPanday only supports Visual Studios English Version, there  are already  issues created for German support as well as Italian and Spanish.", "We  do not have    enough contributors that know the language.", "We would gladly accept  patches for  this so that NPanday can work on other languages as well."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["im experimenting with npanday 1.3-incubating and ran into a few    problems:     * the visual studio add-in starts succesfully, yet there are no    npanday entries    in the solution/project context menu    (add-in installed with installer)     * when i try to create a project from command line, maven doesnt  seem    to find    the archetype        as it fails complaining that there is no pom.xml in the  directory    (the    repository is in my .m2 though)       my configuration    maven 3    visual studio 2010 (german)     thanks for taking the time in testing out 1.3 Chris.", "Unfortunately  at the  moment NPanday only supports Visual Studios English Version, there  are already  issues created for German support as well as Italian and Spanish.", "We  do not have    enough contributors that know the language.", "We would gladly accept  patches for  this so that NPanday can work on other languages as well.", "thanks for your help!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["what would i have to do to add german language support myself?", "You would need to build the source code and in \\dotnet\\assemblies\\NPanday.VisualStudio.Addin\\Connect.cs you need to modify launchNPandayBuildSystem function there is a loop that checks for the specific Control Caption in the running VS  i.e.", "if (control.Caption.Equals(\"C&onfiguration Manager...\"))  you can include the german equivalent for this.", "for the creating of the project what command did you use?", "i used        mvn archetype:generate  -DarchetypeArtifactId=maven-archetype-dotnet-simple  -DarchetypeGroupId=npanday  -DarchetypeVersion=1.3-incubating      as stated in \"Creating a simple project\"."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["if (control.Caption.Equals(\"C&onfiguration Manager...\"))  you can include the german equivalent for this.", "for the creating of the project what command did you use?", "i used        mvn archetype:generate  -DarchetypeArtifactId=maven-archetype-dotnet-simple  -DarchetypeGroupId=npanday  -DarchetypeVersion=1.3-incubating      as stated in \"Creating a simple project\".", "i dont see anything wrong  with it.", "also tested it on my linux machine by now and it worked like a  charm...  the machine where it doesnt work runs windows xp by the way."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["for the creating of the project what command did you use?", "i used        mvn archetype:generate  -DarchetypeArtifactId=maven-archetype-dotnet-simple  -DarchetypeGroupId=npanday  -DarchetypeVersion=1.3-incubating      as stated in \"Creating a simple project\".", "i dont see anything wrong  with it.", "also tested it on my linux machine by now and it worked like a  charm...  the machine where it doesnt work runs windows xp by the way.", "also i never used maven on windows before, so it could be that im  overlooking  something."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What's wrong?", "This is the libraries I set for my project:  adf-faces-api-11-m7-SNAPSHOT.jar adf-faces-impl-11-m7-SNAPSHOT.jar myfaces-api-1.1.4-SNAPSHOT.jar myfaces-impl-1.1.4-SNAPSHOT.jar tomahawk-1.1.4-SNAPSHOT.jar jsf-facelets-1.1.7.jar el-api.jar el-ri.jar commons-beanutils-1.7.0.jar commons-codec-1.3.jar commons-collections-3.1.jar commons-digester-1.6.jar commons-el-1.0.jar commons-fileupload-1.0.jar commons-lang-2.1.jar commons-logging-1.0.4.jar commons-validator-1.1.4.jar jakarta-oro-2.0.7.jar jstl-1.1.0.jar  In my facelets page, I declared the namespaces this way:  <html     xmlns=\"http://www.w3.org/1999/xhtml\"     xmlns:ui=\"http://java.sun.com/jsf/facelets\"     xmlns:f=\"http://java.sun.com/jsf\"     xmlns:h=\"http://java.sun.com/jsf/html\"     xmlns:t=\"http://myfaces.apache.org/tomahawk\"     xmlns:af=\"http://myfaces.apache.org/adf/faces\"     xmlns:afh=\"http://myfaces.apache.org/adf/faces/html\"  What's wrong?", "2006/5/31, Cosma Colanicchia <cosmacol@gmail.com:  2006/5/31, Frank Felix Debatin <ffd@gmx.net:    I'm was starting with the latest release, 1.1.7, but I'll    follow your hint and try 1.0.14 first.", "I was actually hoping you start with 1.1.x to see if you   find the same problems ;-)   Ok, I promise that, if I can work out without problems 1.0.14, I'll  try switch to 1.1.7 :-)     So,  the only required    libraries are those for tomahawk and for the sandbox,   right?", "The facelets page mentions a tomahawk taglib contribution."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hope that helps,    Joe   ----- \"Cihan\" <ctozan@yahoo.com wrote:    Hi   I'm trying to build a .net project with npanday.its giving this error.", "Error resolving version for plugin 'npanday.plugin:maven-aspx-plugin' from the repositories [local (C:\\Documents and Settings\\U053797.KFS\\.m2\\repository), central (http://repo1.maven.org/maven2)]: Plugin not found in any plugin repository - [Help 1]   in my repository, maven-aspx-plugin jar and pom files are present and versions are correct.", "what is the reason for this error?", "Thanks    my pom file:<?", "<  <  <  <  <  <  </  <  <  <  <  <  <  <  <  <  <  <  <  <  </  </  </  </  </xmlversion=\"1.0\"encoding=\"utf-8\"?projectxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"xmlns=\"http://maven.apache.org/POM/4.0.0\"parentartifactIdSolution1-parent</artifactIdgroupIdKFS.Solution1</groupIdversion1.0-SNAPSHOT</versionrelativePath../pom.xml</relativePathparentmodelVersion4.0.0</modelVersionartifactIdWebSite</artifactIdpackagingasp</packagingnameKFS.Solution1 : WebSite</namebuildsourceDirectory.</sourceDirectorypluginsplugingroupIdorg.apache.npanday.plugins</groupId artifactIdmaven-aspx-plugin</artifactIdextensionstrue</extensionsconfigurationframeworkVersion3.5</frameworkVersionconfigurationpluginpluginsbuildproject     full error text:------------------------------------------------------------------  Executing Maven  Pom File: D:\\My Documents\\Visual Studio 2008\\Projects\\Solution1\\pom.xml  Goal: compile  Arguments: compile  NPanday Command: C:\\Documents and Settings\\U053797.KFS\\Desktop\\apache-maven-3.0.3-bin\\apache-maven-3.0.3\\bin\\mvn.bat compile   ------------------------------------------------------------------   [INFO] Scanning for projects...  [WARNING]  [WARNING] Some problems were encountered while building the effective model for KFS.Solution1:WebSite:asp:1.0-SNAPSHOT  [WARNING] 'build.plugins.plugin.version' for org.apache.npanday.plugins:maven-aspx-plugin is missing."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks    my pom file:<?", "<  <  <  <  <  <  </  <  <  <  <  <  <  <  <  <  <  <  <  <  </  </  </  </  </xmlversion=\"1.0\"encoding=\"utf-8\"?projectxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"xmlns=\"http://maven.apache.org/POM/4.0.0\"parentartifactIdSolution1-parent</artifactIdgroupIdKFS.Solution1</groupIdversion1.0-SNAPSHOT</versionrelativePath../pom.xml</relativePathparentmodelVersion4.0.0</modelVersionartifactIdWebSite</artifactIdpackagingasp</packagingnameKFS.Solution1 : WebSite</namebuildsourceDirectory.</sourceDirectorypluginsplugingroupIdorg.apache.npanday.plugins</groupId artifactIdmaven-aspx-plugin</artifactIdextensionstrue</extensionsconfigurationframeworkVersion3.5</frameworkVersionconfigurationpluginpluginsbuildproject     full error text:------------------------------------------------------------------  Executing Maven  Pom File: D:\\My Documents\\Visual Studio 2008\\Projects\\Solution1\\pom.xml  Goal: compile  Arguments: compile  NPanday Command: C:\\Documents and Settings\\U053797.KFS\\Desktop\\apache-maven-3.0.3-bin\\apache-maven-3.0.3\\bin\\mvn.bat compile   ------------------------------------------------------------------   [INFO] Scanning for projects...  [WARNING]  [WARNING] Some problems were encountered while building the effective model for KFS.Solution1:WebSite:asp:1.0-SNAPSHOT  [WARNING] 'build.plugins.plugin.version' for org.apache.npanday.plugins:maven-aspx-plugin is missing.", "@ line 16, column 15  [WARNING]  [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.", "[WARNING]  [WARNING] For this reason, future Maven versions might no longer support building such malformed projects.", "[WARNING]  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Build Order:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent  [INFO] KFS.Solution1 : WebSite  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : Solution1-parent 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO]  [INFO] ------------------------------------------------------------------------  [INFO] Building KFS.Solution1 : WebSite 1.0-SNAPSHOT  [INFO] ------------------------------------------------------------------------  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]  [INFO] KFS.Solution1 : Solution1-parent .................."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["is there any workaround method for this?", "On Thu, Feb 7, 2013 at 11:11 AM, Aaron McCurry <amccurry@gmail.com wrote:  Unfortunately yes the minimum for all the features to work is 0.20.2 (with  appends).", "I have never tried to run it on anything less than 0.20.2.", "Sorry.", "Aaron    On Wed, Feb 6, 2013 at 9:38 PM, Li Li <fancyerii@gmail.com wrote:   I am using hadoop 0.18. does blur need higher version hadoop?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["so   I use another machine to clone by:     git clone https://git-wip-us.apache.org/repos/asf/incubator-blur.git   git checkout 0.2-dev     then I copy it to my build machine   all is fine before start-up   there are still errors in log   RROR 20130206_11:47:00:000_CST [main]   concurrent.SimpleUncaughtExceptionHandler: Unknown error in thread   [Thread[main,5,main]]   java.lang.RuntimeException: Safemode data missing   [/blur/clusters/default/safemode]           at    org.apache.blur.manager.indexserver.DistributedIndexServer.waitInSafeModeIfNeeded(DistributedIndexServer.java:177)           at    org.apache.blur.manager.indexserver.DistributedIndexServer.init(DistributedIndexServer.java:135)           at    org.apache.blur.thrift.ThriftBlurServer.createServer(ThriftBlurServer.java:187)           at   org.apache.blur.thrift.ThriftBlurServer.main(ThriftBlurServer.java:88)       This could be a bug with an empty ZooKeeper (meaning maybe blur isn't   setting things up correctly).", "I'm going to retest that blur sets up   ZooKeeper correctly for safe mode and report back.", "when I run the client   $./apache-blur-0.2.0-SNAPSHOT/bin/blur shell localhost:40020   java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)   Exception in thread \"main\" java.lang.NoSuchMethodError:   org.apache.thrift.meta_data.FieldValueMetaData.<init(BZ)V           at   org.apache.blur.thrift.generated.QueryArgs.<clinit(QueryArgs.java:255)           at java.lang.Class.forName0(Native Method)           at java.lang.Class.forName(Class.java:169)           at $Proxy0.<clinit(Unknown Source)           at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native   Method)           at    sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)           at    sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)           at  java.lang.reflect.Constructor.newInstance(Constructor.java:513)           at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:588)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:116)           at  org.apache.blur.thrift.BlurClient.getClient(BlurClient.java:99)           at org.apache.blur.shell.Main.main(Main.java:171)       This is a very strange error, it's almost like the wrong thrift library  is   in the classpath somewhere.", "What hadoop version are you using?", "Could   there be another version of thrift pulled in from another project  somehow?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What hadoop version are you using?", "Could   there be another version of thrift pulled in from another project  somehow?", "Maybe hbase?", "I know that the latest version of hbase is still using  0.8.0   of thrift.", "On Tue, Feb 5, 2013 at 11:53 PM, Aaron McCurry <amccurry@gmail.com  wrote:    This is an interesting error, because it's looks like there is a  mixture   of    old code the trunk (0.1.x) with an error because of Lucene 4.0."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could   there be another version of thrift pulled in from another project  somehow?", "Maybe hbase?", "I know that the latest version of hbase is still using  0.8.0   of thrift.", "On Tue, Feb 5, 2013 at 11:53 PM, Aaron McCurry <amccurry@gmail.com  wrote:    This is an interesting error, because it's looks like there is a  mixture   of    old code the trunk (0.1.x) with an error because of Lucene 4.0.", "Try   using    and building with the 0.2-dev, should become the trunk soon."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If not, should we have a   jira with pig to track this?", "Thanks,   Aniket     --  *Note that I'm no longer using my Yahoo!", "email address.", "Please email me at billgraham@gmail.com going forward.", "*  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Daniel,  Hey, I did see it, but I'm currently plowed under at work.", "This might be:  http://issues.apache.org/jira/browse/ADFFACES-24 ... which desparately needs some attention.", "-- Adam   On 1/3/07, Daniel Hannum <dhannum@quovadx.com wrote:  Hi.", "I'm reposting this from last week because I'm afraid no one saw it  during the vacation week.", "This is still a very serious issue for me and  I'd welcome any feedback you can provide."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["As expected, I can click  the menu link and go to the other page, and the immediate attribute  bypasses validations.", "However, if I add that page a control that has autoSubmit=\"true\", even  if the autoSubmit has nothing to do with the required field, now when I  click the menu link, it will still stop me with validation errors.", "If I  click the menu link again, it will go to the page as expected.", "I saw  reports on the Oracle forums about ADF issues in this vein [1, 2].", "I  don't know if they have been fixed in Trinidad."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I saw  reports on the Oracle forums about ADF issues in this vein [1, 2].", "I  don't know if they have been fixed in Trinidad.", "Seems like this has to  be a bug, though.", "To recap:     1.", "Make a page with a required text field, an autoSubmit checkbox,  and an immediate link to another page."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I  don't know if they have been fixed in Trinidad.", "Seems like this has to  be a bug, though.", "To recap:     1.", "Make a page with a required text field, an autoSubmit checkbox,  and an immediate link to another page.", "2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Leave the field blank and click the link.", "You go to the page fine.", "Validations are skipped by immediate=\"true\"   3.", "Go back.", "Still leave the field blank, but this time click the  autoSubmit checkbox."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Go back.", "Still leave the field blank, but this time click the  autoSubmit checkbox.", "Now click the link.", "You'll get validation errors  (despite the immediate=\"true\"...", "bug?)"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,   Are you following the instructions at https://gobblin.readthedocs.io/en/latest/user-guide/Gobblin-Schedulers/   Which scheduler are you using to launch the shell script?", "On Wed, Jul 24, 2019 at 12:13 AM Amith Prasanna <amith.prasanna@sentienz.com wrote:   Hi all,   I am working on scheduling a job which pulls records from kafka to hdfs.", "This works fine while running in standalone mode.", "But on trying in  map-reduce mode using gobblin-mapreduce.sh script I'm getting class and  method not found errors.", "I'm using hadoop-2.8.1 and gobblin-0.13."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I want to set these when I spawn a job in client mode.", "Also when I try to set \"spark.master\" through REST through \"conf\" param, it is not being set.", "This is the JSON I'm sending to the Livy API.", "{                 \"file\": \"/user/livy/spark-examples.jar\",                 \"conf\" : {                                 \"spark.master\": \"yarn\",                                 \"spark.submit.deployMode\": \"cluster\"                 },                 \"args\": [\"2\"],                 \"className\": \"org.apache.spark.examples.SparkPi\" }  Thanks and Regards, Sarthak  Privileged/Confidential Information may be contained in this message and is intended only for the use of the addressee.", "If you are not the addressee, or person responsible for delivering it to the addressee, you should not copy or deliver this to anyone else."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you are not the addressee, or person responsible for delivering it to the addressee, you should not copy or deliver this to anyone else.", "If you receive this message by mistake, please delete the message from any/all computer/s and notify the sender immediately by reply Email.", "We appreciate your assistance in preserving the confidentiality of our correspondence.", "Any information in this message that does not relate to the official business of the organization shall be understood as neither given nor endorsed by it.", "Please advise immediately if you or your employer does not consent to Internet Email for messages of this kind."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["1 The JVM  heap usage is erratic.", "Near the beginning of the job start we see frequent GC on the driver jvm.", "The memory usage is also really high.", "We have around 56000 files and the size is 1TB  2 The initial time spent by the job on the driver box is nearly 6times of the map job duration.", "This mean we are spending almost 6times rounding up the files  compare to actual copy."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The memory usage is also really high.", "We have around 56000 files and the size is 1TB  2 The initial time spent by the job on the driver box is nearly 6times of the map job duration.", "This mean we are spending almost 6times rounding up the files  compare to actual copy.", "We would like to know if there is a way we could avoid these two pitfalls while using distcp-NG.", "We are ok if the data is inconsistent at the dataset level as long as individual files are copied correctly."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We have around 56000 files and the size is 1TB  2 The initial time spent by the job on the driver box is nearly 6times of the map job duration.", "This mean we are spending almost 6times rounding up the files  compare to actual copy.", "We would like to know if there is a way we could avoid these two pitfalls while using distcp-NG.", "We are ok if the data is inconsistent at the dataset level as long as individual files are copied correctly.", "Another thing, we see there are a few parameters which are used in the code but I could not find their reference in the docs."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We are ok if the data is inconsistent at the dataset level as long as individual files are copied correctly.", "Another thing, we see there are a few parameters which are used in the code but I could not find their reference in the docs.", "E.g.", "gobblin.prioritization.maxCopy.copyEntities Gobblin.copy.max.concurrent.listing.services Gobblin.copy.binpacking.Maxworkunitperbin Gobblin.copy.binpacking.MaxsizePerBin Gobblin.copy.abortonsinglefailure   Is there a better way to understand these other than looking into the code ?", "--  Cheerio!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Another thing, we see there are a few parameters which are used in the code but I could not find their reference in the docs.", "E.g.", "gobblin.prioritization.maxCopy.copyEntities Gobblin.copy.max.concurrent.listing.services Gobblin.copy.binpacking.Maxworkunitperbin Gobblin.copy.binpacking.MaxsizePerBin Gobblin.copy.abortonsinglefailure   Is there a better way to understand these other than looking into the code ?", "--  Cheerio!", "*Rohit*  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["18:36:43,375 INFO  [Http11BaseProtocol] Initializing Coyote HTTP/1.1 on  http-0.0.0.0-8080  18:36:43,375 INFO  [Catalina] Initialization processed in 391 ms  18:36:43,375 INFO  [StandardService] Starting service jboss.web  18:36:43,375 INFO  [StandardEngine] Starting Servlet Engine: Apache  Tomcat/5.5.20  18:36:43,421 INFO  [StandardHost] XML validation disabled  18:36:43,453 INFO  [Catalina] Server startup in 78 ms  18:36:43,593 INFO  [TomcatDeployer] deploy, ctxPath=/portal-cms,  warUrl=.../tmp/deploy/tmp51467portal-cms-exp.war/  18:36:44,000 INFO  [WebappLoader] Dual registration of jndi stream handler:  factory already defined  18:36:44,156 ERROR [[/portal-cms]] Error configuring application listener of  class org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl  java.lang.ClassNotFoundException:  org.apache.myfaces.trinidadinternal.webapp.TrinidadListenerImpl          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1355)          at  org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.jav  a:1201)          at  org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:  3711)          at  org.apache.catalina.core.StandardContext.start(StandardContext.java:4211)          at  org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:7  59)          at  org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:739)          at  org.apache.catalina.core.StandardHost.addChild(StandardHost.java:524)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.apache.catalina.core.StandardContext.init(StandardContext.java:5052)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.apache.commons.modeler.BaseModelMBean.invoke(BaseModelMBean.java:503)          at  org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeployInternal(TomcatDeployer  .java:297)          at  org.jboss.web.tomcat.tc5.TomcatDeployer.performDeploy(TomcatDeployer.java:10  3)          at  org.jboss.web.AbstractWebDeployer.start(AbstractWebDeployer.java:371)          at org.jboss.web.WebModule.startModule(WebModule.java:83)          at org.jboss.web.WebModule.startService(WebModule.java:61)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy98.start(Unknown Source)          at  org.jboss.web.AbstractWebContainer.start(AbstractWebContainer.java:466)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.system.InterceptorServiceMBeanSupport.invokeNext(InterceptorServic  eMBeanSupport.java:238)          at  org.jboss.ws.integration.jboss.DeployerInterceptor.start(DeployerInterceptor  .java:92)          at  org.jboss.deployment.SubDeployerInterceptorSupport$XMBeanInterceptor.start(S  ubDeployerInterceptorSupport.java:188)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:95)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy99.start(Unknown Source)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at  org.jboss.deployment.MainDeployer.addDeployer(MainDeployer.java:368)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy89.addDeployer(Unknown Source)          at org.jboss.web.tomcat.tc5.Tomcat5.startService(Tomcat5.java:506)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at  org.jboss.mx.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:9  7)          at  org.jboss.deployment.SubDeployerInterceptor.invokeNext(SubDeployerIntercepto  r.java:124)          at  org.jboss.deployment.SubDeployerInterceptor.invoke(SubDeployerInterceptor.ja  va:109)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at  org.jboss.system.ServiceController.start(ServiceController.java:435)          at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy4.start(Unknown Source)          at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)          at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy6.deploy(Unknown Source)          at  org.jboss.deployment.scanner.URLDeploymentScanner.deploy(URLDeploymentScanne  r.java:421)          at  org.jboss.deployment.scanner.URLDeploymentScanner.scan(URLDeploymentScanner.", "java:634)          at  org.jboss.deployment.scanner.AbstractDeploymentScanner$ScannerThread.doScan(  AbstractDeploymentScanner.java:263)          at  org.jboss.deployment.scanner.AbstractDeploymentScanner.startService(Abstract  DeploymentScanner.java:336)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalStart(ServiceMBeanSupport.", "java:289)          at  org.jboss.system.ServiceMBeanSupport.jbossInternalLifecycle(ServiceMBeanSupp  ort.java:245)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at  org.jboss.system.ServiceController$ServiceProxy.invoke(ServiceController.jav  a:978)          at $Proxy0.start(Unknown Source)          at  org.jboss.system.ServiceController.start(ServiceController.java:417)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:86)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy4.start(Unknown Source)          at org.jboss.deployment.SARDeployer.start(SARDeployer.java:302)          at org.jboss.deployment.MainDeployer.start(MainDeployer.java:1025)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:819)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:782)          at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:766)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39  )          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl  .java:25)          at java.lang.reflect.Method.invoke(Method.java:585)          at  org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java  :155)          at org.jboss.mx.server.Invocation.dispatch(Invocation.java:94)          at  org.jboss.mx.interceptor.AbstractInterceptor.invoke(AbstractInterceptor.java  :133)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.interceptor.ModelMBeanOperationInterceptor.invoke(ModelMBeanOpe  rationInterceptor.java:142)          at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)          at  org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:26  4)          at  org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)          at org.jboss.mx.util.MBeanProxyExt.invoke(MBeanProxyExt.java:210)          at $Proxy5.deploy(Unknown Source)          at org.jboss.system.server.ServerImpl.doStart(ServerImpl.java:482)          at org.jboss.system.server.ServerImpl.start(ServerImpl.java:362)          at org.jboss.Main.boot(Main.java:200)          at org.jboss.Main$1.run(Main.java:490)          at java.lang.Thread.run(Thread.java:595)  [/code]   should I copy Trinidad into default/lib path of jboss???"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Don't know if this helps anyone.", "When I was trying different deployment options on the drawing board, one particular config stood out when co-locating DNs & Shard-servers...  1.", "Run Data-Nodes & Shard-servers co-located\u2026 2.", "Make sure all such machines are placed in one rack [Say RackA]\u2026 3.", "Start Data-Nodes alone {without Shard-servers} in a separate RackB\u2026  With such a config in place, hadoop places the 1st write-copy locally in RackA."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps    We are using latest version of 1.7.", "Actually we imported around 2TB of  data as dry-run with just 16GB heap without major GC issues using good old  CMS.", "There is no sorting/faceting during searches also.", "My reluctance stems  from the fact that I am not quite familiar with G1 :)   I am favouring more for write-thru cache [at-least around 50Gb] rather  than read-cache because we have a lot of free RAM available & I feel read  cache is going to use very less RAM.", "But I am not sure about this."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["My reluctance stems  from the fact that I am not quite familiar with G1 :)   I am favouring more for write-thru cache [at-least around 50Gb] rather  than read-cache because we have a lot of free RAM available & I feel read  cache is going to use very less RAM.", "But I am not sure about this.", "Any  pointers will greatly help.", "I would also recommend that you increase the  block cache buffer and file buffer sizes from 8K to 64K    This is one issue we faced during dry-run.", "Marking-up from 8K to 16K  solved the issue."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Any  pointers will greatly help.", "I would also recommend that you increase the  block cache buffer and file buffer sizes from 8K to 64K    This is one issue we faced during dry-run.", "Marking-up from 8K to 16K  solved the issue.", "I thought 32K must be a good fit for us.", "Will surely  explore this\u2026   Thanks again for helping out   On Sat, Feb 28, 2015 at 3:04 AM, Aaron McCurry <amccurry@gmail.com wrote:   On Fri, Feb 27, 2015 at 1:29 AM, Ravikumar Govindarajan <  ravikumar.govindarajan@gmail.com wrote:    Hi,     I need a general guidance on number of machines/shards required for our   first blur set-up     Some data as follows     1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Marking-up from 8K to 16K  solved the issue.", "I thought 32K must be a good fit for us.", "Will surely  explore this\u2026   Thanks again for helping out   On Sat, Feb 28, 2015 at 3:04 AM, Aaron McCurry <amccurry@gmail.com wrote:   On Fri, Feb 27, 2015 at 1:29 AM, Ravikumar Govindarajan <  ravikumar.govindarajan@gmail.com wrote:    Hi,     I need a general guidance on number of machines/shards required for our   first blur set-up     Some data as follows     1.", "Shard-Server Config : 128GB RAM, 16-core dual socket with   hyper-threading.", "32 procs   2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["32 procs   2.", "Total dataset size: 10TB.", "With rep-factor=3, total cluster-size=30TB.", "Pre-populated via       MR or Thrift...   3.", "We receive very less queries per minute [600-900 queries]."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This will greatly depend on your data, how  many fields, how many terms, etc.", "Can each shard-server machine with above specs handle 25 shards?", "Is  such a   configuration over-utilized/under-utilized?", "Again, it likely depends.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is  such a   configuration over-utilized/under-utilized?", "Again, it likely depends.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps.", "Whatever is leftover can  be  allocated to the block cache.", "I would also recommend that you increase  the  block cache buffer and file buffer sizes from 8K to 64K."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Again, it likely depends.", "I would recommend using the latest Java7  (perhaps Java8 but I haven't tested with it), and use the G1 garbage  collector if you plan on running larger heaps.", "Whatever is leftover can  be  allocated to the block cache.", "I would also recommend that you increase  the  block cache buffer and file buffer sizes from 8K to 64K.", "This will also  decrease heap pressure for the number of entries in the block cache lru  map."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Whatever is leftover can  be  allocated to the block cache.", "I would also recommend that you increase  the  block cache buffer and file buffer sizes from 8K to 64K.", "This will also  decrease heap pressure for the number of entries in the block cache lru  map.", "How do folks run in production.", "Any numbers/pointers will be really  helpful     Generally large shard servers (like the ones you are suggesting) with a  few  controllers (6-12) are typical setups."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["(the latter can be mapped to a short-cut making work faster).", "The big advantage of this approach you may edit and save the embedded XML without unzip and zip all the time!", "Regards, Svante  Am 15.04.2014 14:08, schrieb Bruno Girin:  Hi Svante,   I'm happy to help with this.", "However, I'm not sure how that relates to my  problem: I am just trying to read a spreadsheet's content by iterating over  tables, rows and cells in that spreadsheet not knowing how many of those  items I have when I open the file.", "From Nick's comment, it looks like my code is not iterating over the  spreadsheet properly so I would welcome any suggestion as to how I should  do that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The big advantage of this approach you may edit and save the embedded XML without unzip and zip all the time!", "Regards, Svante  Am 15.04.2014 14:08, schrieb Bruno Girin:  Hi Svante,   I'm happy to help with this.", "However, I'm not sure how that relates to my  problem: I am just trying to read a spreadsheet's content by iterating over  tables, rows and cells in that spreadsheet not knowing how many of those  items I have when I open the file.", "From Nick's comment, it looks like my code is not iterating over the  spreadsheet properly so I would welcome any suggestion as to how I should  do that.", "In short, what I'm trying to do is this:   open spreadsheet  for table : spreadsheet {    for row : table {      for cell : row {        store the value in a completely separate structure      }    }  }    Cheers,   Bruno     On 15 April 2014 12:45, Svante Schubert <svante.schubert@gmail.com wrote:   I am still working on a patch for fixing the performance problem for  spreadsheets."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But again this will come a little later this year with a  major contribution and major refactoring is required on my side before  submittable.", "Basically the idea for fix is to separate any functionality for altering  cells into two basic functions to avoid code redundancy:   One function will be selecting the range of cells to be altered (might  be a single cell, row, column, full table or sub-rectangle).", "The  altering might be any arbitrary change to be applied on the range's  column/row/cells (e.g.", "\"draw borders around the selected range\" or for  instance \"alter the styles from those cells containing content/format\").", "This change will be covered by second function (or class with a certain  method/interface a only JDK 8 support Lambda functions, calling a  function with a function as parameter)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Basically the idea for fix is to separate any functionality for altering  cells into two basic functions to avoid code redundancy:   One function will be selecting the range of cells to be altered (might  be a single cell, row, column, full table or sub-rectangle).", "The  altering might be any arbitrary change to be applied on the range's  column/row/cells (e.g.", "\"draw borders around the selected range\" or for  instance \"alter the styles from those cells containing content/format\").", "This change will be covered by second function (or class with a certain  method/interface a only JDK 8 support Lambda functions, calling a  function with a function as parameter).", "By this split, I could reuse the selection part, which is quite  difficult with all the repeated/coverage of columns/rows and cells."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["\"draw borders around the selected range\" or for  instance \"alter the styles from those cells containing content/format\").", "This change will be covered by second function (or class with a certain  method/interface a only JDK 8 support Lambda functions, calling a  function with a function as parameter).", "By this split, I could reuse the selection part, which is quite  difficult with all the repeated/coverage of columns/rows and cells.", "Note: The second function will be called for every column/row/cells  within the given range.", "Seems to have quite a good performance in my  current tests..   Best regard,  Svante   Am 15.04.2014 13:22, schrieb Bruno Girin:  Hi Nick,    On 15 April 2014 11:49, Nicholas Evans <nick.evans@inology.nl wrote:   Dear Bruno,   I have tried out your test code and cannot reproduce the exception that  you  get."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is  the  behaviour I was seeing when I wrote the first implementation using  v0.5-incubating from the Maven repositories; moving to v0.6-incubating  using the .jar directly seemed to fix the hanging problem but triggered  the  exception.", "Obviously what might have happened is that by doing that I  introduced an interfering dependency.", "Is there any plan to make ODF Toolkit available in the Maven repositories  again so that client projects can just reference the Maven repo?", "I can load the spreadsheet in without a problem, and can also query the  spreadsheet as expected.", "I couldn't get your test to pass because it seems to take a long time to  run."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Obviously what might have happened is that by doing that I  introduced an interfering dependency.", "Is there any plan to make ODF Toolkit available in the Maven repositories  again so that client projects can just reference the Maven repo?", "I can load the spreadsheet in without a problem, and can also query the  spreadsheet as expected.", "I couldn't get your test to pass because it seems to take a long time to  run.", "Methods like getRowCount() can return much higher values than you  expect (on your test code it returns 1048576 for me), and  getRowByIndex()  is a very slow method for large numbers of rows."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Is there any plan to make ODF Toolkit available in the Maven repositories  again so that client projects can just reference the Maven repo?", "I can load the spreadsheet in without a problem, and can also query the  spreadsheet as expected.", "I couldn't get your test to pass because it seems to take a long time to  run.", "Methods like getRowCount() can return much higher values than you  expect (on your test code it returns 1048576 for me), and  getRowByIndex()  is a very slow method for large numbers of rows.", "Right, so what is the best way to iterate over all rows in a table and  all  cells in a row?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I couldn't get your test to pass because it seems to take a long time to  run.", "Methods like getRowCount() can return much higher values than you  expect (on your test code it returns 1048576 for me), and  getRowByIndex()  is a very slow method for large numbers of rows.", "Right, so what is the best way to iterate over all rows in a table and  all  cells in a row?", "This is a very simple spreadsheet with 1 table, 1 row  and 3  cells in the first row.", "If getRowCount() and getRowByIndex() are unreliable and slow, should they  be deprecated or at least identified as not safe for general use?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Methods like getRowCount() can return much higher values than you  expect (on your test code it returns 1048576 for me), and  getRowByIndex()  is a very slow method for large numbers of rows.", "Right, so what is the best way to iterate over all rows in a table and  all  cells in a row?", "This is a very simple spreadsheet with 1 table, 1 row  and 3  cells in the first row.", "If getRowCount() and getRowByIndex() are unreliable and slow, should they  be deprecated or at least identified as not safe for general use?", "Are you running this code in a clean project without other dependencies  which might be interfering?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Are you running this code in a clean project without other dependencies  which might be interfering?", "If not perhaps you could try this?", "This is very possible as my project also uses Apache POI so there may be  some dependencies that interfere.", "As explained above, I'm currently using  the 0.6 jar files direct but would prefer to just reference the project  in  a Maven repo to let Maven sort out dependencies.", "Cheers,   Bruno     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi Mari,  Do you have a specific example that does class weighting?", "libffm does not have such feature.", "https://github.com/ycjuan/libffm  Sckit SGD [1] adjust y as follows: \"n_samples / (n_classes * np.bincount(y))\" [1] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html  I think this can easily be achieved using SQL.", "Thanks, Makoto  2019\u5e7410\u670831\u65e5(\u6728) 19:38 Shadi Mari <shadimari@gmail.com:   Hello  I am having extremely imbalanced dataset and trying to find support for class weights in hivemall ffm classifier in specific, however couldnt find any mention in the docs.", "Is this feature supported, otherwise i had to go with negative downsampling."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This problem happens randomly (50%) when a benchmark test begins.", "And then I have to manually restart the glassfish server to solve the issue.", "Could anyone help on this?", "Thanks.", "The error log is <loggercom.sun.faban.driver.engine.TimeThread.13</logger   <levelWARNING</level   <classcom.sun.faban.driver.engine.AgentThread</class   <methodlogError</method   <thread34</thread   <messageUIDriverAgent[0].13.doAddPerson: Broken pipe</message   <exception     <messagejava.net.SocketException: Broken pipe</message     <frame       <classjava.net.SocketOutputStream</class       <methodsocketWrite0</method     </frame     <frame       <classjava.net.SocketOutputStream</class       <methodsocketWrite</method       <line92</line     </frame     <frame       <classjava.net.SocketOutputStream</class       <methodwrite</method       <line136</line     </frame     <frame       <classcom.sun.faban.driver.transport.util.TimedOutputStream</class  - Hao  "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I see some open PRs from you that are not too  active.", "It would be nice to have a release in 3-4 weeks that also targets  full compatibility with Apache so we can graduate to top level.", "Besides  summer break is getting close and after the summer 1.9.0 is scheduled.", "Cheers  Bolke    On 18 May 2017, at 20:54, Bolke de Bruin <bdbruin@gmail.com wrote:     https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow <  https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow     (See higher up in the thread)     Please make sure to address some of the outstanding Apache issues (see  also quote below):     1.", "Your name is still mentioned somewhere as author."], "labels": ["0", "0", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["It would be nice to have a release in 3-4 weeks that also targets  full compatibility with Apache so we can graduate to top level.", "Besides  summer break is getting close and after the summer 1.9.0 is scheduled.", "Cheers  Bolke    On 18 May 2017, at 20:54, Bolke de Bruin <bdbruin@gmail.com wrote:     https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow <  https://cwiki.apache.org/confluence/display/AIRFLOW/Releasing+Airflow     (See higher up in the thread)     Please make sure to address some of the outstanding Apache issues (see  also quote below):     1.", "Your name is still mentioned somewhere as author.", "A patch wasn\u2019t  cherry picked earlier for this   2."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["A patch wasn\u2019t  cherry picked earlier for this   2.", "Copyrights 2016-2017 Apache, before Airbnb / you   3.", "License file formatting     Otherwise it won\u2019t pass the IPMC.", "Quote from the IPMC:   ====     +1, however there's a few issues with the LICENSE file:     - Would be good to list out the locations of each file (or path to a  group   of files) (some have this, and others do not so its hard to follow)   - There's errant /* .. */ around each license declaration, which should  be   removed.", "- Missing license bodies for FooTable v2, jQuery Clock Plugin,     Likewise, your NOTICE has copyright 2011-2017, however Airflow hasn't  been   incubating that long."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["License file formatting     Otherwise it won\u2019t pass the IPMC.", "Quote from the IPMC:   ====     +1, however there's a few issues with the LICENSE file:     - Would be good to list out the locations of each file (or path to a  group   of files) (some have this, and others do not so its hard to follow)   - There's errant /* .. */ around each license declaration, which should  be   removed.", "- Missing license bodies for FooTable v2, jQuery Clock Plugin,     Likewise, your NOTICE has copyright 2011-2017, however Airflow hasn't  been   incubating that long.", "If you like, you can give origination notices to  the   original creators here to specify the original copyright dates.", "I would challenge the podling to see if there's a way to simplify their   LICENSE by instead using npm or some other javascript packaging tool to   build a distribution, rather than shipping the dependencies in the source   release, makes it much easier to use."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Quote from the IPMC:   ====     +1, however there's a few issues with the LICENSE file:     - Would be good to list out the locations of each file (or path to a  group   of files) (some have this, and others do not so its hard to follow)   - There's errant /* .. */ around each license declaration, which should  be   removed.", "- Missing license bodies for FooTable v2, jQuery Clock Plugin,     Likewise, your NOTICE has copyright 2011-2017, however Airflow hasn't  been   incubating that long.", "If you like, you can give origination notices to  the   original creators here to specify the original copyright dates.", "I would challenge the podling to see if there's a way to simplify their   LICENSE by instead using npm or some other javascript packaging tool to   build a distribution, rather than shipping the dependencies in the source   release, makes it much easier to use.", "As the podling matures, would be good to see information about the author   switch from an individual to a community (in setup.cfg, its already in   setup.py so may have been a miss)     It would be great to see a binary distribution in the next vote to see  how   that may work, its not clear how to build it from this."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["I'm planning to make 1.8.2 essentially same as 1.8.1 plus the set of   \"cherries\" that we use at Airbnb in production and every bugfix / minor   feature that looks benign to us.", "Given that, we're committing to try  out RC   along with everyone else.", "What cadence are we aiming at?", "What should be the target date for the  RC?", "Max     On Thu, May 18, 2017 at 11:29 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Max,     Sounds reasonable."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["What should be the target date for the  RC?", "Max     On Thu, May 18, 2017 at 11:29 AM, Bolke de Bruin <bdbruin@gmail.com  <mailto:bdbruin@gmail.com   wrote:     Hi Max,     Sounds reasonable.", "For the Release Manager it is really mostly a   management job.", "Chasing, prioritising etc.", "While it is nice to have a  rm   also being able to run the RCs themselves I don\u2019t think it is an  absolute   requirement."], "labels": ["0", "0", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["As mentioned the 1.8.X release series should focus on bug fixes,   performance issue and minor feature updates (UI fixes, fixes to some   hooks/operators).", "1.9.X is for the larger changes.", "So indeed please  keep   1.8.2 simple!", "Fully understand that business priorities can take precedence.", "I (and  I   guess Chris as well) were just hoping that also some of the other   committers would chime in."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["I (and  I   guess Chris as well) were just hoping that also some of the other   committers would chime in.", "Cheers   Bolke       On 18 May 2017, at 20:18, Maxime Beauchemin <  maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com   wrote:     Hey,     Sorry about the delay answering, I wanted to sync up with the Airflow   team   here at Airbnb before I replied here.", "Quick note to say that the folks at Airbnb are putting a plan  together   as   to how we can move towards smooth releases with higher confidence in  the   future.", "That plan involves improving the build/test process as well  as   our   staging infrastructure, possibly enabling progressive rollouts   internally.", "For context, the team that works on Airflow at Airbnb is \"Data  Platform\"   and is also on the hook for big chunks of non-Airflow-related   infrastructure work that hit us recently and accounts for more than  the   team's bandwidth at this time."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers   Bolke       On 18 May 2017, at 20:18, Maxime Beauchemin <  maximebeauchemin@gmail.com <mailto:maximebeauchemin@gmail.com   wrote:     Hey,     Sorry about the delay answering, I wanted to sync up with the Airflow   team   here at Airbnb before I replied here.", "Quick note to say that the folks at Airbnb are putting a plan  together   as   to how we can move towards smooth releases with higher confidence in  the   future.", "That plan involves improving the build/test process as well  as   our   staging infrastructure, possibly enabling progressive rollouts   internally.", "For context, the team that works on Airflow at Airbnb is \"Data  Platform\"   and is also on the hook for big chunks of non-Airflow-related   infrastructure work that hit us recently and accounts for more than  the   team's bandwidth at this time.", "Given that, the team doesn't want to   commit   the time/risk to deploy RCs in production in the short term."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Apply bug fixes   2.", "Add performance fixes   3.", "Fix some outstanding Apache requirements (Author, Licensing etc)     The process of creating a distribution has been detailed by Chris  here:   https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow   <   https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow <https://cwiki.apache.org/confluence/display/AIRFLOW/  Releasing+Airflow     Now we just need a volunteer (preferably from the committers) to be  the   Release Manager for 1.8.2 :-).", "Who is willing to take this on and make history?", "Regards,   Bolke                  "], "labels": ["1", "1", "1", "0", "0"]}
{"abstract_id": 0, "sentences": ["Yea, you're right.", "I agree.", "I did most integration testing in private environments when RCs were cut.", "Travis seems like it might be a good way to go, if we can make it work.", "I agree with Li Xuan that failures in Travis can be a pain to replicate--I just suffered through this on another open source project--but it is do-able."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Travis seems like it might be a good way to go, if we can make it work.", "I agree with Li Xuan that failures in Travis can be a pain to replicate--I just suffered through this on another open source project--but it is do-able.", "On Tue, Dec 20, 2016 at 12:13 PM, Li Xuan Ji <xuanji@gmail.com wrote:   Chris, is this the authoritative list of services apache provides to  projects?", "https://www.apache.org/dev/services.html   The builds section / page links to https://ci.apache.org/ and lists the  following build services: buildbot, gump, jenkins (the link to continuum is  broken).", "From my brief reading, it seems they use a shared pool of machines  (for all projects) are designed for running unit/integration tests that can  be torn down after running, not long-running services."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I agree with Li Xuan that failures in Travis can be a pain to replicate--I just suffered through this on another open source project--but it is do-able.", "On Tue, Dec 20, 2016 at 12:13 PM, Li Xuan Ji <xuanji@gmail.com wrote:   Chris, is this the authoritative list of services apache provides to  projects?", "https://www.apache.org/dev/services.html   The builds section / page links to https://ci.apache.org/ and lists the  following build services: buildbot, gump, jenkins (the link to continuum is  broken).", "From my brief reading, it seems they use a shared pool of machines  (for all projects) are designed for running unit/integration tests that can  be torn down after running, not long-running services.", "There's no references to Travis on those pages, there are some tickets  related to travis (  https://www.google.ca/search?sitesearch=*.apache.org&q=travis) but I think  they are referring to integration with the public travis-ci.org service  that we already use."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["From my brief reading, it seems they use a shared pool of machines  (for all projects) are designed for running unit/integration tests that can  be torn down after running, not long-running services.", "There's no references to Travis on those pages, there are some tickets  related to travis (  https://www.google.ca/search?sitesearch=*.apache.org&q=travis) but I think  they are referring to integration with the public travis-ci.org service  that we already use.", "The page also says that FreeBSD jails and Ubuntu VMs are available,  although I'm not sure what they are normally used for.", "One thing I don't really like about the \"official\" infrastructure is  access, eg we can't ssh into the travis-ci.org builds to debug errors, and  the travis build script we have is quite verbose, presumably for debugging  purposes.", "On 20 December 2016 at 13:31, Georg Walther <georg.walther@markovian.com  wrote:    Hi,       why would you want these services to be long-running?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hadoop, KDC, Presto, MySQL, etc) in the Jenkins    environment, because that would be sufficient I guess?", "To my knowledge   what    Apache offers is more suited for unit tests (ie.", "the hadoop-qa runs)  than    integration tests.", "What are your thoughts?", "Bolke        Op 19 dec. 2016, om 22:26 heeft Chris Riccomini <  criccomini@apache.org       het volgende geschreven:         Hey Bolke,         Thanks for stepping up."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Aside   from     any     technical requirements we need a place to do it.", "I am willing to   offer    a     place in Lab env I am running or to fund an environment in  AWS/GCloud    if     Apache cannot make these kind of resources available.", "If running in our Lab there is virtually no restriction what we  could    do,     however I will hand select people who have access to this   environment.", "I     will also hold ultimate power to remove access from anyone.", "I even    might     ask for a confirmation that you will behave when using our property     (don\u2019t     worry won\u2019t cover it with legal wording)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I     will also hold ultimate power to remove access from anyone.", "I even    might     ask for a confirmation that you will behave when using our property     (don\u2019t     worry won\u2019t cover it with legal wording).", "This is a IAAS service so   we     need     to cover the things we need ourselves, but the upside is we can and   it    is     free.", "We could setup a Gitlab instance that mirrors from Apache a   kicks     off     runners to do testing.", "Downside 1) it might not be entirely Apache    like."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This is a IAAS service so   we     need     to cover the things we need ourselves, but the upside is we can and   it    is     free.", "We could setup a Gitlab instance that mirrors from Apache a   kicks     off     runners to do testing.", "Downside 1) it might not be entirely Apache    like.", "Sorry cant help that.", "2) there is no guaranteed up time 3) I might   need     to     remove it in the future e.g."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We could setup a Gitlab instance that mirrors from Apache a   kicks     off     runners to do testing.", "Downside 1) it might not be entirely Apache    like.", "Sorry cant help that.", "2) there is no guaranteed up time 3) I might   need     to     remove it in the future e.g.", "when I change jobs for example :)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Sorry cant help that.", "2) there is no guaranteed up time 3) I might   need     to     remove it in the future e.g.", "when I change jobs for example :).", "4)  No     public cloud integration, it\u2019s a private stack after all.", "I can also fund on AWS/GCloud."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["@bolke, I'm curious, how long does it take you to go through one release  cycle?", "Oh, and do you have a documented step by step process for releasing?", "I'd like to add the Pypi part to this doc and add committers that are  interested to have rights on the project on Pypi.", "Max   On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:    So it is a database integrity issue?", "Afaik a start_date should always be   set for a DagRun (create_dagrun) does so  I didn't check the code though."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Oh, and do you have a documented step by step process for releasing?", "I'd like to add the Pypi part to this doc and add committers that are  interested to have rights on the project on Pypi.", "Max   On Wed, Feb 22, 2017 at 2:00 PM, Bolke de Bruin <bdbruin@gmail.com wrote:    So it is a database integrity issue?", "Afaik a start_date should always be   set for a DagRun (create_dagrun) does so  I didn't check the code though.", "Sent from my iPhone      On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.INVALID   wrote:       Should clarify this occurs when a dagrun does not have a start date,  not   a    dag (which makes it even less likely to happen)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Afaik a start_date should always be   set for a DagRun (create_dagrun) does so  I didn't check the code though.", "Sent from my iPhone      On 22 Feb 2017, at 22:19, Dan Davydov <dan.davydov@airbnb.com.INVALID   wrote:       Should clarify this occurs when a dagrun does not have a start date,  not   a    dag (which makes it even less likely to happen).", "I don't think this is  a    blocker for releasing.", "On Wed, Feb 22, 2017 at 1:15 PM, Dan Davydov <dan.davydov@airbnb.com   wrote:       I rolled this out in our prod and the webservers failed to load due to    this commit:       [AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger Dag    7c94d81c390881643f94d5e3d7d6fb351a445b72       This fixed it:    -                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\" title=\"Start   Date:    {{last_run.start_date.strftime('%Y-%m-%d %H:%M')}}\"</span    +                            </a <span id=\"statuses_info\"    class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"</span       This is caused by assuming that all DAGs have start dates set, so a   broken    DAG will take down the whole UI.", "Not sure if we want to make this a   blocker    for the release or not, I'm guessing for most deployments this would   occur    pretty rarely."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["org/mod_mbox/incubator-airflow-dev/201702.mbox/%3C7EB7B6D6-    092E-48D2-AA0F-    15F44376A8FF@gmail.com%3E       Next steps:    1) will start the voting process at the IPMC mailinglist.", "I do  expect    some changes to be required mostly in documentation maybe a license   here    and there.", "So, we might end up with changes to stable.", "As long as   these    are    not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and finalisation I  will    rebrand the RC to Release."], "labels": ["1", "1", "0", "1", "1"]}
{"abstract_id": 0, "sentences": ["As long as   these    are    not (significant) code changes I will not re-raise the vote.", "2) Only after the positive voting on the IPMC and finalisation I  will    rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the tar  ball    needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["2) Only after the positive voting on the IPMC and finalisation I  will    rebrand the RC to Release.", "3) I will upload it to the incubator release page, then the tar  ball    needs to propagate to the mirrors.", "4) Update the website (can someone volunteer please?)", "5) Finally, I will ask Maxime to upload it to pypi.", "It seems we can    keep    the apache branding as lib cloud is doing this as well (    https://libcloud.apache.org/downloads.html#pypi-package <    https://libcloud.apache.org/downloads.html#pypi-package)."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["Read the documentation and/or source code.", "2.", "Search gitter, possibly ask a question there.", "3.", "Search the dev mailing list, possibly ask a question there."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["4.", "File a Jira ticket.", "Before the migration to Apache Incubator, we used a google group which was great for discussions spanning a long time, and for archiving knowledge.", "Gitter is really not good at this.", "And the dev mailing list is fine but it's not a great UI, there's no search, etc."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["File a Jira ticket.", "Before the migration to Apache Incubator, we used a google group which was great for discussions spanning a long time, and for archiving knowledge.", "Gitter is really not good at this.", "And the dev mailing list is fine but it's not a great UI, there's no search, etc.", "I think we need something between steps 2 and 3 that's focused on users."], "labels": ["1", "1", "1", "1", "1"]}
{"abstract_id": 0, "sentences": ["And the dev mailing list is fine but it's not a great UI, there's no search, etc.", "I think we need something between steps 2 and 3 that's focused on users.", "As an example, Pandas encourages posting questions on stackoverflow.", "As a result there's a huge body of curated knowledge there.", "I'm not sure what the best solution is."], "labels": ["1", "1", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Restarting should only be required for clearing up database connections as we are not very good at that yet.", "- Bolke   Op 9 aug. 2016, om 20:30 heeft Lance Norskog <lance.norskog@gmail.com het volgende geschreven:    Yes, it is still current advice.", "My experience is that after running for (let's say) days, the app develops  memory corruption.", "I've seen three different ways that memory corruption  shows up.", "The scheduler failure is just one of these three symptoms."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["- Bolke   Op 9 aug. 2016, om 20:30 heeft Lance Norskog <lance.norskog@gmail.com het volgende geschreven:    Yes, it is still current advice.", "My experience is that after running for (let's say) days, the app develops  memory corruption.", "I've seen three different ways that memory corruption  shows up.", "The scheduler failure is just one of these three symptoms.", "The other two symptoms are  1) the main page of the UI shows a different list of running DAGs than is  what is really configured,  2) a task contains some configuration data that should be in a neighboring  task, and fails."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Tue, Aug 9, 2016 at 8:50 AM, Andrew Phillips <andrewp@apache.org wrote:    Hi all    I just wanted to check to what extent the advice in [1] and [2], namely to  restart the scheduler \"every once in a while\", is still considered accurate?", "\"Restart your scheduler process to get a clean environment every once in a  while.", "Use --num_runs N scheduler CLI option to make it stop after N runs  and have some supervisor ensuring it is always running.", "See issue 698\"    \"The scheduler should be restarted frequently    In our experience, a long running scheduler process, at least with the  CeleryExecutor, ends up not scheduling some tasks.", "We still don\u2019t know the  exact cause, unfortunately."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Use --num_runs N scheduler CLI option to make it stop after N runs  and have some supervisor ensuring it is always running.", "See issue 698\"    \"The scheduler should be restarted frequently    In our experience, a long running scheduler process, at least with the  CeleryExecutor, ends up not scheduling some tasks.", "We still don\u2019t know the  exact cause, unfortunately.", "Fortunately, airflow has a built-in workaround in the form of the \u2014  num_runs flag.", "It specifies a number of iterations for the scheduler to run  of its loop before it quits."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Could you give some guidance on what kind of frequency is  recommended here, or is that very dependent on the particular installation?", "Also, which of the current JIRA issues (if any) is the new version of  \"issue 698\" as mentioned in the first quote?", "There seem to be quite a few  issues relating to the scheduler getting stuck [3] - which one(s) should we  follow and/or add information to to best track progress on this topic?", "Thanks!", "ap      [1] https://cwiki.apache.org/confluence/display/AIRFLOW/Common+Pitfalls  [2] https://medium.com/handy-tech/airflow-tips-tricks-and-pitfal  ls-9ba53fba14eb#.ahcprdr9r  [3] https://issues.apache.org/jira/browse/AIRFLOW-39?jql=project  %20%3D%20AIRFLOW%20AND%20text%20~%20%22scheduler%22          --   Lance Norskog  lance.norskog@gmail.com  Redwood City, CA   "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["We have several dags:   - a daily ETL job   - several reporting jobs (daily, weekly or monthly) which use the data   from previous ETL jobs     I wish to have a dependency such that the reporting jobs depend upon the   last ETL job that the report uses.", "We're happy to set depends_on_past   in the ETL job.", "Daily jobs are easy - ExternalTaskSensor, job done.", "Weekly jobs are a little trickier - we need to work out the   execution_delta - normally +6 for us (we deliberately run a day late to   prioritise other jobs).", "Monthly jobs.... this is where I'm struggling - how to work out the   execution_delta."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Hi,  I have a MySQL table, which will be stored some static information.", "The information could be different for different airflow runs, so I hope to use python code to initialize it whenever airflow starts.", "Where is the best place to put such code ?", "Is the class DagBag's __init__() a good candidate ?", "Please advise."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Where is the best place to put such code ?", "Is the class DagBag's __init__() a good candidate ?", "Please advise.", "Thanks.", "#############################################  class DagBag(LoggingMixin):     \"\"\"     A dagbag is a collection of dags, parsed out of a folder tree and has high     level configuration settings, like what database to use as a backend and     what executor to use to fire off tasks."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Wed, Feb 8, 2017 at 8:33 AM Bolke de Bruin <bdbruin@gmail.com wrote:   On this proposal?", "No, not yet.", "Just popped in my mind yesterday.", "API there  is a bit on the wiki.", "On 8 Feb 2017, at 14:31, Jeremiah Lowin <jlowin@apache.org wrote:     Makes a lot of sense."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I would like to propose a new state  for   tasks named \u201cWAITING_ON_CALLBACK\u201d.", "Currently, we have tasks that have a   kind of polling mechanism (ie.", "Sensors) that wait for an action to  happen   and check if that action happened by regularly polling a particular   backend.", "This will always use a slot from one of the workers and could   starve an airflow cluster for resources.", "What if a callback to Airflow   could happen that task to change its status by calling a callback  mechanism   without taking up a worker slot."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A timeout could (should) be associated   with the required callback so that the task can fail if required.", "So a  bit   more visual:       Task X from DAG Z  does some work and sets \u201cWAITING_ON_CALLBACK\u201d - API   post to /dags/Z/dag_runs/20170101T00:00:00/tasks/X with payload \u201cset  status   to SUCCESS\u201d     DAG Z happily continues.", "Or     Task X from DAG Z sets \u201cWAITING_ON_CALLBACK\u201d with timeout of 300s -  time   passes - scheduler sets task to FAILED.", "Any thoughts?", "- Bolke    "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Static methods are provided for the creation, removal and investigation of  these objects.  \"\"\"", "__tablename__ = \"task_exclusion\"   id = Column(Integer(), primary_key=True)     dag_id = Column(String(ID_LEN), nullable=False)     task_id = Column(String(ID_LEN), nullable=False)     exclusion_type = Column(String(32), nullable=False)     exclusion_start_date = Column(DateTime, nullable=True)     exclusion_end_date = Column(DateTime, nullable=True)     created_by = Column(String(256), nullable=False)     created_on = Column(DateTime, nullable=False)      @classmethod  @provide_session  def set(             cls,             dag_id,             task_id,             exclusion_type,             exclusion_start_date,             exclusion_end_date,             created_by,             session=None):         \"\"\"  Add a task exclusion to prevent a task running under certain  circumstances.", ":param dag_id: The dag_id of the DAG containing the task to exclude  from execution.", ":param task_id: The task_id of the task to exclude from execution.", ":param exclusion_type: The type of circumstances to exclude the task  from execution under."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["This will be ignored if the exclusion_type is INDEFINITE.", ":param exclusion_end_date: The execution_date to stop excluding on.", "This will be ignored if the exclusion_type is INDEFINITE or  SINGLE_DATE.", ":param created_by: Who is creating this exclusion.", "Stored with the  exclusion record for auditing/debugging purposes."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [":param created_by: Who is creating this exclusion.", "Stored with the  exclusion record for auditing/debugging purposes.", ":return: None.  \"\"\"", "session.expunge_all()          # Set up execution date range correctly  if exclusion_type == TaskExclusionType.SINGLE_DATE:             if exclusion_start_date:                 exclusion_end_date = exclusion_start_date             else:                 raise AirflowException(                     \"No exclusion_start_date \"  )         elif exclusion_type == TaskExclusionType.DATE_RANGE:             if exclusion_start_date  exclusion_end_date:                 raise AirflowException(                     \"The exclusion_start_date is after the exclusion_end_date\"  )         elif exclusion_type == TaskExclusionType.INDEFINITE:             exclusion_start_date = None  exclusion_end_date = None  else:             raise AirflowException(                 \"The exclusion_type, {}, is not recognised.\"", ".format(exclusion_type)             )          # remove any duplicate exclusions  session.query(cls).filter(             cls.dag_id == dag_id,             cls.task_id == task_id,             cls.exclusion_type == exclusion_type,             cls.exclusion_start_date == exclusion_start_date,             cls.exclusion_end_date == exclusion_end_date         ).delete()          # insert new exclusion  session.add(TaskExclusion(             dag_id=dag_id,             task_id=task_id,             exclusion_type=exclusion_type,             exclusion_start_date=exclusion_start_date,             exclusion_end_date=exclusion_end_date,             created_by=created_by,             created_on=datetime.now())         )          session.commit()   The unit test:  class TaskExclusionTest(unittest.TestCase):     def test_set_exclusion(self, session=None):          session = settings.Session()          session.expunge_all()          dag_id = 'test_task_exclude'  task_id = 'test_task_exclude'  exec_date = datetime.datetime.now()          TaskExclusion.set(dag_id=dag_id,                           task_id=task_id,                           exclusion_type=TaskExclusionType.SINGLE_DATE,                           exclusion_start_date=exec_date,                           exclusion_end_date=exec_date,                           created_by='airflow')           exclusion = session.query(TaskExclusion).filter(                         TaskExclusion.dag_id == dag_id,                         TaskExclusion.task_id == task_id,                         TaskExclusion.exclusion_type == TaskExclusionType.SINGLE_DATE,                         TaskExclusion.exclusion_start_date == exec_date,                         TaskExclusion.exclusion_end_date == exec_date).first()          self.assertTrue(exclusion)   The unit test passes for postgreSQL and SQLite but fails for MySQL."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Stored with the  exclusion record for auditing/debugging purposes.", ":return: None.  \"\"\"", "session.expunge_all()          # Set up execution date range correctly  if exclusion_type == TaskExclusionType.SINGLE_DATE:             if exclusion_start_date:                 exclusion_end_date = exclusion_start_date             else:                 raise AirflowException(                     \"No exclusion_start_date \"  )         elif exclusion_type == TaskExclusionType.DATE_RANGE:             if exclusion_start_date  exclusion_end_date:                 raise AirflowException(                     \"The exclusion_start_date is after the exclusion_end_date\"  )         elif exclusion_type == TaskExclusionType.INDEFINITE:             exclusion_start_date = None  exclusion_end_date = None  else:             raise AirflowException(                 \"The exclusion_type, {}, is not recognised.\"", ".format(exclusion_type)             )          # remove any duplicate exclusions  session.query(cls).filter(             cls.dag_id == dag_id,             cls.task_id == task_id,             cls.exclusion_type == exclusion_type,             cls.exclusion_start_date == exclusion_start_date,             cls.exclusion_end_date == exclusion_end_date         ).delete()          # insert new exclusion  session.add(TaskExclusion(             dag_id=dag_id,             task_id=task_id,             exclusion_type=exclusion_type,             exclusion_start_date=exclusion_start_date,             exclusion_end_date=exclusion_end_date,             created_by=created_by,             created_on=datetime.now())         )          session.commit()   The unit test:  class TaskExclusionTest(unittest.TestCase):     def test_set_exclusion(self, session=None):          session = settings.Session()          session.expunge_all()          dag_id = 'test_task_exclude'  task_id = 'test_task_exclude'  exec_date = datetime.datetime.now()          TaskExclusion.set(dag_id=dag_id,                           task_id=task_id,                           exclusion_type=TaskExclusionType.SINGLE_DATE,                           exclusion_start_date=exec_date,                           exclusion_end_date=exec_date,                           created_by='airflow')           exclusion = session.query(TaskExclusion).filter(                         TaskExclusion.dag_id == dag_id,                         TaskExclusion.task_id == task_id,                         TaskExclusion.exclusion_type == TaskExclusionType.SINGLE_DATE,                         TaskExclusion.exclusion_start_date == exec_date,                         TaskExclusion.exclusion_end_date == exec_date).first()          self.assertTrue(exclusion)   The unit test passes for postgreSQL and SQLite but fails for MySQL.", "I have checked and the 'exclusion' variable contains a TaskExclusion object for postgreSQL and SQLite but is set to 'None' for MySQL."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["session.expunge_all()          # Set up execution date range correctly  if exclusion_type == TaskExclusionType.SINGLE_DATE:             if exclusion_start_date:                 exclusion_end_date = exclusion_start_date             else:                 raise AirflowException(                     \"No exclusion_start_date \"  )         elif exclusion_type == TaskExclusionType.DATE_RANGE:             if exclusion_start_date  exclusion_end_date:                 raise AirflowException(                     \"The exclusion_start_date is after the exclusion_end_date\"  )         elif exclusion_type == TaskExclusionType.INDEFINITE:             exclusion_start_date = None  exclusion_end_date = None  else:             raise AirflowException(                 \"The exclusion_type, {}, is not recognised.\"", ".format(exclusion_type)             )          # remove any duplicate exclusions  session.query(cls).filter(             cls.dag_id == dag_id,             cls.task_id == task_id,             cls.exclusion_type == exclusion_type,             cls.exclusion_start_date == exclusion_start_date,             cls.exclusion_end_date == exclusion_end_date         ).delete()          # insert new exclusion  session.add(TaskExclusion(             dag_id=dag_id,             task_id=task_id,             exclusion_type=exclusion_type,             exclusion_start_date=exclusion_start_date,             exclusion_end_date=exclusion_end_date,             created_by=created_by,             created_on=datetime.now())         )          session.commit()   The unit test:  class TaskExclusionTest(unittest.TestCase):     def test_set_exclusion(self, session=None):          session = settings.Session()          session.expunge_all()          dag_id = 'test_task_exclude'  task_id = 'test_task_exclude'  exec_date = datetime.datetime.now()          TaskExclusion.set(dag_id=dag_id,                           task_id=task_id,                           exclusion_type=TaskExclusionType.SINGLE_DATE,                           exclusion_start_date=exec_date,                           exclusion_end_date=exec_date,                           created_by='airflow')           exclusion = session.query(TaskExclusion).filter(                         TaskExclusion.dag_id == dag_id,                         TaskExclusion.task_id == task_id,                         TaskExclusion.exclusion_type == TaskExclusionType.SINGLE_DATE,                         TaskExclusion.exclusion_start_date == exec_date,                         TaskExclusion.exclusion_end_date == exec_date).first()          self.assertTrue(exclusion)   The unit test passes for postgreSQL and SQLite but fails for MySQL.", "I have checked and the 'exclusion' variable contains a TaskExclusion object for postgreSQL and SQLite but is set to 'None' for MySQL.", "Any suggestions on what could be causing this would be much appreciated.", "Cheers, Luke Maycock OLIVER WYMAN luke.maycock@affiliate.oliverwyman.com<mailto:luke.maycock@affiliate.oliverwyman.com www.oliverwyman.com<http://www.oliverwyman.com/    ________________________________ From: siddharth anand <sanand@apache.org Sent: 16 November 2016 00:40 To: dev@airflow.incubator.apache.org Subject: Re: Skip task  If your requirement is to skip a portion of tasks in a DagRun based on some state encountered while executing that DagRun, that is what BranchPythonOperator or ShortCircruitOperator (optionally paired with a Trigger Rule specified on a downstream task) is made for."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The callables need to return a Boolean value in the case of the ShortCircruitOperator or a selected choice (i.e.", "branch to take) as in the case of the BranchPythonOperator.", "If you have 20 tasks that all depend on the presence of 20 different files, you would need 20 ShortCircruitOperator or BranchPythonOperator tasks each either sharing a common callable or each with its own callable.", "One could argue that these tasks are \"overhead\" because they just encompass some conditional or control logic and that DAGs should only contain workhorse tasks (i.e.", "tasks that do some  work)."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["branch to take) as in the case of the BranchPythonOperator.", "If you have 20 tasks that all depend on the presence of 20 different files, you would need 20 ShortCircruitOperator or BranchPythonOperator tasks each either sharing a common callable or each with its own callable.", "One could argue that these tasks are \"overhead\" because they just encompass some conditional or control logic and that DAGs should only contain workhorse tasks (i.e.", "tasks that do some  work).", "DAGs with workhorse-only tasks are more of a pure dataflow approach -- i.e."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["no control-logic operators.", "However, I don't see another option.", "In the current system, a callable registered with a ShortCircruitOperator would check for the presence of a file -- if the file were not available, then a series of downstream tasks would be skipped in that DAGRun, until a task with a Trigger_Rule=\"all_done\" were encountered, downstream of which, tasks would no longer be skipped for the DagRun.", "I hope this makes sense.", "A long time ago, I proposed UI functionality to skip a series of DAG runs via the UI, because I knew that no data was available for that time range from an external system."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["It turns out that my company did not end up having such a requirement, so I dropped the feature request.", "If this is what you are asking for, then I am +1.", "Please implement it and submit a PR.", "On Tue, Nov 15, 2016 at 2:50 AM, Maycock, Luke < luke.maycock@affiliate.oliverwyman.com wrote:   Thank you for taking the time to respond.", "This is a great approach if you  know at the time of creating the DAG which tasks you expect to need to  skip."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If this is what you are asking for, then I am +1.", "Please implement it and submit a PR.", "On Tue, Nov 15, 2016 at 2:50 AM, Maycock, Luke < luke.maycock@affiliate.oliverwyman.com wrote:   Thank you for taking the time to respond.", "This is a great approach if you  know at the time of creating the DAG which tasks you expect to need to  skip.", "However, I don't think this is exactly the use case I have."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["For  example, I may be expecting a file to arrive in an FTP folder for loading  into a database but one day it doesn't arrive so I just want to skip that  task on that day.", "Our workflows commonly have around 20 of these types of tasks in.", "I could  configure all of these tasks in the way you suggested in case I ever need  to skip one of them.", "However, I'd prefer not to have to set the tasks up  this way and instead have the ability just to skip a task on an ad-hoc  basis.", "I could then also use this functionality to add the ability to run  from a certain point in a DAG or to a certain point in the DAG."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I could  configure all of these tasks in the way you suggested in case I ever need  to skip one of them.", "However, I'd prefer not to have to set the tasks up  this way and instead have the ability just to skip a task on an ad-hoc  basis.", "I could then also use this functionality to add the ability to run  from a certain point in a DAG or to a certain point in the DAG.", "Thanks,  Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/     ________________________________  From: siddharth anand <sanand@apache.org  Sent: 14 November 2016 19:48  To: dev@airflow.incubator.apache.org  Subject: Re: Skip task   For cases like this, we (Agari) use the following approach :     1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["However, I'd prefer not to have to set the tasks up  this way and instead have the ability just to skip a task on an ad-hoc  basis.", "I could then also use this functionality to add the ability to run  from a certain point in a DAG or to a certain point in the DAG.", "Thanks,  Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/     ________________________________  From: siddharth anand <sanand@apache.org  Sent: 14 November 2016 19:48  To: dev@airflow.incubator.apache.org  Subject: Re: Skip task   For cases like this, we (Agari) use the following approach :     1.", "Create a Variable in the UI of type boolean such as *enable_feature_x*    2."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thanks,  Luke Maycock  OLIVER WYMAN  luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com  www.oliverwyman.com<http://www.oliverwyman.com/     ________________________________  From: siddharth anand <sanand@apache.org  Sent: 14 November 2016 19:48  To: dev@airflow.incubator.apache.org  Subject: Re: Skip task   For cases like this, we (Agari) use the following approach :     1.", "Create a Variable in the UI of type boolean such as *enable_feature_x*    2.", "Use a ShortCircuitOperator (or BranchPythonOperator) to Skip    downstream processing based on the value of *enable_feature_x*    3.", "Assuming that you don't want to skip ALL downstream tasks, you can    use a trigger_rule of all_done to resume processing some portion of your    downstream DAG after skipping an upstream portion   In other words, there is already a means to achieve what you are asking for  today."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Assuming that you don't want to skip ALL downstream tasks, you can    use a trigger_rule of all_done to resume processing some portion of your    downstream DAG after skipping an upstream portion   In other words, there is already a means to achieve what you are asking for  today.", "You can change the value of via *enable_feature_x  *the UI.", "If you'd  like to enhance the UI to better capture this pattern, pls submit a PR.", "-s   On Thu, Nov 10, 2016 at 1:20 PM, Maycock, Luke <  luke.maycock@affiliate.oliverwyman.com wrote:    Hi Gerard,       I see the new status as having a number of uses:      1.", "A user can manually set a task to skip in a DAG run via the UI."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["-s   On Thu, Nov 10, 2016 at 1:20 PM, Maycock, Luke <  luke.maycock@affiliate.oliverwyman.com wrote:    Hi Gerard,       I see the new status as having a number of uses:      1.", "A user can manually set a task to skip in a DAG run via the UI.", "2.", "We can then make use of this new status to add the following   functionality to Airflow:       *   Run a DAG run up to a certain point and have the rest of the  tasks   have the new status.", "*  Run a DAG run from a certain task to the end, setting all   pre-requisite tasks to have this new status."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["A user can manually set a task to skip in a DAG run via the UI.", "2.", "We can then make use of this new status to add the following   functionality to Airflow:       *   Run a DAG run up to a certain point and have the rest of the  tasks   have the new status.", "*  Run a DAG run from a certain task to the end, setting all   pre-requisite tasks to have this new status.", "I am happy to be challenged on the above use cases if there are better   ways to achieve the same things."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["maycock@affiliate.oliverwyman.com    www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________    From: Gerard Toonstra <gtoonstra@gmail.com    Sent: 08 November 2016 18:19    To: dev@airflow.incubator.apache.org    Subject: Re: Skip task       Also in 1.7.1.3, there's the ShortCircuitOperator, which can give you  an    example.", "https://github.com/apache/incubator-airflow/blob/1.7.1.", "3/airflow/operators/python_operator.py       You'd have to modify this to your needs, but the way it works is that  if    the condition evaluates to True, none of the    downstream tasks are actually executed, they'd be skipped.", "The reason  for    putting them into SKIPPED state is that    the DAG final result would still be SUCCESS and not failed.", "You could copy the operator from there and don't do the full \"for  loop\",    only pick the tasks immediately downstream    from this operator and skip that."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["https://github.com/apache/incubator-airflow/blob/1.7.1.", "3/airflow/operators/python_operator.py       You'd have to modify this to your needs, but the way it works is that  if    the condition evaluates to True, none of the    downstream tasks are actually executed, they'd be skipped.", "The reason  for    putting them into SKIPPED state is that    the DAG final result would still be SUCCESS and not failed.", "You could copy the operator from there and don't do the full \"for  loop\",    only pick the tasks immediately downstream    from this operator and skip that.", "Or... if you need to skip additional    tasks downstream, add a parameter \"num_tasks\"    that decide on a halting condition for the for loop."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["3/airflow/operators/python_operator.py       You'd have to modify this to your needs, but the way it works is that  if    the condition evaluates to True, none of the    downstream tasks are actually executed, they'd be skipped.", "The reason  for    putting them into SKIPPED state is that    the DAG final result would still be SUCCESS and not failed.", "You could copy the operator from there and don't do the full \"for  loop\",    only pick the tasks immediately downstream    from this operator and skip that.", "Or... if you need to skip additional    tasks downstream, add a parameter \"num_tasks\"    that decide on a halting condition for the for loop.", "I believe that should work."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Or... if you need to skip additional    tasks downstream, add a parameter \"num_tasks\"    that decide on a halting condition for the for loop.", "I believe that should work.", "I didn't try that here, but you can test  that    and see what it does for you.", "If you want this as a UI capability... for example have a human  operator    decide on skipping this yes or not, then    maybe the best way forward would be some kind of highly custom plugin   with    its own view.", "In the end, you'd basically    do the same action in the backend, whether the python cond evaluates to    True or the button is clicked."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I didn't try that here, but you can test  that    and see what it does for you.", "If you want this as a UI capability... for example have a human  operator    decide on skipping this yes or not, then    maybe the best way forward would be some kind of highly custom plugin   with    its own view.", "In the end, you'd basically    do the same action in the backend, whether the python cond evaluates to    True or the button is clicked.", "In the plugin case though, you'd have to keep the UI and the structure  of    the DAG in sync and aligned, otherwise    it'd become a mess.... Airflow wasn't really developed for  workflow/human    interaction, but in workflows where only    automated processes are involved.", "That doesn't mean that you can't do    anything like that, but it may be costly resource    wise to get this done."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you want this as a UI capability... for example have a human  operator    decide on skipping this yes or not, then    maybe the best way forward would be some kind of highly custom plugin   with    its own view.", "In the end, you'd basically    do the same action in the backend, whether the python cond evaluates to    True or the button is clicked.", "In the plugin case though, you'd have to keep the UI and the structure  of    the DAG in sync and aligned, otherwise    it'd become a mess.... Airflow wasn't really developed for  workflow/human    interaction, but in workflows where only    automated processes are involved.", "That doesn't mean that you can't do    anything like that, but it may be costly resource    wise to get this done.", "For example, on the basis of the BranchOperator,   you    could call an external API to verify if a decision    was taken on a case, then follow branch A or B if the decision is there   or    put the state back into UP_FOR_RETRY."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Rgds,       Gerard                On Tue, Nov 8, 2016 at 5:30 PM, Maycock, Luke <    luke.maycock@affiliate.oliverwyman.com wrote:        Hi All,             I am using Airflow 1.7.1.3 and have a particular requirement, which I     don't think is currently supported by Airflow but just wanted to  check   in     case I was missing something.", "I occasionally wish to skip a particular task in a given DAG run such    that     the task does not run for that DAG run.", "Is this functionality  available    in     Airflow?", "I am aware of the BranchPythonOperator (https://airflow.incubator.", "apache.org/concepts.html#branching) but I don't think believe this  is     exactly what I am looking for."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I occasionally wish to skip a particular task in a given DAG run such    that     the task does not run for that DAG run.", "Is this functionality  available    in     Airflow?", "I am aware of the BranchPythonOperator (https://airflow.incubator.", "apache.org/concepts.html#branching) but I don't think believe this  is     exactly what I am looking for.", "I am thinking that a button in the UI alongside the 'Mark Success'  and     'Run' buttons would be appropriate."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I am thinking that a button in the UI alongside the 'Mark Success'  and     'Run' buttons would be appropriate.", "If the functionality does not exist, does anyone have any suggestions   on     ways to implement this?", "Cheers,     Luke Maycock     OLIVER WYMAN     luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com     www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________     This e-mail and any attachments may be confidential or legally    privileged.", "If you received this message in error or are not the intended   recipient,     you should destroy the e-mail message and any attachments or copies,   and     you are prohibited from retaining, distributing, disclosing or using   any     information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Cheers,     Luke Maycock     OLIVER WYMAN     luke.maycock@affiliate.oliverwyman.com<mailto:luke.", "maycock@affiliate.oliverwyman.com     www.oliverwyman.com<http://www.oliverwyman.com/             ________________________________     This e-mail and any attachments may be confidential or legally    privileged.", "If you received this message in error or are not the intended   recipient,     you should destroy the e-mail message and any attachments or copies,   and     you are prohibited from retaining, distributing, disclosing or using   any     information contained herein.", "Please inform us of the erroneous   delivery    by     return e-mail.", "Thank you for your cooperation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you received this message in error or are not the intended   recipient,     you should destroy the e-mail message and any attachments or copies,   and     you are prohibited from retaining, distributing, disclosing or using   any     information contained herein.", "Please inform us of the erroneous   delivery    by     return e-mail.", "Thank you for your cooperation.", "________________________________    This e-mail and any attachments may be confidential or legally   privileged.", "If you received this message in error or are not the intended  recipient,    you should destroy the e-mail message and any attachments or copies,  and    you are prohibited from retaining, distributing, disclosing or using  any    information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please inform us of the erroneous   delivery    by     return e-mail.", "Thank you for your cooperation.", "________________________________    This e-mail and any attachments may be confidential or legally   privileged.", "If you received this message in error or are not the intended  recipient,    you should destroy the e-mail message and any attachments or copies,  and    you are prohibited from retaining, distributing, disclosing or using  any    information contained herein.", "Please inform us of the erroneous  delivery   by    return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If you received this message in error or are not the intended  recipient,    you should destroy the e-mail message and any attachments or copies,  and    you are prohibited from retaining, distributing, disclosing or using  any    information contained herein.", "Please inform us of the erroneous  delivery   by    return e-mail.", "Thank you for your cooperation.", "________________________________   This e-mail and any attachments may be confidential or legally  privileged.", "If you received this message in error or are not the intended recipient,   you should destroy the e-mail message and any attachments or copies, and   you are prohibited from retaining, distributing, disclosing or using any   information contained herein."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Thank you for your cooperation.", "________________________________  This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient,  you should destroy the e-mail message and any attachments or copies, and  you are prohibited from retaining, distributing, disclosing or using any  information contained herein.", "Please inform us of the erroneous delivery by  return e-mail.", "Thank you for your cooperation."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Please inform us of the erroneous delivery by  return e-mail.", "Thank you for your cooperation.", "________________________________ This e-mail and any attachments may be confidential or legally privileged.", "If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein.", "Please inform us of the erroneous delivery by return e-mail."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I believe they are being ignored as \"unrelated\"  but   they have the potential to mask real issues and should be addressed.", "Unfortunately they're out of my expertise so I'm going to list the ones   I've identified and hope someone smarter than me can see if they can  help!", "In particular, we have a number of simple PR's that should obviously have   no problems (typos, readme edits, etc.)", "that are nonetheless failing  tests,   causing frustration for all.", "Here is one from just this morning:   https://github.com/apache/incubator-airflow/pull/1705/files     Thanks in advance!"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Unfortunately they're out of my expertise so I'm going to list the ones   I've identified and hope someone smarter than me can see if they can  help!", "In particular, we have a number of simple PR's that should obviously have   no problems (typos, readme edits, etc.)", "that are nonetheless failing  tests,   causing frustration for all.", "Here is one from just this morning:   https://github.com/apache/incubator-airflow/pull/1705/files     Thanks in advance!", "1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["In particular, we have a number of simple PR's that should obviously have   no problems (typos, readme edits, etc.)", "that are nonetheless failing  tests,   causing frustration for all.", "Here is one from just this morning:   https://github.com/apache/incubator-airflow/pull/1705/files     Thanks in advance!", "1.", "Python 3 Mysql (this one is pretty common), due to not being able to   find \"beeline\" which I believe is related to Hive."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Here is one from just this morning:   https://github.com/apache/incubator-airflow/pull/1705/files     Thanks in advance!", "1.", "Python 3 Mysql (this one is pretty common), due to not being able to   find \"beeline\" which I believe is related to Hive.", "This is the error:     ======================================================================     ERROR: test_mysql_to_hive_partition (tests.TransferTests)     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/  operators/operators.py\",   line 208, in test_mysql_to_hive_partition        t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 2350, in run        force=force,)      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 1388, in run        result = task_copy.execute(context=context)      File \"/home/travis/build/apache/incubator-airflow/airflow/  operators/mysql_to_hive.py\",   line 131, in execute        recreate=self.recreate)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 322, in load_file        self.run_cli(hql)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 212, in run_cli        cwd=tmp_dir)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 858, in  __init__        restore_signals, start_new_session)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 1456, in   _execute_child        raise child_exception_type(errno_num, err_msg)     nose.proxy.FileNotFoundError: [Errno 2] No such file or directory:  'beeline'       2.", "Python 3 Postgres (this one is really infrequent):     ======================================================================     FAIL: Test that ignore_first_depends_on_past doesn't affect results     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 349, in test_dagrun_deadlock_ignore_depends_on_past        run_kwargs=dict(ignore_first_depends_on_past=True))      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 221, in evaluate_dagrun        self.assertEqual(ti.state, expected_state)     nose.proxy.AssertionError: None != 'success'     3."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Python 3 Mysql (this one is pretty common), due to not being able to   find \"beeline\" which I believe is related to Hive.", "This is the error:     ======================================================================     ERROR: test_mysql_to_hive_partition (tests.TransferTests)     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/  operators/operators.py\",   line 208, in test_mysql_to_hive_partition        t.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, force=True)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 2350, in run        force=force,)      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/airflow/models.py\",   line 1388, in run        result = task_copy.execute(context=context)      File \"/home/travis/build/apache/incubator-airflow/airflow/  operators/mysql_to_hive.py\",   line 131, in execute        recreate=self.recreate)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 322, in load_file        self.run_cli(hql)      File \"/home/travis/build/apache/incubator-airflow/airflow/  hooks/hive_hooks.py\",   line 212, in run_cli        cwd=tmp_dir)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 858, in  __init__        restore_signals, start_new_session)      File \"/opt/python/3.4.2/lib/python3.4/subprocess.py\", line 1456, in   _execute_child        raise child_exception_type(errno_num, err_msg)     nose.proxy.FileNotFoundError: [Errno 2] No such file or directory:  'beeline'       2.", "Python 3 Postgres (this one is really infrequent):     ======================================================================     FAIL: Test that ignore_first_depends_on_past doesn't affect results     ----------------------------------------------------------------------     Traceback (most recent call last):      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 349, in test_dagrun_deadlock_ignore_depends_on_past        run_kwargs=dict(ignore_first_depends_on_past=True))      File \"/home/travis/build/apache/incubator-airflow/airflow/utils/db.py\",   line 54, in wrapper        result = func(*args, **kwargs)      File \"/home/travis/build/apache/incubator-airflow/tests/jobs.py\",   line 221, in evaluate_dagrun        self.assertEqual(ti.state, expected_state)     nose.proxy.AssertionError: None != 'success'     3.", "Mysql (py2 and py3, infrequent).", "This appears to happen when the   SLA code is called wiht mysql."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Mid-january is probably a good target.", "- Joe  On Thu, Dec 15, 2016 at 5:18 PM, Luke Ptz <lukeptzcode@gmail.com wrote:   Cool to see the interest is there!", "I unfortunately can't offer a space for  a meetup, can anyone else?", "If not could always be informal/meet in a public  setting   On Wed, Dec 14, 2016 at 7:08 PM, Andrew Phillips <andrewp@apache.org  wrote:    We at Blue Apron would be very interested.", "Same here."], "labels": ["0", "0", "0", "1", "0"]}
{"abstract_id": 0, "sentences": ["Feel free to downgrade to critical if you like :).", "Cheers Bolke    On 8 Jun 2017, at 02:35, Maxime Beauchemin <maximebeauchemin@gmail.com wrote:    What a pleasant, mind numbing afternoon doing some release management    Notes:  * Added a warning that the package name has changed on Pypi  <https://pypi.python.org/pypi/airflow  * Removed references to my name here  <https://github.com/apache/incubator-airflow/pull/2352 and merged  * Addressed John D. Ament's concerns here  <https://github.com/apache/incubator-airflow/pull/2354, please review!", "* \"footable\" appears to have been removed, not a problem anymore  * that `airflow-jira is a god send!", "thanks Bolke.", "* reviewed list of Airbnb's production cherries and flagged those as `Fix  Version == 1.8.2`  * Started branch v1-8-2.rc1 and started picking cherries using  `airflow-jira compare 1.8.2`    I'll finish going through picking everything that targeted 1.8.2 that does  not create merge conflict."], "labels": ["1", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["On Sun, May 6, 2018 at 3:17 PM, Carlos Santana <csantana23@gmail.com wrote:  ...if many go we can have an OpenWhisk BoF ...", "I'll be at ApacheCon as well, giving a talk on Thursday.", "I'll have a busy schedule, not sure if I'll be able to attend a BoF but interested in principle!", "-Bertrand  "], "labels": ["0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have a DAG < schedule intervel 30 mins .", "It has 3 tasks.", "t1 runs a python program on remote EC2.", "t2 waits for S3 file availability at particular location.", "This S3 file created by t1."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Once the S3 file is available, t3 runs and process the file on S3.", "I have date-time as part of my S3 file location.", "dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')  bucket_key2 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/sucess\"  t1 runs more than 1 hour so second instance of DAG  is already started and it changes the variable dttm2 value so job1 task # t2 is trying to locate the file at different location.", "To overcome this I am planning to use parameter {{execution_date}} instead of getting dttm2 value as shown above.", "In situations like these, is there any better approach to keep same value in a variable through out the particular run of DAG?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["I have date-time as part of my S3 file location.", "dttm2 = datetime.now().strftime('%Y-%m-%d-%H-%M')  bucket_key2 = \"s3://aaaaa/bbbbb/\" + dttm2 + \"/sucess\"  t1 runs more than 1 hour so second instance of DAG  is already started and it changes the variable dttm2 value so job1 task # t2 is trying to locate the file at different location.", "To overcome this I am planning to use parameter {{execution_date}} instead of getting dttm2 value as shown above.", "In situations like these, is there any better approach to keep same value in a variable through out the particular run of DAG?", "Or use XCOM feature to push and pull the values across the tasks with different keys for each run?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["The contrib section is slowly but steadily  growing and with operators/hooks we are particularly dependent on the  community as not all (or even none in some case) of the committers use  these themselves.", "In any case test coverage is required, but that is a given I think.", "Kind regards,  Bolke    On 31 May 2017, at 21:10, Peter Dolan <peterdolan@google.com.INVALID  wrote:     Hello developers,     I work with Google Cloud ML, and my team and I are interested in  contributing a set of Operators to support working with the Cloud ML  platform.", "The platform supports using the TensorFlow deep neural network  framework as a managed system.", "In particular, we would like to contribute    * CloudMLTrainingOperator, which would launch and monitor a Cloud ML  Training Job (  https://cloud.google.com/ml-engine/docs/how-tos/training-jobs <  https://cloud.google.com/ml-engine/docs/how-tos/training-jobs),    * CloudMLBatchPredictionOperator, which would launch and monitor a  Cloud ML Batch Prediction Job (  https://cloud.google.com/ml-engine/docs/how-tos/batch-predict <  https://cloud.google.com/ml-engine/docs/how-tos/batch-predict), and    * CloudMLVersionOperator, which can create, update, and delete  TensorFlow model versions (  https://cloud.google.com/ml-engine/docs/how-tos/managing-models-jobs <  https://cloud.google.com/ml-engine/docs/how-tos/managing-models-jobs)     I'm eager to hear if the Airflow project is open to these  contributions, and if any changes are suggested."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["If the later, consider using the SSH Execute Operator?", "-R  On Tue, May 16, 2017 at 1:43 AM Niranda Perera <niranda.17@cse.mrt.ac.lk wrote:   To clarify the the question more,   I want to run workers on windows, as I have some apps in my workflow which  require windows.", "But I can keep my master instance in linux.", "I hope this issue of gunicorn not compatible with windows [1] would not be  a blocker to run the worker?", "I also want to monitor the status of the windows worker from the airflow  master and I hope not having gunicorn, would not break that functionality?"], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["But I can keep my master instance in linux.", "I hope this issue of gunicorn not compatible with windows [1] would not be  a blocker to run the worker?", "I also want to monitor the status of the windows worker from the airflow  master and I hope not having gunicorn, would not break that functionality?", "[1]   http://stackoverflow.com/questions/32378494/how-to-run-airflow-on-windows/32378495   Best  Nira   Best regards   Niranda Perera  Research Assistant  Dept of CSE, University of Moratuwa  niranda.17@cse.mrt.ac.lk  +94 71 554 8430 <+94%2071%20554%208430  https://lk.linkedin.com/in/niranda   On Tue, May 16, 2017 at 12:00 PM, Niranda Perera <niranda.17@cse.mrt.ac.lk    wrote:    Hi devs,     Does airflow work on the windows environment?", "Best regards     Niranda Perera   Research Assistant   Dept of CSE, University of Moratuwa   niranda.17@cse.mrt.ac.lk   +94 71 554 8430 <+94%2071%20554%208430 <+94%2071%20554%208430   https://lk.linkedin.com/in/niranda     "], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Ran Airflow via docker (https://github.com/puckel/docker-airflow) 2.", "Copied the example from https://pythonhosted.org/airflow/plugins.html 3.", "Copied the postgres hook, renamed a few fields and ended up with https://gist.github.com/shin-nien/2d011972d8631c92675d1de58b000168 (it won't work for mongo yet but I wanted to test that it would be available).", "4.", "Restarted the container and observed pyc in plugins dir was created."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": ["Copied the postgres hook, renamed a few fields and ended up with https://gist.github.com/shin-nien/2d011972d8631c92675d1de58b000168 (it won't work for mongo yet but I wanted to test that it would be available).", "4.", "Restarted the container and observed pyc in plugins dir was created.", "5.", "Went to UI and noticed the test menus 6."], "labels": ["0", "0", "0", "0", "0"]}
{"abstract_id": 0, "sentences": [" We at Blue Apron would be very interested.", "Same here.", "ap  "], "labels": ["0", "0", "0"]}
