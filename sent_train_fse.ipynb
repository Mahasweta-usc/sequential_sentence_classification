{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sent_train_fse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce5e1d6cca2c4d00b2af7c974f4c5a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c60c4d0d26654ba496d7b39ed50bbdf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2684cfddf58d46e09d2c2023c6ab5cea",
              "IPY_MODEL_a1bee93d3c1740379e7152285fe65720",
              "IPY_MODEL_f78921fe8c224eb19eb07f1acac50146"
            ]
          }
        },
        "c60c4d0d26654ba496d7b39ed50bbdf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2684cfddf58d46e09d2c2023c6ab5cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fc220f81c33f4c27b1d286d94292b244",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d7338e8db3a456db4865725185baded"
          }
        },
        "a1bee93d3c1740379e7152285fe65720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d02ad205aa4c4fb5b397156ce0c23887",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 24459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 24459,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43f7d0a6064b412b9de67c775b05812e"
          }
        },
        "f78921fe8c224eb19eb07f1acac50146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14ee86d3437e49829c608671d557e424",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 142k/? [00:00&lt;00:00, 4.44MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05ac41a57cab4e6e90082d6a2fdb42a9"
          }
        },
        "fc220f81c33f4c27b1d286d94292b244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d7338e8db3a456db4865725185baded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d02ad205aa4c4fb5b397156ce0c23887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43f7d0a6064b412b9de67c775b05812e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14ee86d3437e49829c608671d557e424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05ac41a57cab4e6e90082d6a2fdb42a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59I7dt5xLGbo",
        "outputId": "a7aeb124-d482-430f-fe38-bba1a2f66cd6"
      },
      "source": [
        "!pip install transformers email_reply_parser jsonlines\n",
        "#best lr 2e-5, mx_len 256, split seed 42, stratified split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.7 MB/s \n",
            "\u001b[?25hCollecting email_reply_parser\n",
            "  Downloading email_reply_parser-0.5.12-py3-none-any.whl (4.1 kB)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 79.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, jsonlines, email-reply-parser\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed email-reply-parser-0.5.12 huggingface-hub-0.4.0 jsonlines-3.0.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "YFwb00bi0FBX",
        "outputId": "8c72c3f9-5d56-47b0-b203-01467fdf93cb"
      },
      "source": [
        "# Upload the train file from your local drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content\n",
        "import os\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d1d1a987-7b83-4280-841e-153883b2ffaf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d1d1a987-7b83-4280-841e-153883b2ffaf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_IS_v1.csv to all_IS_v1.csv\n",
            "Saving policy.csv to policy.csv\n",
            "Saving scraped_mentor_12_19_2021.csv to scraped_mentor_12_19_2021.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7puwQWjzqqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698,
          "referenced_widgets": [
            "ce5e1d6cca2c4d00b2af7c974f4c5a4a",
            "c60c4d0d26654ba496d7b39ed50bbdf1",
            "2684cfddf58d46e09d2c2023c6ab5cea",
            "a1bee93d3c1740379e7152285fe65720",
            "f78921fe8c224eb19eb07f1acac50146",
            "fc220f81c33f4c27b1d286d94292b244",
            "3d7338e8db3a456db4865725185baded",
            "d02ad205aa4c4fb5b397156ce0c23887",
            "43f7d0a6064b412b9de67c775b05812e",
            "14ee86d3437e49829c608671d557e424",
            "05ac41a57cab4e6e90082d6a2fdb42a9"
          ]
        },
        "outputId": "3a35968e-e02e-45e3-8a84-3f96693800d7"
      },
      "source": [
        "!pip install fuzzywuzzy stanza\n",
        "import os, sys\n",
        "import random\n",
        "import json\n",
        "import urllib\n",
        "from urllib.parse import urlsplit\n",
        "from urllib.parse import urljoin, urlencode, quote_plus\n",
        "from urllib.parse import urlparse\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "import numpy as np\n",
        "import regex as re\n",
        "from fuzzywuzzy import fuzz, process\n",
        "import itertools\n",
        "from itertools import combinations, compress\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests.utils import requote_uri\n",
        "import requests.exceptions\n",
        "import stanza\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "scraped_data = pd.read_csv(\"/content/scraped_mentor.csv\")\n",
        "\n",
        "def get_email(url):\n",
        "  browser = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "  browser.get(url)\n",
        "  html = browser.page_source\n",
        "  soup_3 = BeautifulSoup(html, 'lxml')\n",
        "  email = soup_3.find(\"pre\").text\n",
        "  return email\n",
        "\t# title = \"\"\n",
        "\t# parent =  soup_3.find_all(\"tr\", class_=\"subject\")[0]\n",
        "\t# subject = parent.find_all(\"td\")[-1]  # last cell in the row\n",
        "\n",
        "\n",
        "def process_(text):\n",
        "  text = text.replace(\"\\r\\n\",\" \").replace(\"\\n\",\" \")\n",
        "  text = \" \".join(re.sub('>',\"\",text).split())\n",
        "  return text\n",
        "\n",
        "def sent_break(text):\n",
        "\tdoc = nlp(text)\n",
        "\tlines = [line.text for line in doc.sentences]\n",
        "\treturn lines\n",
        "\n",
        "def decode(seg):\n",
        "\tchunk = tokenizer.convert_ids_to_tokens(seg)\n",
        "\ttext = tokenizer.convert_tokens_to_string(chunk)\n",
        "\treturn text \n",
        "\n",
        "def chunks(lst, n):\n",
        "\tfor i in range(0, len(lst), n):\n",
        "\t\tyield lst[i:i + n]\n",
        "\n",
        "def prune():\n",
        "\tglobal candidate\n",
        "\ttemp = []\n",
        "\tfor elem in candidate:\n",
        "\t\tif any(len(elem) < len(cand) and set(elem).issubset(set(cand)) for cand in candidate):\n",
        "\t\t\ttemp.append(elem)\n",
        "\n",
        "\tfor elem in temp: candidate.remove(elem)\n",
        "\n",
        "def segmenter():\n",
        "  global candidate\n",
        "  global email_sent\n",
        "  sum = 0;\n",
        "  lim = int(0.9*MAX_LEN)\n",
        "  candidate.append([])\n",
        "  for idx, elem in enumerate(email_sent):\n",
        "    sum += len(tokenizer.encode(elem))\n",
        "    remain = lim - sum\n",
        "    if sum > lim:\n",
        "      retain = tokenizer.convert_tokens_to_string(tokenizer.tokenize(elem[:remain]))\n",
        "      carryover = tokenizer.convert_tokens_to_string(tokenizer.tokenize(elem[remain:]))\n",
        "      if not idx: \n",
        "        candidate[-1].append(retain) \n",
        "        # email_sent[idx] = carryover\n",
        "        # email_sent.insert(idx,retain)\n",
        "        # return idx + 1\n",
        "      return idx\n",
        "    \n",
        "    candidate[-1].append(elem)\n",
        "    # break\n",
        "\n",
        "def find_project(url):\n",
        "\tdetails = url.split(\"/\")[-3]\n",
        "\tsub_details = details.split('-')\n",
        "\tfor elem in sub_details: \n",
        "\t\tif elem not in [\"dev\",\"user\",\"incubator\"]: return elem\n",
        "\n",
        "import difflib \n",
        "\n",
        "def find_match(sup, sub):\n",
        "  if sup in sub or sub in sup: return True\n",
        "  if fuzz.partial_ratio(sup,sub) > 90: return True\n",
        "  else: return False\n",
        "\n",
        "def single_entries():\n",
        "  global candidate\n",
        "  for idx, elem in enumerate(candidate):\n",
        "    if len(elem) < 2:\n",
        "      elem = tokenizer.tokenize(elem[0])\n",
        "      length = MAX_LEN - len(elem) - 1\n",
        "      if MAX_LEN > 2*len(elem):\n",
        "        addage = tokenizer.convert_tokens_to_string(elem)\n",
        "        candidate[idx].append(addage)\n",
        "      else:\n",
        "        candidate[idx] = []\n",
        "        for subsent in chunks(elem,int(len(elem)/2) + 1):\n",
        "          addage = tokenizer.convert_tokens_to_string(subsent)\n",
        "          candidate[idx].append(addage)\n",
        "        \n",
        "\n",
        "\n",
        "new_data = pd.read_csv(\"/content/all_IS.csv\")\n",
        "new_data[\"url\"] = new_data[\"url\"].apply(lambda x: x.strip('\"'))\n",
        "scraped_data['label'] = scraped_data['url'].apply(lambda x: 1 if \n",
        "  any([item in x for item in new_data['url'].tolist()]) else 0)\n",
        "\n",
        "candidate = []\n",
        "\n",
        "\n",
        "\n",
        "def create_data(current_df):\n",
        "  all_sentences,all_is,urls = 0,[],[]\n",
        "  IS_record = pd.DataFrame(columns=[\"extract\",\"IS_found\",\"IS_list\",'url'])\n",
        "  new_ = new_data.groupby('url')\n",
        "  table = pd.DataFrame(columns = [\"sentences\",\"labels\",'abstract_id',\"confs\",\"url\"])\n",
        "  not_found = []\n",
        "  row_count = 0\n",
        "  IS_row_count = 0\n",
        "  bad_url = []\n",
        "  all_scores = []\n",
        "  global candidate, email_sent\n",
        "  IS_count = []\n",
        "\n",
        "  for idx,elem in current_df.iterrows():\n",
        "    url = elem[\"url\"]\n",
        "    email_text = elem[\"text\"]\n",
        "    # project = find_project(str(url)) #;print(idx, project) \n",
        "    IS_flag = False\n",
        "\n",
        "    try:\n",
        "      if url not in new_data[\"url\"].unique(): raise Exception\n",
        "      IS_flag = True\n",
        "      IS_row = new_.get_group(url)\n",
        "      ##setting this to true rules out picking positive emails from IS designated emails as '0' label\n",
        "      text = IS_row[\"prof\"].tolist()\n",
        "      if type(text) == float: print(text)\n",
        "      text = list(itertools.chain.from_iterable([item.split('<EOL>') for item in text]))\n",
        "      text = [process_(item).strip() for item in text if len(process_(item).strip()) > 5]\n",
        "    except Exception as e:\n",
        "      text = []\n",
        "\n",
        "    # if url.strip('\"') in def_url: email_sent = sent_tokenize(process_(email_text)); print(\"bad_url\")\n",
        "    email_sent = sent_break(process_(email_text)) \n",
        "    all_sentences += len(set(email_sent));print(all_sentences)\n",
        "    # new_is = [is_ for is_ in text if is_ not in all_is]\n",
        "    if url not in urls:\n",
        "      all_is += text;print(len(all_is),len(urls)); urls += [url]*len(text)\n",
        "    # print(email_sent)\n",
        "    new_IS = [x for x in text]\n",
        "\n",
        "    candidate = []\n",
        "    while True:\n",
        "      pos = segmenter()\n",
        "      #pos is none when don't pop or end reached\n",
        "      # if pos:\n",
        "      #   for _ in range(pos): email_sent.pop(0)\n",
        "      if len(email_sent) > 1: email_sent.pop(0)\n",
        "      else: break\n",
        "\n",
        "    prune()\n",
        "    single_entries()\n",
        "\n",
        "    #change for best result\n",
        "    # candidate = [decode(tokenizer.encode(\" \".join(x))[:MAX_LEN-2]) for x in candidate]\n",
        "    for seg in candidate:\n",
        "      labels = []\n",
        "      for sent in seg:\n",
        "        matches = [IS for IS in text if find_match(sent, IS) and len(sent)]\n",
        "\n",
        "        if matches:\n",
        "          with open(\"/content/matches.txt\",'a+') as op:\n",
        "            op.write(url+\"\\n\"+sent+\"\\n\"+\"  \".join(matches)+\"\\n\")\n",
        "          \n",
        "        for x in matches: \n",
        "          if x in new_IS: new_IS.remove(x)\n",
        "\n",
        "        if matches and IS_flag:\n",
        "          labels.append(1)\n",
        "        else: labels.append(0)\n",
        "      \n",
        "      # if len(seg) == 1:\n",
        "      #   seg += tokenizer.convert_ids_to_tokens([0])\n",
        "      #   labels += [0]\n",
        "\n",
        "      \n",
        "\n",
        "      if IS_flag and labels:\n",
        "        IS_record.at[IS_row_count,\"extract\"] = \" \".join(seg)\n",
        "        IS_record.at[IS_row_count,\"IS_found\"] = matches\n",
        "        IS_record.at[IS_row_count,\"IS_list\"] = text\n",
        "        IS_record.at[IS_row_count,\"url\"] = url.strip('\"')\n",
        "        IS_row_count += 1\n",
        "\n",
        "\n",
        "      # if not sum(labels) and IS_flag: continue\n",
        "      if not seg: continue\n",
        "      table.at[row_count,'labels'] = labels\n",
        "      table.at[row_count,'sentences'] = seg\n",
        "      table.at[row_count,'url'] = url\n",
        "      table.at[row_count,'abstract_id'] = 0\n",
        "      table.at[row_count,'confs'] = None\n",
        "      row_count += 1  \n",
        "      \n",
        "    IS_count += new_IS\n",
        "    if new_IS: bad_url.append(url);print(new_IS,url)\n",
        "    \n",
        "  print(table.shape)\n",
        "  # table = table[~table['url'].isin(bad_url)]\n",
        "  table.dropna(inplace=True,subset=['sentences'])\n",
        "  print(table.shape)\n",
        "  table[\"sum\"] = table[\"labels\"].apply(lambda x: 1 if sum(x) > 0 else 0)\n",
        "  temp = table[table[\"sum\"] > 0]; print(temp.shape,len(IS_count))\n",
        "  table.to_csv(\"sent_class.csv\")\n",
        "  print(table.head(5))\n",
        "  IS_record.to_csv(\"IS_records.csv\")\n",
        "  rules = pd.DataFrame()\n",
        "  rules[\"Coded_IS\"] = all_is\n",
        "  rules[\"URL\"] = urls\n",
        "  rules.to_csv(\"/content/ground_coded_data.csv\")\n",
        "  return table\n",
        "\n",
        "\n",
        "    # for IS in text:\n",
        "    #   IS_cand = [cand for cand in candidate if find_match(cand,IS)] \n",
        "    #   # if not IS_cand and len(tokenizer.encode(IS)) > 60: IS_cand += [IS]\n",
        "    #   matches += IS_cand\n",
        "    #   if IS_cand: IS_matches.append(IS) \n",
        "    #   else: continue\n",
        "    # # IS_matches = list(set(IS_matches))\n",
        "    # IS_miss = set(text).difference(set(IS_matches))\n",
        "    # matched = list(set(matches)) # + list(IS_miss)\n",
        "    # IS_count += len(IS_miss)\n",
        "    # # if matched: match_flag = True; \n",
        "\n",
        "    # candidate = []\n",
        "\n",
        "    # candidate = matched\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.5)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from stanza) (1.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.62.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce5e1d6cca2c4d00b2af7c974f4c5a4a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-17 05:33:14 INFO: Downloading default packages for language: en (English)...\n",
            "2022-02-17 05:33:15 INFO: File exists: /root/stanza_resources/en/default.zip.\n",
            "2022-02-17 05:33:20 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2022-02-17 05:33:20 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "========================\n",
            "\n",
            "2022-02-17 05:33:20 INFO: Use device: gpu\n",
            "2022-02-17 05:33:20 INFO: Loading: tokenize\n",
            "2022-02-17 05:33:20 INFO: Done loading processors!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iIFqMDYp9Jc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18b1ab8f-0bbe-48a3-bf0f-ffacc3f163c8"
      },
      "source": [
        "\n",
        "%cd /content\n",
        "!git config --global user.email \"mahaswec@usc.edu\"\n",
        "!git config --global user.name \"Mahasweta-usc\"\n",
        "!rm -rf  /content/sequential_sentence_classification\n",
        "!git clone https://github.com/Mahasweta-usc/sequential_sentence_classification/\n",
        "\n",
        "%cd /content/sequential_sentence_classification/ \n",
        "!git checkout cscw_rev\n",
        "!ls\n",
        "%pip install -r /content/sequential_sentence_classification/requirements.txt \n",
        "%pip install overrides==4.1.2 \n",
        "!wget https://storage.googleapis.com/tempvaxx/model_muting.tar.gz -P /content/sequential_sentence_classification/trained/\n",
        "# !rm -rf  /content/sequential_sentence_classification\n",
        "# !mkdir sequential_sentence_classification\n",
        "# %cd sequential_sentence_classification\n",
        "# !git init\n",
        "# !git remote add -t allennlp2 -f origin https://github.com/Mahasweta-usc/sequential_sentence_classification/\n",
        "# !git checkout allennlp2\n",
        "# !git remote add origin https://Mahasweta-usc:JohnNash1994@github.com/Mahasweta-usc/sequential_sentence_classification/\n",
        "# !git remote -v\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'sequential_sentence_classification'...\n",
            "remote: Enumerating objects: 813, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 813 (delta 102), reused 106 (delta 84), pack-reused 667\u001b[K\n",
            "Receiving objects: 100% (813/813), 3.17 MiB | 17.29 MiB/s, done.\n",
            "Resolving deltas: 100% (512/512), done.\n",
            "/content/sequential_sentence_classification\n",
            "Branch 'cscw_rev' set up to track remote branch 'cscw_rev' from 'origin'.\n",
            "Switched to a new branch 'cscw_rev'\n",
            "data\t     README.md\t       sequential_sentence_classification  trained\n",
            "history.txt  requirements.txt  setup.sh\n",
            "LICENSE      scripts\t       src\n",
            "Obtaining allennlp from git+git://github.com/ibeltagy/allennlp@ac2b21da6008d0e41d31192ea596153988c000a4#egg=allennlp (from -r /content/sequential_sentence_classification/requirements.txt (line 1))\n",
            "  Cloning git://github.com/ibeltagy/allennlp (to revision ac2b21da6008d0e41d31192ea596153988c000a4) to ./src/allennlp\n",
            "  Running command git clone -q git://github.com/ibeltagy/allennlp /content/sequential_sentence_classification/src/allennlp\n",
            "  Running command git rev-parse -q --verify 'sha^ac2b21da6008d0e41d31192ea596153988c000a4'\n",
            "  Running command git fetch -q git://github.com/ibeltagy/allennlp ac2b21da6008d0e41d31192ea596153988c000a4\n",
            "  Running command git checkout -q ac2b21da6008d0e41d31192ea596153988c000a4\n",
            "Requirement already satisfied: email-reply-parser==0.5.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/sequential_sentence_classification/requirements.txt (line 2)) (0.5.12)\n",
            "Requirement already satisfied: jsonlines==3.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/sequential_sentence_classification/requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (from -r /content/sequential_sentence_classification/requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: stanza==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/sequential_sentence_classification/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/sequential_sentence_classification/requirements.txt (line 6)) (4.62.3)\n",
            "Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/sequential_sentence_classification/requirements.txt (line 7)) (4.16.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.10.0+cu111)\n",
            "Collecting overrides\n",
            "  Downloading overrides-6.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.2.5)\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "  Downloading spacy-2.1.9-cp37-cp37m-manylinux1_x86_64.whl (30.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.21.5)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.21.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 81.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.1.4)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Collecting gevent>=1.3.6\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2018.9)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 78.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.6.4)\n",
            "Collecting flaky\n",
            "  Downloading flaky-3.7.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting responses>=0.7\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading numpydoc-1.2-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting conllu==1.3.1\n",
            "  Downloading conllu-1.3.1-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting parsimonious>=0.8.0\n",
            "  Downloading parsimonious-0.8.1.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.4.2)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "  Downloading pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 84.5 MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
            "\u001b[K     |████████████████████████████████| 592 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines==3.0.0->-r /content/sequential_sentence_classification/requirements.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines==3.0.0->-r /content/sequential_sentence_classification/requirements.txt (line 3)) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza==1.3.0->-r /content/sequential_sentence_classification/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza==1.3.0->-r /content/sequential_sentence_classification/requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from stanza==1.3.0->-r /content/sequential_sentence_classification/requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (4.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (0.11.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 63.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.1.2)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 81.6 MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: sphinx>=1.8 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.8.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.6 MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.8\n",
            "  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 66.6 MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 524 kB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.0.6)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.0.5)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.7.12)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.17.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.9.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (2.2.0)\n",
            "Collecting botocore<1.25.0,>=1.24.1\n",
            "  Downloading botocore-1.24.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 73.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (3.7.0)\n",
            "Collecting typing-utils>=0.0.3\n",
            "  Downloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r /content/sequential_sentence_classification/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.8->numpydoc>=0.8.0->allennlp->-r /content/sequential_sentence_classification/requirements.txt (line 1)) (1.1.5)\n",
            "Building wheels for collected packages: jsonnet, parsimonious, word2number\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994560 sha256=c5246a79bd731e9f6773301a7e248ca19e02e4c8f09105f123d5cfe220f519e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-py3-none-any.whl size=42723 sha256=215a587caa7d154941293117a9ee639d64f428c791465cd541f779d62a21c6b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/5d/ba/f27d8af07306b65ee44f9d3f9cadea1db749a421a6db8a99bf\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=01edb27798d0a0ff0c48962287acf7ff152069a619f4456e0988bbad4b7db7fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built jsonnet parsimonious word2number\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, preshed, plac, blis, zope.interface, zope.event, typing-utils, thinc, sentencepiece, boto3, word2number, unidecode, tensorboardX, spacy, responses, pytorch-transformers, pytorch-pretrained-bert, parsimonious, overrides, numpydoc, jsonpickle, jsonnet, gevent, ftfy, flask-cors, flaky, conllu, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.6\n",
            "    Uninstalling preshed-3.0.6:\n",
            "      Successfully uninstalled preshed-3.0.6\n",
            "  Attempting uninstall: plac\n",
            "    Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Running setup.py develop for allennlp\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed allennlp-0.9.0-unreleased blis-0.2.4 boto3-1.21.1 botocore-1.24.1 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.10 ftfy-6.1.1 gevent-21.12.0 jmespath-0.10.0 jsonnet-0.18.0 jsonpickle-2.1.0 numpydoc-1.2 overrides-6.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.18.0 s3transfer-0.5.1 sentencepiece-0.1.96 spacy-2.1.9 tensorboardX-2.4.1 thinc-7.0.8 typing-utils-0.1.0 unidecode-1.3.2 urllib3-1.25.11 word2number-1.1 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting overrides==4.1.2\n",
            "  Downloading overrides-4.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from overrides==4.1.2) (0.1.0)\n",
            "Installing collected packages: overrides\n",
            "  Attempting uninstall: overrides\n",
            "    Found existing installation: overrides 6.1.0\n",
            "    Uninstalling overrides-6.1.0:\n",
            "      Successfully uninstalled overrides-6.1.0\n",
            "Successfully installed overrides-4.1.2\n",
            "--2022-02-17 05:09:13--  https://storage.googleapis.com/tempvaxx/model_muting.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.135.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-02-17 05:09:14 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "# os.environ[\"SENT_MAX_LEN\"] = str(MAX_LEN)\n",
        "# os.environ[\"NUM_EPOCHS\"] = \"6\"\n",
        "# os.environ[\"BATCH_SIZE\"] = \"16\"\n",
        "# os.environ[\"MAX_SENT_PER_EXAMPLE\"] = str(MAX_LEN)\n",
        "# os.environ[\"BERT_VOCAB\"] = \"bert-base-uncased\"\n",
        "# os.environ[\"BERT_WEIGHTS\"] = \"bert-base-uncased\"\n",
        "# os.environ[\"SEED\"] = \"72270\"\n",
        "policy_data = pd.read_csv(\"/content/policy.csv\")\n",
        "print(policy_data.columns)\n",
        "policy_data[\"sentences\"] = policy_data[\"policy.statement\"].apply(lambda x: sent_break(x))\n",
        "policy_data[\"labels\"] = policy_data[\"sentences\"].apply(lambda x: [1]*len(x))\n",
        "policy_data[\"abstract_id\"] = [0]*policy_data.shape[0] \n",
        "print(policy_data.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4vU9HRU8dsB",
        "outputId": "d83db359-fb05-46a8-fc50-57b57f9ca4d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['policy.doc.name', 'section.name', 'policy.statement',\n",
            "       'ostrom.statement.class', 'rule.category', 'rule.subcategory',\n",
            "       'cost.incurred.by', 'benefit.accrued.to'],\n",
            "      dtype='object')\n",
            "    policy.doc.name       section.name  ... labels abstract_id\n",
            "0  Incubator Policy  podling reporting  ...    [1]           0\n",
            "1  Incubator Policy  podling reporting  ...    [1]           0\n",
            "\n",
            "[2 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''check performance on multiple splits'''\n",
        "!rm -rf tmp_output_dir*\n",
        "print(scraped_data.shape)\n",
        "import os,nltk\n",
        "nltk.download('punkt')\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold, KFold\n",
        "from transformers import BertTokenizer\n",
        "MAX_LEN = 256\n",
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import tensorflow as tf\n",
        "from tensorflow.core.util import event_pb2\n",
        "import glob\n",
        "import statistics\n",
        "\n",
        "\n",
        "\n",
        "def df_to_json(file_, table):\n",
        "\tfor idx, row in table.iterrows():\n",
        "\t\tentry = dict()\n",
        "\t\tentry[\"sentences\"] = row[\"sentences\"]\n",
        "\t\tentry[\"labels\"] = row[\"labels\"]\n",
        "\t\tentry[\"abstract_id\"] = row[\"abstract_id\"]\n",
        "\t\t# entry[\"url\"] = row[\"url\"]\n",
        "\t\t# print(entry)\n",
        "\t\twith jsonlines.open(file_, mode='a') as writer:\n",
        "\t\t  writer.write(entry)\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# _ = create_data(scraped_data)\n",
        "\n",
        "%cd /content/sequential_sentence_classification/\n",
        "oversample = RandomOverSampler(sampling_strategy=1,random_state=42)\n",
        "scraped_train, scraped_test, _,_ = train_test_split(scraped_data, scraped_data.label.values, test_size=0.125, random_state=42, shuffle=True, stratify=scraped_data.label.values)\n",
        "scraped_train.to_csv(\"/content/training.csv\")\n",
        "scraped_test.to_csv(\"/content/test.csv\")\n",
        "#Check multiple splits\n",
        "\n",
        "\n",
        "seeds = [72270 + 1000*i for  i in range(20)]\n",
        "performances = []\n",
        "\n",
        "for index in [\"dev\"]:\n",
        "\tX_test = create_data(scraped_test)\n",
        "\t# X_test = create_data(scraped_data)\n",
        "\tX_train = create_data(scraped_train)\n",
        "\t#segment size length\n",
        "\tseg_lengths = [len(x) for x in X_train['sentences'].tolist()]\n",
        "\tprint(\"num sentences\",np.mean(seg_lengths),min(seg_lengths))\n",
        "\t#add policy data\n",
        "\tX_train[\"labels\"] = X_train[\"labels\"].apply(tuple)\n",
        "\tX_train[\"sentences\"] = X_train[\"sentences\"].apply(tuple)\n",
        "\tprint(X_train.columns, X_train.shape[0])\n",
        "\tX_train = pd.concat([X_train,policy_data], axis=0, ignore_index=True)\n",
        "\tprint(X_train.columns, X_train.shape[0])\n",
        "\tX_train = X_train.dropna(axis=1)\n",
        "\tprint(X_train.columns, X_train.shape[0])\n",
        "\tX_train[\"sum\"] = X_train[\"labels\"].apply(lambda x: int(sum(x) > 0))\n",
        "\t#resample to 1:1\n",
        "\tresampled, _ = oversample.fit_resample(X_train,X_train[\"sum\"].values)\n",
        "\tX_train = pd.DataFrame(resampled,columns=X_train.columns)\n",
        "\tprint(X_train.columns, X_train.shape[0])\n",
        "\tX_train[\"labels\"] = X_train[\"labels\"].apply(list)\n",
        "\tX_train[\"sentences\"] = X_train[\"sentences\"].apply(list)\n",
        "\tos.system(\"rm -rf /content/*.jsonl\")\n",
        "\tos.system(\"rm -rf tmp_output_dir*\")\n",
        "\n",
        "\tdf_to_json(\"/content/test.jsonl\",X_test)\n",
        "\tdf_to_json(\"/content/train.jsonl\",X_train)\n",
        "\tos.system(\"rm /content/sequential*/data/CSAbstruct/*.jsonl\")\n",
        "\tos.system(\"cp /content/test.jsonl /content/dev.jsonl\")\n",
        "\tos.system(\"cp /content/*.jsonl /content/sequential*/data/CSAbstruct/\")\n",
        "\t# for round, seed in enumerate(seeds):\n",
        "\tos.environ[\"TRAINING_DATA_INSTANCES\"] = str(X_train.shape[0])\n",
        "\tos.system(\"scripts/train.sh tmp_output_dir_{} >> /content/output.txt\".format(index))\n",
        "\tos.system(\"cp /content/*.jsonl tmp_output_dir_{}/\".format(index))\n",
        "\n",
        "\tf = glob.glob(\"/content/sequential_sentence_classification/tmp_output_dir_{}/metrics_epoch_*.json\".format(index))\n",
        "\tF1 = []\n",
        "\tfor file_ in f:\n",
        "\t\twith open(file_) as ip:\n",
        "\t\t  json_data = json.load(ip)\n",
        "\t\t  F1.append(json_data[\"validation_1_labelF\"])\n",
        "\tperformances.append(max(F1))\n",
        "\tprint(performances, \"\\n\")\n",
        "\tprint(max(performances),statistics.mean(performances),statistics.median(performances))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-PPGtp0JnCT",
        "outputId": "fb7aaa62-ec92-4efa-eccb-88f833463953"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(313, 6)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (3.10.0.2)\n",
            "Loading BERT tokenizer...\n",
            "/content/sequential_sentence_classification\n",
            "1\n",
            "0 0\n",
            "5\n",
            "0 0\n",
            "19\n",
            "1 0\n",
            "32\n",
            "1 1\n",
            "41\n",
            "1 1\n",
            "47\n",
            "1 1\n",
            "55\n",
            "1 1\n",
            "99\n",
            "1 1\n",
            "140\n",
            "1 1\n",
            "154\n",
            "5 1\n",
            "176\n",
            "5 5\n",
            "182\n",
            "5 5\n",
            "194\n",
            "5 5\n",
            "202\n",
            "5 5\n",
            "226\n",
            "16 5\n",
            "249\n",
            "22 16\n",
            "['IPMC Voting can be found here: http://mail-archives.apache.org/mod_mbox/incubator-general/ 201702.mbox/%%%% 3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%%%%3e < http://mail-archives.apache.org/mod_mbox/incubator-general/ 201702.mbox/%%%% 3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%%%%3E'] http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CABekQG4OMFKgXzaKnON+zYJAZGYNiM=pvTJf8k-DFmmuAH6L+A@mail.gmail.com>\n",
            "369\n",
            "24 22\n",
            "409\n",
            "25 24\n",
            "413\n",
            "25 25\n",
            "419\n",
            "25 25\n",
            "437\n",
            "25 25\n",
            "464\n",
            "39 25\n",
            "494\n",
            "41 39\n",
            "524\n",
            "41 41\n",
            "538\n",
            "42 41\n",
            "555\n",
            "42 42\n",
            "567\n",
            "42 42\n",
            "577\n",
            "42 42\n",
            "638\n",
            "42 42\n",
            "669\n",
            "42 42\n",
            "673\n",
            "42 42\n",
            "725\n",
            "42 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750\n",
            "42 42\n",
            "754\n",
            "42 42\n",
            "790\n",
            "42 42\n",
            "802\n",
            "42 42\n",
            "820\n",
            "42 42\n",
            "823\n",
            "42 42\n",
            "839\n",
            "42 42\n",
            "847\n",
            "42 42\n",
            "(381, 5)\n",
            "(381, 5)\n",
            "(49, 6) 1\n",
            "                                           sentences  ... sum\n",
            "0  [See <https://builds.apache.org/job/provisionr...  ...   0\n",
            "1  [I was wondering how to modify window options ...  ...   0\n",
            "2  [Maybe worth to think about moving parts of th...  ...   0\n",
            "3  [Simply not enough free timeslots as I had hop...  ...   0\n",
            "4  [Kind regards, Andreas On Wed, Sep 2, 2015 at ...  ...   0\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "8\n",
            "2 0\n",
            "20\n",
            "2 2\n",
            "46\n",
            "3 2\n",
            "54\n",
            "3 3\n",
            "83\n",
            "3 3\n",
            "93\n",
            "3 3\n",
            "138\n",
            "7 3\n",
            "145\n",
            "7 7\n",
            "175\n",
            "7 7\n",
            "226\n",
            "21 7\n",
            "230\n",
            "21 21\n",
            "241\n",
            "26 21\n",
            "276\n",
            "26 26\n",
            "288\n",
            "26 26\n",
            "297\n",
            "26 26\n",
            "311\n",
            "26 26\n",
            "324\n",
            "27 26\n",
            "339\n",
            "27 27\n",
            "347\n",
            "27 27\n",
            "364\n",
            "27 27\n",
            "373\n",
            "27 27\n",
            "387\n",
            "27 27\n",
            "398\n",
            "27 27\n",
            "414\n",
            "30 27\n",
            "424\n",
            "36 30\n",
            "430\n",
            "36 36\n",
            "440\n",
            "39 36\n",
            "454\n",
            "39 39\n",
            "474\n",
            "41 39\n",
            "487\n",
            "41 41\n",
            "492\n",
            "41 41\n",
            "493\n",
            "41 41\n",
            "499\n",
            "41 41\n",
            "529\n",
            "41 41\n",
            "534\n",
            "44 41\n",
            "627\n",
            "44 44\n",
            "633\n",
            "44 44\n",
            "660\n",
            "44 44\n",
            "668\n",
            "44 44\n",
            "703\n",
            "44 44\n",
            "711\n",
            "44 44\n",
            "716\n",
            "44 44\n",
            "747\n",
            "44 44\n",
            "760\n",
            "44 44\n",
            "766\n",
            "44 44\n",
            "786\n",
            "44 44\n",
            "810\n",
            "44 44\n",
            "818\n",
            "44 44\n",
            "829\n",
            "44 44\n",
            "845\n",
            "44 44\n",
            "852\n",
            "44 44\n",
            "943\n",
            "51 44\n",
            "959\n",
            "52 51\n",
            "975\n",
            "52 52\n",
            "999\n",
            "55 52\n",
            "1017\n",
            "57 55\n",
            "1028\n",
            "57 57\n",
            "1118\n",
            "57 57\n",
            "1141\n",
            "57 57\n",
            "1144\n",
            "57 57\n",
            "1213\n",
            "57 57\n",
            "1223\n",
            "57 57\n",
            "1234\n",
            "57 57\n",
            "1237\n",
            "57 57\n",
            "1276\n",
            "57 57\n",
            "1310\n",
            "57 57\n",
            "1315\n",
            "57 57\n",
            "1324\n",
            "57 57\n",
            "1332\n",
            "57 57\n",
            "1335\n",
            "57 57\n",
            "1350\n",
            "62 57\n",
            "1361\n",
            "62 62\n",
            "1369\n",
            "62 62\n",
            "1392\n",
            "62 62\n",
            "1437\n",
            "62 62\n",
            "1445\n",
            "62 62\n",
            "1448\n",
            "64 62\n",
            "1455\n",
            "64 64\n",
            "1543\n",
            "65 64\n",
            "1550\n",
            "65 65\n",
            "1560\n",
            "65 65\n",
            "1569\n",
            "65 65\n",
            "1577\n",
            "65 65\n",
            "1600\n",
            "65 65\n",
            "1620\n",
            "65 65\n",
            "1626\n",
            "65 65\n",
            "1640\n",
            "65 65\n",
            "1648\n",
            "65 65\n",
            "1680\n",
            "65 65\n",
            "1694\n",
            "65 65\n",
            "1702\n",
            "65 65\n",
            "1723\n",
            "65 65\n",
            "1730\n",
            "65 65\n",
            "1767\n",
            "80 65\n",
            "1839\n",
            "80 80\n",
            "1844\n",
            "80 80\n",
            "1865\n",
            "80 80\n",
            "1870\n",
            "80 80\n",
            "1881\n",
            "80 80\n",
            "1907\n",
            "80 80\n",
            "1917\n",
            "80 80\n",
            "1926\n",
            "81 80\n",
            "1933\n",
            "83 81\n",
            "1941\n",
            "83 83\n",
            "1964\n",
            "83 83\n",
            "1974\n",
            "83 83\n",
            "2009\n",
            "83 83\n",
            "2023\n",
            "83 83\n",
            "2029\n",
            "83 83\n",
            "2034\n",
            "83 83\n",
            "2048\n",
            "83 83\n",
            "2070\n",
            "83 83\n",
            "2087\n",
            "84 83\n",
            "2111\n",
            "87 84\n",
            "2124\n",
            "90 87\n",
            "2150\n",
            "90 90\n",
            "2154\n",
            "90 90\n",
            "2171\n",
            "90 90\n",
            "2178\n",
            "90 90\n",
            "2182\n",
            "90 90\n",
            "2201\n",
            "90 90\n",
            "2230\n",
            "90 90\n",
            "2263\n",
            "90 90\n",
            "2270\n",
            "90 90\n",
            "2409\n",
            "90 90\n",
            "2419\n",
            "90 90\n",
            "2437\n",
            "90 90\n",
            "2454\n",
            "90 90\n",
            "2459\n",
            "90 90\n",
            "2503\n",
            "90 90\n",
            "2514\n",
            "90 90\n",
            "2528\n",
            "90 90\n",
            "2560\n",
            "90 90\n",
            "2564\n",
            "90 90\n",
            "2578\n",
            "90 90\n",
            "2601\n",
            "91 90\n",
            "2604\n",
            "91 91\n",
            "2615\n",
            "91 91\n",
            "2629\n",
            "94 91\n",
            "2664\n",
            "94 94\n",
            "2699\n",
            "95 94\n",
            "2705\n",
            "95 95\n",
            "2711\n",
            "95 95\n",
            "2715\n",
            "95 95\n",
            "2731\n",
            "95 95\n",
            "2740\n",
            "95 95\n",
            "2744\n",
            "98 95\n",
            "2781\n",
            "98 98\n",
            "2785\n",
            "100 98\n",
            "2818\n",
            "101 100\n",
            "2827\n",
            "101 101\n",
            "2833\n",
            "101 101\n",
            "2835\n",
            "101 101\n",
            "2866\n",
            "101 101\n",
            "2897\n",
            "101 101\n",
            "2921\n",
            "101 101\n",
            "2937\n",
            "101 101\n",
            "2974\n",
            "107 101\n",
            "['IPMC Voting can be found here: http://mail-archives.apache.org/mod_mbox/incubator-general/ 201702.mbox/%%%% 3c676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%%%%3e < http://mail-archives.apache.org/mod_mbox/incubator-general/ 201702.mbox/%%%% 3C676BDC9F-1B55-4469-92A7-9FF309AD0EC8@gmail.com%%%%3E'] http://mail-archives.apache.org/mod_mbox/airflow-dev/201702.mbox/<CANLtMicNtNTysxT+xRmdW2Jry2VMJXuMmwx146PkPDu356utOg@mail.gmail.com>\n",
            "2985\n",
            "107 107\n",
            "3053\n",
            "115 107\n",
            "3076\n",
            "115 115\n",
            "3085\n",
            "115 115\n",
            "3097\n",
            "115 115\n",
            "3124\n",
            "115 115\n",
            "3161\n",
            "115 115\n",
            "3164\n",
            "115 115\n",
            "3194\n",
            "115 115\n",
            "3257\n",
            "118 115\n",
            "3269\n",
            "118 118\n",
            "3278\n",
            "118 118\n",
            "3288\n",
            "118 118\n",
            "3296\n",
            "121 118\n",
            "3298\n",
            "124 121\n",
            "3351\n",
            "125 124\n",
            "3371\n",
            "128 125\n",
            "3385\n",
            "128 128\n",
            "3414\n",
            "128 128\n",
            "3420\n",
            "128 128\n",
            "3429\n",
            "128 128\n",
            "3547\n",
            "131 128\n",
            "3560\n",
            "136 131\n",
            "3587\n",
            "137 136\n",
            "3594\n",
            "137 137\n",
            "3601\n",
            "137 137\n",
            "3651\n",
            "137 137\n",
            "3660\n",
            "137 137\n",
            "3669\n",
            "137 137\n",
            "3682\n",
            "137 137\n",
            "3688\n",
            "137 137\n",
            "3696\n",
            "137 137\n",
            "3710\n",
            "137 137\n",
            "3721\n",
            "137 137\n",
            "3734\n",
            "137 137\n",
            "3738\n",
            "137 137\n",
            "3755\n",
            "137 137\n",
            "3792\n",
            "137 137\n",
            "3889\n",
            "137 137\n",
            "3935\n",
            "137 137\n",
            "4049\n",
            "137 137\n",
            "4054\n",
            "137 137\n",
            "4074\n",
            "137 137\n",
            "4100\n",
            "139 137\n",
            "4109\n",
            "139 139\n",
            "4145\n",
            "139 139\n",
            "4153\n",
            "139 139\n",
            "4157\n",
            "139 139\n",
            "4201\n",
            "139 139\n",
            "4220\n",
            "139 139\n",
            "4238\n",
            "144 139\n",
            "4245\n",
            "145 144\n",
            "4273\n",
            "158 145\n",
            "4276\n",
            "158 158\n",
            "4287\n",
            "158 158\n",
            "4304\n",
            "158 158\n",
            "4342\n",
            "158 158\n",
            "4366\n",
            "158 158\n",
            "4376\n",
            "158 158\n",
            "4380\n",
            "158 158\n",
            "4391\n",
            "158 158\n",
            "4402\n",
            "158 158\n",
            "4432\n",
            "159 158\n",
            "4439\n",
            "159 159\n",
            "4451\n",
            "159 159\n",
            "4458\n",
            "163 159\n",
            "4467\n",
            "163 163\n",
            "4480\n",
            "163 163\n",
            "4546\n",
            "182 163\n",
            "4559\n",
            "182 182\n",
            "4681\n",
            "182 182\n",
            "4696\n",
            "182 182\n",
            "4710\n",
            "182 182\n",
            "4712\n",
            "182 182\n",
            "4761\n",
            "189 182\n",
            "4769\n",
            "189 189\n",
            "4793\n",
            "190 189\n",
            "4801\n",
            "190 190\n",
            "4831\n",
            "190 190\n",
            "4870\n",
            "198 190\n",
            "4898\n",
            "198 198\n",
            "4904\n",
            "202 198\n",
            "4908\n",
            "202 202\n",
            "4922\n",
            "202 202\n",
            "4952\n",
            "203 202\n",
            "4955\n",
            "204 203\n",
            "5062\n",
            "204 204\n",
            "5067\n",
            "204 204\n",
            "5073\n",
            "204 204\n",
            "5087\n",
            "205 204\n",
            "5097\n",
            "205 205\n",
            "5120\n",
            "205 205\n",
            "5224\n",
            "205 205\n",
            "5258\n",
            "205 205\n",
            "5266\n",
            "205 205\n",
            "5314\n",
            "206 205\n",
            "5325\n",
            "207 206\n",
            "5369\n",
            "214 207\n",
            "5374\n",
            "214 214\n",
            "5400\n",
            "214 214\n",
            "5510\n",
            "214 214\n",
            "5511\n",
            "214 214\n",
            "5521\n",
            "214 214\n",
            "5526\n",
            "214 214\n",
            "5536\n",
            "214 214\n",
            "5547\n",
            "214 214\n",
            "5574\n",
            "218 214\n",
            "5575\n",
            "218 218\n",
            "5578\n",
            "219 218\n",
            "5582\n",
            "219 219\n",
            "5585\n",
            "221 219\n",
            "5592\n",
            "221 221\n",
            "5628\n",
            "221 221\n",
            "5656\n",
            "231 221\n",
            "5664\n",
            "231 231\n",
            "(2850, 5)\n",
            "(2850, 5)\n",
            "(293, 6) 1\n",
            "                                           sentences  ... sum\n",
            "0  [Happy to take the extra time to facilitate a ...  ...   1\n",
            "1  [Hi I updated the jackrabbit version from 0.9....  ...   0\n",
            "2  [caused by : javax . jcr . repositoryexception...  ...   0\n",
            "3  [caused by : java . lang . arrayindexoutofboun...  ...   0\n",
            "4  [That's only a recent addition, and I've only ...  ...   0\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "num sentences 6.339298245614035 2\n",
            "Index(['sentences', 'labels', 'abstract_id', 'confs', 'url', 'sum'], dtype='object') 2850\n",
            "Index(['sentences', 'labels', 'abstract_id', 'confs', 'url', 'sum',\n",
            "       'policy.doc.name', 'section.name', 'policy.statement',\n",
            "       'ostrom.statement.class', 'rule.category', 'rule.subcategory',\n",
            "       'cost.incurred.by', 'benefit.accrued.to'],\n",
            "      dtype='object') 3177\n",
            "Index(['sentences', 'labels', 'abstract_id'], dtype='object') 3177\n",
            "Index(['sentences', 'labels', 'abstract_id', 'sum'], dtype='object') 5114\n",
            "[0.6938775181770325] \n",
            "\n",
            "0.6938775181770325 0.6938775181770325 0.6938775181770325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scripts/train.sh tmp_output_dir_{} >> /content/output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMPLIPVdqUBD",
        "outputId": "fddeba5b-f632-4fa6-ac82-94a56d1fe05b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-17 03:10:59,930 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2022-02-17 03:11:00,354 - INFO - pytorch_transformers.modeling_bert - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2022-02-17 03:11:00,356 - INFO - pytorch_transformers.modeling_xlnet - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2022-02-17 03:11:00,658 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:00,658 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:00,659 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:00,659 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:00,740 - INFO - allennlp.common.params - random_seed = 72270\n",
            "2022-02-17 03:11:00,740 - INFO - allennlp.common.params - numpy_seed = 722\n",
            "2022-02-17 03:11:00,740 - INFO - allennlp.common.params - pytorch_seed = 7227\n",
            "2022-02-17 03:11:00,747 - INFO - allennlp.common.checks - Pytorch version: 1.10.0+cu111\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.params - evaluate_on_test = True\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': False, 'max_sent_per_example': '256', 'sci_sum': False, 'sci_sum_fake_scores': False, 'sent_max_len': '256', 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained', 'use_starting_offsets': False}}, 'type': 'SeqClassificationReader', 'use_abstract_scores': False, 'use_sep': 'true', 'word_splitter': 'bert-basic'} and extras set()\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.params - dataset_reader.type = SeqClassificationReader\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.from_params - instantiating class <class 'sequential_sentence_classification.dataset_reader.SeqClassificationReader'> from params {'lazy': False, 'max_sent_per_example': '256', 'sci_sum': False, 'sci_sum_fake_scores': False, 'sent_max_len': '256', 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained', 'use_starting_offsets': False}}, 'use_abstract_scores': False, 'use_sep': 'true', 'word_splitter': 'bert-basic'} and extras set()\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2022-02-17 03:11:00,748 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'do_lowercase': True, 'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained', 'use_starting_offsets': False} and extras set()\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = bert-pretrained\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer'> from params {'do_lowercase': True, 'pretrained_model': 'bert-base-cased', 'use_starting_offsets': False} and extras set()\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.pretrained_model = bert-base-cased\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.use_starting_offsets = False\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.do_lowercase = True\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.never_lowercase = None\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_pieces = 512\n",
            "2022-02-17 03:11:00,749 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.truncate_long_sequences = True\n",
            "2022-02-17 03:11:00,749 - WARNING - allennlp.data.token_indexers.wordpiece_indexer - Your BERT model appears to be cased, but your indexer is lowercasing tokens.\n",
            "2022-02-17 03:11:00,750 - WARNING - pytorch_pretrained_bert.tokenization - The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
            "2022-02-17 03:11:01,026 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.params - dataset_reader.word_splitter = bert-basic\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.registrable - instantiating registered subclass bert-basic of <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'>\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.params - dataset_reader.sent_max_len = 256\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.params - dataset_reader.max_sent_per_example = 256\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.params - dataset_reader.use_sep = true\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.params - dataset_reader.sci_sum = False\n",
            "2022-02-17 03:11:01,057 - INFO - allennlp.common.params - dataset_reader.use_abstract_scores = False\n",
            "2022-02-17 03:11:01,058 - INFO - allennlp.common.params - dataset_reader.sci_sum_fake_scores = False\n",
            "2022-02-17 03:11:01,058 - INFO - allennlp.common.params - dataset_reader.predict = False\n",
            "2022-02-17 03:11:01,191 - INFO - allennlp.common.params - train_data_path = data/CSAbstruct/train.jsonl\n",
            "2022-02-17 03:11:01,191 - INFO - allennlp.training.util - Reading training data from data/CSAbstruct/train.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "2021it [00:10, 202.05it/s]\n",
            "3518it [00:14, 250.89it/s]\n",
            "\n",
            "2022-02-17 03:11:15,214 - INFO - allennlp.common.params - validation_data_path = data/CSAbstruct/dev.jsonl\n",
            "2022-02-17 03:11:15,214 - INFO - allennlp.training.util - Reading validation data from data/CSAbstruct/dev.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "268it [00:02, 132.03it/s]\n",
            "\n",
            "2022-02-17 03:11:17,244 - INFO - allennlp.common.params - test_data_path = data/CSAbstruct/test.jsonl\n",
            "2022-02-17 03:11:17,244 - INFO - allennlp.training.util - Reading test data from data/CSAbstruct/test.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "268it [00:01, 157.91it/s]\n",
            "\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.training.trainer_pieces - From dataset instances, test, train, validation will be considered for vocabulary creation.\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.type = None\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.extend = False\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.directory_path = None\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.min_count = None\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.pretrained_files = {}\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n",
            "2022-02-17 03:11:18,947 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "0it [00:00, ?it/s]\n",
            "4054it [00:00, 35320.27it/s]\n",
            "\n",
            "2022-02-17 03:11:19,063 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'additional_feature_size': 0, 'bert_dropout': 0.1, 'sci_sum': False, 'self_attn': {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100, 'type': 'stacked_self_attention'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-cased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'}}}, 'type': 'SeqClassificationModel', 'use_sep': 'true', 'with_crf': 'false'} and extras {'vocab'}\n",
            "2022-02-17 03:11:19,063 - INFO - allennlp.common.params - model.type = SeqClassificationModel\n",
            "2022-02-17 03:11:19,063 - INFO - allennlp.common.from_params - instantiating class <class 'sequential_sentence_classification.model.SeqClassificationModel'> from params {'additional_feature_size': 0, 'bert_dropout': 0.1, 'sci_sum': False, 'self_attn': {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100, 'type': 'stacked_self_attention'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-cased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'}}}, 'use_sep': 'true', 'with_crf': 'false'} and extras {'vocab'}\n",
            "2022-02-17 03:11:19,063 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-cased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'}}} and extras {'vocab'}\n",
            "2022-02-17 03:11:19,063 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2022-02-17 03:11:19,063 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = True\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'bert-base-cased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'} and extras {'vocab'}\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.type = bert-pretrained\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'pretrained_model': 'bert-base-cased', 'requires_grad': 'all', 'top_layer_only': False} and extras {'vocab'}\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.pretrained_model = bert-base-cased\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.requires_grad = all\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
            "2022-02-17 03:11:19,064 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.scalar_mix_parameters = None\n",
            "2022-02-17 03:11:19,351 - INFO - pytorch_pretrained_bert.modeling - loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "2022-02-17 03:11:19,352 - INFO - pytorch_pretrained_bert.modeling - extracting archive file /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /tmp/tmpkwzryv96\n",
            "2022-02-17 03:11:22,850 - INFO - pytorch_pretrained_bert.modeling - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "2022-02-17 03:11:24,517 - INFO - allennlp.common.params - model.use_sep = true\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.with_crf = false\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100, 'type': 'stacked_self_attention'} and extras {'vocab'}\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.type = stacked_self_attention\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder'> from params {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100} and extras {'vocab'}\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.input_dim = 768\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.hidden_dim = 100\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.projection_dim = 100\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.feedforward_hidden_dim = 50\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.num_layers = 2\n",
            "2022-02-17 03:11:24,518 - INFO - allennlp.common.params - model.self_attn.num_attention_heads = 2\n",
            "2022-02-17 03:11:24,519 - INFO - allennlp.common.params - model.self_attn.use_positional_encoding = True\n",
            "2022-02-17 03:11:24,519 - INFO - allennlp.common.params - model.self_attn.dropout_prob = 0.1\n",
            "2022-02-17 03:11:24,519 - INFO - allennlp.common.params - model.self_attn.residual_dropout_prob = 0.2\n",
            "2022-02-17 03:11:24,519 - INFO - allennlp.common.params - model.self_attn.attention_dropout_prob = 0.1\n",
            "2022-02-17 03:11:24,519 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:24,519 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:24,520 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:24,520 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-02-17 03:11:24,522 - INFO - allennlp.common.params - model.bert_dropout = 0.1\n",
            "2022-02-17 03:11:24,522 - INFO - allennlp.common.params - model.sci_sum = False\n",
            "2022-02-17 03:11:24,522 - INFO - allennlp.common.params - model.additional_feature_size = 0\n",
            "2022-02-17 03:11:24,524 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'biggest_batch_first': True, 'cache_instances': True, 'sorting_keys': [['sentences', 'num_fields']], 'type': 'bucket'} and extras set()\n",
            "2022-02-17 03:11:24,524 - INFO - allennlp.common.params - iterator.type = bucket\n",
            "2022-02-17 03:11:24,524 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'biggest_batch_first': True, 'cache_instances': True, 'sorting_keys': [['sentences', 'num_fields']]} and extras set()\n",
            "2022-02-17 03:11:24,524 - INFO - allennlp.common.params - iterator.sorting_keys = [['sentences', 'num_fields']]\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.padding_noise = 0.1\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.biggest_batch_first = True\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.batch_size = 16\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.cache_instances = True\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.track_epoch = False\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - iterator.skip_smaller_batches = False\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - validation_iterator = None\n",
            "2022-02-17 03:11:24,525 - INFO - allennlp.common.params - trainer.no_grad = ()\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - Following parameters are Frozen  (without gradient):\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - Following parameters are Tunable (with gradient):\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
            "2022-02-17 03:11:24,527 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
            "2022-02-17 03:11:24,528 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,529 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,530 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,531 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
            "2022-02-17 03:11:24,532 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,596 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,597 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
            "2022-02-17 03:11:24,598 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
            "2022-02-17 03:11:24,599 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,600 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,601 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,602 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
            "2022-02-17 03:11:24,603 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_0._linear_layers.0.weight\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_0._linear_layers.0.bias\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_0._linear_layers.1.weight\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_0._linear_layers.1.bias\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_layer_norm_0.gamma\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_layer_norm_0.beta\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_0._combined_projection.weight\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_0._combined_projection.bias\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_0._output_projection.weight\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_0._output_projection.bias\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.layer_norm_0.gamma\n",
            "2022-02-17 03:11:24,604 - INFO - allennlp.training.trainer_pieces - self_attn.layer_norm_0.beta\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_1._linear_layers.0.weight\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_1._linear_layers.0.bias\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_1._linear_layers.1.weight\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_1._linear_layers.1.bias\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_layer_norm_1.gamma\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.feedforward_layer_norm_1.beta\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_1._combined_projection.weight\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_1._combined_projection.bias\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_1._output_projection.weight\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.self_attention_1._output_projection.bias\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.layer_norm_1.gamma\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - self_attn.layer_norm_1.beta\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - time_distributed_aggregate_feedforward._module.weight\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.training.trainer_pieces - time_distributed_aggregate_feedforward._module.bias\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.common.params - trainer.patience = 5\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.common.params - trainer.min_delta = 0.001\n",
            "2022-02-17 03:11:24,605 - INFO - allennlp.common.params - trainer.validation_metric = +acc\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.shuffle = True\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.num_epochs = 6\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.grad_clipping = 1\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.fp16 = False\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.gradient_accumulation_batch_size = 32\n",
            "2022-02-17 03:11:24,606 - INFO - allennlp.common.params - trainer.num_steps_reset_metrics = None\n",
            "2022-02-17 03:11:27,648 - INFO - allennlp.common.params - trainer.optimizer.type = bert_adam\n",
            "2022-02-17 03:11:27,648 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2022-02-17 03:11:27,648 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2022-02-17 03:11:27,648 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups.0.1.weight_decay = 0\n",
            "2022-02-17 03:11:27,650 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2022-02-17 03:11:27,650 - INFO - allennlp.training.optimizers - Group 0: ['text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias', 'self_attn.feedforward_1._linear_layers.1.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'self_attn.feedforward_1._linear_layers.0.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias', 'self_attn.self_attention_0._combined_projection.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias', 'self_attn.self_attention_0._output_projection.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias', 'self_attn.feedforward_0._linear_layers.1.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias', 'self_attn.feedforward_0._linear_layers.0.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias', 'self_attn.self_attention_1._combined_projection.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'self_attn.self_attention_1._output_projection.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias', 'time_distributed_aggregate_feedforward._module.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias'], {'weight_decay': 0}\n",
            "2022-02-17 03:11:27,719 - INFO - allennlp.training.optimizers - Group 1: ['text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight', 'self_attn.layer_norm_1.gamma', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight', 'self_attn.feedforward_0._linear_layers.1.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight', 'self_attn.feedforward_1._linear_layers.0.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight', 'self_attn.layer_norm_1.beta', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3', 'text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight', 'self_attn.layer_norm_0.gamma', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight', 'self_attn.feedforward_1._linear_layers.1.weight', 'self_attn.self_attention_1._combined_projection.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2', 'self_attn.feedforward_layer_norm_1.gamma', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight', 'self_attn.self_attention_0._output_projection.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.gamma', 'self_attn.self_attention_0._combined_projection.weight', 'self_attn.feedforward_0._linear_layers.0.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1', 'self_attn.feedforward_layer_norm_0.beta', 'self_attn.feedforward_layer_norm_1.beta', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight', 'self_attn.layer_norm_0.beta', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight', 'self_attn.self_attention_1._output_projection.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight', 'time_distributed_aggregate_feedforward._module.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight', 'self_attn.feedforward_layer_norm_0.gamma', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10'], {}\n",
            "2022-02-17 03:11:27,720 - WARNING - allennlp.training.optimizers - When constructing parameter groups,  layer_norm.weight not match any parameter name\n",
            "2022-02-17 03:11:27,720 - INFO - allennlp.training.optimizers - Number of trainable parameters: 108447123\n",
            "2022-02-17 03:11:27,720 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\n",
            "2022-02-17 03:11:27,720 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2022-02-17 03:11:27,721 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2022-02-17 03:11:27,721 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-5\n",
            "2022-02-17 03:11:27,721 - INFO - allennlp.common.params - trainer.optimizer.max_grad_norm = 1\n",
            "2022-02-17 03:11:27,721 - INFO - allennlp.common.params - trainer.optimizer.t_total = -1\n",
            "2022-02-17 03:11:27,721 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.01\n",
            "2022-02-17 03:11:27,721 - INFO - allennlp.common.registrable - instantiating registered subclass bert_adam of <class 'allennlp.training.optimizers.Optimizer'>\n",
            "2022-02-17 03:11:27,721 - WARNING - pytorch_pretrained_bert.optimization - t_total value of -1 results in schedule not being applied\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.registrable - instantiating registered subclass slanted_triangular of <class 'allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler'>\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.1\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_epochs = 6\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_steps_per_epoch = 109.9375\n",
            "2022-02-17 03:11:27,722 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 20\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.model_save_interval = 3600\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.summary_interval = 100\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.histogram_interval = None\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.should_log_learning_rate = True\n",
            "2022-02-17 03:11:27,723 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\n",
            "2022-02-17 03:11:27,727 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2022-02-17 03:11:27,727 - INFO - allennlp.training.trainer - Epoch 0/5\n",
            "2022-02-17 03:11:27,727 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4971.196\n",
            "2022-02-17 03:11:27,817 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2634\n",
            "2022-02-17 03:11:27,819 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/220 [00:00<?, ?it/s]\n",
            "2022-02-17 03:11:27,844 - WARNING - allennlp.data.token_indexers.wordpiece_indexer - Too many wordpieces, truncating sequence. If you would like a sliding window, set `truncate_long_sequences` to False.The offending input was: ['caused', 'by', ':', 'java', '.', 'lang', '.', 'arrayindexoutofboundsexception', ':', '0', '20', ':', '10', ':', '43', ',', '118', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'nodeindexer', '.', 'addbinaryvalue', '(', 'nodeindexer', '.', 'java', ':', '271', ')', '20', ':', '10', ':', '43', ',', '118', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'nodeindexer', '.', 'addvalue', '(', 'nodeindexer', '.', 'java', ':', '210', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'nodeindexer', '.', 'createdoc', '(', 'nodeindexer', '.', 'java', ':', '162', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'nodeindexer', '.', 'createdocument', '(', 'nodeindexer', '.', 'java', ':', '116', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'searchind', '[SEP]', '#', '#', 'ex', '.', 'createdocument', '(', 'searchindex', '.', 'java', ':', '254', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'multiindex', '.', 'addnodepersistent', '(', 'multiindex', '.', 'java', ':', '456', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'multiindex', '.', '<', 'init', '(', 'multiindex', '.', 'java', ':', '209', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'lucene', '.', 'searchindex', '.', 'doinit', '(', 'searchindex', '.', 'java', ':', '88', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'query', '.', 'abstractqueryhandler', '.', 'init', '(', 'abstractqueryhandler', '.', 'java', ':', '39', ')', '20', ':', '10', ':', '43', ',', '134', 'info', '[', 'stdout', ']', 'at', 'org', '.', 'apache', '.', 'jackrabbit', '.', 'core', '.', 'searchmanager', '.', '<', 'init', '(', 'searchmanager', '.', 'java', ':', '123', ')'].To avoid polluting your logs we will not warn about this again.\n",
            "  1%|          | 2/220 [00:07<13:12,  3.64s/it]\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/run.py\", line 21, in <module>\n",
            "    run()\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/run.py\", line 18, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/commands/__init__.py\", line 102, in main\n",
            "    args.func(args)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/commands/train.py\", line 124, in train_model_from_args\n",
            "    args.cache_prefix)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/commands/train.py\", line 168, in train_model_from_file\n",
            "    cache_directory, cache_prefix)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/commands/train.py\", line 252, in train_model\n",
            "    metrics = trainer.train()\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/training/trainer.py\", line 539, in train\n",
            "    train_metrics = self._train_epoch(epoch)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/training/trainer.py\", line 368, in _train_epoch\n",
            "    loss = self.batch_loss(this_batch, for_training=True)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/training/trainer.py\", line 279, in batch_loss\n",
            "    output_dict = self.model(**batch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/sequential_sentence_classification/sequential_sentence_classification/model.py\", line 96, in forward\n",
            "    embedded_sentences = self.text_field_embedder(sentences)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\", line 118, in forward\n",
            "    token_vectors = embedder(*tensors, **forward_params_values)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/sequential_sentence_classification/src/allennlp/allennlp/modules/token_embedders/bert_token_embedder.py\", line 176, in forward\n",
            "    all_encoder_layers = torch.stack(all_encoder_layers)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 15.90 GiB total capacity; 13.17 GiB already allocated; 220.75 MiB free; 13.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv5_t4RZnrw-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cc0164b-4590-444f-edb6-76a3dc9ce8e8"
      },
      "source": [
        "# %env PYTHONPATH=/env/python:/root/.local/bin:/root/.local/lib:/root/.local/lib/python3.7/site-packages \n",
        "# !echo $PYTHONPATH\n",
        "# %pip install typing_utils --user\n",
        "\n",
        "# %pip install -r /content/sequential_sentence_classification/requirements.txt \n",
        "# %pip install overrides==4.1.2 \n",
        "import os\n",
        "import statistics\n",
        "%cd /content/sequential_sentence_classification/\n",
        "# !scripts/train.sh tmp_output_dir\n",
        "%pip install overrides==4.1.2 \n",
        "!pip install allennlp \n",
        "!pip install urllib3==1.25.10\n",
        "!pip install jsonlines\n",
        "missed = pd.DataFrame(columns=[\"segment\",\"IS\",\"pred\",\"url\"])\n",
        "row_count = 0\n",
        "\n",
        "!cp /content/test.jsonl /content/sequential_sentence_classification/trained/\n",
        "!cp /content/test.jsonl /content/sequential_sentence_classification/tmp_output_dir_dev/\n",
        "import jsonlines\n",
        "\n",
        "for i in [\"tmp_output_dir_dev\"]:\n",
        "  misses_path = \"/content/misses.csv\"\n",
        "  os.environ[\"file_path\"] = \"/content/sequential_sentence_classification/{}/predictions.jsonl\".format(i)\n",
        "  os.system(\"allennlp predict  /content/sequential_sentence_classification/{}/model.tar.gz /content/sequential_sentence_classification/{}/test.jsonl --include-package sequential_sentence_classification  --predictor SeqClassificationPredictor >> output.txt\".format(i,i))\n",
        "  with jsonlines.open(os.environ[\"file_path\"],'r') as reader:\n",
        "    # pred_data = [json.load(item) for item in ip.readlines()]\n",
        "    true_p, pred_p, all = [],[],[]\n",
        "    for data in reader:\n",
        "      data[\"predictions\"] = [int(item.split(\"_\")[0]) for item in data[\"predictions\"]]\n",
        "      batch_true = data[\"labels\"]\n",
        "      batch_pred = data[\"predictions\"]\n",
        "      true_p += itertools.compress(data[\"sentences\"],data[\"labels\"])\n",
        "      pred_p += itertools.compress(data[\"sentences\"],data[\"predictions\"])\n",
        "      all.extend(data[\"sentences\"])\n",
        "\n",
        "      # if not all(x == y for x,y in zip(batch_true,batch_pred)):\n",
        "      #     # missed.at[row_count,\"segment\"] = \" \".join(data[\"sentences\"])\n",
        "      #     # missed.at[row_count,\"IS\"] = \"<EOL>\".join(itertools.compress(data[\"sentences\"],data[\"labels\"]))\n",
        "      #     # missed.at[row_count,\"url\"] = data[\"url\"]\n",
        "      #     # missed.at[row_count,\"pred\"] = \"<EOL>\".join(itertools.compress(data[\"sentences\"],data[\"predictions\"]))\n",
        "      #     row_count += 1\n",
        "      #     missed.drop_duplicates(subset=[\"segment\",\"IS\"])\n",
        "      #     missed.to_csv(\"/content/missed.csv\")\n",
        "    print(len(set(all)))\n",
        "    true_p = set(true_p); pred_p = set(pred_p); all = set(all) #set([x for x in pred_ if len(x) > 5])\n",
        "    true_n = all.difference(true_p); pred_n = all.difference(pred_p)\n",
        "\n",
        "    actual = [1 if elem in true_p else 0 for elem in all] \n",
        "    predicted = [1 if elem in pred_p else 0 for elem in all]\n",
        "    print(len(list(zip(actual,predicted))))\n",
        "    print(f1_score(actual,predicted,pos_label=1))\n",
        "    print(precision_score(actual,predicted,pos_label=1))\n",
        "    print(recall_score(actual,predicted,pos_label=1))\n",
        "    print(f1_score(actual,predicted,average='micro'))\n",
        "    print(f1_score(actual,predicted,average='macro'))\n",
        "    print(accuracy_score(actual,predicted))\n",
        "\n",
        "    # print(true,pred)\n",
        "\n",
        "    # TP = len(true.intersection(pred))\n",
        "    # FP = len(pred.difference(true))\n",
        "    # FN = len(true.difference(pred))\n",
        "    # precision = TP/(TP+FP)\n",
        "    # recall = TP/(TP+FN)\n",
        "    # F1 = (2*precision*recall)/(precision+recall)\n",
        "    # print(precision,recall,F1)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sequential_sentence_classification\n",
            "Requirement already satisfied: overrides==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from overrides==4.1.2) (0.1.0)\n",
            "Requirement already satisfied: allennlp in ./src/allennlp (0.9.0-unreleased)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.10.0+cu111)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.21.5)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.21.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.4)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.10)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from allennlp) (21.12.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.3)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.7.0)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.18.0)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.2)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.8.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp) (6.1.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.2)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.6.2)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.1.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.96)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.7->allennlp) (1.15.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp) (4.5.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->allennlp) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: sphinx>=1.8 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.10.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (2.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (21.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (2.9.1)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (0.17.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (1.3.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=0.8.0->allennlp) (2.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->allennlp) (3.10.0.2)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp) (0.5.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.1 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp) (1.24.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp) (0.2.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.7.0)\n",
            "Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from overrides->allennlp) (0.1.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (21.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (8.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (3.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.8->numpydoc>=0.8.0->allennlp) (1.1.5)\n",
            "Collecting urllib3==1.25.10\n",
            "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.25.11\n",
            "    Uninstalling urllib3-1.25.11:\n",
            "      Successfully uninstalled urllib3-1.25.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (3.10.0.2)\n",
            "855\n",
            "855\n",
            "0.6736842105263158\n",
            "0.6956521739130435\n",
            "0.6530612244897959\n",
            "0.9637426900584796\n",
            "0.8272445820433436\n",
            "0.9637426900584796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!allennlp predict  /content/sequential_sentence_classification/trained/model.tar.gz /content/sequential_sentence_classification/trained/test.jsonl --include-package sequential_sentence_classification  --predictor SeqClassificationPredictor >> output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu7Wf-CcV4j1",
        "outputId": "256eeb56-f98d-4c64-de5e-0a1decc369a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-10 16:11:18,101 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2022-01-10 16:11:18,522 - INFO - pytorch_transformers.modeling_bert - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2022-01-10 16:11:18,525 - INFO - pytorch_transformers.modeling_xlnet - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2022-01-10 16:11:18,817 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:18,818 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:18,819 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:18,819 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:18,873 - INFO - allennlp.models.archival - loading archive file /content/sequential_sentence_classification/trained/model.tar.gz\n",
            "2022-01-10 16:11:18,874 - INFO - allennlp.models.archival - extracting archive file /content/sequential_sentence_classification/trained/model.tar.gz to temp dir /tmp/tmpqd1xeiu6\n",
            "2022-01-10 16:11:22,537 - INFO - allennlp.common.registrable - instantiating registered subclass SeqClassificationModel of <class 'allennlp.models.model.Model'>\n",
            "2022-01-10 16:11:22,537 - INFO - allennlp.common.params - type = default\n",
            "2022-01-10 16:11:22,537 - INFO - allennlp.common.registrable - instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
            "2022-01-10 16:11:22,537 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpqd1xeiu6/vocabulary.\n",
            "2022-01-10 16:11:22,538 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'additional_feature_size': 0, 'bert_dropout': 0.1, 'sci_sum': False, 'self_attn': {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100, 'type': 'stacked_self_attention'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'}}}, 'type': 'SeqClassificationModel', 'use_sep': 'true', 'with_crf': 'false'} and extras {'vocab'}\n",
            "2022-01-10 16:11:22,538 - INFO - allennlp.common.params - model.type = SeqClassificationModel\n",
            "2022-01-10 16:11:22,538 - INFO - allennlp.common.from_params - instantiating class <class 'sequential_sentence_classification.model.SeqClassificationModel'> from params {'additional_feature_size': 0, 'bert_dropout': 0.1, 'sci_sum': False, 'self_attn': {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100, 'type': 'stacked_self_attention'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'}}}, 'use_sep': 'true', 'with_crf': 'false'} and extras {'vocab'}\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'}}} and extras {'vocab'}\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = True\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'requires_grad': 'all', 'top_layer_only': False, 'type': 'bert-pretrained'} and extras {'vocab'}\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.type = bert-pretrained\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'requires_grad': 'all', 'top_layer_only': False} and extras {'vocab'}\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.pretrained_model = bert-base-uncased\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.requires_grad = all\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
            "2022-01-10 16:11:22,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.scalar_mix_parameters = None\n",
            "2022-01-10 16:11:22,849 - INFO - pytorch_pretrained_bert.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpt1r3yvn2\n",
            "100% 407873900/407873900 [00:10<00:00, 37520333.12B/s]\n",
            "2022-01-10 16:11:34,128 - INFO - pytorch_pretrained_bert.file_utils - copying /tmp/tmpt1r3yvn2 to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "2022-01-10 16:11:35,074 - INFO - pytorch_pretrained_bert.file_utils - creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "2022-01-10 16:11:35,074 - INFO - pytorch_pretrained_bert.file_utils - removing temp file /tmp/tmpt1r3yvn2\n",
            "2022-01-10 16:11:35,113 - INFO - pytorch_pretrained_bert.modeling - loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "2022-01-10 16:11:35,114 - INFO - pytorch_pretrained_bert.modeling - extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpuay_2c4_\n",
            "2022-01-10 16:11:38,658 - INFO - pytorch_pretrained_bert.modeling - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.use_sep = true\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.with_crf = false\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100, 'type': 'stacked_self_attention'} and extras {'vocab'}\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.type = stacked_self_attention\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder'> from params {'feedforward_hidden_dim': 50, 'hidden_dim': 100, 'input_dim': 768, 'num_attention_heads': 2, 'num_layers': 2, 'projection_dim': 100} and extras {'vocab'}\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.input_dim = 768\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.hidden_dim = 100\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.projection_dim = 100\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.feedforward_hidden_dim = 50\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.num_layers = 2\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.num_attention_heads = 2\n",
            "2022-01-10 16:11:40,516 - INFO - allennlp.common.params - model.self_attn.use_positional_encoding = True\n",
            "2022-01-10 16:11:40,517 - INFO - allennlp.common.params - model.self_attn.dropout_prob = 0.1\n",
            "2022-01-10 16:11:40,517 - INFO - allennlp.common.params - model.self_attn.residual_dropout_prob = 0.2\n",
            "2022-01-10 16:11:40,517 - INFO - allennlp.common.params - model.self_attn.attention_dropout_prob = 0.1\n",
            "2022-01-10 16:11:40,517 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:40,517 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:40,518 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:40,518 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2022-01-10 16:11:40,519 - INFO - allennlp.common.params - model.bert_dropout = 0.1\n",
            "2022-01-10 16:11:40,519 - INFO - allennlp.common.params - model.sci_sum = False\n",
            "2022-01-10 16:11:40,519 - INFO - allennlp.common.params - model.additional_feature_size = 0\n",
            "2022-01-10 16:11:40,886 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': False, 'max_sent_per_example': '256', 'sci_sum': False, 'sci_sum_fake_scores': False, 'sent_max_len': '256', 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}}, 'type': 'SeqClassificationReader', 'use_abstract_scores': False, 'use_sep': 'true', 'word_splitter': 'bert-basic'} and extras set()\n",
            "2022-01-10 16:11:40,886 - INFO - allennlp.common.params - dataset_reader.type = SeqClassificationReader\n",
            "2022-01-10 16:11:40,886 - INFO - allennlp.common.from_params - instantiating class <class 'sequential_sentence_classification.dataset_reader.SeqClassificationReader'> from params {'lazy': False, 'max_sent_per_example': '256', 'sci_sum': False, 'sci_sum_fake_scores': False, 'sent_max_len': '256', 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}}, 'use_abstract_scores': False, 'use_sep': 'true', 'word_splitter': 'bert-basic'} and extras set()\n",
            "2022-01-10 16:11:40,886 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False} and extras set()\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = bert-pretrained\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer'> from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'use_starting_offsets': False} and extras set()\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.pretrained_model = bert-base-uncased\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.use_starting_offsets = False\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.do_lowercase = True\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.never_lowercase = None\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_pieces = 512\n",
            "2022-01-10 16:11:40,887 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.truncate_long_sequences = True\n",
            "2022-01-10 16:11:41,171 - INFO - pytorch_pretrained_bert.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpp4fkd6sc\n",
            "100% 231508/231508 [00:00<00:00, 909557.07B/s]\n",
            "2022-01-10 16:11:41,714 - INFO - pytorch_pretrained_bert.file_utils - copying /tmp/tmpp4fkd6sc to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2022-01-10 16:11:41,714 - INFO - pytorch_pretrained_bert.file_utils - creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2022-01-10 16:11:41,714 - INFO - pytorch_pretrained_bert.file_utils - removing temp file /tmp/tmpp4fkd6sc\n",
            "2022-01-10 16:11:41,714 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.word_splitter = bert-basic\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.registrable - instantiating registered subclass bert-basic of <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'>\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.sent_max_len = 256\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.max_sent_per_example = 256\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.use_sep = true\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.sci_sum = False\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.use_abstract_scores = False\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.sci_sum_fake_scores = False\n",
            "2022-01-10 16:11:41,791 - INFO - allennlp.common.params - dataset_reader.predict = False\n",
            "2022-01-10 16:11:41,933 - INFO - allennlp.common.registrable - instantiating registered subclass SeqClassificationPredictor of <class 'allennlp.predictors.predictor.Predictor'>\n",
            "2022-01-10 16:11:52,195 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpqd1xeiu6\n"
          ]
        }
      ]
    }
  ]
}